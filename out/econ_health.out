
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=1,2,3,4,5
L2_REG_LAMBDA=0.0
LABELS_DATA_FILE=./nlp_features/econ_health_onehotlabels.npy
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=200
NUM_FILTERS=128
SENTENCES_DATA_FILE=./nlp_features/econ_health_sentences_list_of_strings.npy
UNROLLED_LSTM=False

Loading data...
Vocabulary Size: 12499
Train/Dev split: 2610/289
Writing to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215

2017-03-02T23:32:23.042916: step 1, loss 3.72976, acc 0.34375
2017-03-02T23:32:23.153792: step 2, loss 2.75646, acc 0.46875
2017-03-02T23:32:23.262151: step 3, loss 2.41539, acc 0.453125
2017-03-02T23:32:23.374722: step 4, loss 2.48761, acc 0.59375
2017-03-02T23:32:23.480803: step 5, loss 2.0819, acc 0.453125
2017-03-02T23:32:23.573565: step 6, loss 3.86937, acc 0.296875
2017-03-02T23:32:23.664761: step 7, loss 1.91142, acc 0.53125
2017-03-02T23:32:23.769225: step 8, loss 2.84098, acc 0.453125
2017-03-02T23:32:23.876783: step 9, loss 2.29883, acc 0.484375
2017-03-02T23:32:23.976247: step 10, loss 2.63186, acc 0.421875
2017-03-02T23:32:24.080389: step 11, loss 3.75415, acc 0.4375
2017-03-02T23:32:24.180733: step 12, loss 2.76966, acc 0.40625
2017-03-02T23:32:24.290382: step 13, loss 2.15203, acc 0.484375
2017-03-02T23:32:24.389339: step 14, loss 3.0408, acc 0.453125
2017-03-02T23:32:24.479959: step 15, loss 2.40229, acc 0.5
2017-03-02T23:32:24.582681: step 16, loss 2.58192, acc 0.421875
2017-03-02T23:32:24.684323: step 17, loss 2.96065, acc 0.375
2017-03-02T23:32:24.788941: step 18, loss 2.82685, acc 0.46875
2017-03-02T23:32:24.890173: step 19, loss 2.91805, acc 0.484375
2017-03-02T23:32:24.992627: step 20, loss 2.68184, acc 0.484375
2017-03-02T23:32:25.094461: step 21, loss 2.96591, acc 0.421875
2017-03-02T23:32:25.197822: step 22, loss 2.8683, acc 0.34375
2017-03-02T23:32:25.304344: step 23, loss 2.42226, acc 0.4375
2017-03-02T23:32:25.406126: step 24, loss 2.39304, acc 0.515625
2017-03-02T23:32:25.510982: step 25, loss 2.32298, acc 0.515625
2017-03-02T23:32:25.619614: step 26, loss 2.8522, acc 0.5
2017-03-02T23:32:25.721311: step 27, loss 2.55421, acc 0.484375
2017-03-02T23:32:25.823282: step 28, loss 2.21069, acc 0.5
2017-03-02T23:32:25.913695: step 29, loss 3.10281, acc 0.34375
2017-03-02T23:32:26.036738: step 30, loss 2.44614, acc 0.546875
2017-03-02T23:32:26.138114: step 31, loss 1.577, acc 0.53125
2017-03-02T23:32:26.245462: step 32, loss 2.4265, acc 0.453125
2017-03-02T23:32:26.347395: step 33, loss 1.98004, acc 0.46875
2017-03-02T23:32:26.454772: step 34, loss 1.76581, acc 0.4375
2017-03-02T23:32:26.556033: step 35, loss 1.74588, acc 0.578125
2017-03-02T23:32:26.658350: step 36, loss 1.98326, acc 0.5625
2017-03-02T23:32:26.765222: step 37, loss 2.19324, acc 0.4375
2017-03-02T23:32:26.872351: step 38, loss 2.69011, acc 0.40625
2017-03-02T23:32:26.979089: step 39, loss 2.33278, acc 0.5
2017-03-02T23:32:27.084104: step 40, loss 2.4364, acc 0.453125
2017-03-02T23:32:40.173565: step 41, loss 3.04869, acc 0.42
2017-03-02T23:32:40.282665: step 42, loss 2.29001, acc 0.515625
2017-03-02T23:32:40.384388: step 43, loss 1.8335, acc 0.515625
2017-03-02T23:32:40.494210: step 44, loss 1.86124, acc 0.515625
2017-03-02T23:32:40.594801: step 45, loss 2.05497, acc 0.4375
2017-03-02T23:32:40.688907: step 46, loss 2.18689, acc 0.46875
2017-03-02T23:32:40.796199: step 47, loss 2.45856, acc 0.453125
2017-03-02T23:32:40.908682: step 48, loss 1.68911, acc 0.515625
2017-03-02T23:32:41.017248: step 49, loss 1.56571, acc 0.546875
2017-03-02T23:32:41.121020: step 50, loss 1.95472, acc 0.53125
2017-03-02T23:32:41.235332: step 51, loss 2.00701, acc 0.4375
2017-03-02T23:32:41.332838: step 52, loss 2.21188, acc 0.5625
2017-03-02T23:32:41.434224: step 53, loss 2.02527, acc 0.546875
2017-03-02T23:32:41.535119: step 54, loss 1.93688, acc 0.578125
2017-03-02T23:32:41.637105: step 55, loss 2.87961, acc 0.4375
2017-03-02T23:32:41.753239: step 56, loss 2.0525, acc 0.46875
2017-03-02T23:32:41.855538: step 57, loss 1.78651, acc 0.515625
2017-03-02T23:32:41.953195: step 58, loss 2.35536, acc 0.453125
2017-03-02T23:32:42.047343: step 59, loss 1.9618, acc 0.46875
2017-03-02T23:32:42.159910: step 60, loss 1.95356, acc 0.4375
2017-03-02T23:32:42.261567: step 61, loss 2.12466, acc 0.40625
2017-03-02T23:32:42.370575: step 62, loss 2.11835, acc 0.421875
2017-03-02T23:32:42.475098: step 63, loss 1.5253, acc 0.578125
2017-03-02T23:32:42.575384: step 64, loss 2.53491, acc 0.375
2017-03-02T23:32:42.682029: step 65, loss 1.83342, acc 0.65625
2017-03-02T23:32:42.778211: step 66, loss 1.65376, acc 0.640625
2017-03-02T23:32:42.875697: step 67, loss 1.74975, acc 0.5625
2017-03-02T23:32:42.979640: step 68, loss 1.96616, acc 0.5
2017-03-02T23:32:43.097261: step 69, loss 1.85969, acc 0.515625
2017-03-02T23:32:43.205556: step 70, loss 1.71074, acc 0.625
2017-03-02T23:32:43.305592: step 71, loss 1.92105, acc 0.453125
2017-03-02T23:32:43.408450: step 72, loss 1.59447, acc 0.625
2017-03-02T23:32:43.497948: step 73, loss 1.82112, acc 0.5625
2017-03-02T23:32:43.589134: step 74, loss 1.85325, acc 0.546875
2017-03-02T23:32:43.689134: step 75, loss 1.636, acc 0.546875
2017-03-02T23:32:43.794655: step 76, loss 1.69281, acc 0.515625
2017-03-02T23:32:43.909925: step 77, loss 1.63751, acc 0.5
2017-03-02T23:32:44.013092: step 78, loss 1.57144, acc 0.546875
2017-03-02T23:32:44.119686: step 79, loss 1.8213, acc 0.484375
2017-03-02T23:32:44.231317: step 80, loss 1.75649, acc 0.484375
2017-03-02T23:32:44.321890: step 81, loss 1.9509, acc 0.453125
2017-03-02T23:32:44.418105: step 82, loss 1.29253, acc 0.72
2017-03-02T23:32:44.531155: step 83, loss 1.22397, acc 0.625
2017-03-02T23:32:44.636334: step 84, loss 1.58839, acc 0.59375
2017-03-02T23:32:44.746542: step 85, loss 1.68118, acc 0.53125
2017-03-02T23:32:44.855179: step 86, loss 0.975896, acc 0.625
2017-03-02T23:32:44.960510: step 87, loss 1.53197, acc 0.515625
2017-03-02T23:32:45.054291: step 88, loss 1.74873, acc 0.453125
2017-03-02T23:32:45.152708: step 89, loss 1.44365, acc 0.546875
2017-03-02T23:32:45.259592: step 90, loss 1.97364, acc 0.4375
2017-03-02T23:32:45.363629: step 91, loss 1.71537, acc 0.546875
2017-03-02T23:32:45.484213: step 92, loss 1.83136, acc 0.53125
2017-03-02T23:32:45.590287: step 93, loss 1.86707, acc 0.5
2017-03-02T23:32:45.697212: step 94, loss 1.43587, acc 0.5625
2017-03-02T23:32:45.786365: step 95, loss 1.93457, acc 0.53125
2017-03-02T23:32:45.875253: step 96, loss 1.95415, acc 0.53125
2017-03-02T23:32:45.979665: step 97, loss 1.45813, acc 0.515625
2017-03-02T23:32:46.093597: step 98, loss 1.44072, acc 0.546875
2017-03-02T23:32:46.196249: step 99, loss 1.76884, acc 0.421875
2017-03-02T23:32:46.302126: step 100, loss 1.59636, acc 0.59375

Evaluation:
2017-03-02T23:32:46.508121: step 100, loss 1.08534, acc 0.553633

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-100

2017-03-02T23:32:47.042829: step 101, loss 1.03313, acc 0.625
2017-03-02T23:32:47.145007: step 102, loss 1.26955, acc 0.546875
2017-03-02T23:32:47.253865: step 103, loss 1.28713, acc 0.5625
2017-03-02T23:32:47.350824: step 104, loss 1.4488, acc 0.578125
2017-03-02T23:32:47.449632: step 105, loss 1.2881, acc 0.578125
2017-03-02T23:32:47.554860: step 106, loss 1.38262, acc 0.609375
2017-03-02T23:32:47.658810: step 107, loss 1.61246, acc 0.5625
2017-03-02T23:32:47.761728: step 108, loss 1.64049, acc 0.5625
2017-03-02T23:32:47.864674: step 109, loss 1.6095, acc 0.53125
2017-03-02T23:32:47.968563: step 110, loss 1.62157, acc 0.578125
2017-03-02T23:32:48.054112: step 111, loss 1.83165, acc 0.5
2017-03-02T23:32:48.146033: step 112, loss 1.42044, acc 0.5625
2017-03-02T23:32:48.250189: step 113, loss 1.24409, acc 0.640625
2017-03-02T23:32:48.356060: step 114, loss 1.77337, acc 0.5
2017-03-02T23:32:48.462970: step 115, loss 1.92597, acc 0.46875
2017-03-02T23:32:48.600594: step 116, loss 1.14824, acc 0.53125
2017-03-02T23:32:48.703158: step 117, loss 1.81598, acc 0.484375
2017-03-02T23:32:48.803340: step 118, loss 1.61482, acc 0.515625
2017-03-02T23:32:48.908605: step 119, loss 1.32124, acc 0.640625
2017-03-02T23:32:49.012022: step 120, loss 1.73256, acc 0.546875
2017-03-02T23:32:49.122957: step 121, loss 1.43401, acc 0.59375
2017-03-02T23:32:49.227373: step 122, loss 1.61327, acc 0.53125
2017-03-02T23:32:49.324794: step 123, loss 1.41007, acc 0.58
2017-03-02T23:32:49.434542: step 124, loss 1.48242, acc 0.546875
2017-03-02T23:32:49.540901: step 125, loss 1.30608, acc 0.578125
2017-03-02T23:32:49.626392: step 126, loss 1.31656, acc 0.625
2017-03-02T23:32:49.728236: step 127, loss 1.18418, acc 0.59375
2017-03-02T23:32:49.832336: step 128, loss 1.62102, acc 0.578125
2017-03-02T23:32:49.942810: step 129, loss 1.43975, acc 0.625
2017-03-02T23:32:50.044951: step 130, loss 1.54496, acc 0.59375
2017-03-02T23:32:50.151406: step 131, loss 1.90061, acc 0.59375
2017-03-02T23:32:50.256420: step 132, loss 0.946163, acc 0.625
2017-03-02T23:32:50.354415: step 133, loss 1.22582, acc 0.5625
2017-03-02T23:32:50.459113: step 134, loss 1.42666, acc 0.625
2017-03-02T23:32:50.562179: step 135, loss 1.3053, acc 0.5625
2017-03-02T23:32:50.662530: step 136, loss 1.72433, acc 0.5625
2017-03-02T23:32:50.778014: step 137, loss 1.33834, acc 0.609375
2017-03-02T23:32:50.880983: step 138, loss 1.35934, acc 0.609375
2017-03-02T23:32:50.983873: step 139, loss 0.939486, acc 0.59375
2017-03-02T23:32:51.082372: step 140, loss 1.64493, acc 0.5625
2017-03-02T23:32:51.181471: step 141, loss 1.354, acc 0.515625
2017-03-02T23:32:51.286496: step 142, loss 1.44251, acc 0.609375
2017-03-02T23:32:51.391913: step 143, loss 1.10114, acc 0.578125
2017-03-02T23:32:51.500466: step 144, loss 1.01232, acc 0.578125
2017-03-02T23:32:51.611308: step 145, loss 1.03326, acc 0.5625
2017-03-02T23:32:51.722291: step 146, loss 1.15542, acc 0.671875
2017-03-02T23:32:51.826218: step 147, loss 1.37373, acc 0.515625
2017-03-02T23:32:51.918716: step 148, loss 1.33842, acc 0.640625
2017-03-02T23:32:52.026603: step 149, loss 1.16869, acc 0.5625
2017-03-02T23:32:52.153053: step 150, loss 1.38158, acc 0.546875
2017-03-02T23:32:52.262603: step 151, loss 1.19615, acc 0.59375
2017-03-02T23:32:52.367654: step 152, loss 1.33229, acc 0.640625
2017-03-02T23:32:52.474351: step 153, loss 1.49731, acc 0.59375
2017-03-02T23:32:52.580593: step 154, loss 1.24028, acc 0.578125
2017-03-02T23:32:52.671982: step 155, loss 1.17371, acc 0.671875
2017-03-02T23:32:52.775456: step 156, loss 1.49263, acc 0.5
2017-03-02T23:32:52.882540: step 157, loss 1.05677, acc 0.671875
2017-03-02T23:32:52.983705: step 158, loss 1.81321, acc 0.5
2017-03-02T23:32:53.090686: step 159, loss 1.19363, acc 0.609375
2017-03-02T23:32:53.193676: step 160, loss 1.41206, acc 0.546875
2017-03-02T23:32:53.304072: step 161, loss 1.54747, acc 0.578125
2017-03-02T23:32:53.388333: step 162, loss 1.74214, acc 0.53125
2017-03-02T23:32:53.486416: step 163, loss 1.44342, acc 0.46875
2017-03-02T23:32:53.582500: step 164, loss 1.44282, acc 0.46
2017-03-02T23:32:53.686551: step 165, loss 0.857299, acc 0.75
2017-03-02T23:32:53.789898: step 166, loss 1.51954, acc 0.546875
2017-03-02T23:32:53.896004: step 167, loss 0.934073, acc 0.59375
2017-03-02T23:32:54.004039: step 168, loss 1.27917, acc 0.59375
2017-03-02T23:32:54.106936: step 169, loss 1.1253, acc 0.6875
2017-03-02T23:32:54.192346: step 170, loss 1.34328, acc 0.5
2017-03-02T23:32:54.296593: step 171, loss 1.13814, acc 0.625
2017-03-02T23:32:54.404928: step 172, loss 0.981023, acc 0.640625
2017-03-02T23:32:54.514027: step 173, loss 1.09244, acc 0.609375
2017-03-02T23:32:54.642018: step 174, loss 1.17873, acc 0.59375
2017-03-02T23:32:54.744867: step 175, loss 1.36388, acc 0.609375
2017-03-02T23:32:54.855792: step 176, loss 0.986367, acc 0.703125
2017-03-02T23:32:54.953325: step 177, loss 1.32376, acc 0.59375
2017-03-02T23:32:55.055175: step 178, loss 1.42298, acc 0.625
2017-03-02T23:32:55.158517: step 179, loss 1.45817, acc 0.515625
2017-03-02T23:32:55.260611: step 180, loss 0.980142, acc 0.6875
2017-03-02T23:32:55.364592: step 181, loss 1.06748, acc 0.703125
2017-03-02T23:32:55.471921: step 182, loss 0.994854, acc 0.671875
2017-03-02T23:32:55.573212: step 183, loss 1.09058, acc 0.59375
2017-03-02T23:32:55.661683: step 184, loss 1.25258, acc 0.53125
2017-03-02T23:32:55.764425: step 185, loss 1.11257, acc 0.609375
2017-03-02T23:32:55.869459: step 186, loss 1.20592, acc 0.5625
2017-03-02T23:32:55.989720: step 187, loss 0.855484, acc 0.703125
2017-03-02T23:32:56.093576: step 188, loss 0.631368, acc 0.796875
2017-03-02T23:32:56.213853: step 189, loss 1.04354, acc 0.671875
2017-03-02T23:32:56.323477: step 190, loss 1.05075, acc 0.703125
2017-03-02T23:32:56.423853: step 191, loss 1.35475, acc 0.609375
2017-03-02T23:32:56.538064: step 192, loss 1.0186, acc 0.65625
2017-03-02T23:32:56.646654: step 193, loss 0.880541, acc 0.671875
2017-03-02T23:32:56.751108: step 194, loss 1.21323, acc 0.53125
2017-03-02T23:32:56.854554: step 195, loss 0.866494, acc 0.71875
2017-03-02T23:32:56.956558: step 196, loss 1.24466, acc 0.5625
2017-03-02T23:32:57.052296: step 197, loss 1.41701, acc 0.484375
2017-03-02T23:32:57.144942: step 198, loss 1.00137, acc 0.59375
2017-03-02T23:32:57.236636: step 199, loss 1.19046, acc 0.625
2017-03-02T23:32:57.340762: step 200, loss 0.954478, acc 0.71875

Evaluation:
2017-03-02T23:32:57.396391: step 200, loss 1.27584, acc 0.570934

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-200

2017-03-02T23:32:57.841236: step 201, loss 0.990771, acc 0.6875
2017-03-02T23:32:57.937129: step 202, loss 1.14353, acc 0.609375
2017-03-02T23:32:58.040696: step 203, loss 1.46745, acc 0.484375
2017-03-02T23:32:58.138863: step 204, loss 0.794357, acc 0.71875
2017-03-02T23:32:58.232911: step 205, loss 1.58478, acc 0.58
2017-03-02T23:32:58.340789: step 206, loss 1.10376, acc 0.671875
2017-03-02T23:32:58.446261: step 207, loss 0.981648, acc 0.6875
2017-03-02T23:32:58.555859: step 208, loss 0.868561, acc 0.625
2017-03-02T23:32:58.647626: step 209, loss 0.859257, acc 0.65625
2017-03-02T23:32:58.746805: step 210, loss 1.06924, acc 0.640625
2017-03-02T23:32:58.857943: step 211, loss 1.11239, acc 0.625
2017-03-02T23:32:58.957844: step 212, loss 1.00227, acc 0.640625
2017-03-02T23:32:59.065111: step 213, loss 1.34482, acc 0.578125
2017-03-02T23:32:59.169112: step 214, loss 1.02393, acc 0.671875
2017-03-02T23:32:59.269224: step 215, loss 0.853248, acc 0.6875
2017-03-02T23:32:59.365895: step 216, loss 0.905938, acc 0.65625
2017-03-02T23:32:59.466598: step 217, loss 0.944217, acc 0.65625
2017-03-02T23:32:59.566274: step 218, loss 1.05292, acc 0.640625
2017-03-02T23:32:59.666383: step 219, loss 1.1386, acc 0.671875
2017-03-02T23:32:59.774350: step 220, loss 1.25377, acc 0.609375
2017-03-02T23:32:59.875855: step 221, loss 1.31742, acc 0.609375
2017-03-02T23:32:59.985115: step 222, loss 0.818397, acc 0.6875
2017-03-02T23:33:00.076624: step 223, loss 0.989589, acc 0.671875
2017-03-02T23:33:00.165915: step 224, loss 0.859789, acc 0.625
2017-03-02T23:33:00.264624: step 225, loss 1.04483, acc 0.625
2017-03-02T23:33:00.357391: step 226, loss 1.02104, acc 0.578125
2017-03-02T23:33:00.462284: step 227, loss 0.921511, acc 0.578125
2017-03-02T23:33:00.552922: step 228, loss 0.96668, acc 0.625
2017-03-02T23:33:00.658085: step 229, loss 1.05961, acc 0.640625
2017-03-02T23:33:00.760683: step 230, loss 0.841363, acc 0.6875
2017-03-02T23:33:00.850375: step 231, loss 0.769696, acc 0.625
2017-03-02T23:33:00.943605: step 232, loss 1.01593, acc 0.5625
2017-03-02T23:33:01.044336: step 233, loss 0.976823, acc 0.59375
2017-03-02T23:33:01.152467: step 234, loss 0.953125, acc 0.625
2017-03-02T23:33:01.262009: step 235, loss 1.10857, acc 0.546875
2017-03-02T23:33:01.365370: step 236, loss 1.21747, acc 0.609375
2017-03-02T23:33:01.470298: step 237, loss 1.22618, acc 0.640625
2017-03-02T23:33:01.572384: step 238, loss 1.28711, acc 0.6875
2017-03-02T23:33:01.663135: step 239, loss 0.889776, acc 0.6875
2017-03-02T23:33:01.769156: step 240, loss 1.37583, acc 0.5625
2017-03-02T23:33:01.868023: step 241, loss 1.27599, acc 0.640625
2017-03-02T23:33:01.976901: step 242, loss 0.868637, acc 0.734375
2017-03-02T23:33:02.085041: step 243, loss 1.11634, acc 0.59375
2017-03-02T23:33:02.188631: step 244, loss 1.25621, acc 0.59375
2017-03-02T23:33:02.290483: step 245, loss 1.1635, acc 0.671875
2017-03-02T23:33:02.372668: step 246, loss 0.815216, acc 0.74
2017-03-02T23:33:02.454734: step 247, loss 1.00001, acc 0.640625
2017-03-02T23:33:02.562904: step 248, loss 1.09202, acc 0.546875
2017-03-02T23:33:02.657771: step 249, loss 1.14564, acc 0.609375
2017-03-02T23:33:02.766389: step 250, loss 1.06298, acc 0.546875
2017-03-02T23:33:02.878166: step 251, loss 0.86545, acc 0.640625
2017-03-02T23:33:02.983784: step 252, loss 0.924965, acc 0.671875
2017-03-02T23:33:03.096296: step 253, loss 0.782831, acc 0.734375
2017-03-02T23:33:03.177685: step 254, loss 0.927441, acc 0.625
2017-03-02T23:33:03.275853: step 255, loss 1.19232, acc 0.703125
2017-03-02T23:33:03.395374: step 256, loss 0.731116, acc 0.6875
2017-03-02T23:33:03.502721: step 257, loss 0.781442, acc 0.6875
2017-03-02T23:33:03.612100: step 258, loss 0.840386, acc 0.703125
2017-03-02T23:33:03.715022: step 259, loss 0.615792, acc 0.75
2017-03-02T23:33:03.822606: step 260, loss 0.7426, acc 0.75
2017-03-02T23:33:03.911351: step 261, loss 0.605311, acc 0.765625
2017-03-02T23:33:04.014348: step 262, loss 1.028, acc 0.640625
2017-03-02T23:33:04.115717: step 263, loss 0.745106, acc 0.6875
2017-03-02T23:33:04.219197: step 264, loss 0.849395, acc 0.6875
2017-03-02T23:33:04.327304: step 265, loss 0.907259, acc 0.75
2017-03-02T23:33:04.436371: step 266, loss 0.577517, acc 0.8125
2017-03-02T23:33:04.554649: step 267, loss 1.39901, acc 0.59375
2017-03-02T23:33:04.648643: step 268, loss 1.13659, acc 0.625
2017-03-02T23:33:04.734615: step 269, loss 1.27739, acc 0.640625
2017-03-02T23:33:04.837849: step 270, loss 1.00196, acc 0.640625
2017-03-02T23:33:04.943416: step 271, loss 0.929717, acc 0.65625
2017-03-02T23:33:05.049079: step 272, loss 1.28432, acc 0.53125
2017-03-02T23:33:05.156746: step 273, loss 1.1094, acc 0.59375
2017-03-02T23:33:05.256688: step 274, loss 0.710643, acc 0.734375
2017-03-02T23:33:05.354741: step 275, loss 1.04962, acc 0.671875
2017-03-02T23:33:05.444539: step 276, loss 0.759783, acc 0.734375
2017-03-02T23:33:05.541649: step 277, loss 1.04376, acc 0.609375
2017-03-02T23:33:05.643188: step 278, loss 0.693848, acc 0.6875
2017-03-02T23:33:05.765592: step 279, loss 0.898723, acc 0.734375
2017-03-02T23:33:05.869187: step 280, loss 0.851388, acc 0.671875
2017-03-02T23:33:05.972230: step 281, loss 1.17334, acc 0.625
2017-03-02T23:33:06.078430: step 282, loss 1.07765, acc 0.609375
2017-03-02T23:33:06.173976: step 283, loss 0.886066, acc 0.671875
2017-03-02T23:33:06.269070: step 284, loss 0.731624, acc 0.703125
2017-03-02T23:33:06.373111: step 285, loss 1.06508, acc 0.59375
2017-03-02T23:33:06.478793: step 286, loss 1.03044, acc 0.59375
2017-03-02T23:33:06.590385: step 287, loss 0.701097, acc 0.74
2017-03-02T23:33:06.697033: step 288, loss 0.747326, acc 0.75
2017-03-02T23:33:06.798553: step 289, loss 0.754209, acc 0.65625
2017-03-02T23:33:06.901961: step 290, loss 0.578132, acc 0.75
2017-03-02T23:33:06.999601: step 291, loss 0.778325, acc 0.734375
2017-03-02T23:33:07.110882: step 292, loss 0.714703, acc 0.765625
2017-03-02T23:33:07.217427: step 293, loss 0.984744, acc 0.671875
2017-03-02T23:33:07.328226: step 294, loss 1.10456, acc 0.609375
2017-03-02T23:33:07.435991: step 295, loss 0.710644, acc 0.71875
2017-03-02T23:33:07.539342: step 296, loss 0.531289, acc 0.765625
2017-03-02T23:33:07.642309: step 297, loss 0.621277, acc 0.703125
2017-03-02T23:33:07.734008: step 298, loss 0.996491, acc 0.609375
2017-03-02T23:33:07.838416: step 299, loss 1.08483, acc 0.5625
2017-03-02T23:33:07.943897: step 300, loss 0.956319, acc 0.59375

Evaluation:
2017-03-02T23:33:08.009821: step 300, loss 1.0325, acc 0.564014

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-300

2017-03-02T23:33:08.456798: step 301, loss 0.909074, acc 0.609375
2017-03-02T23:33:08.580183: step 302, loss 0.68497, acc 0.671875
2017-03-02T23:33:08.690016: step 303, loss 0.545845, acc 0.78125
2017-03-02T23:33:08.787199: step 304, loss 0.46801, acc 0.734375
2017-03-02T23:33:08.883308: step 305, loss 1.15936, acc 0.578125
2017-03-02T23:33:08.985976: step 306, loss 0.758166, acc 0.65625
2017-03-02T23:33:09.101493: step 307, loss 0.599545, acc 0.796875
2017-03-02T23:33:09.191620: step 308, loss 0.742729, acc 0.671875
2017-03-02T23:33:09.306906: step 309, loss 0.649148, acc 0.6875
2017-03-02T23:33:09.405169: step 310, loss 1.02024, acc 0.578125
2017-03-02T23:33:09.507607: step 311, loss 0.814222, acc 0.65625
2017-03-02T23:33:09.610798: step 312, loss 0.5733, acc 0.765625
2017-03-02T23:33:09.727714: step 313, loss 0.963627, acc 0.640625
2017-03-02T23:33:09.838131: step 314, loss 1.01686, acc 0.703125
2017-03-02T23:33:09.930934: step 315, loss 0.787374, acc 0.6875
2017-03-02T23:33:10.029096: step 316, loss 0.792398, acc 0.671875
2017-03-02T23:33:10.127959: step 317, loss 0.890591, acc 0.71875
2017-03-02T23:33:10.244603: step 318, loss 0.920675, acc 0.625
2017-03-02T23:33:10.349694: step 319, loss 1.03598, acc 0.59375
2017-03-02T23:33:10.454660: step 320, loss 0.810833, acc 0.609375
2017-03-02T23:33:10.549358: step 321, loss 0.619007, acc 0.6875
2017-03-02T23:33:10.639946: step 322, loss 0.735268, acc 0.671875
2017-03-02T23:33:10.726169: step 323, loss 0.858493, acc 0.671875
2017-03-02T23:33:10.833130: step 324, loss 0.768212, acc 0.625
2017-03-02T23:33:10.942818: step 325, loss 1.1599, acc 0.5625
2017-03-02T23:33:11.053559: step 326, loss 0.939074, acc 0.703125
2017-03-02T23:33:11.160006: step 327, loss 0.755688, acc 0.640625
2017-03-02T23:33:11.259569: step 328, loss 0.742388, acc 0.74
2017-03-02T23:33:11.364316: step 329, loss 0.844041, acc 0.671875
2017-03-02T23:33:11.457133: step 330, loss 0.616195, acc 0.765625
2017-03-02T23:33:11.562111: step 331, loss 0.932238, acc 0.6875
2017-03-02T23:33:11.666477: step 332, loss 0.59845, acc 0.78125
2017-03-02T23:33:11.762802: step 333, loss 0.827446, acc 0.65625
2017-03-02T23:33:11.870429: step 334, loss 0.840265, acc 0.75
2017-03-02T23:33:11.963393: step 335, loss 1.10149, acc 0.5625
2017-03-02T23:33:12.058855: step 336, loss 0.70497, acc 0.703125
2017-03-02T23:33:12.155341: step 337, loss 0.827749, acc 0.65625
2017-03-02T23:33:12.256370: step 338, loss 0.914623, acc 0.640625
2017-03-02T23:33:12.362956: step 339, loss 0.744734, acc 0.703125
2017-03-02T23:33:12.464610: step 340, loss 0.678455, acc 0.796875
2017-03-02T23:33:12.564208: step 341, loss 0.810757, acc 0.71875
2017-03-02T23:33:12.671000: step 342, loss 0.692677, acc 0.78125
2017-03-02T23:33:12.773985: step 343, loss 0.779942, acc 0.765625
2017-03-02T23:33:12.875706: step 344, loss 1.18892, acc 0.5625
2017-03-02T23:33:13.000022: step 345, loss 0.777893, acc 0.671875
2017-03-02T23:33:13.121609: step 346, loss 0.572202, acc 0.75
2017-03-02T23:33:13.218816: step 347, loss 0.593526, acc 0.796875
2017-03-02T23:33:13.326480: step 348, loss 0.877944, acc 0.640625
2017-03-02T23:33:13.436101: step 349, loss 0.742727, acc 0.625
2017-03-02T23:33:13.536620: step 350, loss 1.00846, acc 0.59375
2017-03-02T23:33:13.631193: step 351, loss 1.15161, acc 0.578125
2017-03-02T23:33:13.717635: step 352, loss 0.881188, acc 0.703125
2017-03-02T23:33:13.822166: step 353, loss 0.803203, acc 0.625
2017-03-02T23:33:13.933342: step 354, loss 0.764937, acc 0.703125
2017-03-02T23:33:14.037933: step 355, loss 0.662582, acc 0.71875
2017-03-02T23:33:14.153399: step 356, loss 0.71748, acc 0.6875
2017-03-02T23:33:14.257368: step 357, loss 1.2345, acc 0.625
2017-03-02T23:33:14.361629: step 358, loss 0.730149, acc 0.71875
2017-03-02T23:33:14.452086: step 359, loss 0.735178, acc 0.71875
2017-03-02T23:33:14.566216: step 360, loss 0.744375, acc 0.75
2017-03-02T23:33:14.665163: step 361, loss 0.651542, acc 0.734375
2017-03-02T23:33:14.767288: step 362, loss 0.779116, acc 0.75
2017-03-02T23:33:14.870457: step 363, loss 0.538727, acc 0.78125
2017-03-02T23:33:14.973233: step 364, loss 0.805359, acc 0.65625
2017-03-02T23:33:15.081773: step 365, loss 0.686411, acc 0.71875
2017-03-02T23:33:15.181814: step 366, loss 0.607058, acc 0.734375
2017-03-02T23:33:15.281009: step 367, loss 0.878698, acc 0.65625
2017-03-02T23:33:15.383774: step 368, loss 0.757176, acc 0.6875
2017-03-02T23:33:15.478298: step 369, loss 0.737488, acc 0.68
2017-03-02T23:33:15.577553: step 370, loss 0.678538, acc 0.671875
2017-03-02T23:33:15.686554: step 371, loss 0.809804, acc 0.640625
2017-03-02T23:33:15.791623: step 372, loss 0.731775, acc 0.625
2017-03-02T23:33:15.895878: step 373, loss 0.89904, acc 0.625
2017-03-02T23:33:15.987964: step 374, loss 0.692266, acc 0.734375
2017-03-02T23:33:16.089743: step 375, loss 0.725182, acc 0.671875
2017-03-02T23:33:16.207407: step 376, loss 0.656512, acc 0.734375
2017-03-02T23:33:16.315445: step 377, loss 0.499525, acc 0.78125
2017-03-02T23:33:16.411524: step 378, loss 0.931194, acc 0.5625
2017-03-02T23:33:16.520808: step 379, loss 0.702099, acc 0.75
2017-03-02T23:33:16.630491: step 380, loss 0.67891, acc 0.6875
2017-03-02T23:33:16.724157: step 381, loss 0.727785, acc 0.65625
2017-03-02T23:33:16.822361: step 382, loss 0.817524, acc 0.671875
2017-03-02T23:33:16.929248: step 383, loss 0.808026, acc 0.625
2017-03-02T23:33:17.032830: step 384, loss 0.736711, acc 0.71875
2017-03-02T23:33:17.134257: step 385, loss 0.564628, acc 0.734375
2017-03-02T23:33:17.236432: step 386, loss 0.788325, acc 0.65625
2017-03-02T23:33:17.335225: step 387, loss 0.912259, acc 0.671875
2017-03-02T23:33:17.429268: step 388, loss 0.599315, acc 0.8125
2017-03-02T23:33:17.523073: step 389, loss 0.698674, acc 0.71875
2017-03-02T23:33:17.632087: step 390, loss 0.578869, acc 0.765625
2017-03-02T23:33:17.761005: step 391, loss 0.863105, acc 0.625
2017-03-02T23:33:17.865263: step 392, loss 0.554877, acc 0.78125
2017-03-02T23:33:17.971644: step 393, loss 0.779102, acc 0.640625
2017-03-02T23:33:18.078493: step 394, loss 0.692755, acc 0.765625
2017-03-02T23:33:18.181432: step 395, loss 0.628064, acc 0.765625
2017-03-02T23:33:18.277381: step 396, loss 0.747515, acc 0.671875
2017-03-02T23:33:18.376761: step 397, loss 0.642441, acc 0.71875
2017-03-02T23:33:18.483117: step 398, loss 0.93099, acc 0.6875
2017-03-02T23:33:18.578685: step 399, loss 0.44633, acc 0.8125
2017-03-02T23:33:18.693857: step 400, loss 0.728818, acc 0.703125

Evaluation:
2017-03-02T23:33:18.755011: step 400, loss 1.13989, acc 0.557093

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-400

2017-03-02T23:33:19.186623: step 401, loss 0.724271, acc 0.71875
2017-03-02T23:33:19.294542: step 402, loss 0.667786, acc 0.734375
2017-03-02T23:33:19.398866: step 403, loss 0.860579, acc 0.671875
2017-03-02T23:33:19.533319: step 404, loss 0.523905, acc 0.734375
2017-03-02T23:33:19.642240: step 405, loss 0.829899, acc 0.6875
2017-03-02T23:33:19.740582: step 406, loss 0.621596, acc 0.734375
2017-03-02T23:33:19.835368: step 407, loss 0.906369, acc 0.625
2017-03-02T23:33:19.941813: step 408, loss 0.751836, acc 0.640625
2017-03-02T23:33:20.051106: step 409, loss 1.00102, acc 0.609375
2017-03-02T23:33:20.161174: step 410, loss 0.866139, acc 0.7
2017-03-02T23:33:20.270018: step 411, loss 0.60261, acc 0.75
2017-03-02T23:33:20.385318: step 412, loss 0.667837, acc 0.75
2017-03-02T23:33:20.476587: step 413, loss 0.752211, acc 0.765625
2017-03-02T23:33:20.587134: step 414, loss 0.635336, acc 0.734375
2017-03-02T23:33:20.699909: step 415, loss 0.60103, acc 0.734375
2017-03-02T23:33:20.810998: step 416, loss 0.627916, acc 0.75
2017-03-02T23:33:20.913244: step 417, loss 0.560077, acc 0.78125
2017-03-02T23:33:21.013440: step 418, loss 0.558439, acc 0.796875
2017-03-02T23:33:21.117011: step 419, loss 0.464408, acc 0.765625
2017-03-02T23:33:21.212695: step 420, loss 0.709972, acc 0.703125
2017-03-02T23:33:21.305672: step 421, loss 0.567559, acc 0.71875
2017-03-02T23:33:21.405469: step 422, loss 0.520483, acc 0.8125
2017-03-02T23:33:21.513499: step 423, loss 0.645014, acc 0.734375
2017-03-02T23:33:21.618662: step 424, loss 0.576097, acc 0.75
2017-03-02T23:33:21.725599: step 425, loss 0.537955, acc 0.765625
2017-03-02T23:33:21.828189: step 426, loss 0.610071, acc 0.71875
2017-03-02T23:33:21.931472: step 427, loss 0.547409, acc 0.734375
2017-03-02T23:33:22.018802: step 428, loss 0.698071, acc 0.765625
2017-03-02T23:33:22.123170: step 429, loss 0.79053, acc 0.609375
2017-03-02T23:33:22.231439: step 430, loss 0.539449, acc 0.796875
2017-03-02T23:33:22.337013: step 431, loss 0.673125, acc 0.75
2017-03-02T23:33:22.440084: step 432, loss 0.564377, acc 0.765625
2017-03-02T23:33:22.542369: step 433, loss 0.704118, acc 0.71875
2017-03-02T23:33:22.652006: step 434, loss 0.609071, acc 0.78125
2017-03-02T23:33:22.743665: step 435, loss 0.615659, acc 0.765625
2017-03-02T23:33:22.843791: step 436, loss 0.541133, acc 0.78125
2017-03-02T23:33:22.948792: step 437, loss 0.653695, acc 0.796875
2017-03-02T23:33:23.060652: step 438, loss 0.682614, acc 0.671875
2017-03-02T23:33:23.175066: step 439, loss 0.732311, acc 0.671875
2017-03-02T23:33:23.278633: step 440, loss 0.584776, acc 0.8125
2017-03-02T23:33:23.382704: step 441, loss 0.652782, acc 0.71875
2017-03-02T23:33:23.474795: step 442, loss 0.539776, acc 0.71875
2017-03-02T23:33:23.571193: step 443, loss 0.864128, acc 0.71875
2017-03-02T23:33:23.676728: step 444, loss 0.433487, acc 0.796875
2017-03-02T23:33:23.781260: step 445, loss 0.61519, acc 0.796875
2017-03-02T23:33:23.884752: step 446, loss 0.651824, acc 0.734375
2017-03-02T23:33:23.988097: step 447, loss 0.462908, acc 0.78125
2017-03-02T23:33:24.094676: step 448, loss 0.619575, acc 0.734375
2017-03-02T23:33:24.201812: step 449, loss 0.683994, acc 0.640625
2017-03-02T23:33:24.293404: step 450, loss 0.703525, acc 0.75
2017-03-02T23:33:24.386582: step 451, loss 0.46344, acc 0.88
2017-03-02T23:33:24.492665: step 452, loss 0.712732, acc 0.78125
2017-03-02T23:33:24.597772: step 453, loss 0.37218, acc 0.859375
2017-03-02T23:33:24.707465: step 454, loss 0.766888, acc 0.734375
2017-03-02T23:33:24.805032: step 455, loss 0.406209, acc 0.84375
2017-03-02T23:33:24.913847: step 456, loss 0.374705, acc 0.796875
2017-03-02T23:33:25.010215: step 457, loss 0.533563, acc 0.796875
2017-03-02T23:33:25.106744: step 458, loss 0.732251, acc 0.703125
2017-03-02T23:33:25.210917: step 459, loss 0.755392, acc 0.71875
2017-03-02T23:33:25.316975: step 460, loss 0.639801, acc 0.765625
2017-03-02T23:33:25.418857: step 461, loss 0.603017, acc 0.78125
2017-03-02T23:33:25.527262: step 462, loss 0.552208, acc 0.75
2017-03-02T23:33:25.633332: step 463, loss 0.45742, acc 0.828125
2017-03-02T23:33:25.730978: step 464, loss 0.509367, acc 0.765625
2017-03-02T23:33:25.822416: step 465, loss 0.672007, acc 0.734375
2017-03-02T23:33:25.928166: step 466, loss 0.594944, acc 0.75
2017-03-02T23:33:26.062496: step 467, loss 0.638788, acc 0.71875
2017-03-02T23:33:26.165624: step 468, loss 0.4399, acc 0.8125
2017-03-02T23:33:26.273029: step 469, loss 0.619115, acc 0.75
2017-03-02T23:33:26.371343: step 470, loss 0.505868, acc 0.828125
2017-03-02T23:33:26.482188: step 471, loss 0.685298, acc 0.671875
2017-03-02T23:33:26.579751: step 472, loss 0.592985, acc 0.78125
2017-03-02T23:33:26.684116: step 473, loss 0.51912, acc 0.796875
2017-03-02T23:33:26.792175: step 474, loss 0.901378, acc 0.59375
2017-03-02T23:33:26.898669: step 475, loss 0.852893, acc 0.765625
2017-03-02T23:33:27.011954: step 476, loss 0.490216, acc 0.78125
2017-03-02T23:33:27.115006: step 477, loss 0.642421, acc 0.75
2017-03-02T23:33:27.226849: step 478, loss 0.425341, acc 0.8125
2017-03-02T23:33:27.316540: step 479, loss 0.49925, acc 0.8125
2017-03-02T23:33:27.432233: step 480, loss 0.74367, acc 0.703125
2017-03-02T23:33:27.547029: step 481, loss 0.638371, acc 0.765625
2017-03-02T23:33:27.647391: step 482, loss 0.532716, acc 0.75
2017-03-02T23:33:27.755469: step 483, loss 0.598163, acc 0.78125
2017-03-02T23:33:27.866615: step 484, loss 0.74483, acc 0.6875
2017-03-02T23:33:27.976799: step 485, loss 0.923027, acc 0.5625
2017-03-02T23:33:28.072235: step 486, loss 0.785553, acc 0.609375
2017-03-02T23:33:28.173749: step 487, loss 0.499362, acc 0.828125
2017-03-02T23:33:28.282067: step 488, loss 0.527168, acc 0.765625
2017-03-02T23:33:28.389841: step 489, loss 0.565803, acc 0.734375
2017-03-02T23:33:28.497409: step 490, loss 0.667993, acc 0.71875
2017-03-02T23:33:28.603091: step 491, loss 0.637407, acc 0.796875
2017-03-02T23:33:28.699101: step 492, loss 0.585357, acc 0.8
2017-03-02T23:33:28.790266: step 493, loss 0.458031, acc 0.765625
2017-03-02T23:33:28.891063: step 494, loss 0.36659, acc 0.875
2017-03-02T23:33:29.001182: step 495, loss 0.723471, acc 0.71875
2017-03-02T23:33:29.106387: step 496, loss 0.65253, acc 0.75
2017-03-02T23:33:29.216879: step 497, loss 0.545557, acc 0.734375
2017-03-02T23:33:29.310765: step 498, loss 0.564157, acc 0.734375
2017-03-02T23:33:29.418225: step 499, loss 0.476019, acc 0.796875
2017-03-02T23:33:29.508599: step 500, loss 0.810566, acc 0.65625

Evaluation:
2017-03-02T23:33:29.562348: step 500, loss 1.02443, acc 0.560554

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-500

2017-03-02T23:33:30.013651: step 501, loss 0.499045, acc 0.78125
2017-03-02T23:33:30.115700: step 502, loss 0.586582, acc 0.703125
2017-03-02T23:33:30.218185: step 503, loss 0.470816, acc 0.78125
2017-03-02T23:33:30.317349: step 504, loss 0.61873, acc 0.796875
2017-03-02T23:33:30.430011: step 505, loss 0.45085, acc 0.828125
2017-03-02T23:33:30.537584: step 506, loss 0.754146, acc 0.765625
2017-03-02T23:33:30.628473: step 507, loss 0.632794, acc 0.78125
2017-03-02T23:33:30.727243: step 508, loss 0.638114, acc 0.765625
2017-03-02T23:33:30.831898: step 509, loss 0.419146, acc 0.875
2017-03-02T23:33:30.942924: step 510, loss 0.568294, acc 0.71875
2017-03-02T23:33:31.031498: step 511, loss 0.578372, acc 0.75
2017-03-02T23:33:31.135692: step 512, loss 0.612619, acc 0.75
2017-03-02T23:33:31.242152: step 513, loss 0.579745, acc 0.75
2017-03-02T23:33:31.344379: step 514, loss 0.680781, acc 0.703125
2017-03-02T23:33:31.453132: step 515, loss 0.450899, acc 0.84375
2017-03-02T23:33:31.547768: step 516, loss 0.559393, acc 0.75
2017-03-02T23:33:31.662777: step 517, loss 0.449103, acc 0.828125
2017-03-02T23:33:31.752614: step 518, loss 0.471775, acc 0.78125
2017-03-02T23:33:31.857711: step 519, loss 0.625827, acc 0.765625
2017-03-02T23:33:31.962942: step 520, loss 0.381516, acc 0.84375
2017-03-02T23:33:32.071475: step 521, loss 0.671801, acc 0.75
2017-03-02T23:33:32.181523: step 522, loss 0.469312, acc 0.84375
2017-03-02T23:33:32.287139: step 523, loss 0.52792, acc 0.765625
2017-03-02T23:33:32.390926: step 524, loss 0.515058, acc 0.796875
2017-03-02T23:33:32.481753: step 525, loss 0.503426, acc 0.8125
2017-03-02T23:33:32.587489: step 526, loss 0.504923, acc 0.75
2017-03-02T23:33:32.714643: step 527, loss 0.527255, acc 0.734375
2017-03-02T23:33:32.829774: step 528, loss 0.747115, acc 0.671875
2017-03-02T23:33:32.929361: step 529, loss 0.66918, acc 0.703125
2017-03-02T23:33:33.029733: step 530, loss 0.506424, acc 0.765625
2017-03-02T23:33:33.133170: step 531, loss 0.624823, acc 0.734375
2017-03-02T23:33:33.221699: step 532, loss 0.438276, acc 0.796875
2017-03-02T23:33:33.311025: step 533, loss 0.441118, acc 0.78
2017-03-02T23:33:33.421612: step 534, loss 0.473657, acc 0.703125
2017-03-02T23:33:33.526304: step 535, loss 0.542037, acc 0.75
2017-03-02T23:33:33.643657: step 536, loss 0.462902, acc 0.859375
2017-03-02T23:33:33.749494: step 537, loss 0.379081, acc 0.859375
2017-03-02T23:33:33.851124: step 538, loss 0.4939, acc 0.78125
2017-03-02T23:33:33.940305: step 539, loss 0.456663, acc 0.84375
2017-03-02T23:33:34.055764: step 540, loss 0.369519, acc 0.828125
2017-03-02T23:33:34.162497: step 541, loss 0.54671, acc 0.75
2017-03-02T23:33:34.255367: step 542, loss 0.500038, acc 0.84375
2017-03-02T23:33:34.361879: step 543, loss 0.496625, acc 0.796875
2017-03-02T23:33:34.464812: step 544, loss 0.454837, acc 0.84375
2017-03-02T23:33:34.567476: step 545, loss 0.598609, acc 0.703125
2017-03-02T23:33:34.662596: step 546, loss 0.54104, acc 0.765625
2017-03-02T23:33:34.755066: step 547, loss 0.546793, acc 0.734375
2017-03-02T23:33:34.863005: step 548, loss 0.433881, acc 0.8125
2017-03-02T23:33:34.966582: step 549, loss 0.688287, acc 0.703125
2017-03-02T23:33:35.090375: step 550, loss 0.567554, acc 0.78125
2017-03-02T23:33:35.197533: step 551, loss 0.409298, acc 0.765625
2017-03-02T23:33:35.303966: step 552, loss 0.461411, acc 0.84375
2017-03-02T23:33:35.433864: step 553, loss 0.460191, acc 0.78125
2017-03-02T23:33:35.521753: step 554, loss 0.427044, acc 0.828125
2017-03-02T23:33:35.626133: step 555, loss 0.517586, acc 0.734375
2017-03-02T23:33:35.731325: step 556, loss 0.423215, acc 0.796875
2017-03-02T23:33:35.838769: step 557, loss 0.496805, acc 0.8125
2017-03-02T23:33:35.954700: step 558, loss 0.648079, acc 0.734375
2017-03-02T23:33:36.057950: step 559, loss 0.341954, acc 0.84375
2017-03-02T23:33:36.174977: step 560, loss 0.365585, acc 0.859375
2017-03-02T23:33:36.262106: step 561, loss 0.418638, acc 0.859375
2017-03-02T23:33:36.363479: step 562, loss 0.485198, acc 0.8125
2017-03-02T23:33:36.466172: step 563, loss 0.7876, acc 0.734375
2017-03-02T23:33:36.572119: step 564, loss 0.523146, acc 0.765625
2017-03-02T23:33:36.681643: step 565, loss 0.413351, acc 0.78125
2017-03-02T23:33:36.778970: step 566, loss 0.385683, acc 0.859375
2017-03-02T23:33:36.887416: step 567, loss 0.749316, acc 0.734375
2017-03-02T23:33:36.978998: step 568, loss 0.254603, acc 0.875
2017-03-02T23:33:37.080230: step 569, loss 0.54783, acc 0.765625
2017-03-02T23:33:37.183875: step 570, loss 0.677934, acc 0.734375
2017-03-02T23:33:37.296860: step 571, loss 0.633362, acc 0.75
2017-03-02T23:33:37.414791: step 572, loss 0.550352, acc 0.796875
2017-03-02T23:33:37.517888: step 573, loss 0.732503, acc 0.703125
2017-03-02T23:33:37.637806: step 574, loss 0.64504, acc 0.66
2017-03-02T23:33:37.725515: step 575, loss 0.462599, acc 0.796875
2017-03-02T23:33:37.822201: step 576, loss 0.303259, acc 0.859375
2017-03-02T23:33:37.925729: step 577, loss 0.396184, acc 0.890625
2017-03-02T23:33:38.030898: step 578, loss 0.476916, acc 0.75
2017-03-02T23:33:38.136372: step 579, loss 0.393875, acc 0.84375
2017-03-02T23:33:38.263706: step 580, loss 0.525429, acc 0.796875
2017-03-02T23:33:38.371789: step 581, loss 0.5202, acc 0.78125
2017-03-02T23:33:38.476643: step 582, loss 0.512645, acc 0.765625
2017-03-02T23:33:38.589908: step 583, loss 0.483916, acc 0.84375
2017-03-02T23:33:38.695672: step 584, loss 0.521456, acc 0.765625
2017-03-02T23:33:38.815534: step 585, loss 0.56089, acc 0.71875
2017-03-02T23:33:38.930484: step 586, loss 0.641827, acc 0.765625
2017-03-02T23:33:39.042615: step 587, loss 0.507282, acc 0.765625
2017-03-02T23:33:39.144180: step 588, loss 0.331359, acc 0.890625
2017-03-02T23:33:39.236942: step 589, loss 0.362252, acc 0.859375
2017-03-02T23:33:39.349771: step 590, loss 0.638246, acc 0.75
2017-03-02T23:33:39.454464: step 591, loss 0.27135, acc 0.90625
2017-03-02T23:33:39.562017: step 592, loss 0.39953, acc 0.859375
2017-03-02T23:33:39.667053: step 593, loss 0.468291, acc 0.78125
2017-03-02T23:33:39.767612: step 594, loss 0.420889, acc 0.78125
2017-03-02T23:33:39.863390: step 595, loss 0.635355, acc 0.78125
2017-03-02T23:33:39.955727: step 596, loss 0.492658, acc 0.84375
2017-03-02T23:33:40.051931: step 597, loss 0.452917, acc 0.765625
2017-03-02T23:33:40.171919: step 598, loss 0.548688, acc 0.78125
2017-03-02T23:33:40.278918: step 599, loss 0.441123, acc 0.8125
2017-03-02T23:33:40.381861: step 600, loss 0.488211, acc 0.828125

Evaluation:
2017-03-02T23:33:40.441934: step 600, loss 1.11448, acc 0.550173

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-600

2017-03-02T23:33:40.906440: step 601, loss 0.708454, acc 0.71875
2017-03-02T23:33:41.011956: step 602, loss 0.365807, acc 0.90625
2017-03-02T23:33:41.135301: step 603, loss 0.536518, acc 0.78125
2017-03-02T23:33:41.240071: step 604, loss 0.675899, acc 0.71875
2017-03-02T23:33:41.341821: step 605, loss 0.466134, acc 0.8125
2017-03-02T23:33:41.432623: step 606, loss 0.359834, acc 0.875
2017-03-02T23:33:41.530963: step 607, loss 0.407098, acc 0.859375
2017-03-02T23:33:41.635419: step 608, loss 0.551753, acc 0.8125
2017-03-02T23:33:41.743485: step 609, loss 0.538484, acc 0.78125
2017-03-02T23:33:41.852504: step 610, loss 0.676862, acc 0.71875
2017-03-02T23:33:41.963633: step 611, loss 0.441663, acc 0.75
2017-03-02T23:33:42.071730: step 612, loss 0.355558, acc 0.875
2017-03-02T23:33:42.161731: step 613, loss 0.314303, acc 0.84375
2017-03-02T23:33:42.261771: step 614, loss 0.505232, acc 0.75
2017-03-02T23:33:42.354538: step 615, loss 0.478069, acc 0.8
2017-03-02T23:33:42.463427: step 616, loss 0.484735, acc 0.78125
2017-03-02T23:33:42.569640: step 617, loss 0.523797, acc 0.765625
2017-03-02T23:33:42.677034: step 618, loss 0.392624, acc 0.859375
2017-03-02T23:33:42.785885: step 619, loss 0.558143, acc 0.828125
2017-03-02T23:33:42.904677: step 620, loss 0.407, acc 0.78125
2017-03-02T23:33:42.994899: step 621, loss 0.548247, acc 0.8125
2017-03-02T23:33:43.100908: step 622, loss 0.524318, acc 0.765625
2017-03-02T23:33:43.202176: step 623, loss 0.478052, acc 0.859375
2017-03-02T23:33:43.314144: step 624, loss 0.513802, acc 0.8125
2017-03-02T23:33:43.415480: step 625, loss 0.481921, acc 0.765625
2017-03-02T23:33:43.527584: step 626, loss 0.522968, acc 0.8125
2017-03-02T23:33:43.628377: step 627, loss 0.492813, acc 0.859375
2017-03-02T23:33:43.720306: step 628, loss 0.401899, acc 0.828125
2017-03-02T23:33:43.815165: step 629, loss 0.418482, acc 0.78125
2017-03-02T23:33:43.917869: step 630, loss 0.378524, acc 0.78125
2017-03-02T23:33:44.060636: step 631, loss 0.386626, acc 0.8125
2017-03-02T23:33:44.165450: step 632, loss 0.42932, acc 0.859375
2017-03-02T23:33:44.266996: step 633, loss 0.381256, acc 0.875
2017-03-02T23:33:44.369440: step 634, loss 0.454007, acc 0.8125
2017-03-02T23:33:44.463087: step 635, loss 0.41225, acc 0.84375
2017-03-02T23:33:44.554011: step 636, loss 0.422443, acc 0.84375
2017-03-02T23:33:44.662596: step 637, loss 0.768307, acc 0.734375
2017-03-02T23:33:44.767521: step 638, loss 0.583784, acc 0.78125
2017-03-02T23:33:44.876488: step 639, loss 0.518088, acc 0.765625
2017-03-02T23:33:44.975810: step 640, loss 0.504325, acc 0.765625
2017-03-02T23:33:45.080943: step 641, loss 0.33138, acc 0.890625
2017-03-02T23:33:45.188505: step 642, loss 0.26562, acc 0.921875
2017-03-02T23:33:45.283053: step 643, loss 0.495456, acc 0.8125
2017-03-02T23:33:45.388645: step 644, loss 0.456109, acc 0.78125
2017-03-02T23:33:45.491006: step 645, loss 0.611586, acc 0.75
2017-03-02T23:33:45.595718: step 646, loss 0.388301, acc 0.890625
2017-03-02T23:33:45.704839: step 647, loss 0.633982, acc 0.765625
2017-03-02T23:33:45.811581: step 648, loss 0.60086, acc 0.734375
2017-03-02T23:33:45.919178: step 649, loss 0.391176, acc 0.828125
2017-03-02T23:33:46.013907: step 650, loss 0.346948, acc 0.875
2017-03-02T23:33:46.111737: step 651, loss 0.572261, acc 0.765625
2017-03-02T23:33:46.229958: step 652, loss 0.413894, acc 0.859375
2017-03-02T23:33:46.335770: step 653, loss 0.513033, acc 0.734375
2017-03-02T23:33:46.440327: step 654, loss 0.384025, acc 0.8125
2017-03-02T23:33:46.542057: step 655, loss 0.343901, acc 0.859375
2017-03-02T23:33:46.636792: step 656, loss 0.272702, acc 0.9
2017-03-02T23:33:46.738204: step 657, loss 0.347402, acc 0.84375
2017-03-02T23:33:46.823810: step 658, loss 0.429245, acc 0.8125
2017-03-02T23:33:46.926524: step 659, loss 0.292888, acc 0.90625
2017-03-02T23:33:47.028601: step 660, loss 0.490449, acc 0.828125
2017-03-02T23:33:47.135595: step 661, loss 0.486628, acc 0.84375
2017-03-02T23:33:47.252722: step 662, loss 0.442733, acc 0.765625
2017-03-02T23:33:47.351863: step 663, loss 0.390608, acc 0.875
2017-03-02T23:33:47.455707: step 664, loss 0.476051, acc 0.828125
2017-03-02T23:33:47.541918: step 665, loss 0.514999, acc 0.78125
2017-03-02T23:33:47.641966: step 666, loss 0.441658, acc 0.8125
2017-03-02T23:33:47.742289: step 667, loss 0.425375, acc 0.8125
2017-03-02T23:33:47.843915: step 668, loss 0.518371, acc 0.8125
2017-03-02T23:33:47.948228: step 669, loss 0.599586, acc 0.78125
2017-03-02T23:33:48.050190: step 670, loss 0.43591, acc 0.796875
2017-03-02T23:33:48.156840: step 671, loss 0.194233, acc 0.890625
2017-03-02T23:33:48.242523: step 672, loss 0.407444, acc 0.796875
2017-03-02T23:33:48.339992: step 673, loss 0.283174, acc 0.890625
2017-03-02T23:33:48.444033: step 674, loss 0.405871, acc 0.828125
2017-03-02T23:33:48.544566: step 675, loss 0.417657, acc 0.859375
2017-03-02T23:33:48.646121: step 676, loss 0.502561, acc 0.78125
2017-03-02T23:33:48.750544: step 677, loss 0.372217, acc 0.84375
2017-03-02T23:33:48.853137: step 678, loss 0.511894, acc 0.796875
2017-03-02T23:33:48.958679: step 679, loss 0.400757, acc 0.828125
2017-03-02T23:33:49.049683: step 680, loss 0.438396, acc 0.84375
2017-03-02T23:33:49.153686: step 681, loss 0.303823, acc 0.890625
2017-03-02T23:33:49.256157: step 682, loss 0.601189, acc 0.75
2017-03-02T23:33:49.364939: step 683, loss 0.383073, acc 0.828125
2017-03-02T23:33:49.466180: step 684, loss 0.521062, acc 0.8125
2017-03-02T23:33:49.570764: step 685, loss 0.60936, acc 0.75
2017-03-02T23:33:49.680865: step 686, loss 0.33483, acc 0.84375
2017-03-02T23:33:49.771475: step 687, loss 0.526263, acc 0.78125
2017-03-02T23:33:49.873213: step 688, loss 0.402079, acc 0.84375
2017-03-02T23:33:49.983355: step 689, loss 0.459991, acc 0.8125
2017-03-02T23:33:50.089791: step 690, loss 0.536144, acc 0.765625
2017-03-02T23:33:50.190939: step 691, loss 1.08595, acc 0.703125
2017-03-02T23:33:50.293729: step 692, loss 0.477572, acc 0.828125
2017-03-02T23:33:50.396626: step 693, loss 0.391431, acc 0.84375
2017-03-02T23:33:50.485188: step 694, loss 0.516436, acc 0.765625
2017-03-02T23:33:50.591483: step 695, loss 0.555743, acc 0.75
2017-03-02T23:33:50.698055: step 696, loss 0.447331, acc 0.8125
2017-03-02T23:33:50.796088: step 697, loss 0.461375, acc 0.82
2017-03-02T23:33:50.906518: step 698, loss 0.378868, acc 0.875
2017-03-02T23:33:51.013446: step 699, loss 0.294374, acc 0.890625
2017-03-02T23:33:51.124773: step 700, loss 0.331626, acc 0.859375

Evaluation:
2017-03-02T23:33:51.166905: step 700, loss 1.38313, acc 0.570934

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-700

2017-03-02T23:33:51.611566: step 701, loss 0.248704, acc 0.90625
2017-03-02T23:33:51.716890: step 702, loss 0.543379, acc 0.8125
2017-03-02T23:33:51.825921: step 703, loss 0.640431, acc 0.765625
2017-03-02T23:33:51.921663: step 704, loss 0.526521, acc 0.8125
2017-03-02T23:33:52.013426: step 705, loss 0.34033, acc 0.875
2017-03-02T23:33:52.115156: step 706, loss 0.429444, acc 0.796875
2017-03-02T23:33:52.219128: step 707, loss 0.592499, acc 0.734375
2017-03-02T23:33:52.339437: step 708, loss 0.490314, acc 0.859375
2017-03-02T23:33:52.440365: step 709, loss 0.563115, acc 0.734375
2017-03-02T23:33:52.545212: step 710, loss 0.45796, acc 0.84375
2017-03-02T23:33:52.658518: step 711, loss 0.333733, acc 0.875
2017-03-02T23:33:52.751928: step 712, loss 0.267117, acc 0.890625
2017-03-02T23:33:52.858890: step 713, loss 0.275454, acc 0.875
2017-03-02T23:33:52.967451: step 714, loss 0.429282, acc 0.78125
2017-03-02T23:33:53.066497: step 715, loss 0.659308, acc 0.75
2017-03-02T23:33:53.164994: step 716, loss 0.222043, acc 0.90625
2017-03-02T23:33:53.266845: step 717, loss 0.41049, acc 0.765625
2017-03-02T23:33:53.382235: step 718, loss 0.277125, acc 0.84375
2017-03-02T23:33:53.469893: step 719, loss 0.383483, acc 0.796875
2017-03-02T23:33:53.579517: step 720, loss 0.650094, acc 0.703125
2017-03-02T23:33:53.683439: step 721, loss 0.619474, acc 0.8125
2017-03-02T23:33:53.785494: step 722, loss 0.381579, acc 0.875
2017-03-02T23:33:53.888615: step 723, loss 0.261821, acc 0.890625
2017-03-02T23:33:54.000368: step 724, loss 0.399088, acc 0.828125
2017-03-02T23:33:54.119910: step 725, loss 0.406789, acc 0.84375
2017-03-02T23:33:54.212782: step 726, loss 0.489543, acc 0.8125
2017-03-02T23:33:54.310349: step 727, loss 0.571709, acc 0.640625
2017-03-02T23:33:54.413189: step 728, loss 0.420546, acc 0.859375
2017-03-02T23:33:54.516372: step 729, loss 0.419359, acc 0.84375
2017-03-02T23:33:54.623991: step 730, loss 0.462105, acc 0.859375
2017-03-02T23:33:54.729028: step 731, loss 0.559753, acc 0.765625
2017-03-02T23:33:54.835455: step 732, loss 0.48578, acc 0.765625
2017-03-02T23:33:54.936594: step 733, loss 0.338585, acc 0.84375
2017-03-02T23:33:55.030382: step 734, loss 0.425828, acc 0.796875
2017-03-02T23:33:55.135364: step 735, loss 0.29453, acc 0.90625
2017-03-02T23:33:55.241927: step 736, loss 0.494436, acc 0.765625
2017-03-02T23:33:55.359700: step 737, loss 0.385559, acc 0.8125
2017-03-02T23:33:55.451733: step 738, loss 0.466342, acc 0.78
2017-03-02T23:33:55.555843: step 739, loss 0.254611, acc 0.90625
2017-03-02T23:33:55.666393: step 740, loss 0.500333, acc 0.796875
2017-03-02T23:33:55.749345: step 741, loss 0.45683, acc 0.796875
2017-03-02T23:33:55.853051: step 742, loss 0.239993, acc 0.890625
2017-03-02T23:33:55.947214: step 743, loss 0.432414, acc 0.796875
2017-03-02T23:33:56.057799: step 744, loss 0.578977, acc 0.75
2017-03-02T23:33:56.168649: step 745, loss 0.47158, acc 0.8125
2017-03-02T23:33:56.274959: step 746, loss 0.594215, acc 0.765625
2017-03-02T23:33:56.388548: step 747, loss 0.274921, acc 0.890625
2017-03-02T23:33:56.492802: step 748, loss 0.383381, acc 0.84375
2017-03-02T23:33:56.589212: step 749, loss 0.429271, acc 0.84375
2017-03-02T23:33:56.696770: step 750, loss 0.255204, acc 0.921875
2017-03-02T23:33:56.796767: step 751, loss 0.372377, acc 0.859375
2017-03-02T23:33:56.902351: step 752, loss 0.436337, acc 0.8125
2017-03-02T23:33:57.014116: step 753, loss 0.430114, acc 0.84375
2017-03-02T23:33:57.123624: step 754, loss 0.281866, acc 0.90625
2017-03-02T23:33:57.222547: step 755, loss 0.414244, acc 0.8125
2017-03-02T23:33:57.311676: step 756, loss 0.526555, acc 0.765625
2017-03-02T23:33:57.418898: step 757, loss 0.348756, acc 0.84375
2017-03-02T23:33:57.524096: step 758, loss 0.399661, acc 0.875
2017-03-02T23:33:57.644193: step 759, loss 0.289163, acc 0.90625
2017-03-02T23:33:57.755764: step 760, loss 0.311677, acc 0.875
2017-03-02T23:33:57.859595: step 761, loss 0.412552, acc 0.828125
2017-03-02T23:33:57.967403: step 762, loss 0.297255, acc 0.890625
2017-03-02T23:33:58.057208: step 763, loss 0.405907, acc 0.796875
2017-03-02T23:33:58.151877: step 764, loss 0.508817, acc 0.796875
2017-03-02T23:33:58.259571: step 765, loss 0.316074, acc 0.90625
2017-03-02T23:33:58.362913: step 766, loss 0.519412, acc 0.8125
2017-03-02T23:33:58.469659: step 767, loss 0.243868, acc 0.90625
2017-03-02T23:33:58.574198: step 768, loss 0.519635, acc 0.734375
2017-03-02T23:33:58.677061: step 769, loss 0.478645, acc 0.828125
2017-03-02T23:33:58.763547: step 770, loss 0.488014, acc 0.8125
2017-03-02T23:33:58.849689: step 771, loss 0.293707, acc 0.84375
2017-03-02T23:33:58.959628: step 772, loss 0.43208, acc 0.75
2017-03-02T23:33:59.069299: step 773, loss 0.226505, acc 0.921875
2017-03-02T23:33:59.167226: step 774, loss 0.244449, acc 0.890625
2017-03-02T23:33:59.268943: step 775, loss 0.254536, acc 0.875
2017-03-02T23:33:59.376974: step 776, loss 0.422589, acc 0.859375
2017-03-02T23:33:59.482211: step 777, loss 0.35061, acc 0.859375
2017-03-02T23:33:59.568854: step 778, loss 0.522109, acc 0.828125
2017-03-02T23:33:59.667551: step 779, loss 0.586751, acc 0.78
2017-03-02T23:33:59.774872: step 780, loss 0.365081, acc 0.8125
2017-03-02T23:33:59.878646: step 781, loss 0.339053, acc 0.890625
2017-03-02T23:33:59.983124: step 782, loss 0.405878, acc 0.875
2017-03-02T23:34:00.083203: step 783, loss 0.55109, acc 0.75
2017-03-02T23:34:00.188669: step 784, loss 0.295357, acc 0.828125
2017-03-02T23:34:00.290213: step 785, loss 0.450157, acc 0.84375
2017-03-02T23:34:00.393472: step 786, loss 0.328939, acc 0.890625
2017-03-02T23:34:00.491883: step 787, loss 0.304978, acc 0.890625
2017-03-02T23:34:00.599143: step 788, loss 0.363215, acc 0.890625
2017-03-02T23:34:00.700597: step 789, loss 0.211603, acc 0.90625
2017-03-02T23:34:00.803071: step 790, loss 0.287561, acc 0.890625
2017-03-02T23:34:00.911411: step 791, loss 0.42296, acc 0.8125
2017-03-02T23:34:01.012042: step 792, loss 0.407949, acc 0.8125
2017-03-02T23:34:01.108590: step 793, loss 0.345369, acc 0.796875
2017-03-02T23:34:01.213246: step 794, loss 0.345925, acc 0.875
2017-03-02T23:34:01.318012: step 795, loss 0.234107, acc 0.921875
2017-03-02T23:34:01.422123: step 796, loss 0.250603, acc 0.90625
2017-03-02T23:34:01.516369: step 797, loss 0.369392, acc 0.828125
2017-03-02T23:34:01.621055: step 798, loss 0.438862, acc 0.796875
2017-03-02T23:34:01.737089: step 799, loss 0.400347, acc 0.8125
2017-03-02T23:34:01.825448: step 800, loss 0.367631, acc 0.828125

Evaluation:
2017-03-02T23:34:01.881091: step 800, loss 1.15151, acc 0.567474

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-800

2017-03-02T23:34:02.322588: step 801, loss 0.452373, acc 0.8125
2017-03-02T23:34:02.429059: step 802, loss 0.309268, acc 0.890625
2017-03-02T23:34:02.515551: step 803, loss 0.444087, acc 0.8125
2017-03-02T23:34:02.617868: step 804, loss 0.292922, acc 0.828125
2017-03-02T23:34:02.724364: step 805, loss 0.489618, acc 0.8125
2017-03-02T23:34:02.827514: step 806, loss 0.313656, acc 0.859375
2017-03-02T23:34:02.952045: step 807, loss 0.308659, acc 0.90625
2017-03-02T23:34:03.054868: step 808, loss 0.414715, acc 0.78125
2017-03-02T23:34:03.156167: step 809, loss 0.299728, acc 0.875
2017-03-02T23:34:03.246347: step 810, loss 0.509828, acc 0.796875
2017-03-02T23:34:03.336629: step 811, loss 0.338295, acc 0.890625
2017-03-02T23:34:03.441089: step 812, loss 0.308937, acc 0.90625
2017-03-02T23:34:03.538508: step 813, loss 0.317727, acc 0.859375
2017-03-02T23:34:03.645026: step 814, loss 0.504064, acc 0.8125
2017-03-02T23:34:03.751208: step 815, loss 0.343105, acc 0.875
2017-03-02T23:34:03.852311: step 816, loss 0.419421, acc 0.828125
2017-03-02T23:34:03.967997: step 817, loss 0.293394, acc 0.875
2017-03-02T23:34:04.056221: step 818, loss 0.249863, acc 0.921875
2017-03-02T23:34:04.157166: step 819, loss 0.341283, acc 0.84375
2017-03-02T23:34:04.260523: step 820, loss 0.293811, acc 0.88
2017-03-02T23:34:04.364821: step 821, loss 0.431614, acc 0.828125
2017-03-02T23:34:04.472213: step 822, loss 0.392624, acc 0.859375
2017-03-02T23:34:04.571659: step 823, loss 0.374457, acc 0.8125
2017-03-02T23:34:04.683422: step 824, loss 0.325061, acc 0.828125
2017-03-02T23:34:04.773726: step 825, loss 0.380318, acc 0.890625
2017-03-02T23:34:04.874529: step 826, loss 0.311434, acc 0.875
2017-03-02T23:34:04.980547: step 827, loss 0.324116, acc 0.859375
2017-03-02T23:34:05.098933: step 828, loss 0.42642, acc 0.8125
2017-03-02T23:34:05.204136: step 829, loss 0.491736, acc 0.78125
2017-03-02T23:34:05.309134: step 830, loss 0.471753, acc 0.796875
2017-03-02T23:34:05.421674: step 831, loss 0.431569, acc 0.84375
2017-03-02T23:34:05.516251: step 832, loss 0.471453, acc 0.8125
2017-03-02T23:34:05.612562: step 833, loss 0.330556, acc 0.859375
2017-03-02T23:34:05.716237: step 834, loss 0.38352, acc 0.8125
2017-03-02T23:34:05.819150: step 835, loss 0.458113, acc 0.859375
2017-03-02T23:34:05.923871: step 836, loss 0.316837, acc 0.890625
2017-03-02T23:34:06.027796: step 837, loss 0.332206, acc 0.875
2017-03-02T23:34:06.130183: step 838, loss 0.231149, acc 0.890625
2017-03-02T23:34:06.238029: step 839, loss 0.602207, acc 0.734375
2017-03-02T23:34:06.325414: step 840, loss 0.354499, acc 0.84375
2017-03-02T23:34:06.429923: step 841, loss 0.349094, acc 0.875
2017-03-02T23:34:06.537656: step 842, loss 0.266264, acc 0.890625
2017-03-02T23:34:06.640411: step 843, loss 0.264825, acc 0.921875
2017-03-02T23:34:06.750975: step 844, loss 0.507498, acc 0.84375
2017-03-02T23:34:06.851196: step 845, loss 0.564999, acc 0.796875
2017-03-02T23:34:06.959812: step 846, loss 0.370844, acc 0.796875
2017-03-02T23:34:07.050032: step 847, loss 0.288021, acc 0.890625
2017-03-02T23:34:07.151578: step 848, loss 0.290497, acc 0.875
2017-03-02T23:34:07.255169: step 849, loss 0.361322, acc 0.828125
2017-03-02T23:34:07.360339: step 850, loss 0.339081, acc 0.890625
2017-03-02T23:34:07.462800: step 851, loss 0.556634, acc 0.84375
2017-03-02T23:34:07.566541: step 852, loss 0.469256, acc 0.84375
2017-03-02T23:34:07.674296: step 853, loss 0.373491, acc 0.8125
2017-03-02T23:34:07.771495: step 854, loss 0.32142, acc 0.921875
2017-03-02T23:34:07.859337: step 855, loss 0.345851, acc 0.828125
2017-03-02T23:34:07.963402: step 856, loss 0.283152, acc 0.90625
2017-03-02T23:34:08.065601: step 857, loss 0.454111, acc 0.78125
2017-03-02T23:34:08.181041: step 858, loss 0.425508, acc 0.84375
2017-03-02T23:34:08.286399: step 859, loss 0.378392, acc 0.90625
2017-03-02T23:34:08.385956: step 860, loss 0.378667, acc 0.84375
2017-03-02T23:34:08.482566: step 861, loss 0.347441, acc 0.84
2017-03-02T23:34:08.575755: step 862, loss 0.462372, acc 0.796875
2017-03-02T23:34:08.674104: step 863, loss 0.385138, acc 0.875
2017-03-02T23:34:08.779763: step 864, loss 0.23694, acc 0.921875
2017-03-02T23:34:08.875931: step 865, loss 0.294686, acc 0.84375
2017-03-02T23:34:08.986595: step 866, loss 0.382206, acc 0.875
2017-03-02T23:34:09.090319: step 867, loss 0.239122, acc 0.90625
2017-03-02T23:34:09.198367: step 868, loss 0.318494, acc 0.859375
2017-03-02T23:34:09.295758: step 869, loss 0.564687, acc 0.78125
2017-03-02T23:34:09.394126: step 870, loss 0.341842, acc 0.84375
2017-03-02T23:34:09.497703: step 871, loss 0.269514, acc 0.921875
2017-03-02T23:34:09.601524: step 872, loss 0.473263, acc 0.8125
2017-03-02T23:34:09.705066: step 873, loss 0.36763, acc 0.828125
2017-03-02T23:34:09.809302: step 874, loss 0.306281, acc 0.859375
2017-03-02T23:34:09.910675: step 875, loss 0.247285, acc 0.890625
2017-03-02T23:34:10.016686: step 876, loss 0.4668, acc 0.828125
2017-03-02T23:34:10.118762: step 877, loss 0.330626, acc 0.859375
2017-03-02T23:34:10.221638: step 878, loss 0.265544, acc 0.890625
2017-03-02T23:34:10.332702: step 879, loss 0.480196, acc 0.765625
2017-03-02T23:34:10.437029: step 880, loss 0.424191, acc 0.84375
2017-03-02T23:34:10.543683: step 881, loss 0.343586, acc 0.84375
2017-03-02T23:34:10.650216: step 882, loss 0.369801, acc 0.828125
2017-03-02T23:34:10.761892: step 883, loss 0.372256, acc 0.875
2017-03-02T23:34:10.854700: step 884, loss 0.40585, acc 0.84375
2017-03-02T23:34:10.953601: step 885, loss 0.428191, acc 0.796875
2017-03-02T23:34:11.056124: step 886, loss 0.546245, acc 0.734375
2017-03-02T23:34:11.159790: step 887, loss 0.219953, acc 0.921875
2017-03-02T23:34:11.264775: step 888, loss 0.434626, acc 0.828125
2017-03-02T23:34:11.371523: step 889, loss 0.304631, acc 0.875
2017-03-02T23:34:11.479313: step 890, loss 0.258429, acc 0.90625
2017-03-02T23:34:11.581394: step 891, loss 0.305315, acc 0.828125
2017-03-02T23:34:11.682558: step 892, loss 0.219305, acc 0.90625
2017-03-02T23:34:11.786301: step 893, loss 0.394218, acc 0.859375
2017-03-02T23:34:11.891607: step 894, loss 0.340838, acc 0.890625
2017-03-02T23:34:12.000438: step 895, loss 0.311108, acc 0.90625
2017-03-02T23:34:12.107085: step 896, loss 0.344506, acc 0.875
2017-03-02T23:34:12.210659: step 897, loss 0.308063, acc 0.890625
2017-03-02T23:34:12.297646: step 898, loss 0.264102, acc 0.875
2017-03-02T23:34:12.395674: step 899, loss 0.326665, acc 0.828125
2017-03-02T23:34:12.500976: step 900, loss 0.186161, acc 0.9375

Evaluation:
2017-03-02T23:34:12.560738: step 900, loss 1.24659, acc 0.570934

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-900

2017-03-02T23:34:12.993612: step 901, loss 0.158927, acc 0.9375
2017-03-02T23:34:13.071072: step 902, loss 0.423131, acc 0.82
2017-03-02T23:34:13.174173: step 903, loss 0.347141, acc 0.859375
2017-03-02T23:34:13.275959: step 904, loss 0.269863, acc 0.90625
2017-03-02T23:34:13.385184: step 905, loss 0.46109, acc 0.8125
2017-03-02T23:34:13.489855: step 906, loss 0.501446, acc 0.78125
2017-03-02T23:34:13.594644: step 907, loss 0.196794, acc 0.9375
2017-03-02T23:34:13.708792: step 908, loss 0.243692, acc 0.90625
2017-03-02T23:34:13.800582: step 909, loss 0.298101, acc 0.9375
2017-03-02T23:34:13.907861: step 910, loss 0.467542, acc 0.78125
2017-03-02T23:34:14.012079: step 911, loss 0.250694, acc 0.875
2017-03-02T23:34:14.137262: step 912, loss 0.360659, acc 0.875
2017-03-02T23:34:14.240166: step 913, loss 0.289264, acc 0.875
2017-03-02T23:34:14.347739: step 914, loss 0.460195, acc 0.828125
2017-03-02T23:34:14.457070: step 915, loss 0.279616, acc 0.921875
2017-03-02T23:34:14.545640: step 916, loss 0.301175, acc 0.875
2017-03-02T23:34:14.662127: step 917, loss 0.35509, acc 0.859375
2017-03-02T23:34:14.765566: step 918, loss 0.188884, acc 0.953125
2017-03-02T23:34:14.870832: step 919, loss 0.322396, acc 0.84375
2017-03-02T23:34:14.969480: step 920, loss 0.34804, acc 0.828125
2017-03-02T23:34:15.079701: step 921, loss 0.323239, acc 0.890625
2017-03-02T23:34:15.200427: step 922, loss 0.310102, acc 0.890625
2017-03-02T23:34:15.291705: step 923, loss 0.319737, acc 0.875
2017-03-02T23:34:15.393677: step 924, loss 0.18587, acc 0.96875
2017-03-02T23:34:15.493231: step 925, loss 0.287187, acc 0.875
2017-03-02T23:34:15.599358: step 926, loss 0.265893, acc 0.90625
2017-03-02T23:34:15.708415: step 927, loss 0.290146, acc 0.859375
2017-03-02T23:34:15.818077: step 928, loss 0.32699, acc 0.90625
2017-03-02T23:34:15.928583: step 929, loss 0.192081, acc 0.9375
2017-03-02T23:34:16.018014: step 930, loss 0.392833, acc 0.84375
2017-03-02T23:34:16.112620: step 931, loss 0.357008, acc 0.890625
2017-03-02T23:34:16.218664: step 932, loss 0.374643, acc 0.84375
2017-03-02T23:34:16.326912: step 933, loss 0.22667, acc 0.875
2017-03-02T23:34:16.430617: step 934, loss 0.20129, acc 0.90625
2017-03-02T23:34:16.535110: step 935, loss 0.325269, acc 0.84375
2017-03-02T23:34:16.634613: step 936, loss 0.359653, acc 0.78125
2017-03-02T23:34:16.746416: step 937, loss 0.358017, acc 0.859375
2017-03-02T23:34:16.839187: step 938, loss 0.320835, acc 0.921875
2017-03-02T23:34:16.939289: step 939, loss 0.298324, acc 0.84375
2017-03-02T23:34:17.046393: step 940, loss 0.265477, acc 0.90625
2017-03-02T23:34:17.155712: step 941, loss 0.297079, acc 0.921875
2017-03-02T23:34:17.264104: step 942, loss 0.423425, acc 0.84375
2017-03-02T23:34:17.361080: step 943, loss 0.334785, acc 0.88
2017-03-02T23:34:17.473178: step 944, loss 0.328468, acc 0.875
2017-03-02T23:34:17.560870: step 945, loss 0.150585, acc 0.953125
2017-03-02T23:34:17.656200: step 946, loss 0.452283, acc 0.78125
2017-03-02T23:34:17.757651: step 947, loss 0.323842, acc 0.890625
2017-03-02T23:34:17.870862: step 948, loss 0.286061, acc 0.921875
2017-03-02T23:34:17.975836: step 949, loss 0.427315, acc 0.828125
2017-03-02T23:34:18.079724: step 950, loss 0.290205, acc 0.90625
2017-03-02T23:34:18.187230: step 951, loss 0.273558, acc 0.90625
2017-03-02T23:34:18.304430: step 952, loss 0.305637, acc 0.890625
2017-03-02T23:34:18.396495: step 953, loss 0.404813, acc 0.84375
2017-03-02T23:34:18.499086: step 954, loss 0.466899, acc 0.8125
2017-03-02T23:34:18.618186: step 955, loss 0.207022, acc 0.90625
2017-03-02T23:34:18.721838: step 956, loss 0.366653, acc 0.84375
2017-03-02T23:34:18.826028: step 957, loss 0.341015, acc 0.859375
2017-03-02T23:34:18.931928: step 958, loss 0.317837, acc 0.859375
2017-03-02T23:34:19.039755: step 959, loss 0.280941, acc 0.90625
2017-03-02T23:34:19.130390: step 960, loss 0.235291, acc 0.90625
2017-03-02T23:34:19.229768: step 961, loss 0.439184, acc 0.828125
2017-03-02T23:34:19.331711: step 962, loss 0.208176, acc 0.953125
2017-03-02T23:34:19.433825: step 963, loss 0.310338, acc 0.875
2017-03-02T23:34:19.539803: step 964, loss 0.280969, acc 0.875
2017-03-02T23:34:19.644417: step 965, loss 0.227722, acc 0.921875
2017-03-02T23:34:19.752813: step 966, loss 0.295114, acc 0.9375
2017-03-02T23:34:19.842182: step 967, loss 0.286694, acc 0.859375
2017-03-02T23:34:19.929456: step 968, loss 0.398547, acc 0.875
2017-03-02T23:34:20.050943: step 969, loss 0.310473, acc 0.875
2017-03-02T23:34:20.156906: step 970, loss 0.482408, acc 0.84375
2017-03-02T23:34:20.262892: step 971, loss 0.263707, acc 0.859375
2017-03-02T23:34:20.370394: step 972, loss 0.235654, acc 0.921875
2017-03-02T23:34:20.468444: step 973, loss 0.252117, acc 0.9375
2017-03-02T23:34:20.576787: step 974, loss 0.365491, acc 0.875
2017-03-02T23:34:20.671268: step 975, loss 0.24248, acc 0.90625
2017-03-02T23:34:20.771622: step 976, loss 0.28117, acc 0.890625
2017-03-02T23:34:20.872201: step 977, loss 0.151643, acc 0.96875
2017-03-02T23:34:20.975526: step 978, loss 0.249155, acc 0.890625
2017-03-02T23:34:21.082522: step 979, loss 0.210031, acc 0.90625
2017-03-02T23:34:21.188631: step 980, loss 0.313124, acc 0.875
2017-03-02T23:34:21.307737: step 981, loss 0.353501, acc 0.828125
2017-03-02T23:34:21.397111: step 982, loss 0.317429, acc 0.890625
2017-03-02T23:34:21.492059: step 983, loss 0.36693, acc 0.890625
2017-03-02T23:34:21.591399: step 984, loss 0.325149, acc 0.9
2017-03-02T23:34:21.693889: step 985, loss 0.275768, acc 0.875
2017-03-02T23:34:21.800407: step 986, loss 0.308264, acc 0.84375
2017-03-02T23:34:21.899368: step 987, loss 0.181591, acc 0.921875
2017-03-02T23:34:22.009558: step 988, loss 0.277084, acc 0.890625
2017-03-02T23:34:22.111213: step 989, loss 0.353586, acc 0.84375
2017-03-02T23:34:22.199371: step 990, loss 0.261585, acc 0.875
2017-03-02T23:34:22.305365: step 991, loss 0.377809, acc 0.859375
2017-03-02T23:34:22.413321: step 992, loss 0.271911, acc 0.875
2017-03-02T23:34:22.521440: step 993, loss 0.420514, acc 0.765625
2017-03-02T23:34:22.619843: step 994, loss 0.328173, acc 0.84375
2017-03-02T23:34:22.725509: step 995, loss 0.415063, acc 0.78125
2017-03-02T23:34:22.837877: step 996, loss 0.304255, acc 0.84375
2017-03-02T23:34:22.931763: step 997, loss 0.363549, acc 0.828125
2017-03-02T23:34:23.037849: step 998, loss 0.191128, acc 0.9375
2017-03-02T23:34:23.150813: step 999, loss 0.450753, acc 0.765625
2017-03-02T23:34:23.262223: step 1000, loss 0.28453, acc 0.875

Evaluation:
2017-03-02T23:34:23.325603: step 1000, loss 1.14799, acc 0.553633

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1000

2017-03-02T23:34:23.781454: step 1001, loss 0.168931, acc 0.9375
2017-03-02T23:34:23.910894: step 1002, loss 0.243425, acc 0.921875
2017-03-02T23:34:24.020350: step 1003, loss 0.390842, acc 0.875
2017-03-02T23:34:24.125696: step 1004, loss 0.20354, acc 0.921875
2017-03-02T23:34:24.228979: step 1005, loss 0.217607, acc 0.890625
2017-03-02T23:34:24.339129: step 1006, loss 0.443375, acc 0.78125
2017-03-02T23:34:24.430377: step 1007, loss 0.247979, acc 0.90625
2017-03-02T23:34:24.547503: step 1008, loss 0.283329, acc 0.90625
2017-03-02T23:34:24.645226: step 1009, loss 0.34865, acc 0.890625
2017-03-02T23:34:24.745942: step 1010, loss 0.331839, acc 0.859375
2017-03-02T23:34:24.850502: step 1011, loss 0.244481, acc 0.921875
2017-03-02T23:34:24.951374: step 1012, loss 0.216602, acc 0.921875
2017-03-02T23:34:25.054399: step 1013, loss 0.393703, acc 0.8125
2017-03-02T23:34:25.156670: step 1014, loss 0.458358, acc 0.78125
2017-03-02T23:34:25.259807: step 1015, loss 0.393082, acc 0.859375
2017-03-02T23:34:25.364058: step 1016, loss 0.353452, acc 0.828125
2017-03-02T23:34:25.473498: step 1017, loss 0.295561, acc 0.859375
2017-03-02T23:34:25.580671: step 1018, loss 0.258031, acc 0.875
2017-03-02T23:34:25.688641: step 1019, loss 0.24433, acc 0.890625
2017-03-02T23:34:25.790017: step 1020, loss 0.172378, acc 0.921875
2017-03-02T23:34:25.882505: step 1021, loss 0.375549, acc 0.84375
2017-03-02T23:34:25.969950: step 1022, loss 0.320625, acc 0.875
2017-03-02T23:34:26.073143: step 1023, loss 0.340045, acc 0.875
2017-03-02T23:34:26.178312: step 1024, loss 0.306846, acc 0.890625
2017-03-02T23:34:26.298982: step 1025, loss 0.443962, acc 0.78
2017-03-02T23:34:26.407251: step 1026, loss 0.229528, acc 0.953125
2017-03-02T23:34:26.513243: step 1027, loss 0.309637, acc 0.84375
2017-03-02T23:34:26.614352: step 1028, loss 0.234016, acc 0.921875
2017-03-02T23:34:26.706018: step 1029, loss 0.260629, acc 0.90625
2017-03-02T23:34:26.816892: step 1030, loss 0.330453, acc 0.859375
2017-03-02T23:34:26.926311: step 1031, loss 0.306305, acc 0.890625
2017-03-02T23:34:27.030582: step 1032, loss 0.273581, acc 0.875
2017-03-02T23:34:27.147180: step 1033, loss 0.293261, acc 0.875
2017-03-02T23:34:27.249832: step 1034, loss 0.225604, acc 0.921875
2017-03-02T23:34:27.356667: step 1035, loss 0.267597, acc 0.859375
2017-03-02T23:34:27.443336: step 1036, loss 0.273783, acc 0.875
2017-03-02T23:34:27.541803: step 1037, loss 0.271991, acc 0.875
2017-03-02T23:34:27.640427: step 1038, loss 0.166544, acc 0.9375
2017-03-02T23:34:27.741778: step 1039, loss 0.174586, acc 0.9375
2017-03-02T23:34:27.849157: step 1040, loss 0.345738, acc 0.84375
2017-03-02T23:34:27.949895: step 1041, loss 0.28233, acc 0.90625
2017-03-02T23:34:28.054046: step 1042, loss 0.294454, acc 0.828125
2017-03-02T23:34:28.140814: step 1043, loss 0.207289, acc 0.9375
2017-03-02T23:34:28.234339: step 1044, loss 0.259934, acc 0.890625
2017-03-02T23:34:28.342089: step 1045, loss 0.251474, acc 0.921875
2017-03-02T23:34:28.441595: step 1046, loss 0.42211, acc 0.875
2017-03-02T23:34:28.547092: step 1047, loss 0.226819, acc 0.890625
2017-03-02T23:34:28.651637: step 1048, loss 0.140489, acc 0.953125
2017-03-02T23:34:28.763453: step 1049, loss 0.460013, acc 0.8125
2017-03-02T23:34:28.879943: step 1050, loss 0.373879, acc 0.8125
2017-03-02T23:34:28.967538: step 1051, loss 0.303731, acc 0.921875
2017-03-02T23:34:29.071558: step 1052, loss 0.331573, acc 0.875
2017-03-02T23:34:29.184831: step 1053, loss 0.252322, acc 0.875
2017-03-02T23:34:29.290053: step 1054, loss 0.345297, acc 0.84375
2017-03-02T23:34:29.393761: step 1055, loss 0.347358, acc 0.828125
2017-03-02T23:34:29.497825: step 1056, loss 0.281376, acc 0.875
2017-03-02T23:34:29.599775: step 1057, loss 0.258705, acc 0.890625
2017-03-02T23:34:29.696040: step 1058, loss 0.393135, acc 0.828125
2017-03-02T23:34:29.796112: step 1059, loss 0.191812, acc 0.953125
2017-03-02T23:34:29.910259: step 1060, loss 0.195723, acc 0.921875
2017-03-02T23:34:30.020576: step 1061, loss 0.255075, acc 0.90625
2017-03-02T23:34:30.126162: step 1062, loss 0.237905, acc 0.9375
2017-03-02T23:34:30.224495: step 1063, loss 0.225275, acc 0.921875
2017-03-02T23:34:30.331125: step 1064, loss 0.211772, acc 0.90625
2017-03-02T23:34:30.423047: step 1065, loss 0.309138, acc 0.875
2017-03-02T23:34:30.505086: step 1066, loss 0.320616, acc 0.86
2017-03-02T23:34:30.613723: step 1067, loss 0.295307, acc 0.890625
2017-03-02T23:34:30.716735: step 1068, loss 0.232802, acc 0.9375
2017-03-02T23:34:30.817492: step 1069, loss 0.271066, acc 0.90625
2017-03-02T23:34:30.920056: step 1070, loss 0.185437, acc 0.921875
2017-03-02T23:34:31.022773: step 1071, loss 0.268962, acc 0.921875
2017-03-02T23:34:31.133215: step 1072, loss 0.338662, acc 0.875
2017-03-02T23:34:31.230466: step 1073, loss 0.259374, acc 0.90625
2017-03-02T23:34:31.329855: step 1074, loss 0.21741, acc 0.890625
2017-03-02T23:34:31.435287: step 1075, loss 0.311042, acc 0.875
2017-03-02T23:34:31.548846: step 1076, loss 0.233892, acc 0.9375
2017-03-02T23:34:31.654269: step 1077, loss 0.325421, acc 0.84375
2017-03-02T23:34:31.761857: step 1078, loss 0.169091, acc 0.953125
2017-03-02T23:34:31.867771: step 1079, loss 0.331444, acc 0.875
2017-03-02T23:34:31.957363: step 1080, loss 0.25233, acc 0.921875
2017-03-02T23:34:32.059772: step 1081, loss 0.277504, acc 0.9375
2017-03-02T23:34:32.175578: step 1082, loss 0.243648, acc 0.890625
2017-03-02T23:34:32.278282: step 1083, loss 0.299776, acc 0.84375
2017-03-02T23:34:32.388967: step 1084, loss 0.252124, acc 0.90625
2017-03-02T23:34:32.492626: step 1085, loss 0.230122, acc 0.921875
2017-03-02T23:34:32.598379: step 1086, loss 0.195837, acc 0.921875
2017-03-02T23:34:32.693536: step 1087, loss 0.153725, acc 0.984375
2017-03-02T23:34:32.782335: step 1088, loss 0.242605, acc 0.890625
2017-03-02T23:34:32.887054: step 1089, loss 0.27008, acc 0.90625
2017-03-02T23:34:32.999266: step 1090, loss 0.372049, acc 0.859375
2017-03-02T23:34:33.108536: step 1091, loss 0.213449, acc 0.90625
2017-03-02T23:34:33.221077: step 1092, loss 0.170451, acc 0.953125
2017-03-02T23:34:33.328449: step 1093, loss 0.405139, acc 0.859375
2017-03-02T23:34:33.454206: step 1094, loss 0.16944, acc 0.953125
2017-03-02T23:34:33.539788: step 1095, loss 0.212355, acc 0.921875
2017-03-02T23:34:33.643953: step 1096, loss 0.350791, acc 0.875
2017-03-02T23:34:33.766688: step 1097, loss 0.211727, acc 0.9375
2017-03-02T23:34:33.872588: step 1098, loss 0.234873, acc 0.90625
2017-03-02T23:34:33.974857: step 1099, loss 0.219811, acc 0.921875
2017-03-02T23:34:34.091396: step 1100, loss 0.317036, acc 0.875

Evaluation:
2017-03-02T23:34:34.152182: step 1100, loss 1.47839, acc 0.577855

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1100

2017-03-02T23:34:34.616692: step 1101, loss 0.39699, acc 0.859375
2017-03-02T23:34:34.724012: step 1102, loss 0.279096, acc 0.890625
2017-03-02T23:34:34.817272: step 1103, loss 0.25957, acc 0.921875
2017-03-02T23:34:34.930715: step 1104, loss 0.378739, acc 0.859375
2017-03-02T23:34:35.023714: step 1105, loss 0.258605, acc 0.875
2017-03-02T23:34:35.133113: step 1106, loss 0.267115, acc 0.875
2017-03-02T23:34:35.231024: step 1107, loss 0.304226, acc 0.9
2017-03-02T23:34:35.342439: step 1108, loss 0.404036, acc 0.828125
2017-03-02T23:34:35.445884: step 1109, loss 0.160397, acc 0.9375
2017-03-02T23:34:35.551072: step 1110, loss 0.17725, acc 0.9375
2017-03-02T23:34:35.651379: step 1111, loss 0.23155, acc 0.875
2017-03-02T23:34:35.745173: step 1112, loss 0.366507, acc 0.859375
2017-03-02T23:34:35.843337: step 1113, loss 0.180694, acc 0.9375
2017-03-02T23:34:35.948560: step 1114, loss 0.222903, acc 0.9375
2017-03-02T23:34:36.057498: step 1115, loss 0.234904, acc 0.9375
2017-03-02T23:34:36.155243: step 1116, loss 0.278249, acc 0.890625
2017-03-02T23:34:36.260036: step 1117, loss 0.174187, acc 0.953125
2017-03-02T23:34:36.367391: step 1118, loss 0.110183, acc 0.96875
2017-03-02T23:34:36.467287: step 1119, loss 0.286467, acc 0.90625
2017-03-02T23:34:36.559037: step 1120, loss 0.160656, acc 0.9375
2017-03-02T23:34:36.656124: step 1121, loss 0.228187, acc 0.90625
2017-03-02T23:34:36.763904: step 1122, loss 0.188405, acc 0.953125
2017-03-02T23:34:36.870362: step 1123, loss 0.214573, acc 0.90625
2017-03-02T23:34:36.972614: step 1124, loss 0.159258, acc 0.96875
2017-03-02T23:34:37.082917: step 1125, loss 0.128935, acc 0.9375
2017-03-02T23:34:37.201915: step 1126, loss 0.218358, acc 0.890625
2017-03-02T23:34:37.291656: step 1127, loss 0.254601, acc 0.875
2017-03-02T23:34:37.391285: step 1128, loss 0.352299, acc 0.84375
2017-03-02T23:34:37.495259: step 1129, loss 0.267269, acc 0.890625
2017-03-02T23:34:37.594711: step 1130, loss 0.156799, acc 0.9375
2017-03-02T23:34:37.705152: step 1131, loss 0.273567, acc 0.875
2017-03-02T23:34:37.810393: step 1132, loss 0.193944, acc 0.9375
2017-03-02T23:34:37.915784: step 1133, loss 0.256311, acc 0.90625
2017-03-02T23:34:38.008664: step 1134, loss 0.190802, acc 0.9375
2017-03-02T23:34:38.097578: step 1135, loss 0.14461, acc 0.96875
2017-03-02T23:34:38.203674: step 1136, loss 0.182529, acc 0.921875
2017-03-02T23:34:38.313112: step 1137, loss 0.270789, acc 0.90625
2017-03-02T23:34:38.423941: step 1138, loss 0.256609, acc 0.90625
2017-03-02T23:34:38.541412: step 1139, loss 0.251924, acc 0.890625
2017-03-02T23:34:38.646468: step 1140, loss 0.217513, acc 0.953125
2017-03-02T23:34:38.758106: step 1141, loss 0.332847, acc 0.890625
2017-03-02T23:34:38.849606: step 1142, loss 0.23443, acc 0.890625
2017-03-02T23:34:38.951823: step 1143, loss 0.204498, acc 0.90625
2017-03-02T23:34:39.059898: step 1144, loss 0.236828, acc 0.890625
2017-03-02T23:34:39.166008: step 1145, loss 0.194344, acc 0.9375
2017-03-02T23:34:39.263937: step 1146, loss 0.180147, acc 0.9375
2017-03-02T23:34:39.368151: step 1147, loss 0.226496, acc 0.890625
2017-03-02T23:34:39.464778: step 1148, loss 0.33859, acc 0.86
2017-03-02T23:34:39.554664: step 1149, loss 0.235156, acc 0.875
2017-03-02T23:34:39.655426: step 1150, loss 0.0992045, acc 0.984375
2017-03-02T23:34:39.760995: step 1151, loss 0.108976, acc 0.984375
2017-03-02T23:34:39.877413: step 1152, loss 0.234898, acc 0.9375
2017-03-02T23:34:39.982544: step 1153, loss 0.140449, acc 0.921875
2017-03-02T23:34:40.088210: step 1154, loss 0.3128, acc 0.921875
2017-03-02T23:34:40.196701: step 1155, loss 0.170688, acc 0.953125
2017-03-02T23:34:40.287589: step 1156, loss 0.26509, acc 0.875
2017-03-02T23:34:40.389365: step 1157, loss 0.278085, acc 0.875
2017-03-02T23:34:40.495081: step 1158, loss 0.176656, acc 0.921875
2017-03-02T23:34:40.598738: step 1159, loss 0.128804, acc 0.9375
2017-03-02T23:34:40.701803: step 1160, loss 0.295246, acc 0.875
2017-03-02T23:34:40.812368: step 1161, loss 0.258415, acc 0.921875
2017-03-02T23:34:40.912249: step 1162, loss 0.200575, acc 0.9375
2017-03-02T23:34:41.025932: step 1163, loss 0.132102, acc 0.96875
2017-03-02T23:34:41.114996: step 1164, loss 0.126195, acc 1
2017-03-02T23:34:41.218757: step 1165, loss 0.198185, acc 0.9375
2017-03-02T23:34:41.324246: step 1166, loss 0.215018, acc 0.90625
2017-03-02T23:34:41.430758: step 1167, loss 0.181997, acc 0.90625
2017-03-02T23:34:41.533691: step 1168, loss 0.223485, acc 0.90625
2017-03-02T23:34:41.635246: step 1169, loss 0.234418, acc 0.9375
2017-03-02T23:34:41.736238: step 1170, loss 0.191269, acc 0.921875
2017-03-02T23:34:41.829572: step 1171, loss 0.273375, acc 0.90625
2017-03-02T23:34:41.936697: step 1172, loss 0.199159, acc 0.90625
2017-03-02T23:34:42.043342: step 1173, loss 0.191881, acc 0.90625
2017-03-02T23:34:42.144578: step 1174, loss 0.295111, acc 0.890625
2017-03-02T23:34:42.259131: step 1175, loss 0.268623, acc 0.921875
2017-03-02T23:34:42.385896: step 1176, loss 0.283666, acc 0.890625
2017-03-02T23:34:42.492759: step 1177, loss 0.222539, acc 0.90625
2017-03-02T23:34:42.577889: step 1178, loss 0.215058, acc 0.875
2017-03-02T23:34:42.680718: step 1179, loss 0.260541, acc 0.890625
2017-03-02T23:34:42.804543: step 1180, loss 0.199299, acc 0.921875
2017-03-02T23:34:42.908759: step 1181, loss 0.15097, acc 0.953125
2017-03-02T23:34:43.018282: step 1182, loss 0.239815, acc 0.890625
2017-03-02T23:34:43.123239: step 1183, loss 0.12095, acc 0.96875
2017-03-02T23:34:43.223678: step 1184, loss 0.25889, acc 0.90625
2017-03-02T23:34:43.309788: step 1185, loss 0.176572, acc 0.921875
2017-03-02T23:34:43.413298: step 1186, loss 0.113551, acc 0.96875
2017-03-02T23:34:43.517999: step 1187, loss 0.329441, acc 0.84375
2017-03-02T23:34:43.619845: step 1188, loss 0.339707, acc 0.84375
2017-03-02T23:34:43.713419: step 1189, loss 0.316497, acc 0.82
2017-03-02T23:34:43.820804: step 1190, loss 0.201638, acc 0.90625
2017-03-02T23:34:43.914560: step 1191, loss 0.215872, acc 0.90625
2017-03-02T23:34:44.026673: step 1192, loss 0.243557, acc 0.90625
2017-03-02T23:34:44.118051: step 1193, loss 0.0959931, acc 0.96875
2017-03-02T23:34:44.225571: step 1194, loss 0.0748924, acc 0.984375
2017-03-02T23:34:44.333459: step 1195, loss 0.136074, acc 0.953125
2017-03-02T23:34:44.436457: step 1196, loss 0.213027, acc 0.953125
2017-03-02T23:34:44.554084: step 1197, loss 0.131974, acc 0.9375
2017-03-02T23:34:44.658645: step 1198, loss 0.181495, acc 0.953125
2017-03-02T23:34:44.764929: step 1199, loss 0.27304, acc 0.90625
2017-03-02T23:34:44.861152: step 1200, loss 0.181505, acc 0.9375

Evaluation:
2017-03-02T23:34:44.920671: step 1200, loss 1.22422, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1200

2017-03-02T23:34:45.367672: step 1201, loss 0.244873, acc 0.90625
2017-03-02T23:34:45.477670: step 1202, loss 0.166083, acc 0.921875
2017-03-02T23:34:45.566407: step 1203, loss 0.213956, acc 0.9375
2017-03-02T23:34:45.660872: step 1204, loss 0.155864, acc 0.9375
2017-03-02T23:34:45.764726: step 1205, loss 0.302731, acc 0.875
2017-03-02T23:34:45.872439: step 1206, loss 0.220284, acc 0.890625
2017-03-02T23:34:45.977631: step 1207, loss 0.300328, acc 0.875
2017-03-02T23:34:46.089737: step 1208, loss 0.167047, acc 0.921875
2017-03-02T23:34:46.187858: step 1209, loss 0.301896, acc 0.875
2017-03-02T23:34:46.281044: step 1210, loss 0.209892, acc 0.90625
2017-03-02T23:34:46.372632: step 1211, loss 0.202769, acc 0.953125
2017-03-02T23:34:46.475128: step 1212, loss 0.348243, acc 0.890625
2017-03-02T23:34:46.582277: step 1213, loss 0.204098, acc 0.921875
2017-03-02T23:34:46.691328: step 1214, loss 0.218615, acc 0.890625
2017-03-02T23:34:46.798808: step 1215, loss 0.29384, acc 0.890625
2017-03-02T23:34:46.934456: step 1216, loss 0.213423, acc 0.9375
2017-03-02T23:34:47.056237: step 1217, loss 0.322008, acc 0.84375
2017-03-02T23:34:47.149183: step 1218, loss 0.232403, acc 0.90625
2017-03-02T23:34:47.253362: step 1219, loss 0.135106, acc 0.96875
2017-03-02T23:34:47.364519: step 1220, loss 0.21037, acc 0.90625
2017-03-02T23:34:47.460010: step 1221, loss 0.367525, acc 0.859375
2017-03-02T23:34:47.560510: step 1222, loss 0.0835059, acc 1
2017-03-02T23:34:47.661468: step 1223, loss 0.261865, acc 0.859375
2017-03-02T23:34:47.773329: step 1224, loss 0.256356, acc 0.921875
2017-03-02T23:34:47.879251: step 1225, loss 0.248576, acc 0.890625
2017-03-02T23:34:47.980570: step 1226, loss 0.222672, acc 0.921875
2017-03-02T23:34:48.086200: step 1227, loss 0.163405, acc 0.9375
2017-03-02T23:34:48.193651: step 1228, loss 0.113213, acc 0.96875
2017-03-02T23:34:48.299285: step 1229, loss 0.267071, acc 0.90625
2017-03-02T23:34:48.395572: step 1230, loss 0.225553, acc 0.92
2017-03-02T23:34:48.497791: step 1231, loss 0.311624, acc 0.90625
2017-03-02T23:34:48.590633: step 1232, loss 0.188565, acc 0.921875
2017-03-02T23:34:48.681169: step 1233, loss 0.14195, acc 0.96875
2017-03-02T23:34:48.784849: step 1234, loss 0.106652, acc 0.984375
2017-03-02T23:34:48.889798: step 1235, loss 0.252888, acc 0.90625
2017-03-02T23:34:49.000885: step 1236, loss 0.160391, acc 0.953125
2017-03-02T23:34:49.110847: step 1237, loss 0.20543, acc 0.90625
2017-03-02T23:34:49.211794: step 1238, loss 0.35254, acc 0.953125
2017-03-02T23:34:49.320189: step 1239, loss 0.114932, acc 0.984375
2017-03-02T23:34:49.414339: step 1240, loss 0.246815, acc 0.921875
2017-03-02T23:34:49.535771: step 1241, loss 0.215553, acc 0.890625
2017-03-02T23:34:49.642294: step 1242, loss 0.168458, acc 0.921875
2017-03-02T23:34:49.755071: step 1243, loss 0.195789, acc 0.9375
2017-03-02T23:34:49.858287: step 1244, loss 0.168762, acc 0.9375
2017-03-02T23:34:49.970597: step 1245, loss 0.226533, acc 0.90625
2017-03-02T23:34:50.083570: step 1246, loss 0.209366, acc 0.9375
2017-03-02T23:34:50.174401: step 1247, loss 0.157864, acc 0.9375
2017-03-02T23:34:50.276557: step 1248, loss 0.283105, acc 0.90625
2017-03-02T23:34:50.382219: step 1249, loss 0.150808, acc 0.96875
2017-03-02T23:34:50.497386: step 1250, loss 0.249857, acc 0.890625
2017-03-02T23:34:50.609712: step 1251, loss 0.27724, acc 0.875
2017-03-02T23:34:50.711955: step 1252, loss 0.146872, acc 0.921875
2017-03-02T23:34:50.820000: step 1253, loss 0.289021, acc 0.90625
2017-03-02T23:34:50.911550: step 1254, loss 0.312292, acc 0.90625
2017-03-02T23:34:51.019730: step 1255, loss 0.0954803, acc 0.96875
2017-03-02T23:34:51.125484: step 1256, loss 0.174624, acc 0.9375
2017-03-02T23:34:51.242036: step 1257, loss 0.174082, acc 0.9375
2017-03-02T23:34:51.347790: step 1258, loss 0.156941, acc 0.96875
2017-03-02T23:34:51.454589: step 1259, loss 0.179858, acc 0.953125
2017-03-02T23:34:51.566093: step 1260, loss 0.150918, acc 0.921875
2017-03-02T23:34:51.657291: step 1261, loss 0.23535, acc 0.875
2017-03-02T23:34:51.757826: step 1262, loss 0.195073, acc 0.90625
2017-03-02T23:34:51.865096: step 1263, loss 0.181134, acc 0.9375
2017-03-02T23:34:51.969675: step 1264, loss 0.191815, acc 0.953125
2017-03-02T23:34:52.079181: step 1265, loss 0.277276, acc 0.921875
2017-03-02T23:34:52.186489: step 1266, loss 0.216551, acc 0.921875
2017-03-02T23:34:52.293767: step 1267, loss 0.319586, acc 0.859375
2017-03-02T23:34:52.390211: step 1268, loss 0.178701, acc 0.953125
2017-03-02T23:34:52.485214: step 1269, loss 0.23325, acc 0.90625
2017-03-02T23:34:52.589578: step 1270, loss 0.14594, acc 0.9375
2017-03-02T23:34:52.686014: step 1271, loss 0.223507, acc 0.9
2017-03-02T23:34:52.805913: step 1272, loss 0.280572, acc 0.875
2017-03-02T23:34:52.905457: step 1273, loss 0.0861367, acc 0.984375
2017-03-02T23:34:53.014076: step 1274, loss 0.235933, acc 0.90625
2017-03-02T23:34:53.114781: step 1275, loss 0.15881, acc 0.9375
2017-03-02T23:34:53.212701: step 1276, loss 0.165019, acc 0.9375
2017-03-02T23:34:53.317516: step 1277, loss 0.201063, acc 0.90625
2017-03-02T23:34:53.426846: step 1278, loss 0.186173, acc 0.9375
2017-03-02T23:34:53.532326: step 1279, loss 0.169828, acc 0.921875
2017-03-02T23:34:53.636455: step 1280, loss 0.124858, acc 0.953125
2017-03-02T23:34:53.742722: step 1281, loss 0.147302, acc 0.921875
2017-03-02T23:34:53.848832: step 1282, loss 0.166211, acc 0.921875
2017-03-02T23:34:53.929087: step 1283, loss 0.180538, acc 0.953125
2017-03-02T23:34:54.031161: step 1284, loss 0.203197, acc 0.875
2017-03-02T23:34:54.140064: step 1285, loss 0.101775, acc 0.96875
2017-03-02T23:34:54.248361: step 1286, loss 0.0925466, acc 0.96875
2017-03-02T23:34:54.349122: step 1287, loss 0.341226, acc 0.90625
2017-03-02T23:34:54.451788: step 1288, loss 0.206153, acc 0.921875
2017-03-02T23:34:54.552566: step 1289, loss 0.138449, acc 0.953125
2017-03-02T23:34:54.645193: step 1290, loss 0.187736, acc 0.96875
2017-03-02T23:34:54.744430: step 1291, loss 0.258074, acc 0.890625
2017-03-02T23:34:54.847393: step 1292, loss 0.195738, acc 0.9375
2017-03-02T23:34:54.948479: step 1293, loss 0.146726, acc 0.953125
2017-03-02T23:34:55.057673: step 1294, loss 0.19607, acc 0.890625
2017-03-02T23:34:55.156037: step 1295, loss 0.148318, acc 0.953125
2017-03-02T23:34:55.263526: step 1296, loss 0.141746, acc 0.953125
2017-03-02T23:34:55.366837: step 1297, loss 0.127925, acc 0.96875
2017-03-02T23:34:55.451346: step 1298, loss 0.302883, acc 0.875
2017-03-02T23:34:55.551048: step 1299, loss 0.169036, acc 0.90625
2017-03-02T23:34:55.654853: step 1300, loss 0.188579, acc 0.953125

Evaluation:
2017-03-02T23:34:55.715672: step 1300, loss 1.21803, acc 0.560554

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1300

2017-03-02T23:34:56.224187: step 1301, loss 0.140993, acc 0.921875
2017-03-02T23:34:56.332886: step 1302, loss 0.187472, acc 0.90625
2017-03-02T23:34:56.435399: step 1303, loss 0.19629, acc 0.9375
2017-03-02T23:34:56.554743: step 1304, loss 0.286919, acc 0.90625
2017-03-02T23:34:56.659326: step 1305, loss 0.125194, acc 0.96875
2017-03-02T23:34:56.758318: step 1306, loss 0.20867, acc 0.921875
2017-03-02T23:34:56.851498: step 1307, loss 0.295512, acc 0.875
2017-03-02T23:34:56.943496: step 1308, loss 0.147139, acc 0.9375
2017-03-02T23:34:57.051707: step 1309, loss 0.134107, acc 0.953125
2017-03-02T23:34:57.153896: step 1310, loss 0.245446, acc 0.921875
2017-03-02T23:34:57.260061: step 1311, loss 0.163057, acc 0.9375
2017-03-02T23:34:57.357148: step 1312, loss 0.0999968, acc 0.98
2017-03-02T23:34:57.463142: step 1313, loss 0.0897545, acc 0.96875
2017-03-02T23:34:57.572887: step 1314, loss 0.171066, acc 0.890625
2017-03-02T23:34:57.667988: step 1315, loss 0.192132, acc 0.921875
2017-03-02T23:34:57.764346: step 1316, loss 0.179745, acc 0.921875
2017-03-02T23:34:57.871481: step 1317, loss 0.122231, acc 0.953125
2017-03-02T23:34:57.986723: step 1318, loss 0.130306, acc 0.953125
2017-03-02T23:34:58.093742: step 1319, loss 0.0998443, acc 0.96875
2017-03-02T23:34:58.198654: step 1320, loss 0.135921, acc 0.984375
2017-03-02T23:34:58.304519: step 1321, loss 0.133502, acc 0.953125
2017-03-02T23:34:58.396458: step 1322, loss 0.228052, acc 0.9375
2017-03-02T23:34:58.509847: step 1323, loss 0.0897548, acc 0.96875
2017-03-02T23:34:58.609471: step 1324, loss 0.23377, acc 0.921875
2017-03-02T23:34:58.713302: step 1325, loss 0.204643, acc 0.90625
2017-03-02T23:34:58.817955: step 1326, loss 0.162166, acc 0.96875
2017-03-02T23:34:58.928534: step 1327, loss 0.213848, acc 0.890625
2017-03-02T23:34:59.032901: step 1328, loss 0.152425, acc 0.953125
2017-03-02T23:34:59.120051: step 1329, loss 0.156354, acc 0.953125
2017-03-02T23:34:59.210816: step 1330, loss 0.28816, acc 0.90625
2017-03-02T23:34:59.328072: step 1331, loss 0.165734, acc 0.921875
2017-03-02T23:34:59.461263: step 1332, loss 0.170193, acc 0.90625
2017-03-02T23:34:59.563924: step 1333, loss 0.162895, acc 0.921875
2017-03-02T23:34:59.691601: step 1334, loss 0.156422, acc 0.921875
2017-03-02T23:34:59.795287: step 1335, loss 0.168961, acc 0.96875
2017-03-02T23:34:59.881858: step 1336, loss 0.134372, acc 0.921875
2017-03-02T23:34:59.983803: step 1337, loss 0.108895, acc 0.96875
2017-03-02T23:35:00.090079: step 1338, loss 0.105111, acc 0.96875
2017-03-02T23:35:00.197311: step 1339, loss 0.126105, acc 0.984375
2017-03-02T23:35:00.303332: step 1340, loss 0.175622, acc 0.953125
2017-03-02T23:35:00.406744: step 1341, loss 0.182136, acc 0.921875
2017-03-02T23:35:00.518547: step 1342, loss 0.189664, acc 0.921875
2017-03-02T23:35:00.615764: step 1343, loss 0.194737, acc 0.921875
2017-03-02T23:35:00.704715: step 1344, loss 0.356668, acc 0.859375
2017-03-02T23:35:00.809376: step 1345, loss 0.22611, acc 0.9375
2017-03-02T23:35:00.921961: step 1346, loss 0.16543, acc 0.96875
2017-03-02T23:35:01.026236: step 1347, loss 0.0861557, acc 0.984375
2017-03-02T23:35:01.125507: step 1348, loss 0.238744, acc 0.9375
2017-03-02T23:35:01.223599: step 1349, loss 0.163441, acc 0.9375
2017-03-02T23:35:01.343160: step 1350, loss 0.125511, acc 0.953125
2017-03-02T23:35:01.440630: step 1351, loss 0.14629, acc 0.9375
2017-03-02T23:35:01.544899: step 1352, loss 0.144813, acc 0.9375
2017-03-02T23:35:01.652005: step 1353, loss 0.177904, acc 0.88
2017-03-02T23:35:01.767693: step 1354, loss 0.234594, acc 0.90625
2017-03-02T23:35:01.867686: step 1355, loss 0.14518, acc 0.96875
2017-03-02T23:35:01.970259: step 1356, loss 0.0954958, acc 0.96875
2017-03-02T23:35:02.074236: step 1357, loss 0.231198, acc 0.890625
2017-03-02T23:35:02.168337: step 1358, loss 0.173178, acc 0.953125
2017-03-02T23:35:02.267184: step 1359, loss 0.234282, acc 0.9375
2017-03-02T23:35:02.377899: step 1360, loss 0.226349, acc 0.9375
2017-03-02T23:35:02.486520: step 1361, loss 0.151993, acc 0.953125
2017-03-02T23:35:02.595069: step 1362, loss 0.0833593, acc 0.96875
2017-03-02T23:35:02.698047: step 1363, loss 0.184593, acc 0.921875
2017-03-02T23:35:02.802568: step 1364, loss 0.286414, acc 0.90625
2017-03-02T23:35:02.899593: step 1365, loss 0.20038, acc 0.890625
2017-03-02T23:35:02.986430: step 1366, loss 0.130892, acc 0.953125
2017-03-02T23:35:03.119633: step 1367, loss 0.188884, acc 0.921875
2017-03-02T23:35:03.227804: step 1368, loss 0.151423, acc 0.953125
2017-03-02T23:35:03.340179: step 1369, loss 0.321797, acc 0.890625
2017-03-02T23:35:03.444490: step 1370, loss 0.0943015, acc 1
2017-03-02T23:35:03.548719: step 1371, loss 0.197888, acc 0.90625
2017-03-02T23:35:03.658776: step 1372, loss 0.127843, acc 0.953125
2017-03-02T23:35:03.746060: step 1373, loss 0.129551, acc 0.96875
2017-03-02T23:35:03.850498: step 1374, loss 0.0954948, acc 0.96875
2017-03-02T23:35:03.949140: step 1375, loss 0.181673, acc 0.953125
2017-03-02T23:35:04.058215: step 1376, loss 0.264506, acc 0.90625
2017-03-02T23:35:04.163359: step 1377, loss 0.139594, acc 0.953125
2017-03-02T23:35:04.269016: step 1378, loss 0.27571, acc 0.890625
2017-03-02T23:35:04.379580: step 1379, loss 0.165034, acc 0.9375
2017-03-02T23:35:04.482217: step 1380, loss 0.0982957, acc 0.96875
2017-03-02T23:35:04.583371: step 1381, loss 0.263792, acc 0.921875
2017-03-02T23:35:04.690146: step 1382, loss 0.178476, acc 0.953125
2017-03-02T23:35:04.808427: step 1383, loss 0.120784, acc 0.96875
2017-03-02T23:35:04.906750: step 1384, loss 0.302054, acc 0.921875
2017-03-02T23:35:05.013625: step 1385, loss 0.239243, acc 0.90625
2017-03-02T23:35:05.123408: step 1386, loss 0.159582, acc 0.953125
2017-03-02T23:35:05.214877: step 1387, loss 0.238868, acc 0.9375
2017-03-02T23:35:05.326237: step 1388, loss 0.165938, acc 0.921875
2017-03-02T23:35:05.443630: step 1389, loss 0.147184, acc 0.953125
2017-03-02T23:35:05.554059: step 1390, loss 0.193996, acc 0.9375
2017-03-02T23:35:05.659612: step 1391, loss 0.250805, acc 0.921875
2017-03-02T23:35:05.767167: step 1392, loss 0.124406, acc 0.9375
2017-03-02T23:35:05.872016: step 1393, loss 0.122537, acc 0.953125
2017-03-02T23:35:05.960786: step 1394, loss 0.172292, acc 0.92
2017-03-02T23:35:06.060204: step 1395, loss 0.173229, acc 0.9375
2017-03-02T23:35:06.165470: step 1396, loss 0.165956, acc 0.9375
2017-03-02T23:35:06.265135: step 1397, loss 0.103477, acc 0.953125
2017-03-02T23:35:06.370639: step 1398, loss 0.107562, acc 0.953125
2017-03-02T23:35:06.477015: step 1399, loss 0.155211, acc 0.921875
2017-03-02T23:35:06.583939: step 1400, loss 0.105989, acc 0.9375

Evaluation:
2017-03-02T23:35:06.621647: step 1400, loss 1.4326, acc 0.560554

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1400

2017-03-02T23:35:08.019350: step 1401, loss 0.16759, acc 0.921875
2017-03-02T23:35:08.120568: step 1402, loss 0.197805, acc 0.921875
2017-03-02T23:35:08.225833: step 1403, loss 0.174613, acc 0.921875
2017-03-02T23:35:08.325048: step 1404, loss 0.121086, acc 0.96875
2017-03-02T23:35:08.426283: step 1405, loss 0.155465, acc 0.953125
2017-03-02T23:35:08.535393: step 1406, loss 0.110715, acc 0.953125
2017-03-02T23:35:08.634140: step 1407, loss 0.222475, acc 0.9375
2017-03-02T23:35:08.723576: step 1408, loss 0.270973, acc 0.859375
2017-03-02T23:35:08.827300: step 1409, loss 0.14319, acc 0.9375
2017-03-02T23:35:08.941111: step 1410, loss 0.142496, acc 0.953125
2017-03-02T23:35:09.046013: step 1411, loss 0.34946, acc 0.859375
2017-03-02T23:35:09.151578: step 1412, loss 0.241339, acc 0.9375
2017-03-02T23:35:09.259937: step 1413, loss 0.100637, acc 0.96875
2017-03-02T23:35:09.377004: step 1414, loss 0.0823804, acc 0.984375
2017-03-02T23:35:09.476536: step 1415, loss 0.198818, acc 0.921875
2017-03-02T23:35:09.565717: step 1416, loss 0.21192, acc 0.90625
2017-03-02T23:35:09.668619: step 1417, loss 0.172468, acc 0.921875
2017-03-02T23:35:09.776572: step 1418, loss 0.19401, acc 0.90625
2017-03-02T23:35:09.883859: step 1419, loss 0.259187, acc 0.90625
2017-03-02T23:35:10.001313: step 1420, loss 0.164717, acc 0.921875
2017-03-02T23:35:10.101997: step 1421, loss 0.218249, acc 0.90625
2017-03-02T23:35:10.221891: step 1422, loss 0.0576604, acc 1
2017-03-02T23:35:10.316444: step 1423, loss 0.140506, acc 0.9375
2017-03-02T23:35:10.425328: step 1424, loss 0.0674948, acc 0.984375
2017-03-02T23:35:10.541900: step 1425, loss 0.132856, acc 0.953125
2017-03-02T23:35:10.642128: step 1426, loss 0.175391, acc 0.9375
2017-03-02T23:35:10.746714: step 1427, loss 0.100864, acc 1
2017-03-02T23:35:10.846913: step 1428, loss 0.124976, acc 0.953125
2017-03-02T23:35:10.951252: step 1429, loss 0.103884, acc 0.96875
2017-03-02T23:35:11.039070: step 1430, loss 0.126237, acc 0.953125
2017-03-02T23:35:11.127551: step 1431, loss 0.17674, acc 0.90625
2017-03-02T23:35:11.228836: step 1432, loss 0.142902, acc 0.90625
2017-03-02T23:35:11.332931: step 1433, loss 0.0764176, acc 0.984375
2017-03-02T23:35:11.448651: step 1434, loss 0.120747, acc 1
2017-03-02T23:35:11.553799: step 1435, loss 0.0927512, acc 0.98
2017-03-02T23:35:11.662378: step 1436, loss 0.0893479, acc 0.953125
2017-03-02T23:35:11.769146: step 1437, loss 0.13538, acc 0.953125
2017-03-02T23:35:11.857790: step 1438, loss 0.0964676, acc 0.984375
2017-03-02T23:35:11.976206: step 1439, loss 0.162466, acc 0.921875
2017-03-02T23:35:12.083536: step 1440, loss 0.136173, acc 0.9375
2017-03-02T23:35:12.201741: step 1441, loss 0.15437, acc 0.9375
2017-03-02T23:35:12.304218: step 1442, loss 0.240242, acc 0.90625
2017-03-02T23:35:12.407100: step 1443, loss 0.225969, acc 0.90625
2017-03-02T23:35:12.511227: step 1444, loss 0.135214, acc 0.953125
2017-03-02T23:35:12.598274: step 1445, loss 0.172139, acc 0.9375
2017-03-02T23:35:12.707197: step 1446, loss 0.0998399, acc 0.984375
2017-03-02T23:35:12.816602: step 1447, loss 0.135399, acc 0.9375
2017-03-02T23:35:12.926362: step 1448, loss 0.117704, acc 0.96875
2017-03-02T23:35:13.029813: step 1449, loss 0.123707, acc 0.953125
2017-03-02T23:35:13.135723: step 1450, loss 0.0918695, acc 0.96875
2017-03-02T23:35:13.249843: step 1451, loss 0.14755, acc 0.9375
2017-03-02T23:35:13.338288: step 1452, loss 0.198105, acc 0.90625
2017-03-02T23:35:13.436486: step 1453, loss 0.213587, acc 0.890625
2017-03-02T23:35:13.542314: step 1454, loss 0.0919715, acc 0.96875
2017-03-02T23:35:13.650894: step 1455, loss 0.0978966, acc 0.96875
2017-03-02T23:35:13.753457: step 1456, loss 0.161164, acc 0.9375
2017-03-02T23:35:13.856996: step 1457, loss 0.125988, acc 0.953125
2017-03-02T23:35:13.957609: step 1458, loss 0.137236, acc 0.9375
2017-03-02T23:35:14.070405: step 1459, loss 0.261057, acc 0.90625
2017-03-02T23:35:14.175987: step 1460, loss 0.145751, acc 0.953125
2017-03-02T23:35:14.281655: step 1461, loss 0.109989, acc 0.984375
2017-03-02T23:35:14.387475: step 1462, loss 0.144365, acc 0.953125
2017-03-02T23:35:14.494976: step 1463, loss 0.134558, acc 0.953125
2017-03-02T23:35:14.602110: step 1464, loss 0.21651, acc 0.921875
2017-03-02T23:35:14.708738: step 1465, loss 0.143662, acc 0.9375
2017-03-02T23:35:14.810284: step 1466, loss 0.105152, acc 0.953125
2017-03-02T23:35:14.900622: step 1467, loss 0.186548, acc 0.953125
2017-03-02T23:35:15.012684: step 1468, loss 0.168036, acc 0.921875
2017-03-02T23:35:15.121903: step 1469, loss 0.240325, acc 0.890625
2017-03-02T23:35:15.237773: step 1470, loss 0.247927, acc 0.90625
2017-03-02T23:35:15.333768: step 1471, loss 0.0677893, acc 0.984375
2017-03-02T23:35:15.429299: step 1472, loss 0.15794, acc 0.9375
2017-03-02T23:35:15.536458: step 1473, loss 0.238516, acc 0.921875
2017-03-02T23:35:15.625281: step 1474, loss 0.0925258, acc 0.96875
2017-03-02T23:35:15.728122: step 1475, loss 0.165431, acc 0.953125
2017-03-02T23:35:15.823039: step 1476, loss 0.180711, acc 0.92
2017-03-02T23:35:15.934352: step 1477, loss 0.0661572, acc 0.984375
2017-03-02T23:35:16.041980: step 1478, loss 0.180267, acc 0.921875
2017-03-02T23:35:16.149333: step 1479, loss 0.15175, acc 0.953125
2017-03-02T23:35:16.253366: step 1480, loss 0.0678009, acc 0.96875
2017-03-02T23:35:16.343190: step 1481, loss 0.158846, acc 0.953125
2017-03-02T23:35:16.429178: step 1482, loss 0.094293, acc 0.984375
2017-03-02T23:35:16.537045: step 1483, loss 0.176432, acc 0.9375
2017-03-02T23:35:16.643855: step 1484, loss 0.130291, acc 0.96875
2017-03-02T23:35:16.753208: step 1485, loss 0.113624, acc 0.953125
2017-03-02T23:35:16.858002: step 1486, loss 0.171918, acc 0.9375
2017-03-02T23:35:16.964764: step 1487, loss 0.129193, acc 0.953125
2017-03-02T23:35:17.074708: step 1488, loss 0.145421, acc 0.953125
2017-03-02T23:35:17.161193: step 1489, loss 0.115528, acc 0.96875
2017-03-02T23:35:17.262955: step 1490, loss 0.162255, acc 0.921875
2017-03-02T23:35:17.364930: step 1491, loss 0.202755, acc 0.921875
2017-03-02T23:35:17.472244: step 1492, loss 0.0592872, acc 1
2017-03-02T23:35:17.578540: step 1493, loss 0.188464, acc 0.9375
2017-03-02T23:35:17.686000: step 1494, loss 0.196166, acc 0.921875
2017-03-02T23:35:17.792554: step 1495, loss 0.155361, acc 0.90625
2017-03-02T23:35:17.882617: step 1496, loss 0.0995921, acc 0.96875
2017-03-02T23:35:17.976100: step 1497, loss 0.212386, acc 0.953125
2017-03-02T23:35:18.079472: step 1498, loss 0.193608, acc 0.921875
2017-03-02T23:35:18.187143: step 1499, loss 0.219973, acc 0.921875
2017-03-02T23:35:18.297419: step 1500, loss 0.0999141, acc 0.953125

Evaluation:
2017-03-02T23:35:18.357606: step 1500, loss 1.60955, acc 0.574394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1500

2017-03-02T23:35:18.814056: step 1501, loss 0.159347, acc 0.921875
2017-03-02T23:35:18.922610: step 1502, loss 0.0857205, acc 0.953125
2017-03-02T23:35:19.045426: step 1503, loss 0.228056, acc 0.921875
2017-03-02T23:35:19.151200: step 1504, loss 0.144738, acc 0.953125
2017-03-02T23:35:19.258155: step 1505, loss 0.211952, acc 0.90625
2017-03-02T23:35:19.351761: step 1506, loss 0.1807, acc 0.921875
2017-03-02T23:35:19.456210: step 1507, loss 0.235145, acc 0.90625
2017-03-02T23:35:19.556838: step 1508, loss 0.129583, acc 0.96875
2017-03-02T23:35:19.662283: step 1509, loss 0.0851712, acc 0.984375
2017-03-02T23:35:19.783483: step 1510, loss 0.139259, acc 0.9375
2017-03-02T23:35:19.879826: step 1511, loss 0.102195, acc 0.96875
2017-03-02T23:35:19.986755: step 1512, loss 0.163772, acc 0.90625
2017-03-02T23:35:20.080325: step 1513, loss 0.149392, acc 0.953125
2017-03-02T23:35:20.207578: step 1514, loss 0.26635, acc 0.90625
2017-03-02T23:35:20.315247: step 1515, loss 0.135112, acc 0.953125
2017-03-02T23:35:20.421046: step 1516, loss 0.170843, acc 0.953125
2017-03-02T23:35:20.515842: step 1517, loss 0.172241, acc 0.94
2017-03-02T23:35:20.622088: step 1518, loss 0.207932, acc 0.90625
2017-03-02T23:35:20.728670: step 1519, loss 0.0880836, acc 0.96875
2017-03-02T23:35:20.820579: step 1520, loss 0.135001, acc 0.9375
2017-03-02T23:35:20.914899: step 1521, loss 0.0579719, acc 0.984375
2017-03-02T23:35:21.003694: step 1522, loss 0.121137, acc 0.96875
2017-03-02T23:35:21.107438: step 1523, loss 0.108784, acc 0.96875
2017-03-02T23:35:21.213357: step 1524, loss 0.107929, acc 0.953125
2017-03-02T23:35:21.310593: step 1525, loss 0.215046, acc 0.9375
2017-03-02T23:35:21.420046: step 1526, loss 0.0882456, acc 0.96875
2017-03-02T23:35:21.528980: step 1527, loss 0.161838, acc 0.890625
2017-03-02T23:35:21.621394: step 1528, loss 0.146771, acc 0.921875
2017-03-02T23:35:21.721501: step 1529, loss 0.154498, acc 0.921875
2017-03-02T23:35:21.827776: step 1530, loss 0.0869131, acc 0.96875
2017-03-02T23:35:21.932804: step 1531, loss 0.0844484, acc 0.953125
2017-03-02T23:35:22.039444: step 1532, loss 0.326944, acc 0.890625
2017-03-02T23:35:22.140929: step 1533, loss 0.0865451, acc 0.96875
2017-03-02T23:35:22.240444: step 1534, loss 0.111336, acc 0.953125
2017-03-02T23:35:22.334447: step 1535, loss 0.142337, acc 0.953125
2017-03-02T23:35:22.432981: step 1536, loss 0.220521, acc 0.9375
2017-03-02T23:35:22.537841: step 1537, loss 0.114163, acc 0.96875
2017-03-02T23:35:22.644025: step 1538, loss 0.138404, acc 0.96875
2017-03-02T23:35:22.747390: step 1539, loss 0.103917, acc 1
2017-03-02T23:35:22.865640: step 1540, loss 0.126057, acc 0.984375
2017-03-02T23:35:22.970048: step 1541, loss 0.0722604, acc 0.984375
2017-03-02T23:35:23.064769: step 1542, loss 0.180174, acc 0.921875
2017-03-02T23:35:23.156943: step 1543, loss 0.133715, acc 0.953125
2017-03-02T23:35:23.257397: step 1544, loss 0.140926, acc 0.953125
2017-03-02T23:35:23.362459: step 1545, loss 0.115943, acc 0.96875
2017-03-02T23:35:23.484729: step 1546, loss 0.113118, acc 0.953125
2017-03-02T23:35:23.602697: step 1547, loss 0.196869, acc 0.96875
2017-03-02T23:35:23.708169: step 1548, loss 0.125041, acc 0.9375
2017-03-02T23:35:23.815537: step 1549, loss 0.125526, acc 0.9375
2017-03-02T23:35:23.905494: step 1550, loss 0.116673, acc 0.953125
2017-03-02T23:35:24.010867: step 1551, loss 0.0839237, acc 0.984375
2017-03-02T23:35:24.120176: step 1552, loss 0.351555, acc 0.859375
2017-03-02T23:35:24.223781: step 1553, loss 0.100932, acc 0.953125
2017-03-02T23:35:24.329098: step 1554, loss 0.0731747, acc 0.984375
2017-03-02T23:35:24.435571: step 1555, loss 0.0922603, acc 0.96875
2017-03-02T23:35:24.541199: step 1556, loss 0.120912, acc 0.984375
2017-03-02T23:35:24.645782: step 1557, loss 0.0886551, acc 0.96875
2017-03-02T23:35:24.739009: step 1558, loss 0.0868426, acc 0.96
2017-03-02T23:35:24.841012: step 1559, loss 0.0733059, acc 0.984375
2017-03-02T23:35:24.944111: step 1560, loss 0.140856, acc 0.9375
2017-03-02T23:35:25.058609: step 1561, loss 0.13899, acc 0.9375
2017-03-02T23:35:25.157601: step 1562, loss 0.107259, acc 0.9375
2017-03-02T23:35:25.259228: step 1563, loss 0.0909692, acc 0.96875
2017-03-02T23:35:25.355024: step 1564, loss 0.146173, acc 0.953125
2017-03-02T23:35:25.449588: step 1565, loss 0.0435028, acc 0.984375
2017-03-02T23:35:25.558700: step 1566, loss 0.225603, acc 0.9375
2017-03-02T23:35:25.670682: step 1567, loss 0.240276, acc 0.921875
2017-03-02T23:35:25.774671: step 1568, loss 0.122931, acc 0.96875
2017-03-02T23:35:25.887880: step 1569, loss 0.132158, acc 0.953125
2017-03-02T23:35:25.998507: step 1570, loss 0.120612, acc 0.96875
2017-03-02T23:35:26.094671: step 1571, loss 0.18191, acc 0.9375
2017-03-02T23:35:26.185712: step 1572, loss 0.232682, acc 0.9375
2017-03-02T23:35:26.289685: step 1573, loss 0.106491, acc 0.96875
2017-03-02T23:35:26.394004: step 1574, loss 0.206844, acc 0.90625
2017-03-02T23:35:26.496614: step 1575, loss 0.0634084, acc 0.984375
2017-03-02T23:35:26.603072: step 1576, loss 0.0790448, acc 0.96875
2017-03-02T23:35:26.709010: step 1577, loss 0.128231, acc 0.953125
2017-03-02T23:35:26.818627: step 1578, loss 0.0940991, acc 0.96875
2017-03-02T23:35:26.918990: step 1579, loss 0.0809031, acc 0.96875
2017-03-02T23:35:27.022344: step 1580, loss 0.102519, acc 0.96875
2017-03-02T23:35:27.143479: step 1581, loss 0.143675, acc 0.953125
2017-03-02T23:35:27.246583: step 1582, loss 0.174142, acc 0.9375
2017-03-02T23:35:27.352689: step 1583, loss 0.181415, acc 0.9375
2017-03-02T23:35:27.453672: step 1584, loss 0.151028, acc 0.9375
2017-03-02T23:35:27.563368: step 1585, loss 0.136176, acc 0.953125
2017-03-02T23:35:27.655428: step 1586, loss 0.124844, acc 0.96875
2017-03-02T23:35:27.759207: step 1587, loss 0.149671, acc 0.953125
2017-03-02T23:35:27.864762: step 1588, loss 0.15624, acc 0.921875
2017-03-02T23:35:27.967587: step 1589, loss 0.0749941, acc 0.96875
2017-03-02T23:35:28.070066: step 1590, loss 0.134447, acc 0.9375
2017-03-02T23:35:28.176099: step 1591, loss 0.107348, acc 0.9375
2017-03-02T23:35:28.277739: step 1592, loss 0.0962408, acc 0.953125
2017-03-02T23:35:28.374557: step 1593, loss 0.213165, acc 0.90625
2017-03-02T23:35:28.463827: step 1594, loss 0.201661, acc 0.921875
2017-03-02T23:35:28.560033: step 1595, loss 0.172697, acc 0.921875
2017-03-02T23:35:28.663529: step 1596, loss 0.110923, acc 0.953125
2017-03-02T23:35:28.771426: step 1597, loss 0.0949241, acc 0.96875
2017-03-02T23:35:28.873138: step 1598, loss 0.230082, acc 0.90625
2017-03-02T23:35:28.955898: step 1599, loss 0.153497, acc 0.92
2017-03-02T23:35:29.061879: step 1600, loss 0.131441, acc 0.953125

Evaluation:
2017-03-02T23:35:29.111498: step 1600, loss 1.39219, acc 0.560554

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1600

2017-03-02T23:35:29.580710: step 1601, loss 0.0489128, acc 1
2017-03-02T23:35:29.687077: step 1602, loss 0.112531, acc 0.9375
2017-03-02T23:35:29.798842: step 1603, loss 0.179536, acc 0.96875
2017-03-02T23:35:29.887624: step 1604, loss 0.115841, acc 0.953125
2017-03-02T23:35:29.985508: step 1605, loss 0.104047, acc 0.96875
2017-03-02T23:35:30.092156: step 1606, loss 0.0955524, acc 0.96875
2017-03-02T23:35:30.199344: step 1607, loss 0.298974, acc 0.859375
2017-03-02T23:35:30.302071: step 1608, loss 0.123205, acc 0.9375
2017-03-02T23:35:30.405720: step 1609, loss 0.10966, acc 0.953125
2017-03-02T23:35:30.511739: step 1610, loss 0.114131, acc 0.953125
2017-03-02T23:35:30.619385: step 1611, loss 0.216829, acc 0.921875
2017-03-02T23:35:30.711788: step 1612, loss 0.147137, acc 0.9375
2017-03-02T23:35:30.814075: step 1613, loss 0.0531527, acc 0.984375
2017-03-02T23:35:30.920201: step 1614, loss 0.178038, acc 0.96875
2017-03-02T23:35:31.031678: step 1615, loss 0.095842, acc 0.96875
2017-03-02T23:35:31.137004: step 1616, loss 0.12094, acc 0.9375
2017-03-02T23:35:31.236789: step 1617, loss 0.19189, acc 0.890625
2017-03-02T23:35:31.340321: step 1618, loss 0.137125, acc 0.9375
2017-03-02T23:35:31.433939: step 1619, loss 0.12942, acc 0.9375
2017-03-02T23:35:31.540852: step 1620, loss 0.165465, acc 0.9375
2017-03-02T23:35:31.651237: step 1621, loss 0.117336, acc 0.96875
2017-03-02T23:35:31.759217: step 1622, loss 0.182765, acc 0.9375
2017-03-02T23:35:31.865563: step 1623, loss 0.176659, acc 0.9375
2017-03-02T23:35:31.967248: step 1624, loss 0.144639, acc 0.921875
2017-03-02T23:35:32.074717: step 1625, loss 0.0532689, acc 0.984375
2017-03-02T23:35:32.180467: step 1626, loss 0.0685605, acc 0.984375
2017-03-02T23:35:32.309038: step 1627, loss 0.133248, acc 0.9375
2017-03-02T23:35:32.424768: step 1628, loss 0.237081, acc 0.90625
2017-03-02T23:35:32.529885: step 1629, loss 0.133592, acc 0.984375
2017-03-02T23:35:32.634412: step 1630, loss 0.31335, acc 0.921875
2017-03-02T23:35:32.738901: step 1631, loss 0.0787404, acc 1
2017-03-02T23:35:32.846422: step 1632, loss 0.0880619, acc 0.984375
2017-03-02T23:35:32.937462: step 1633, loss 0.0547281, acc 1
2017-03-02T23:35:33.045520: step 1634, loss 0.0894222, acc 0.96875
2017-03-02T23:35:33.146446: step 1635, loss 0.150654, acc 0.96875
2017-03-02T23:35:33.249580: step 1636, loss 0.0864352, acc 0.96875
2017-03-02T23:35:33.362984: step 1637, loss 0.0805234, acc 0.96875
2017-03-02T23:35:33.472959: step 1638, loss 0.278195, acc 0.90625
2017-03-02T23:35:33.581222: step 1639, loss 0.075349, acc 0.984375
2017-03-02T23:35:33.669017: step 1640, loss 0.187123, acc 0.94
2017-03-02T23:35:33.771430: step 1641, loss 0.100874, acc 0.984375
2017-03-02T23:35:33.876777: step 1642, loss 0.054665, acc 0.984375
2017-03-02T23:35:33.989493: step 1643, loss 0.0551225, acc 1
2017-03-02T23:35:34.103615: step 1644, loss 0.0715671, acc 0.96875
2017-03-02T23:35:34.204749: step 1645, loss 0.12253, acc 0.96875
2017-03-02T23:35:34.311358: step 1646, loss 0.0983985, acc 0.953125
2017-03-02T23:35:34.402898: step 1647, loss 0.172075, acc 0.921875
2017-03-02T23:35:34.507430: step 1648, loss 0.044057, acc 1
2017-03-02T23:35:34.617727: step 1649, loss 0.0916613, acc 0.96875
2017-03-02T23:35:34.722514: step 1650, loss 0.12412, acc 0.953125
2017-03-02T23:35:34.825877: step 1651, loss 0.114342, acc 0.96875
2017-03-02T23:35:34.932468: step 1652, loss 0.121965, acc 0.96875
2017-03-02T23:35:35.032002: step 1653, loss 0.131489, acc 0.953125
2017-03-02T23:35:35.125464: step 1654, loss 0.163581, acc 0.9375
2017-03-02T23:35:35.217771: step 1655, loss 0.111374, acc 0.96875
2017-03-02T23:35:35.318506: step 1656, loss 0.0832523, acc 1
2017-03-02T23:35:35.424170: step 1657, loss 0.121384, acc 0.96875
2017-03-02T23:35:35.530426: step 1658, loss 0.11533, acc 0.9375
2017-03-02T23:35:35.648946: step 1659, loss 0.0773393, acc 0.984375
2017-03-02T23:35:35.753094: step 1660, loss 0.174733, acc 0.921875
2017-03-02T23:35:35.870574: step 1661, loss 0.128666, acc 0.9375
2017-03-02T23:35:35.966008: step 1662, loss 0.0831898, acc 0.953125
2017-03-02T23:35:36.073218: step 1663, loss 0.121936, acc 0.953125
2017-03-02T23:35:36.184045: step 1664, loss 0.124718, acc 0.9375
2017-03-02T23:35:36.291774: step 1665, loss 0.0996292, acc 0.984375
2017-03-02T23:35:36.403125: step 1666, loss 0.19424, acc 0.90625
2017-03-02T23:35:36.515636: step 1667, loss 0.0942861, acc 0.96875
2017-03-02T23:35:36.621543: step 1668, loss 0.137916, acc 0.984375
2017-03-02T23:35:36.720801: step 1669, loss 0.0745391, acc 0.953125
2017-03-02T23:35:36.831842: step 1670, loss 0.144576, acc 0.921875
2017-03-02T23:35:36.946602: step 1671, loss 0.120098, acc 0.9375
2017-03-02T23:35:37.047518: step 1672, loss 0.085686, acc 0.984375
2017-03-02T23:35:37.157575: step 1673, loss 0.0881903, acc 0.96875
2017-03-02T23:35:37.253320: step 1674, loss 0.153569, acc 0.953125
2017-03-02T23:35:37.359098: step 1675, loss 0.123733, acc 0.953125
2017-03-02T23:35:37.446075: step 1676, loss 0.125008, acc 0.9375
2017-03-02T23:35:37.547800: step 1677, loss 0.120832, acc 0.953125
2017-03-02T23:35:37.652452: step 1678, loss 0.191828, acc 0.875
2017-03-02T23:35:37.759816: step 1679, loss 0.0711387, acc 0.984375
2017-03-02T23:35:37.878978: step 1680, loss 0.105643, acc 0.953125
2017-03-02T23:35:37.979359: step 1681, loss 0.135789, acc 0.94
2017-03-02T23:35:38.083696: step 1682, loss 0.237941, acc 0.921875
2017-03-02T23:35:38.173396: step 1683, loss 0.0865379, acc 0.953125
2017-03-02T23:35:38.281019: step 1684, loss 0.0775207, acc 0.96875
2017-03-02T23:35:38.383824: step 1685, loss 0.134537, acc 0.9375
2017-03-02T23:35:38.497090: step 1686, loss 0.0688413, acc 0.984375
2017-03-02T23:35:38.606112: step 1687, loss 0.12999, acc 0.9375
2017-03-02T23:35:38.699149: step 1688, loss 0.0470824, acc 0.984375
2017-03-02T23:35:38.808832: step 1689, loss 0.0522069, acc 0.984375
2017-03-02T23:35:38.913153: step 1690, loss 0.0972491, acc 0.953125
2017-03-02T23:35:39.002632: step 1691, loss 0.115931, acc 0.9375
2017-03-02T23:35:39.105051: step 1692, loss 0.116603, acc 0.984375
2017-03-02T23:35:39.212690: step 1693, loss 0.102959, acc 0.9375
2017-03-02T23:35:39.320008: step 1694, loss 0.0876498, acc 0.96875
2017-03-02T23:35:39.429625: step 1695, loss 0.0925466, acc 0.96875
2017-03-02T23:35:39.537950: step 1696, loss 0.153641, acc 0.9375
2017-03-02T23:35:39.643519: step 1697, loss 0.0876572, acc 0.96875
2017-03-02T23:35:39.736439: step 1698, loss 0.117817, acc 0.9375
2017-03-02T23:35:39.831585: step 1699, loss 0.0848971, acc 0.96875
2017-03-02T23:35:39.922524: step 1700, loss 0.107521, acc 0.96875

Evaluation:
2017-03-02T23:35:39.975055: step 1700, loss 1.43382, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1700

2017-03-02T23:35:40.424977: step 1701, loss 0.0836055, acc 0.984375
2017-03-02T23:35:40.517710: step 1702, loss 0.116833, acc 0.96875
2017-03-02T23:35:40.624675: step 1703, loss 0.0628005, acc 0.984375
2017-03-02T23:35:40.726693: step 1704, loss 0.0885918, acc 0.96875
2017-03-02T23:35:40.829781: step 1705, loss 0.133677, acc 0.953125
2017-03-02T23:35:40.931228: step 1706, loss 0.130857, acc 0.984375
2017-03-02T23:35:41.038600: step 1707, loss 0.110937, acc 0.9375
2017-03-02T23:35:41.141503: step 1708, loss 0.090619, acc 0.96875
2017-03-02T23:35:41.235020: step 1709, loss 0.0866385, acc 0.96875
2017-03-02T23:35:41.339927: step 1710, loss 0.21305, acc 0.9375
2017-03-02T23:35:41.443552: step 1711, loss 0.177675, acc 0.9375
2017-03-02T23:35:41.551889: step 1712, loss 0.104717, acc 0.96875
2017-03-02T23:35:41.652866: step 1713, loss 0.208121, acc 0.921875
2017-03-02T23:35:41.756903: step 1714, loss 0.132439, acc 0.9375
2017-03-02T23:35:41.862131: step 1715, loss 0.151794, acc 0.9375
2017-03-02T23:35:41.958578: step 1716, loss 0.120103, acc 0.984375
2017-03-02T23:35:42.058589: step 1717, loss 0.0804252, acc 0.96875
2017-03-02T23:35:42.163385: step 1718, loss 0.141873, acc 0.953125
2017-03-02T23:35:42.259502: step 1719, loss 0.18219, acc 0.921875
2017-03-02T23:35:42.369474: step 1720, loss 0.120097, acc 0.953125
2017-03-02T23:35:42.473949: step 1721, loss 0.0815656, acc 0.953125
2017-03-02T23:35:42.571157: step 1722, loss 0.0475084, acc 0.98
2017-03-02T23:35:42.686394: step 1723, loss 0.126017, acc 0.96875
2017-03-02T23:35:42.800554: step 1724, loss 0.125203, acc 0.96875
2017-03-02T23:35:42.899687: step 1725, loss 0.171595, acc 0.953125
2017-03-02T23:35:43.006419: step 1726, loss 0.114057, acc 0.953125
2017-03-02T23:35:43.113790: step 1727, loss 0.0598786, acc 0.984375
2017-03-02T23:35:43.223773: step 1728, loss 0.106165, acc 0.96875
2017-03-02T23:35:43.329560: step 1729, loss 0.152183, acc 0.921875
2017-03-02T23:35:43.434058: step 1730, loss 0.0583267, acc 0.984375
2017-03-02T23:35:43.522850: step 1731, loss 0.0757653, acc 0.96875
2017-03-02T23:35:43.626247: step 1732, loss 0.0537409, acc 0.984375
2017-03-02T23:35:43.721409: step 1733, loss 0.0280744, acc 1
2017-03-02T23:35:43.819826: step 1734, loss 0.0570651, acc 0.984375
2017-03-02T23:35:43.919433: step 1735, loss 0.0675851, acc 0.984375
2017-03-02T23:35:44.025217: step 1736, loss 0.106421, acc 0.96875
2017-03-02T23:35:44.139479: step 1737, loss 0.119425, acc 0.953125
2017-03-02T23:35:44.224505: step 1738, loss 0.107001, acc 0.984375
2017-03-02T23:35:44.315096: step 1739, loss 0.101439, acc 0.953125
2017-03-02T23:35:44.424962: step 1740, loss 0.100276, acc 0.96875
2017-03-02T23:35:44.531844: step 1741, loss 0.1511, acc 0.953125
2017-03-02T23:35:44.637957: step 1742, loss 0.0624444, acc 0.984375
2017-03-02T23:35:44.743304: step 1743, loss 0.0857424, acc 0.984375
2017-03-02T23:35:44.846464: step 1744, loss 0.154336, acc 0.921875
2017-03-02T23:35:44.954980: step 1745, loss 0.136505, acc 0.90625
2017-03-02T23:35:45.046690: step 1746, loss 0.0558498, acc 0.984375
2017-03-02T23:35:45.145724: step 1747, loss 0.0520177, acc 1
2017-03-02T23:35:45.250981: step 1748, loss 0.139651, acc 0.953125
2017-03-02T23:35:45.356461: step 1749, loss 0.145858, acc 0.953125
2017-03-02T23:35:45.447968: step 1750, loss 0.0691189, acc 0.96875
2017-03-02T23:35:45.550838: step 1751, loss 0.0703664, acc 0.96875
2017-03-02T23:35:45.655892: step 1752, loss 0.141714, acc 0.953125
2017-03-02T23:35:45.744888: step 1753, loss 0.0947282, acc 0.9375
2017-03-02T23:35:45.836901: step 1754, loss 0.118967, acc 0.953125
2017-03-02T23:35:45.945653: step 1755, loss 0.146271, acc 0.9375
2017-03-02T23:35:46.047953: step 1756, loss 0.0737056, acc 0.984375
2017-03-02T23:35:46.153420: step 1757, loss 0.0591327, acc 0.984375
2017-03-02T23:35:46.262924: step 1758, loss 0.187607, acc 0.921875
2017-03-02T23:35:46.367805: step 1759, loss 0.103639, acc 0.96875
2017-03-02T23:35:46.477924: step 1760, loss 0.0698821, acc 0.96875
2017-03-02T23:35:46.571389: step 1761, loss 0.162951, acc 0.9375
2017-03-02T23:35:46.674604: step 1762, loss 0.0493286, acc 1
2017-03-02T23:35:46.770237: step 1763, loss 0.0455025, acc 1
2017-03-02T23:35:46.872619: step 1764, loss 0.200229, acc 0.9375
2017-03-02T23:35:46.977871: step 1765, loss 0.0622803, acc 1
2017-03-02T23:35:47.075397: step 1766, loss 0.110911, acc 0.953125
2017-03-02T23:35:47.182436: step 1767, loss 0.173885, acc 0.953125
2017-03-02T23:35:47.269346: step 1768, loss 0.0797067, acc 0.96875
2017-03-02T23:35:47.360876: step 1769, loss 0.110824, acc 0.9375
2017-03-02T23:35:47.461095: step 1770, loss 0.101712, acc 0.96875
2017-03-02T23:35:47.564541: step 1771, loss 0.148565, acc 0.953125
2017-03-02T23:35:47.670764: step 1772, loss 0.059433, acc 0.984375
2017-03-02T23:35:47.779018: step 1773, loss 0.0545638, acc 1
2017-03-02T23:35:47.878949: step 1774, loss 0.139765, acc 0.9375
2017-03-02T23:35:47.983493: step 1775, loss 0.109303, acc 0.96875
2017-03-02T23:35:48.072114: step 1776, loss 0.0736505, acc 0.96875
2017-03-02T23:35:48.191345: step 1777, loss 0.0321005, acc 1
2017-03-02T23:35:48.296701: step 1778, loss 0.0504277, acc 0.984375
2017-03-02T23:35:48.413216: step 1779, loss 0.0815146, acc 0.96875
2017-03-02T23:35:48.515769: step 1780, loss 0.0563159, acc 0.96875
2017-03-02T23:35:48.621193: step 1781, loss 0.0625741, acc 0.984375
2017-03-02T23:35:48.732799: step 1782, loss 0.0628277, acc 0.984375
2017-03-02T23:35:48.830141: step 1783, loss 0.0871311, acc 0.96875
2017-03-02T23:35:48.923371: step 1784, loss 0.0785005, acc 0.984375
2017-03-02T23:35:49.030702: step 1785, loss 0.0283003, acc 1
2017-03-02T23:35:49.139596: step 1786, loss 0.0952346, acc 0.953125
2017-03-02T23:35:49.236435: step 1787, loss 0.0509086, acc 0.96875
2017-03-02T23:35:49.340439: step 1788, loss 0.140656, acc 0.9375
2017-03-02T23:35:49.451382: step 1789, loss 0.087691, acc 0.984375
2017-03-02T23:35:49.551546: step 1790, loss 0.0842116, acc 0.984375
2017-03-02T23:35:49.642122: step 1791, loss 0.0880336, acc 0.96875
2017-03-02T23:35:49.747210: step 1792, loss 0.0875925, acc 0.953125
2017-03-02T23:35:49.857469: step 1793, loss 0.101512, acc 0.96875
2017-03-02T23:35:49.968016: step 1794, loss 0.0866678, acc 0.96875
2017-03-02T23:35:50.077405: step 1795, loss 0.0673046, acc 1
2017-03-02T23:35:50.180054: step 1796, loss 0.142257, acc 0.9375
2017-03-02T23:35:50.285675: step 1797, loss 0.119331, acc 0.96875
2017-03-02T23:35:50.381474: step 1798, loss 0.106841, acc 0.96875
2017-03-02T23:35:50.489469: step 1799, loss 0.0511247, acc 0.984375
2017-03-02T23:35:50.598734: step 1800, loss 0.149121, acc 0.953125

Evaluation:
2017-03-02T23:35:50.654462: step 1800, loss 1.47686, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1800

2017-03-02T23:35:51.093310: step 1801, loss 0.122523, acc 0.96875
2017-03-02T23:35:51.192834: step 1802, loss 0.0769054, acc 0.96875
2017-03-02T23:35:51.302167: step 1803, loss 0.0650896, acc 0.96875
2017-03-02T23:35:51.395054: step 1804, loss 0.0622237, acc 1
2017-03-02T23:35:51.525701: step 1805, loss 0.0933895, acc 0.96875
2017-03-02T23:35:51.630985: step 1806, loss 0.112294, acc 0.953125
2017-03-02T23:35:51.742769: step 1807, loss 0.0661506, acc 0.984375
2017-03-02T23:35:51.834166: step 1808, loss 0.0362823, acc 1
2017-03-02T23:35:51.947904: step 1809, loss 0.161084, acc 0.921875
2017-03-02T23:35:52.062048: step 1810, loss 0.168876, acc 0.953125
2017-03-02T23:35:52.171782: step 1811, loss 0.133168, acc 0.9375
2017-03-02T23:35:52.273124: step 1812, loss 0.0807377, acc 0.9375
2017-03-02T23:35:52.370490: step 1813, loss 0.0552272, acc 0.984375
2017-03-02T23:35:52.485951: step 1814, loss 0.108516, acc 0.984375
2017-03-02T23:35:52.580382: step 1815, loss 0.0675314, acc 0.96875
2017-03-02T23:35:52.686407: step 1816, loss 0.132784, acc 0.953125
2017-03-02T23:35:52.790787: step 1817, loss 0.124187, acc 0.953125
2017-03-02T23:35:52.909543: step 1818, loss 0.0490778, acc 1
2017-03-02T23:35:53.015661: step 1819, loss 0.0696877, acc 0.96875
2017-03-02T23:35:53.121624: step 1820, loss 0.049111, acc 1
2017-03-02T23:35:53.229548: step 1821, loss 0.16326, acc 0.9375
2017-03-02T23:35:53.319456: step 1822, loss 0.0701221, acc 0.96875
2017-03-02T23:35:53.413024: step 1823, loss 0.0724259, acc 0.96875
2017-03-02T23:35:53.535380: step 1824, loss 0.0824132, acc 0.984375
2017-03-02T23:35:53.636341: step 1825, loss 0.0382377, acc 1
2017-03-02T23:35:53.743787: step 1826, loss 0.203963, acc 0.9375
2017-03-02T23:35:53.845513: step 1827, loss 0.0622519, acc 0.984375
2017-03-02T23:35:53.949829: step 1828, loss 0.0431685, acc 0.984375
2017-03-02T23:35:54.046948: step 1829, loss 0.064897, acc 1
2017-03-02T23:35:54.133988: step 1830, loss 0.134858, acc 0.9375
2017-03-02T23:35:54.238479: step 1831, loss 0.0735187, acc 0.96875
2017-03-02T23:35:54.338649: step 1832, loss 0.0646866, acc 0.984375
2017-03-02T23:35:54.441047: step 1833, loss 0.0971691, acc 0.96875
2017-03-02T23:35:54.540727: step 1834, loss 0.06034, acc 0.96875
2017-03-02T23:35:54.634729: step 1835, loss 0.0739259, acc 0.96875
2017-03-02T23:35:54.745106: step 1836, loss 0.106391, acc 0.953125
2017-03-02T23:35:54.849304: step 1837, loss 0.0357207, acc 1
2017-03-02T23:35:54.962019: step 1838, loss 0.0483848, acc 1
2017-03-02T23:35:55.071824: step 1839, loss 0.0475346, acc 0.984375
2017-03-02T23:35:55.169108: step 1840, loss 0.0783822, acc 0.96875
2017-03-02T23:35:55.268653: step 1841, loss 0.121696, acc 0.921875
2017-03-02T23:35:55.379920: step 1842, loss 0.140828, acc 0.953125
2017-03-02T23:35:55.484121: step 1843, loss 0.12878, acc 0.9375
2017-03-02T23:35:55.578537: step 1844, loss 0.188512, acc 0.90625
2017-03-02T23:35:55.656079: step 1845, loss 0.0422423, acc 1
2017-03-02T23:35:55.764218: step 1846, loss 0.0770763, acc 0.96875
2017-03-02T23:35:55.876062: step 1847, loss 0.123434, acc 0.96875
2017-03-02T23:35:55.981917: step 1848, loss 0.0816031, acc 0.953125
2017-03-02T23:35:56.088121: step 1849, loss 0.0803149, acc 0.96875
2017-03-02T23:35:56.199311: step 1850, loss 0.160133, acc 0.9375
2017-03-02T23:35:56.302536: step 1851, loss 0.101272, acc 0.953125
2017-03-02T23:35:56.397027: step 1852, loss 0.118381, acc 0.953125
2017-03-02T23:35:56.503458: step 1853, loss 0.0886862, acc 0.984375
2017-03-02T23:35:56.605434: step 1854, loss 0.0363989, acc 1
2017-03-02T23:35:56.719225: step 1855, loss 0.0895609, acc 0.96875
2017-03-02T23:35:56.834409: step 1856, loss 0.134915, acc 0.96875
2017-03-02T23:35:56.936364: step 1857, loss 0.0641366, acc 0.96875
2017-03-02T23:35:57.046560: step 1858, loss 0.0454476, acc 1
2017-03-02T23:35:57.136715: step 1859, loss 0.0714966, acc 0.96875
2017-03-02T23:35:57.237440: step 1860, loss 0.0516393, acc 0.984375
2017-03-02T23:35:57.359214: step 1861, loss 0.0353513, acc 1
2017-03-02T23:35:57.475773: step 1862, loss 0.113013, acc 0.953125
2017-03-02T23:35:57.582953: step 1863, loss 0.0992531, acc 0.96875
2017-03-02T23:35:57.684609: step 1864, loss 0.0950372, acc 0.96875
2017-03-02T23:35:57.786698: step 1865, loss 0.173584, acc 0.953125
2017-03-02T23:35:57.874144: step 1866, loss 0.0552041, acc 0.984375
2017-03-02T23:35:57.991287: step 1867, loss 0.154334, acc 0.96875
2017-03-02T23:35:58.098114: step 1868, loss 0.0843037, acc 0.96875
2017-03-02T23:35:58.192538: step 1869, loss 0.0596994, acc 0.984375
2017-03-02T23:35:58.295877: step 1870, loss 0.0353617, acc 1
2017-03-02T23:35:58.401812: step 1871, loss 0.0918875, acc 0.953125
2017-03-02T23:35:58.506525: step 1872, loss 0.0726855, acc 0.96875
2017-03-02T23:35:58.605368: step 1873, loss 0.0443837, acc 0.984375
2017-03-02T23:35:58.705923: step 1874, loss 0.0581171, acc 1
2017-03-02T23:35:58.809105: step 1875, loss 0.116641, acc 0.9375
2017-03-02T23:35:58.923617: step 1876, loss 0.122207, acc 0.96875
2017-03-02T23:35:59.043872: step 1877, loss 0.105376, acc 0.953125
2017-03-02T23:35:59.151159: step 1878, loss 0.0361239, acc 1
2017-03-02T23:35:59.254498: step 1879, loss 0.0606602, acc 0.984375
2017-03-02T23:35:59.347982: step 1880, loss 0.152057, acc 0.9375
2017-03-02T23:35:59.437517: step 1881, loss 0.0735237, acc 0.96875
2017-03-02T23:35:59.540211: step 1882, loss 0.167586, acc 0.96875
2017-03-02T23:35:59.643207: step 1883, loss 0.134442, acc 0.9375
2017-03-02T23:35:59.745110: step 1884, loss 0.075395, acc 0.96875
2017-03-02T23:35:59.862311: step 1885, loss 0.0939506, acc 0.953125
2017-03-02T23:35:59.959630: step 1886, loss 0.128094, acc 0.92
2017-03-02T23:36:00.067900: step 1887, loss 0.217702, acc 0.90625
2017-03-02T23:36:00.161109: step 1888, loss 0.0721745, acc 0.984375
2017-03-02T23:36:00.262633: step 1889, loss 0.0344226, acc 0.984375
2017-03-02T23:36:00.369049: step 1890, loss 0.102655, acc 0.96875
2017-03-02T23:36:00.473678: step 1891, loss 0.113382, acc 0.953125
2017-03-02T23:36:00.570616: step 1892, loss 0.0836088, acc 0.96875
2017-03-02T23:36:00.677403: step 1893, loss 0.0663963, acc 0.984375
2017-03-02T23:36:00.779863: step 1894, loss 0.0423068, acc 1
2017-03-02T23:36:00.871742: step 1895, loss 0.0634814, acc 0.984375
2017-03-02T23:36:00.963705: step 1896, loss 0.0501049, acc 1
2017-03-02T23:36:01.064769: step 1897, loss 0.0868174, acc 0.984375
2017-03-02T23:36:01.166739: step 1898, loss 0.0677207, acc 0.984375
2017-03-02T23:36:01.279858: step 1899, loss 0.0740424, acc 0.96875
2017-03-02T23:36:01.385529: step 1900, loss 0.0989011, acc 0.984375

Evaluation:
2017-03-02T23:36:01.447785: step 1900, loss 1.60095, acc 0.550173

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-1900

2017-03-02T23:36:01.911432: step 1901, loss 0.105971, acc 0.953125
2017-03-02T23:36:02.013622: step 1902, loss 0.106077, acc 0.953125
2017-03-02T23:36:02.119852: step 1903, loss 0.0799114, acc 0.96875
2017-03-02T23:36:02.221477: step 1904, loss 0.0989601, acc 0.984375
2017-03-02T23:36:02.315705: step 1905, loss 0.108223, acc 0.953125
2017-03-02T23:36:02.414773: step 1906, loss 0.0362211, acc 1
2017-03-02T23:36:02.512975: step 1907, loss 0.0877792, acc 0.984375
2017-03-02T23:36:02.614404: step 1908, loss 0.041816, acc 1
2017-03-02T23:36:02.720505: step 1909, loss 0.053123, acc 0.984375
2017-03-02T23:36:02.820908: step 1910, loss 0.0810053, acc 0.96875
2017-03-02T23:36:02.925189: step 1911, loss 0.0594002, acc 0.984375
2017-03-02T23:36:03.038185: step 1912, loss 0.0588537, acc 0.984375
2017-03-02T23:36:03.128561: step 1913, loss 0.111517, acc 0.953125
2017-03-02T23:36:03.229778: step 1914, loss 0.0442055, acc 1
2017-03-02T23:36:03.338289: step 1915, loss 0.0614353, acc 0.984375
2017-03-02T23:36:03.459368: step 1916, loss 0.0718185, acc 0.984375
2017-03-02T23:36:03.562430: step 1917, loss 0.0426217, acc 1
2017-03-02T23:36:03.670524: step 1918, loss 0.0385673, acc 0.984375
2017-03-02T23:36:03.782032: step 1919, loss 0.0471825, acc 0.96875
2017-03-02T23:36:03.892637: step 1920, loss 0.0938366, acc 0.96875
2017-03-02T23:36:04.013042: step 1921, loss 0.0990366, acc 0.96875
2017-03-02T23:36:04.136237: step 1922, loss 0.125353, acc 0.953125
2017-03-02T23:36:04.238997: step 1923, loss 0.0220566, acc 1
2017-03-02T23:36:04.342145: step 1924, loss 0.0386941, acc 0.984375
2017-03-02T23:36:04.448660: step 1925, loss 0.0635229, acc 0.96875
2017-03-02T23:36:04.553691: step 1926, loss 0.0544932, acc 0.96875
2017-03-02T23:36:04.648715: step 1927, loss 0.120194, acc 0.98
2017-03-02T23:36:04.759394: step 1928, loss 0.0457181, acc 1
2017-03-02T23:36:04.877859: step 1929, loss 0.0501805, acc 0.984375
2017-03-02T23:36:04.982164: step 1930, loss 0.0379422, acc 1
2017-03-02T23:36:05.086303: step 1931, loss 0.0605327, acc 0.984375
2017-03-02T23:36:05.190423: step 1932, loss 0.0869929, acc 0.96875
2017-03-02T23:36:05.294767: step 1933, loss 0.0700027, acc 0.96875
2017-03-02T23:36:05.383972: step 1934, loss 0.0721472, acc 0.984375
2017-03-02T23:36:05.493963: step 1935, loss 0.138929, acc 0.96875
2017-03-02T23:36:05.594502: step 1936, loss 0.0362, acc 0.984375
2017-03-02T23:36:05.701737: step 1937, loss 0.190281, acc 0.9375
2017-03-02T23:36:05.806154: step 1938, loss 0.0321077, acc 1
2017-03-02T23:36:05.910048: step 1939, loss 0.0689656, acc 0.984375
2017-03-02T23:36:06.014069: step 1940, loss 0.0814056, acc 0.953125
2017-03-02T23:36:06.107743: step 1941, loss 0.069965, acc 0.953125
2017-03-02T23:36:06.210182: step 1942, loss 0.0850353, acc 0.96875
2017-03-02T23:36:06.321561: step 1943, loss 0.0924556, acc 0.96875
2017-03-02T23:36:06.428069: step 1944, loss 0.189068, acc 0.921875
2017-03-02T23:36:06.533860: step 1945, loss 0.0402635, acc 1
2017-03-02T23:36:06.644573: step 1946, loss 0.0902407, acc 0.96875
2017-03-02T23:36:06.752670: step 1947, loss 0.0919681, acc 0.953125
2017-03-02T23:36:06.850826: step 1948, loss 0.0593454, acc 1
2017-03-02T23:36:06.953379: step 1949, loss 0.0176563, acc 1
2017-03-02T23:36:07.053628: step 1950, loss 0.0885998, acc 0.96875
2017-03-02T23:36:07.174174: step 1951, loss 0.0504374, acc 0.96875
2017-03-02T23:36:07.281658: step 1952, loss 0.104751, acc 0.96875
2017-03-02T23:36:07.387645: step 1953, loss 0.0196077, acc 1
2017-03-02T23:36:07.495643: step 1954, loss 0.145196, acc 0.921875
2017-03-02T23:36:07.590915: step 1955, loss 0.182331, acc 0.921875
2017-03-02T23:36:07.679013: step 1956, loss 0.0354476, acc 1
2017-03-02T23:36:07.783819: step 1957, loss 0.0726469, acc 0.984375
2017-03-02T23:36:07.894945: step 1958, loss 0.0552301, acc 0.984375
2017-03-02T23:36:08.000146: step 1959, loss 0.0893331, acc 0.984375
2017-03-02T23:36:08.106773: step 1960, loss 0.0541143, acc 0.984375
2017-03-02T23:36:08.217024: step 1961, loss 0.0638304, acc 0.96875
2017-03-02T23:36:08.332046: step 1962, loss 0.0420475, acc 0.984375
2017-03-02T23:36:08.422111: step 1963, loss 0.107414, acc 0.96875
2017-03-02T23:36:08.530649: step 1964, loss 0.0832854, acc 0.96875
2017-03-02T23:36:08.640155: step 1965, loss 0.040227, acc 0.984375
2017-03-02T23:36:08.747324: step 1966, loss 0.0550866, acc 0.984375
2017-03-02T23:36:08.852328: step 1967, loss 0.0915467, acc 0.96875
2017-03-02T23:36:08.947904: step 1968, loss 0.0384093, acc 1
2017-03-02T23:36:09.075493: step 1969, loss 0.128288, acc 0.9375
2017-03-02T23:36:09.171631: step 1970, loss 0.0759, acc 0.96875
2017-03-02T23:36:09.273911: step 1971, loss 0.0798835, acc 0.96875
2017-03-02T23:36:09.379136: step 1972, loss 0.0414363, acc 1
2017-03-02T23:36:09.483469: step 1973, loss 0.0519552, acc 0.984375
2017-03-02T23:36:09.598599: step 1974, loss 0.0554529, acc 0.984375
2017-03-02T23:36:09.711927: step 1975, loss 0.100194, acc 0.96875
2017-03-02T23:36:09.822130: step 1976, loss 0.026706, acc 1
2017-03-02T23:36:09.907721: step 1977, loss 0.0467642, acc 0.984375
2017-03-02T23:36:10.010022: step 1978, loss 0.115884, acc 0.953125
2017-03-02T23:36:10.109665: step 1979, loss 0.123187, acc 0.953125
2017-03-02T23:36:10.220824: step 1980, loss 0.0804897, acc 0.984375
2017-03-02T23:36:10.329010: step 1981, loss 0.0737799, acc 0.96875
2017-03-02T23:36:10.435629: step 1982, loss 0.0598294, acc 0.984375
2017-03-02T23:36:10.537924: step 1983, loss 0.0884629, acc 0.953125
2017-03-02T23:36:10.639217: step 1984, loss 0.0894788, acc 0.9375
2017-03-02T23:36:10.728620: step 1985, loss 0.0509056, acc 0.984375
2017-03-02T23:36:10.839800: step 1986, loss 0.141017, acc 0.9375
2017-03-02T23:36:10.942697: step 1987, loss 0.0643271, acc 0.96875
2017-03-02T23:36:11.059519: step 1988, loss 0.0914356, acc 0.96875
2017-03-02T23:36:11.165005: step 1989, loss 0.058311, acc 0.984375
2017-03-02T23:36:11.268046: step 1990, loss 0.096988, acc 0.953125
2017-03-02T23:36:11.371437: step 1991, loss 0.100414, acc 0.96875
2017-03-02T23:36:11.467605: step 1992, loss 0.0601646, acc 0.984375
2017-03-02T23:36:11.573479: step 1993, loss 0.0263962, acc 0.984375
2017-03-02T23:36:11.674998: step 1994, loss 0.0502144, acc 0.96875
2017-03-02T23:36:11.772963: step 1995, loss 0.0669116, acc 0.984375
2017-03-02T23:36:11.878656: step 1996, loss 0.0232574, acc 1
2017-03-02T23:36:11.986114: step 1997, loss 0.0772187, acc 0.96875
2017-03-02T23:36:12.091293: step 1998, loss 0.0471087, acc 0.984375
2017-03-02T23:36:12.184404: step 1999, loss 0.118751, acc 0.953125
2017-03-02T23:36:12.281646: step 2000, loss 0.0832082, acc 0.953125

Evaluation:
2017-03-02T23:36:12.341368: step 2000, loss 1.59016, acc 0.536332

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2000

2017-03-02T23:36:12.803449: step 2001, loss 0.143518, acc 0.9375
2017-03-02T23:36:12.896903: step 2002, loss 0.0974166, acc 0.953125
2017-03-02T23:36:13.003482: step 2003, loss 0.099372, acc 0.9375
2017-03-02T23:36:13.109144: step 2004, loss 0.00978382, acc 1
2017-03-02T23:36:13.225042: step 2005, loss 0.103349, acc 0.96875
2017-03-02T23:36:13.334252: step 2006, loss 0.0597909, acc 0.96875
2017-03-02T23:36:13.438510: step 2007, loss 0.0642043, acc 0.96875
2017-03-02T23:36:13.550242: step 2008, loss 0.0755542, acc 0.953125
2017-03-02T23:36:13.636142: step 2009, loss 0.136577, acc 0.94
2017-03-02T23:36:13.738359: step 2010, loss 0.0247402, acc 1
2017-03-02T23:36:13.853090: step 2011, loss 0.0149946, acc 1
2017-03-02T23:36:13.955975: step 2012, loss 0.0607538, acc 0.984375
2017-03-02T23:36:14.061741: step 2013, loss 0.0556911, acc 0.96875
2017-03-02T23:36:14.168295: step 2014, loss 0.0178366, acc 1
2017-03-02T23:36:14.275828: step 2015, loss 0.0693531, acc 0.96875
2017-03-02T23:36:14.371600: step 2016, loss 0.0167828, acc 1
2017-03-02T23:36:14.473382: step 2017, loss 0.037439, acc 0.96875
2017-03-02T23:36:14.579345: step 2018, loss 0.0455546, acc 0.984375
2017-03-02T23:36:14.688876: step 2019, loss 0.0283777, acc 0.984375
2017-03-02T23:36:14.793816: step 2020, loss 0.0649015, acc 0.984375
2017-03-02T23:36:14.896217: step 2021, loss 0.0519462, acc 0.96875
2017-03-02T23:36:15.007502: step 2022, loss 0.0764409, acc 0.953125
2017-03-02T23:36:15.096503: step 2023, loss 0.0553065, acc 0.96875
2017-03-02T23:36:15.188605: step 2024, loss 0.0194495, acc 1
2017-03-02T23:36:15.295484: step 2025, loss 0.0439184, acc 0.984375
2017-03-02T23:36:15.409486: step 2026, loss 0.0191668, acc 1
2017-03-02T23:36:15.516675: step 2027, loss 0.105912, acc 0.953125
2017-03-02T23:36:15.621729: step 2028, loss 0.0523884, acc 0.984375
2017-03-02T23:36:15.728655: step 2029, loss 0.152857, acc 0.953125
2017-03-02T23:36:15.839409: step 2030, loss 0.0384699, acc 0.984375
2017-03-02T23:36:15.930940: step 2031, loss 0.0864557, acc 0.984375
2017-03-02T23:36:16.059698: step 2032, loss 0.094859, acc 0.96875
2017-03-02T23:36:16.184251: step 2033, loss 0.0446228, acc 1
2017-03-02T23:36:16.307323: step 2034, loss 0.0298868, acc 1
2017-03-02T23:36:16.418428: step 2035, loss 0.0452988, acc 0.984375
2017-03-02T23:36:16.526670: step 2036, loss 0.0464878, acc 0.984375
2017-03-02T23:36:16.623523: step 2037, loss 0.0768157, acc 0.984375
2017-03-02T23:36:16.742295: step 2038, loss 0.0280355, acc 0.984375
2017-03-02T23:36:16.843695: step 2039, loss 0.0889986, acc 0.96875
2017-03-02T23:36:16.949225: step 2040, loss 0.0616453, acc 0.984375
2017-03-02T23:36:17.059658: step 2041, loss 0.0272086, acc 1
2017-03-02T23:36:17.167245: step 2042, loss 0.0415189, acc 0.984375
2017-03-02T23:36:17.275978: step 2043, loss 0.0801648, acc 0.96875
2017-03-02T23:36:17.367633: step 2044, loss 0.201273, acc 0.9375
2017-03-02T23:36:17.473976: step 2045, loss 0.0337519, acc 0.984375
2017-03-02T23:36:17.580380: step 2046, loss 0.0728989, acc 0.953125
2017-03-02T23:36:17.683971: step 2047, loss 0.126865, acc 0.96875
2017-03-02T23:36:17.792335: step 2048, loss 0.075968, acc 0.96875
2017-03-02T23:36:17.892836: step 2049, loss 0.096666, acc 0.96875
2017-03-02T23:36:17.988063: step 2050, loss 0.0834259, acc 0.96
2017-03-02T23:36:18.084401: step 2051, loss 0.0410402, acc 1
2017-03-02T23:36:18.176154: step 2052, loss 0.0440852, acc 0.984375
2017-03-02T23:36:18.291234: step 2053, loss 0.0538951, acc 0.984375
2017-03-02T23:36:18.395500: step 2054, loss 0.0524888, acc 0.96875
2017-03-02T23:36:18.519511: step 2055, loss 0.0610015, acc 0.984375
2017-03-02T23:36:18.624771: step 2056, loss 0.0500188, acc 0.984375
2017-03-02T23:36:18.728177: step 2057, loss 0.0765067, acc 0.953125
2017-03-02T23:36:18.826209: step 2058, loss 0.0475537, acc 0.984375
2017-03-02T23:36:18.919867: step 2059, loss 0.0297801, acc 1
2017-03-02T23:36:19.027515: step 2060, loss 0.045647, acc 0.96875
2017-03-02T23:36:19.146639: step 2061, loss 0.0407497, acc 1
2017-03-02T23:36:19.274849: step 2062, loss 0.128063, acc 0.96875
2017-03-02T23:36:19.379126: step 2063, loss 0.0219134, acc 1
2017-03-02T23:36:19.498880: step 2064, loss 0.0538485, acc 0.984375
2017-03-02T23:36:19.586508: step 2065, loss 0.0505365, acc 1
2017-03-02T23:36:19.699816: step 2066, loss 0.0432095, acc 0.984375
2017-03-02T23:36:19.808379: step 2067, loss 0.0585763, acc 0.984375
2017-03-02T23:36:19.915730: step 2068, loss 0.0572318, acc 0.96875
2017-03-02T23:36:20.024677: step 2069, loss 0.0994855, acc 0.953125
2017-03-02T23:36:20.131228: step 2070, loss 0.0396747, acc 0.984375
2017-03-02T23:36:20.238198: step 2071, loss 0.0582192, acc 0.984375
2017-03-02T23:36:20.328733: step 2072, loss 0.0553817, acc 0.984375
2017-03-02T23:36:20.431692: step 2073, loss 0.0736003, acc 0.96875
2017-03-02T23:36:20.539560: step 2074, loss 0.0514858, acc 0.984375
2017-03-02T23:36:20.645863: step 2075, loss 0.06486, acc 0.96875
2017-03-02T23:36:20.755603: step 2076, loss 0.0924759, acc 0.953125
2017-03-02T23:36:20.866013: step 2077, loss 0.122665, acc 0.96875
2017-03-02T23:36:20.983613: step 2078, loss 0.0337989, acc 0.984375
2017-03-02T23:36:21.074386: step 2079, loss 0.0968305, acc 0.96875
2017-03-02T23:36:21.176495: step 2080, loss 0.0419755, acc 1
2017-03-02T23:36:21.276307: step 2081, loss 0.0917109, acc 0.953125
2017-03-02T23:36:21.379549: step 2082, loss 0.056954, acc 0.984375
2017-03-02T23:36:21.510702: step 2083, loss 0.0286158, acc 1
2017-03-02T23:36:21.612322: step 2084, loss 0.0329882, acc 1
2017-03-02T23:36:21.721622: step 2085, loss 0.026123, acc 1
2017-03-02T23:36:21.813021: step 2086, loss 0.0833847, acc 0.96875
2017-03-02T23:36:21.911949: step 2087, loss 0.0349124, acc 0.984375
2017-03-02T23:36:22.007406: step 2088, loss 0.0150505, acc 1
2017-03-02T23:36:22.124344: step 2089, loss 0.0279847, acc 1
2017-03-02T23:36:22.230803: step 2090, loss 0.0403197, acc 0.984375
2017-03-02T23:36:22.335198: step 2091, loss 0.091973, acc 0.98
2017-03-02T23:36:22.437565: step 2092, loss 0.0658628, acc 0.96875
2017-03-02T23:36:22.536895: step 2093, loss 0.0658488, acc 0.953125
2017-03-02T23:36:22.625043: step 2094, loss 0.120447, acc 0.96875
2017-03-02T23:36:22.728740: step 2095, loss 0.0290028, acc 1
2017-03-02T23:36:22.832761: step 2096, loss 0.0514067, acc 0.984375
2017-03-02T23:36:22.929643: step 2097, loss 0.0142335, acc 1
2017-03-02T23:36:23.034188: step 2098, loss 0.0271768, acc 0.984375
2017-03-02T23:36:23.136578: step 2099, loss 0.026026, acc 1
2017-03-02T23:36:23.241217: step 2100, loss 0.0591692, acc 0.96875

Evaluation:
2017-03-02T23:36:23.285405: step 2100, loss 1.61363, acc 0.543253

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2100

2017-03-02T23:36:23.755052: step 2101, loss 0.0354332, acc 0.984375
2017-03-02T23:36:23.862310: step 2102, loss 0.0401708, acc 1
2017-03-02T23:36:23.965876: step 2103, loss 0.0450717, acc 0.984375
2017-03-02T23:36:24.064235: step 2104, loss 0.0419974, acc 0.984375
2017-03-02T23:36:24.170068: step 2105, loss 0.0720576, acc 0.984375
2017-03-02T23:36:24.277977: step 2106, loss 0.0224628, acc 1
2017-03-02T23:36:24.392546: step 2107, loss 0.0600528, acc 0.984375
2017-03-02T23:36:24.504348: step 2108, loss 0.0712736, acc 0.96875
2017-03-02T23:36:24.606758: step 2109, loss 0.0494896, acc 0.984375
2017-03-02T23:36:24.708940: step 2110, loss 0.0307463, acc 1
2017-03-02T23:36:24.802012: step 2111, loss 0.0472176, acc 0.984375
2017-03-02T23:36:24.903651: step 2112, loss 0.0567183, acc 0.984375
2017-03-02T23:36:25.005891: step 2113, loss 0.032023, acc 0.984375
2017-03-02T23:36:25.111847: step 2114, loss 0.0876406, acc 0.953125
2017-03-02T23:36:25.214124: step 2115, loss 0.0762804, acc 0.96875
2017-03-02T23:36:25.310578: step 2116, loss 0.0643631, acc 0.984375
2017-03-02T23:36:25.413270: step 2117, loss 0.0332861, acc 1
2017-03-02T23:36:25.514678: step 2118, loss 0.0842306, acc 0.96875
2017-03-02T23:36:25.602677: step 2119, loss 0.171517, acc 0.953125
2017-03-02T23:36:25.712509: step 2120, loss 0.0450475, acc 0.984375
2017-03-02T23:36:25.816172: step 2121, loss 0.0534934, acc 0.984375
2017-03-02T23:36:25.930754: step 2122, loss 0.0125914, acc 1
2017-03-02T23:36:26.037495: step 2123, loss 0.0431664, acc 0.984375
2017-03-02T23:36:26.150716: step 2124, loss 0.0831176, acc 0.96875
2017-03-02T23:36:26.261666: step 2125, loss 0.0555598, acc 0.984375
2017-03-02T23:36:26.349650: step 2126, loss 0.0767833, acc 0.953125
2017-03-02T23:36:26.455535: step 2127, loss 0.0502527, acc 0.984375
2017-03-02T23:36:26.563207: step 2128, loss 0.122007, acc 0.9375
2017-03-02T23:36:26.671577: step 2129, loss 0.0329195, acc 1
2017-03-02T23:36:26.784824: step 2130, loss 0.0694348, acc 0.984375
2017-03-02T23:36:26.884137: step 2131, loss 0.0377056, acc 0.984375
2017-03-02T23:36:26.984615: step 2132, loss 0.0156006, acc 1
2017-03-02T23:36:27.076251: step 2133, loss 0.0322828, acc 1
2017-03-02T23:36:27.179604: step 2134, loss 0.0479973, acc 0.984375
2017-03-02T23:36:27.284627: step 2135, loss 0.0821212, acc 0.984375
2017-03-02T23:36:27.389685: step 2136, loss 0.0604874, acc 1
2017-03-02T23:36:27.490423: step 2137, loss 0.0813184, acc 0.96875
2017-03-02T23:36:27.616518: step 2138, loss 0.0615549, acc 0.984375
2017-03-02T23:36:27.725241: step 2139, loss 0.0312952, acc 1
2017-03-02T23:36:27.817909: step 2140, loss 0.0655933, acc 0.96875
2017-03-02T23:36:27.911601: step 2141, loss 0.114803, acc 0.96875
2017-03-02T23:36:28.025523: step 2142, loss 0.0683222, acc 0.984375
2017-03-02T23:36:28.128171: step 2143, loss 0.0566865, acc 0.96875
2017-03-02T23:36:28.225875: step 2144, loss 0.0367939, acc 1
2017-03-02T23:36:28.329873: step 2145, loss 0.0243372, acc 1
2017-03-02T23:36:28.446238: step 2146, loss 0.0599251, acc 0.984375
2017-03-02T23:36:28.552735: step 2147, loss 0.117024, acc 0.96875
2017-03-02T23:36:28.643176: step 2148, loss 0.0376224, acc 1
2017-03-02T23:36:28.748400: step 2149, loss 0.0321661, acc 1
2017-03-02T23:36:28.855076: step 2150, loss 0.037319, acc 0.984375
2017-03-02T23:36:28.978609: step 2151, loss 0.136187, acc 0.953125
2017-03-02T23:36:29.086288: step 2152, loss 0.0415724, acc 0.984375
2017-03-02T23:36:29.196414: step 2153, loss 0.0606428, acc 0.984375
2017-03-02T23:36:29.298862: step 2154, loss 0.0245819, acc 1
2017-03-02T23:36:29.386451: step 2155, loss 0.0390631, acc 1
2017-03-02T23:36:29.490281: step 2156, loss 0.0362826, acc 1
2017-03-02T23:36:29.584947: step 2157, loss 0.039102, acc 0.984375
2017-03-02T23:36:29.701095: step 2158, loss 0.0442554, acc 0.96875
2017-03-02T23:36:29.807372: step 2159, loss 0.0801214, acc 0.96875
2017-03-02T23:36:29.910460: step 2160, loss 0.130494, acc 0.953125
2017-03-02T23:36:30.013379: step 2161, loss 0.0192285, acc 1
2017-03-02T23:36:30.112750: step 2162, loss 0.048783, acc 0.984375
2017-03-02T23:36:30.216130: step 2163, loss 0.0672536, acc 0.984375
2017-03-02T23:36:30.321222: step 2164, loss 0.0223067, acc 1
2017-03-02T23:36:30.431820: step 2165, loss 0.0474375, acc 0.984375
2017-03-02T23:36:30.536394: step 2166, loss 0.120539, acc 0.921875
2017-03-02T23:36:30.637840: step 2167, loss 0.0625271, acc 0.96875
2017-03-02T23:36:30.741795: step 2168, loss 0.0174863, acc 1
2017-03-02T23:36:30.837228: step 2169, loss 0.0771034, acc 0.96875
2017-03-02T23:36:30.926524: step 2170, loss 0.0270061, acc 0.984375
2017-03-02T23:36:31.030323: step 2171, loss 0.0572945, acc 0.984375
2017-03-02T23:36:31.135280: step 2172, loss 0.0813468, acc 0.953125
2017-03-02T23:36:31.229665: step 2173, loss 0.089009, acc 0.96
2017-03-02T23:36:31.333568: step 2174, loss 0.0214697, acc 1
2017-03-02T23:36:31.440542: step 2175, loss 0.0643941, acc 0.96875
2017-03-02T23:36:31.550785: step 2176, loss 0.0671576, acc 0.96875
2017-03-02T23:36:31.643689: step 2177, loss 0.0141604, acc 1
2017-03-02T23:36:31.739621: step 2178, loss 0.0646604, acc 0.96875
2017-03-02T23:36:31.837104: step 2179, loss 0.0781718, acc 0.984375
2017-03-02T23:36:31.935520: step 2180, loss 0.0593926, acc 0.96875
2017-03-02T23:36:32.040426: step 2181, loss 0.105137, acc 0.96875
2017-03-02T23:36:32.148601: step 2182, loss 0.0601671, acc 0.984375
2017-03-02T23:36:32.255147: step 2183, loss 0.0612254, acc 0.984375
2017-03-02T23:36:32.355096: step 2184, loss 0.102462, acc 0.953125
2017-03-02T23:36:32.460052: step 2185, loss 0.0861206, acc 0.96875
2017-03-02T23:36:32.569264: step 2186, loss 0.0336077, acc 0.984375
2017-03-02T23:36:32.683544: step 2187, loss 0.0542673, acc 0.96875
2017-03-02T23:36:32.794701: step 2188, loss 0.0456516, acc 0.984375
2017-03-02T23:36:32.904357: step 2189, loss 0.0443519, acc 0.984375
2017-03-02T23:36:33.011697: step 2190, loss 0.0346116, acc 1
2017-03-02T23:36:33.103644: step 2191, loss 0.079688, acc 0.984375
2017-03-02T23:36:33.195545: step 2192, loss 0.0167501, acc 1
2017-03-02T23:36:33.311731: step 2193, loss 0.0830378, acc 0.984375
2017-03-02T23:36:33.418348: step 2194, loss 0.0424985, acc 0.984375
2017-03-02T23:36:33.523454: step 2195, loss 0.0182853, acc 1
2017-03-02T23:36:33.633238: step 2196, loss 0.0324518, acc 1
2017-03-02T23:36:33.745317: step 2197, loss 0.0595852, acc 0.984375
2017-03-02T23:36:33.854282: step 2198, loss 0.0386884, acc 1
2017-03-02T23:36:33.941210: step 2199, loss 0.0661703, acc 0.96875
2017-03-02T23:36:34.049250: step 2200, loss 0.0784723, acc 0.984375

Evaluation:
2017-03-02T23:36:34.104463: step 2200, loss 1.74, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2200

2017-03-02T23:36:34.550645: step 2201, loss 0.13974, acc 0.96875
2017-03-02T23:36:34.641884: step 2202, loss 0.10075, acc 0.953125
2017-03-02T23:36:34.752526: step 2203, loss 0.0136071, acc 1
2017-03-02T23:36:34.857279: step 2204, loss 0.0607534, acc 0.96875
2017-03-02T23:36:34.964103: step 2205, loss 0.0866465, acc 0.96875
2017-03-02T23:36:35.091913: step 2206, loss 0.0314954, acc 1
2017-03-02T23:36:35.198617: step 2207, loss 0.0580344, acc 0.984375
2017-03-02T23:36:35.308351: step 2208, loss 0.0985775, acc 0.96875
2017-03-02T23:36:35.397905: step 2209, loss 0.0485413, acc 0.984375
2017-03-02T23:36:35.498003: step 2210, loss 0.0377525, acc 0.984375
2017-03-02T23:36:35.610355: step 2211, loss 0.02007, acc 1
2017-03-02T23:36:35.713163: step 2212, loss 0.098705, acc 0.96875
2017-03-02T23:36:35.815163: step 2213, loss 0.0484484, acc 0.984375
2017-03-02T23:36:35.913214: step 2214, loss 0.119331, acc 0.96
2017-03-02T23:36:36.021735: step 2215, loss 0.0664582, acc 0.96875
2017-03-02T23:36:36.108102: step 2216, loss 0.0204515, acc 1
2017-03-02T23:36:36.194517: step 2217, loss 0.0866113, acc 0.984375
2017-03-02T23:36:36.313175: step 2218, loss 0.0555777, acc 0.984375
2017-03-02T23:36:36.419423: step 2219, loss 0.0320937, acc 1
2017-03-02T23:36:36.525129: step 2220, loss 0.056432, acc 0.984375
2017-03-02T23:36:36.634926: step 2221, loss 0.0505578, acc 0.96875
2017-03-02T23:36:36.741584: step 2222, loss 0.0720101, acc 0.984375
2017-03-02T23:36:36.841208: step 2223, loss 0.147193, acc 0.9375
2017-03-02T23:36:36.934570: step 2224, loss 0.0361407, acc 0.984375
2017-03-02T23:36:37.046068: step 2225, loss 0.0555002, acc 0.984375
2017-03-02T23:36:37.150249: step 2226, loss 0.0111358, acc 1
2017-03-02T23:36:37.265921: step 2227, loss 0.0462029, acc 1
2017-03-02T23:36:37.375305: step 2228, loss 0.0535462, acc 0.984375
2017-03-02T23:36:37.485340: step 2229, loss 0.0601451, acc 0.96875
2017-03-02T23:36:37.594472: step 2230, loss 0.0412362, acc 0.984375
2017-03-02T23:36:37.680916: step 2231, loss 0.0976298, acc 0.9375
2017-03-02T23:36:37.792525: step 2232, loss 0.0113946, acc 1
2017-03-02T23:36:37.904617: step 2233, loss 0.0155755, acc 1
2017-03-02T23:36:38.014282: step 2234, loss 0.0265879, acc 1
2017-03-02T23:36:38.122459: step 2235, loss 0.0811805, acc 0.953125
2017-03-02T23:36:38.227374: step 2236, loss 0.0153334, acc 1
2017-03-02T23:36:38.328062: step 2237, loss 0.0461125, acc 0.984375
2017-03-02T23:36:38.422253: step 2238, loss 0.0954867, acc 0.96875
2017-03-02T23:36:38.536100: step 2239, loss 0.0313108, acc 1
2017-03-02T23:36:38.641652: step 2240, loss 0.0170322, acc 1
2017-03-02T23:36:38.744797: step 2241, loss 0.0727458, acc 0.953125
2017-03-02T23:36:38.850836: step 2242, loss 0.0304585, acc 1
2017-03-02T23:36:38.952484: step 2243, loss 0.0274989, acc 1
2017-03-02T23:36:39.058895: step 2244, loss 0.0904935, acc 0.953125
2017-03-02T23:36:39.152478: step 2245, loss 0.0317735, acc 1
2017-03-02T23:36:39.251660: step 2246, loss 0.0681182, acc 0.984375
2017-03-02T23:36:39.356418: step 2247, loss 0.0421772, acc 0.984375
2017-03-02T23:36:39.463631: step 2248, loss 0.0447473, acc 0.984375
2017-03-02T23:36:39.565295: step 2249, loss 0.0376711, acc 1
2017-03-02T23:36:39.670929: step 2250, loss 0.0134535, acc 1
2017-03-02T23:36:39.775288: step 2251, loss 0.0271927, acc 1
2017-03-02T23:36:39.874368: step 2252, loss 0.0665628, acc 0.96875
2017-03-02T23:36:39.973926: step 2253, loss 0.0143452, acc 1
2017-03-02T23:36:40.078581: step 2254, loss 0.0216501, acc 1
2017-03-02T23:36:40.171177: step 2255, loss 0.138119, acc 0.96
2017-03-02T23:36:40.280805: step 2256, loss 0.0854862, acc 0.984375
2017-03-02T23:36:40.392610: step 2257, loss 0.0152554, acc 1
2017-03-02T23:36:40.494539: step 2258, loss 0.024152, acc 1
2017-03-02T23:36:40.611400: step 2259, loss 0.0159243, acc 1
2017-03-02T23:36:40.702980: step 2260, loss 0.0329384, acc 1
2017-03-02T23:36:40.809847: step 2261, loss 0.0229534, acc 1
2017-03-02T23:36:40.916906: step 2262, loss 0.0592002, acc 0.984375
2017-03-02T23:36:41.018629: step 2263, loss 0.0251134, acc 0.984375
2017-03-02T23:36:41.122609: step 2264, loss 0.0429551, acc 1
2017-03-02T23:36:41.222985: step 2265, loss 0.0912403, acc 0.953125
2017-03-02T23:36:41.328634: step 2266, loss 0.0458899, acc 0.984375
2017-03-02T23:36:41.423896: step 2267, loss 0.0447632, acc 0.984375
2017-03-02T23:36:41.526588: step 2268, loss 0.0502625, acc 0.984375
2017-03-02T23:36:41.639881: step 2269, loss 0.0253697, acc 1
2017-03-02T23:36:41.744163: step 2270, loss 0.0222648, acc 1
2017-03-02T23:36:41.851128: step 2271, loss 0.026354, acc 1
2017-03-02T23:36:41.961094: step 2272, loss 0.0940702, acc 0.96875
2017-03-02T23:36:42.061686: step 2273, loss 0.013385, acc 1
2017-03-02T23:36:42.164380: step 2274, loss 0.0135835, acc 1
2017-03-02T23:36:42.251539: step 2275, loss 0.0469282, acc 0.984375
2017-03-02T23:36:42.355177: step 2276, loss 0.0354568, acc 1
2017-03-02T23:36:42.469302: step 2277, loss 0.050472, acc 0.96875
2017-03-02T23:36:42.569344: step 2278, loss 0.0804094, acc 0.953125
2017-03-02T23:36:42.673715: step 2279, loss 0.0500689, acc 0.984375
2017-03-02T23:36:42.776302: step 2280, loss 0.0421747, acc 0.984375
2017-03-02T23:36:42.878373: step 2281, loss 0.0385798, acc 0.984375
2017-03-02T23:36:42.972534: step 2282, loss 0.0585081, acc 0.96875
2017-03-02T23:36:43.079866: step 2283, loss 0.0691691, acc 0.96875
2017-03-02T23:36:43.199354: step 2284, loss 0.0328174, acc 1
2017-03-02T23:36:43.311936: step 2285, loss 0.0885981, acc 0.953125
2017-03-02T23:36:43.419786: step 2286, loss 0.0171278, acc 1
2017-03-02T23:36:43.521932: step 2287, loss 0.0937585, acc 0.96875
2017-03-02T23:36:43.637241: step 2288, loss 0.0392095, acc 0.984375
2017-03-02T23:36:43.728448: step 2289, loss 0.202516, acc 0.9375
2017-03-02T23:36:43.826853: step 2290, loss 0.021955, acc 1
2017-03-02T23:36:43.943860: step 2291, loss 0.0184869, acc 1
2017-03-02T23:36:44.048694: step 2292, loss 0.121859, acc 0.9375
2017-03-02T23:36:44.165773: step 2293, loss 0.0384194, acc 0.984375
2017-03-02T23:36:44.274769: step 2294, loss 0.0306735, acc 1
2017-03-02T23:36:44.383947: step 2295, loss 0.053371, acc 0.984375
2017-03-02T23:36:44.475930: step 2296, loss 0.105472, acc 0.98
2017-03-02T23:36:44.580390: step 2297, loss 0.0738123, acc 0.96875
2017-03-02T23:36:44.682782: step 2298, loss 0.0162915, acc 1
2017-03-02T23:36:44.790671: step 2299, loss 0.0418856, acc 0.984375
2017-03-02T23:36:44.892602: step 2300, loss 0.0305627, acc 0.984375

Evaluation:
2017-03-02T23:36:44.944314: step 2300, loss 1.67809, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2300

2017-03-02T23:36:45.397746: step 2301, loss 0.0592086, acc 0.96875
2017-03-02T23:36:45.504146: step 2302, loss 0.0416285, acc 0.96875
2017-03-02T23:36:45.621655: step 2303, loss 0.063262, acc 0.96875
2017-03-02T23:36:45.729299: step 2304, loss 0.0347356, acc 0.984375
2017-03-02T23:36:45.836380: step 2305, loss 0.0567287, acc 0.984375
2017-03-02T23:36:45.929784: step 2306, loss 0.019944, acc 1
2017-03-02T23:36:46.020124: step 2307, loss 0.0207307, acc 1
2017-03-02T23:36:46.125857: step 2308, loss 0.0822707, acc 0.96875
2017-03-02T23:36:46.227692: step 2309, loss 0.0176155, acc 1
2017-03-02T23:36:46.331078: step 2310, loss 0.0520098, acc 0.96875
2017-03-02T23:36:46.434926: step 2311, loss 0.0552531, acc 0.984375
2017-03-02T23:36:46.538366: step 2312, loss 0.123887, acc 0.953125
2017-03-02T23:36:46.653309: step 2313, loss 0.0324736, acc 1
2017-03-02T23:36:46.743455: step 2314, loss 0.0624392, acc 0.96875
2017-03-02T23:36:46.847053: step 2315, loss 0.0322973, acc 0.96875
2017-03-02T23:36:46.965140: step 2316, loss 0.0120868, acc 1
2017-03-02T23:36:47.082172: step 2317, loss 0.0319875, acc 0.984375
2017-03-02T23:36:47.185867: step 2318, loss 0.0140951, acc 1
2017-03-02T23:36:47.295174: step 2319, loss 0.0411198, acc 0.984375
2017-03-02T23:36:47.401189: step 2320, loss 0.0307965, acc 1
2017-03-02T23:36:47.501058: step 2321, loss 0.0581489, acc 0.984375
2017-03-02T23:36:47.604135: step 2322, loss 0.0306762, acc 0.984375
2017-03-02T23:36:47.709297: step 2323, loss 0.0197686, acc 1
2017-03-02T23:36:47.823547: step 2324, loss 0.0206578, acc 1
2017-03-02T23:36:47.934973: step 2325, loss 0.0334069, acc 1
2017-03-02T23:36:48.037371: step 2326, loss 0.0264069, acc 1
2017-03-02T23:36:48.154915: step 2327, loss 0.032616, acc 1
2017-03-02T23:36:48.247067: step 2328, loss 0.0428761, acc 0.984375
2017-03-02T23:36:48.348531: step 2329, loss 0.0141781, acc 1
2017-03-02T23:36:48.464768: step 2330, loss 0.0413632, acc 0.984375
2017-03-02T23:36:48.564757: step 2331, loss 0.0808481, acc 0.96875
2017-03-02T23:36:48.668300: step 2332, loss 0.0181493, acc 1
2017-03-02T23:36:48.773111: step 2333, loss 0.0436613, acc 0.984375
2017-03-02T23:36:48.876961: step 2334, loss 0.0155482, acc 1
2017-03-02T23:36:48.979796: step 2335, loss 0.0984192, acc 0.984375
2017-03-02T23:36:49.077291: step 2336, loss 0.0214426, acc 1
2017-03-02T23:36:49.176773: step 2337, loss 0.0145144, acc 1
2017-03-02T23:36:49.282914: step 2338, loss 0.0525175, acc 0.984375
2017-03-02T23:36:49.389495: step 2339, loss 0.0598151, acc 0.984375
2017-03-02T23:36:49.500649: step 2340, loss 0.0582049, acc 0.96875
2017-03-02T23:36:49.610793: step 2341, loss 0.0243454, acc 1
2017-03-02T23:36:49.700008: step 2342, loss 0.0360259, acc 0.984375
2017-03-02T23:36:49.796565: step 2343, loss 0.0542189, acc 0.96875
2017-03-02T23:36:49.912349: step 2344, loss 0.0611962, acc 0.984375
2017-03-02T23:36:50.015612: step 2345, loss 0.0167913, acc 1
2017-03-02T23:36:50.120990: step 2346, loss 0.0159126, acc 1
2017-03-02T23:36:50.230975: step 2347, loss 0.0219754, acc 0.984375
2017-03-02T23:36:50.337079: step 2348, loss 0.0542593, acc 0.984375
2017-03-02T23:36:50.445193: step 2349, loss 0.0310316, acc 0.984375
2017-03-02T23:36:50.540383: step 2350, loss 0.0263445, acc 0.984375
2017-03-02T23:36:50.652762: step 2351, loss 0.0527245, acc 0.984375
2017-03-02T23:36:50.759299: step 2352, loss 0.0987499, acc 0.984375
2017-03-02T23:36:50.866301: step 2353, loss 0.0421223, acc 1
2017-03-02T23:36:50.972501: step 2354, loss 0.0218856, acc 1
2017-03-02T23:36:51.086066: step 2355, loss 0.0332117, acc 0.984375
2017-03-02T23:36:51.183981: step 2356, loss 0.0248715, acc 1
2017-03-02T23:36:51.269666: step 2357, loss 0.0411042, acc 0.984375
2017-03-02T23:36:51.386499: step 2358, loss 0.0263622, acc 1
2017-03-02T23:36:51.491848: step 2359, loss 0.0377466, acc 0.984375
2017-03-02T23:36:51.619630: step 2360, loss 0.0972967, acc 0.96875
2017-03-02T23:36:51.727311: step 2361, loss 0.00674468, acc 1
2017-03-02T23:36:51.834212: step 2362, loss 0.0679925, acc 0.96875
2017-03-02T23:36:51.941824: step 2363, loss 0.0280464, acc 0.984375
2017-03-02T23:36:52.032892: step 2364, loss 0.0169285, acc 1
2017-03-02T23:36:52.127581: step 2365, loss 0.0642888, acc 0.984375
2017-03-02T23:36:52.228370: step 2366, loss 0.0132742, acc 1
2017-03-02T23:36:52.327505: step 2367, loss 0.0416357, acc 1
2017-03-02T23:36:52.429291: step 2368, loss 0.0207143, acc 1
2017-03-02T23:36:52.533497: step 2369, loss 0.156022, acc 0.9375
2017-03-02T23:36:52.637302: step 2370, loss 0.0673136, acc 0.984375
2017-03-02T23:36:52.729032: step 2371, loss 0.0261781, acc 0.984375
2017-03-02T23:36:52.823522: step 2372, loss 0.0260709, acc 0.984375
2017-03-02T23:36:52.921966: step 2373, loss 0.0183545, acc 1
2017-03-02T23:36:53.043908: step 2374, loss 0.0286149, acc 1
2017-03-02T23:36:53.147010: step 2375, loss 0.0302547, acc 1
2017-03-02T23:36:53.254353: step 2376, loss 0.053778, acc 0.96875
2017-03-02T23:36:53.358762: step 2377, loss 0.0247631, acc 1
2017-03-02T23:36:53.456419: step 2378, loss 0.0253382, acc 1
2017-03-02T23:36:53.544545: step 2379, loss 0.0152872, acc 1
2017-03-02T23:36:53.641295: step 2380, loss 0.0411168, acc 0.984375
2017-03-02T23:36:53.749699: step 2381, loss 0.0622418, acc 0.984375
2017-03-02T23:36:53.855948: step 2382, loss 0.0281072, acc 0.984375
2017-03-02T23:36:53.959711: step 2383, loss 0.0439405, acc 0.984375
2017-03-02T23:36:54.060840: step 2384, loss 0.0406265, acc 0.984375
2017-03-02T23:36:54.167552: step 2385, loss 0.0168728, acc 1
2017-03-02T23:36:54.286520: step 2386, loss 0.0133796, acc 1
2017-03-02T23:36:54.374241: step 2387, loss 0.0168885, acc 1
2017-03-02T23:36:54.495009: step 2388, loss 0.0352153, acc 0.984375
2017-03-02T23:36:54.603876: step 2389, loss 0.0482863, acc 0.984375
2017-03-02T23:36:54.707068: step 2390, loss 0.0525176, acc 0.96875
2017-03-02T23:36:54.813706: step 2391, loss 0.0065501, acc 1
2017-03-02T23:36:54.918086: step 2392, loss 0.0837568, acc 0.984375
2017-03-02T23:36:55.025591: step 2393, loss 0.0751218, acc 0.96875
2017-03-02T23:36:55.117955: step 2394, loss 0.0532796, acc 0.984375
2017-03-02T23:36:55.220629: step 2395, loss 0.077438, acc 0.96875
2017-03-02T23:36:55.335877: step 2396, loss 0.1039, acc 0.953125
2017-03-02T23:36:55.438662: step 2397, loss 0.0099098, acc 1
2017-03-02T23:36:55.543358: step 2398, loss 0.00893433, acc 1
2017-03-02T23:36:55.654916: step 2399, loss 0.0410202, acc 0.96875
2017-03-02T23:36:55.757707: step 2400, loss 0.0300483, acc 1

Evaluation:
2017-03-02T23:36:55.803332: step 2400, loss 1.88996, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2400

2017-03-02T23:36:56.279635: step 2401, loss 0.0299338, acc 0.984375
2017-03-02T23:36:56.384597: step 2402, loss 0.0747363, acc 0.96875
2017-03-02T23:36:56.494506: step 2403, loss 0.0804282, acc 0.96875
2017-03-02T23:36:56.586231: step 2404, loss 0.0345577, acc 1
2017-03-02T23:36:56.707808: step 2405, loss 0.0272756, acc 0.984375
2017-03-02T23:36:56.819848: step 2406, loss 0.0229049, acc 1
2017-03-02T23:36:56.916076: step 2407, loss 0.0465854, acc 0.96875
2017-03-02T23:36:57.019429: step 2408, loss 0.024009, acc 1
2017-03-02T23:36:57.134423: step 2409, loss 0.0813158, acc 0.953125
2017-03-02T23:36:57.239216: step 2410, loss 0.101594, acc 0.953125
2017-03-02T23:36:57.333147: step 2411, loss 0.0570181, acc 0.984375
2017-03-02T23:36:57.436745: step 2412, loss 0.0200208, acc 1
2017-03-02T23:36:57.537548: step 2413, loss 0.0233098, acc 1
2017-03-02T23:36:57.645082: step 2414, loss 0.045102, acc 0.984375
2017-03-02T23:36:57.742372: step 2415, loss 0.0404734, acc 1
2017-03-02T23:36:57.846609: step 2416, loss 0.0315994, acc 0.984375
2017-03-02T23:36:57.956617: step 2417, loss 0.0408287, acc 0.984375
2017-03-02T23:36:58.051924: step 2418, loss 0.0690559, acc 0.953125
2017-03-02T23:36:58.146501: step 2419, loss 0.0238198, acc 1
2017-03-02T23:36:58.247477: step 2420, loss 0.0503358, acc 0.96875
2017-03-02T23:36:58.350598: step 2421, loss 0.0558007, acc 0.96875
2017-03-02T23:36:58.452396: step 2422, loss 0.0171553, acc 1
2017-03-02T23:36:58.558061: step 2423, loss 0.00558429, acc 1
2017-03-02T23:36:58.662607: step 2424, loss 0.0461291, acc 0.984375
2017-03-02T23:36:58.776400: step 2425, loss 0.0129766, acc 1
2017-03-02T23:36:58.879448: step 2426, loss 0.0315357, acc 1
2017-03-02T23:36:58.990064: step 2427, loss 0.0232798, acc 1
2017-03-02T23:36:59.104647: step 2428, loss 0.0148544, acc 1
2017-03-02T23:36:59.213427: step 2429, loss 0.035565, acc 0.984375
2017-03-02T23:36:59.323154: step 2430, loss 0.0219111, acc 1
2017-03-02T23:36:59.425889: step 2431, loss 0.0528519, acc 0.96875
2017-03-02T23:36:59.547704: step 2432, loss 0.014534, acc 1
2017-03-02T23:36:59.648885: step 2433, loss 0.00960346, acc 1
2017-03-02T23:36:59.769077: step 2434, loss 0.0332444, acc 0.984375
2017-03-02T23:36:59.864044: step 2435, loss 0.00970846, acc 1
2017-03-02T23:36:59.966677: step 2436, loss 0.0187363, acc 1
2017-03-02T23:37:00.068002: step 2437, loss 0.0151008, acc 1
2017-03-02T23:37:00.176804: step 2438, loss 0.0328386, acc 1
2017-03-02T23:37:00.273068: step 2439, loss 0.0109455, acc 1
2017-03-02T23:37:00.364592: step 2440, loss 0.0240576, acc 0.984375
2017-03-02T23:37:00.459345: step 2441, loss 0.0470111, acc 0.96875
2017-03-02T23:37:00.552236: step 2442, loss 0.034664, acc 1
2017-03-02T23:37:00.656625: step 2443, loss 0.0261793, acc 1
2017-03-02T23:37:00.768231: step 2444, loss 0.0321498, acc 0.984375
2017-03-02T23:37:00.872646: step 2445, loss 0.162926, acc 0.953125
2017-03-02T23:37:00.989619: step 2446, loss 0.0315766, acc 0.984375
2017-03-02T23:37:01.079572: step 2447, loss 0.068703, acc 0.984375
2017-03-02T23:37:01.180631: step 2448, loss 0.0145338, acc 1
2017-03-02T23:37:01.284094: step 2449, loss 0.0359259, acc 0.984375
2017-03-02T23:37:01.387693: step 2450, loss 0.0063665, acc 1
2017-03-02T23:37:01.496871: step 2451, loss 0.0811598, acc 0.953125
2017-03-02T23:37:01.598858: step 2452, loss 0.0197646, acc 1
2017-03-02T23:37:01.701092: step 2453, loss 0.0178007, acc 1
2017-03-02T23:37:01.811219: step 2454, loss 0.0285876, acc 0.984375
2017-03-02T23:37:01.909087: step 2455, loss 0.0172385, acc 1
2017-03-02T23:37:02.016002: step 2456, loss 0.0229505, acc 1
2017-03-02T23:37:02.121714: step 2457, loss 0.0852484, acc 0.984375
2017-03-02T23:37:02.233454: step 2458, loss 0.0247792, acc 0.984375
2017-03-02T23:37:02.338634: step 2459, loss 0.0621229, acc 0.96875
2017-03-02T23:37:02.433229: step 2460, loss 0.0417562, acc 0.98
2017-03-02T23:37:02.551227: step 2461, loss 0.0249763, acc 1
2017-03-02T23:37:02.641505: step 2462, loss 0.0272234, acc 1
2017-03-02T23:37:02.754289: step 2463, loss 0.0182913, acc 1
2017-03-02T23:37:02.865890: step 2464, loss 0.0253377, acc 0.984375
2017-03-02T23:37:02.974128: step 2465, loss 0.00540755, acc 1
2017-03-02T23:37:03.086296: step 2466, loss 0.0432058, acc 0.984375
2017-03-02T23:37:03.190564: step 2467, loss 0.107828, acc 0.9375
2017-03-02T23:37:03.296450: step 2468, loss 0.0060966, acc 1
2017-03-02T23:37:03.394138: step 2469, loss 0.0110197, acc 1
2017-03-02T23:37:03.497960: step 2470, loss 0.0597959, acc 0.96875
2017-03-02T23:37:03.599926: step 2471, loss 0.0348573, acc 1
2017-03-02T23:37:03.701049: step 2472, loss 0.0697122, acc 0.96875
2017-03-02T23:37:03.809676: step 2473, loss 0.0297122, acc 1
2017-03-02T23:37:03.910623: step 2474, loss 0.0366172, acc 0.984375
2017-03-02T23:37:04.021429: step 2475, loss 0.00871151, acc 1
2017-03-02T23:37:04.109736: step 2476, loss 0.0456983, acc 0.984375
2017-03-02T23:37:04.206483: step 2477, loss 0.0163835, acc 1
2017-03-02T23:37:04.312017: step 2478, loss 0.0265693, acc 0.984375
2017-03-02T23:37:04.416149: step 2479, loss 0.0400132, acc 0.984375
2017-03-02T23:37:04.520037: step 2480, loss 0.012908, acc 1
2017-03-02T23:37:04.621233: step 2481, loss 0.0239823, acc 1
2017-03-02T23:37:04.727006: step 2482, loss 0.0603502, acc 0.953125
2017-03-02T23:37:04.824011: step 2483, loss 0.0592394, acc 0.96875
2017-03-02T23:37:04.928298: step 2484, loss 0.0529529, acc 0.984375
2017-03-02T23:37:05.026785: step 2485, loss 0.0464348, acc 0.96875
2017-03-02T23:37:05.130952: step 2486, loss 0.0147731, acc 1
2017-03-02T23:37:05.233897: step 2487, loss 0.00792606, acc 1
2017-03-02T23:37:05.335379: step 2488, loss 0.0847007, acc 0.984375
2017-03-02T23:37:05.441921: step 2489, loss 0.0556252, acc 0.953125
2017-03-02T23:37:05.547963: step 2490, loss 0.0200839, acc 1
2017-03-02T23:37:05.648774: step 2491, loss 0.0203825, acc 1
2017-03-02T23:37:05.754497: step 2492, loss 0.108134, acc 0.984375
2017-03-02T23:37:05.861769: step 2493, loss 0.0277227, acc 0.984375
2017-03-02T23:37:05.970853: step 2494, loss 0.0334465, acc 1
2017-03-02T23:37:06.079839: step 2495, loss 0.0459377, acc 0.984375
2017-03-02T23:37:06.188238: step 2496, loss 0.0251607, acc 1
2017-03-02T23:37:06.295981: step 2497, loss 0.01104, acc 1
2017-03-02T23:37:06.382292: step 2498, loss 0.0397435, acc 0.984375
2017-03-02T23:37:06.486056: step 2499, loss 0.0350692, acc 0.984375
2017-03-02T23:37:06.598484: step 2500, loss 0.0223129, acc 1

Evaluation:
2017-03-02T23:37:06.658767: step 2500, loss 1.82683, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2500

2017-03-02T23:37:07.142972: step 2501, loss 0.012679, acc 1
2017-03-02T23:37:07.246921: step 2502, loss 0.0231193, acc 0.984375
2017-03-02T23:37:07.374493: step 2503, loss 0.0178723, acc 1
2017-03-02T23:37:07.477293: step 2504, loss 0.0153323, acc 1
2017-03-02T23:37:07.576151: step 2505, loss 0.0131769, acc 1
2017-03-02T23:37:07.668423: step 2506, loss 0.0455995, acc 1
2017-03-02T23:37:07.782339: step 2507, loss 0.0135044, acc 1
2017-03-02T23:37:07.880547: step 2508, loss 0.0209435, acc 1
2017-03-02T23:37:07.980514: step 2509, loss 0.0247194, acc 0.984375
2017-03-02T23:37:08.086186: step 2510, loss 0.0605036, acc 0.984375
2017-03-02T23:37:08.191137: step 2511, loss 0.0109669, acc 1
2017-03-02T23:37:08.297013: step 2512, loss 0.059356, acc 0.984375
2017-03-02T23:37:08.406379: step 2513, loss 0.0770027, acc 0.96875
2017-03-02T23:37:08.508637: step 2514, loss 0.0154542, acc 1
2017-03-02T23:37:08.605932: step 2515, loss 0.0376841, acc 0.984375
2017-03-02T23:37:08.705002: step 2516, loss 0.00832925, acc 1
2017-03-02T23:37:08.810511: step 2517, loss 0.0132701, acc 1
2017-03-02T23:37:08.914919: step 2518, loss 0.0579209, acc 0.984375
2017-03-02T23:37:09.028035: step 2519, loss 0.0638946, acc 0.96875
2017-03-02T23:37:09.132161: step 2520, loss 0.0518368, acc 0.984375
2017-03-02T23:37:09.230276: step 2521, loss 0.0109655, acc 1
2017-03-02T23:37:09.320146: step 2522, loss 0.0214882, acc 0.984375
2017-03-02T23:37:09.414091: step 2523, loss 0.0547524, acc 0.96875
2017-03-02T23:37:09.516339: step 2524, loss 0.0176485, acc 1
2017-03-02T23:37:09.640339: step 2525, loss 0.0768491, acc 0.96875
2017-03-02T23:37:09.745004: step 2526, loss 0.0363658, acc 1
2017-03-02T23:37:09.848703: step 2527, loss 0.0102116, acc 1
2017-03-02T23:37:09.952777: step 2528, loss 0.0109763, acc 1
2017-03-02T23:37:10.054690: step 2529, loss 0.0288556, acc 0.984375
2017-03-02T23:37:10.143659: step 2530, loss 0.0273703, acc 1
2017-03-02T23:37:10.246757: step 2531, loss 0.0891373, acc 0.96875
2017-03-02T23:37:10.370593: step 2532, loss 0.0082855, acc 1
2017-03-02T23:37:10.480525: step 2533, loss 0.0941229, acc 0.984375
2017-03-02T23:37:10.585572: step 2534, loss 0.0204307, acc 1
2017-03-02T23:37:10.682946: step 2535, loss 0.0322184, acc 0.984375
2017-03-02T23:37:10.788533: step 2536, loss 0.0272823, acc 0.984375
2017-03-02T23:37:10.880272: step 2537, loss 0.0236138, acc 1
2017-03-02T23:37:10.981322: step 2538, loss 0.022023, acc 1
2017-03-02T23:37:11.083996: step 2539, loss 0.0482634, acc 0.984375
2017-03-02T23:37:11.191587: step 2540, loss 0.0476994, acc 0.984375
2017-03-02T23:37:11.307739: step 2541, loss 0.0294307, acc 1
2017-03-02T23:37:11.409216: step 2542, loss 0.110958, acc 0.98
2017-03-02T23:37:11.521075: step 2543, loss 0.0219985, acc 1
2017-03-02T23:37:11.618037: step 2544, loss 0.033739, acc 0.984375
2017-03-02T23:37:11.720084: step 2545, loss 0.00854858, acc 1
2017-03-02T23:37:11.822499: step 2546, loss 0.00613267, acc 1
2017-03-02T23:37:11.926387: step 2547, loss 0.0148782, acc 1
2017-03-02T23:37:12.030626: step 2548, loss 0.0214426, acc 1
2017-03-02T23:37:12.134105: step 2549, loss 0.0177519, acc 1
2017-03-02T23:37:12.238756: step 2550, loss 0.0508605, acc 0.984375
2017-03-02T23:37:12.331497: step 2551, loss 0.0224971, acc 1
2017-03-02T23:37:12.420111: step 2552, loss 0.0231712, acc 0.984375
2017-03-02T23:37:12.525814: step 2553, loss 0.0386582, acc 0.984375
2017-03-02T23:37:12.633295: step 2554, loss 0.131113, acc 0.96875
2017-03-02T23:37:12.742906: step 2555, loss 0.0312218, acc 0.984375
2017-03-02T23:37:12.849923: step 2556, loss 0.0804474, acc 0.953125
2017-03-02T23:37:12.953189: step 2557, loss 0.0769475, acc 0.984375
2017-03-02T23:37:13.048764: step 2558, loss 0.0171884, acc 1
2017-03-02T23:37:13.149952: step 2559, loss 0.039701, acc 1
2017-03-02T23:37:13.250721: step 2560, loss 0.0375362, acc 0.984375
2017-03-02T23:37:13.356572: step 2561, loss 0.00756413, acc 1
2017-03-02T23:37:13.469117: step 2562, loss 0.0618858, acc 0.96875
2017-03-02T23:37:13.579124: step 2563, loss 0.0882823, acc 0.96875
2017-03-02T23:37:13.681587: step 2564, loss 0.0439674, acc 0.984375
2017-03-02T23:37:13.786231: step 2565, loss 0.106094, acc 0.96875
2017-03-02T23:37:13.887470: step 2566, loss 0.0838844, acc 0.9375
2017-03-02T23:37:13.987271: step 2567, loss 0.0992737, acc 0.953125
2017-03-02T23:37:14.092819: step 2568, loss 0.00930375, acc 1
2017-03-02T23:37:14.205352: step 2569, loss 0.0394903, acc 0.96875
2017-03-02T23:37:14.318461: step 2570, loss 0.0235902, acc 1
2017-03-02T23:37:14.420962: step 2571, loss 0.0159868, acc 1
2017-03-02T23:37:14.525133: step 2572, loss 0.0336241, acc 0.984375
2017-03-02T23:37:14.613763: step 2573, loss 0.0352438, acc 1
2017-03-02T23:37:14.713013: step 2574, loss 0.0195996, acc 1
2017-03-02T23:37:14.815046: step 2575, loss 0.0261595, acc 1
2017-03-02T23:37:14.926241: step 2576, loss 0.0433753, acc 0.984375
2017-03-02T23:37:15.035599: step 2577, loss 0.0424984, acc 0.984375
2017-03-02T23:37:15.141217: step 2578, loss 0.00438485, acc 1
2017-03-02T23:37:15.238460: step 2579, loss 0.026688, acc 1
2017-03-02T23:37:15.348517: step 2580, loss 0.0338732, acc 0.984375
2017-03-02T23:37:15.440480: step 2581, loss 0.0472611, acc 0.984375
2017-03-02T23:37:15.547020: step 2582, loss 0.0511451, acc 1
2017-03-02T23:37:15.653281: step 2583, loss 0.0487026, acc 0.98
2017-03-02T23:37:15.763950: step 2584, loss 0.00973762, acc 1
2017-03-02T23:37:15.873362: step 2585, loss 0.0281517, acc 0.984375
2017-03-02T23:37:15.981782: step 2586, loss 0.00976434, acc 1
2017-03-02T23:37:16.090060: step 2587, loss 0.0256894, acc 1
2017-03-02T23:37:16.178123: step 2588, loss 0.0152874, acc 1
2017-03-02T23:37:16.285747: step 2589, loss 0.00411797, acc 1
2017-03-02T23:37:16.389739: step 2590, loss 0.0307168, acc 1
2017-03-02T23:37:16.491785: step 2591, loss 0.0319031, acc 0.984375
2017-03-02T23:37:16.599109: step 2592, loss 0.00799129, acc 1
2017-03-02T23:37:16.703792: step 2593, loss 0.0129706, acc 1
2017-03-02T23:37:16.808713: step 2594, loss 0.0387814, acc 0.984375
2017-03-02T23:37:16.900944: step 2595, loss 0.00874573, acc 1
2017-03-02T23:37:17.000030: step 2596, loss 0.0299814, acc 0.984375
2017-03-02T23:37:17.107577: step 2597, loss 0.0370231, acc 0.984375
2017-03-02T23:37:17.208359: step 2598, loss 0.0212015, acc 1
2017-03-02T23:37:17.313877: step 2599, loss 0.061218, acc 0.96875
2017-03-02T23:37:17.423890: step 2600, loss 0.0347589, acc 0.984375

Evaluation:
2017-03-02T23:37:17.478139: step 2600, loss 1.86141, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2600

2017-03-02T23:37:17.925341: step 2601, loss 0.0161145, acc 1
2017-03-02T23:37:18.031341: step 2602, loss 0.008875, acc 1
2017-03-02T23:37:18.137924: step 2603, loss 0.0193757, acc 1
2017-03-02T23:37:18.247316: step 2604, loss 0.0156701, acc 1
2017-03-02T23:37:18.352442: step 2605, loss 0.0154704, acc 1
2017-03-02T23:37:18.439176: step 2606, loss 0.0159619, acc 1
2017-03-02T23:37:18.556744: step 2607, loss 0.0155575, acc 0.984375
2017-03-02T23:37:18.662460: step 2608, loss 0.0116554, acc 1
2017-03-02T23:37:18.784847: step 2609, loss 0.012268, acc 1
2017-03-02T23:37:18.888109: step 2610, loss 0.0109253, acc 1
2017-03-02T23:37:18.996746: step 2611, loss 0.0109253, acc 1
2017-03-02T23:37:19.101482: step 2612, loss 0.0380879, acc 0.984375
2017-03-02T23:37:19.183901: step 2613, loss 0.034167, acc 0.984375
2017-03-02T23:37:19.286905: step 2614, loss 0.0156457, acc 1
2017-03-02T23:37:19.400329: step 2615, loss 0.0100502, acc 1
2017-03-02T23:37:19.500994: step 2616, loss 0.0289534, acc 0.984375
2017-03-02T23:37:19.614901: step 2617, loss 0.0128651, acc 1
2017-03-02T23:37:19.716980: step 2618, loss 0.00878672, acc 1
2017-03-02T23:37:19.822453: step 2619, loss 0.0143468, acc 1
2017-03-02T23:37:19.915181: step 2620, loss 0.0158723, acc 1
2017-03-02T23:37:20.019645: step 2621, loss 0.0774952, acc 0.984375
2017-03-02T23:37:20.122524: step 2622, loss 0.0120899, acc 1
2017-03-02T23:37:20.228231: step 2623, loss 0.0344965, acc 0.984375
2017-03-02T23:37:20.324482: step 2624, loss 0.0372246, acc 0.98
2017-03-02T23:37:20.426423: step 2625, loss 0.128369, acc 0.9375
2017-03-02T23:37:20.527592: step 2626, loss 0.0149161, acc 1
2017-03-02T23:37:20.619207: step 2627, loss 0.00878141, acc 1
2017-03-02T23:37:20.710062: step 2628, loss 0.0429064, acc 0.96875
2017-03-02T23:37:20.812688: step 2629, loss 0.0267771, acc 1
2017-03-02T23:37:20.913801: step 2630, loss 0.0158046, acc 1
2017-03-02T23:37:21.019943: step 2631, loss 0.0126405, acc 1
2017-03-02T23:37:21.116007: step 2632, loss 0.0118507, acc 1
2017-03-02T23:37:21.213528: step 2633, loss 0.0184321, acc 1
2017-03-02T23:37:21.322146: step 2634, loss 0.0268099, acc 0.984375
2017-03-02T23:37:21.416529: step 2635, loss 0.008399, acc 1
2017-03-02T23:37:21.515843: step 2636, loss 0.0204449, acc 1
2017-03-02T23:37:21.620361: step 2637, loss 0.0118103, acc 1
2017-03-02T23:37:21.721992: step 2638, loss 0.0274184, acc 0.984375
2017-03-02T23:37:21.826004: step 2639, loss 0.0218288, acc 1
2017-03-02T23:37:21.937734: step 2640, loss 0.0131523, acc 1
2017-03-02T23:37:22.042786: step 2641, loss 0.0137485, acc 1
2017-03-02T23:37:22.156684: step 2642, loss 0.0104817, acc 1
2017-03-02T23:37:22.244940: step 2643, loss 0.00861617, acc 1
2017-03-02T23:37:22.349208: step 2644, loss 0.0616988, acc 0.96875
2017-03-02T23:37:22.468382: step 2645, loss 0.00685705, acc 1
2017-03-02T23:37:22.579160: step 2646, loss 0.0199121, acc 1
2017-03-02T23:37:22.688724: step 2647, loss 0.00787611, acc 1
2017-03-02T23:37:22.794221: step 2648, loss 0.00682637, acc 1
2017-03-02T23:37:22.899068: step 2649, loss 0.0150937, acc 1
2017-03-02T23:37:22.987179: step 2650, loss 0.0206962, acc 1
2017-03-02T23:37:23.084762: step 2651, loss 0.0490526, acc 0.984375
2017-03-02T23:37:23.193939: step 2652, loss 0.044495, acc 0.984375
2017-03-02T23:37:23.297832: step 2653, loss 0.00918327, acc 1
2017-03-02T23:37:23.398597: step 2654, loss 0.0871524, acc 0.96875
2017-03-02T23:37:23.509363: step 2655, loss 0.0302544, acc 0.984375
2017-03-02T23:37:23.613961: step 2656, loss 0.0117302, acc 1
2017-03-02T23:37:23.704623: step 2657, loss 0.0402704, acc 0.984375
2017-03-02T23:37:23.796615: step 2658, loss 0.0286092, acc 0.984375
2017-03-02T23:37:23.901841: step 2659, loss 0.0122988, acc 1
2017-03-02T23:37:24.003109: step 2660, loss 0.0363631, acc 0.96875
2017-03-02T23:37:24.108238: step 2661, loss 0.0672936, acc 0.984375
2017-03-02T23:37:24.213157: step 2662, loss 0.104235, acc 0.96875
2017-03-02T23:37:24.312516: step 2663, loss 0.0766861, acc 0.96875
2017-03-02T23:37:24.418193: step 2664, loss 0.0183444, acc 1
2017-03-02T23:37:24.502240: step 2665, loss 0.142581, acc 0.96
2017-03-02T23:37:24.608181: step 2666, loss 0.0318605, acc 0.984375
2017-03-02T23:37:24.718042: step 2667, loss 0.0367707, acc 0.984375
2017-03-02T23:37:24.821324: step 2668, loss 0.0320748, acc 0.984375
2017-03-02T23:37:24.923415: step 2669, loss 0.012512, acc 1
2017-03-02T23:37:25.027175: step 2670, loss 0.0808352, acc 0.96875
2017-03-02T23:37:25.127594: step 2671, loss 0.0308856, acc 1
2017-03-02T23:37:25.215282: step 2672, loss 0.00713961, acc 1
2017-03-02T23:37:25.324967: step 2673, loss 0.00756657, acc 1
2017-03-02T23:37:25.432518: step 2674, loss 0.031575, acc 0.984375
2017-03-02T23:37:25.540658: step 2675, loss 0.0107714, acc 1
2017-03-02T23:37:25.653078: step 2676, loss 0.0212207, acc 1
2017-03-02T23:37:25.745732: step 2677, loss 0.0537096, acc 0.984375
2017-03-02T23:37:25.853114: step 2678, loss 0.0381249, acc 0.984375
2017-03-02T23:37:25.944833: step 2679, loss 0.147666, acc 0.9375
2017-03-02T23:37:26.045510: step 2680, loss 0.0801123, acc 0.984375
2017-03-02T23:37:26.149179: step 2681, loss 0.0125543, acc 1
2017-03-02T23:37:26.267460: step 2682, loss 0.0461357, acc 0.984375
2017-03-02T23:37:26.371074: step 2683, loss 0.0306967, acc 1
2017-03-02T23:37:26.476181: step 2684, loss 0.0240843, acc 0.984375
2017-03-02T23:37:26.587518: step 2685, loss 0.0984644, acc 0.953125
2017-03-02T23:37:26.689750: step 2686, loss 0.024134, acc 1
2017-03-02T23:37:26.778729: step 2687, loss 0.0920441, acc 0.984375
2017-03-02T23:37:26.887550: step 2688, loss 0.0335105, acc 1
2017-03-02T23:37:26.993944: step 2689, loss 0.0249138, acc 1
2017-03-02T23:37:27.095160: step 2690, loss 0.030073, acc 0.984375
2017-03-02T23:37:27.200455: step 2691, loss 0.0171951, acc 1
2017-03-02T23:37:27.299307: step 2692, loss 0.0413382, acc 1
2017-03-02T23:37:27.406208: step 2693, loss 0.0108282, acc 1
2017-03-02T23:37:27.499631: step 2694, loss 0.0173198, acc 1
2017-03-02T23:37:27.598442: step 2695, loss 0.011274, acc 1
2017-03-02T23:37:27.701832: step 2696, loss 0.0160855, acc 1
2017-03-02T23:37:27.810938: step 2697, loss 0.00874414, acc 1
2017-03-02T23:37:27.927907: step 2698, loss 0.0242064, acc 1
2017-03-02T23:37:28.033636: step 2699, loss 0.0461278, acc 0.984375
2017-03-02T23:37:28.139166: step 2700, loss 0.0196, acc 1

Evaluation:
2017-03-02T23:37:28.180993: step 2700, loss 2.13279, acc 0.550173

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2700

2017-03-02T23:37:28.627059: step 2701, loss 0.00373551, acc 1
2017-03-02T23:37:28.732071: step 2702, loss 0.0299766, acc 0.96875
2017-03-02T23:37:28.846534: step 2703, loss 0.0355055, acc 0.984375
2017-03-02T23:37:28.938184: step 2704, loss 0.0260174, acc 1
2017-03-02T23:37:29.048180: step 2705, loss 0.0185302, acc 1
2017-03-02T23:37:29.141099: step 2706, loss 0.0718533, acc 0.98
2017-03-02T23:37:29.244212: step 2707, loss 0.0113022, acc 1
2017-03-02T23:37:29.348339: step 2708, loss 0.00531073, acc 1
2017-03-02T23:37:29.474887: step 2709, loss 0.0249689, acc 1
2017-03-02T23:37:29.584245: step 2710, loss 0.0736591, acc 0.984375
2017-03-02T23:37:29.677674: step 2711, loss 0.0478159, acc 0.984375
2017-03-02T23:37:29.780674: step 2712, loss 0.0219073, acc 1
2017-03-02T23:37:29.884973: step 2713, loss 0.00673476, acc 1
2017-03-02T23:37:29.985034: step 2714, loss 0.0476705, acc 0.984375
2017-03-02T23:37:30.090761: step 2715, loss 0.0186678, acc 1
2017-03-02T23:37:30.195972: step 2716, loss 0.0108284, acc 1
2017-03-02T23:37:30.307632: step 2717, loss 0.00468349, acc 1
2017-03-02T23:37:30.403274: step 2718, loss 0.011767, acc 1
2017-03-02T23:37:30.513704: step 2719, loss 0.0142022, acc 1
2017-03-02T23:37:30.624685: step 2720, loss 0.00527784, acc 1
2017-03-02T23:37:30.734931: step 2721, loss 0.0391293, acc 0.984375
2017-03-02T23:37:30.842302: step 2722, loss 0.0169873, acc 1
2017-03-02T23:37:30.946355: step 2723, loss 0.0112431, acc 1
2017-03-02T23:37:31.045027: step 2724, loss 0.0798494, acc 0.984375
2017-03-02T23:37:31.134183: step 2725, loss 0.0560654, acc 0.984375
2017-03-02T23:37:31.223889: step 2726, loss 0.0142126, acc 1
2017-03-02T23:37:31.326449: step 2727, loss 0.0225946, acc 0.984375
2017-03-02T23:37:31.428586: step 2728, loss 0.048048, acc 0.96875
2017-03-02T23:37:31.534526: step 2729, loss 0.0138613, acc 1
2017-03-02T23:37:31.641510: step 2730, loss 0.0795048, acc 0.953125
2017-03-02T23:37:31.753642: step 2731, loss 0.00822459, acc 1
2017-03-02T23:37:31.858246: step 2732, loss 0.00928855, acc 1
2017-03-02T23:37:31.949368: step 2733, loss 0.0255781, acc 1
2017-03-02T23:37:32.054654: step 2734, loss 0.0430984, acc 0.984375
2017-03-02T23:37:32.167354: step 2735, loss 0.0401223, acc 0.984375
2017-03-02T23:37:32.275687: step 2736, loss 0.0242763, acc 0.984375
2017-03-02T23:37:32.385252: step 2737, loss 0.0171515, acc 0.984375
2017-03-02T23:37:32.491142: step 2738, loss 0.00416114, acc 1
2017-03-02T23:37:32.599066: step 2739, loss 0.0529555, acc 0.984375
2017-03-02T23:37:32.690100: step 2740, loss 0.0428611, acc 0.96875
2017-03-02T23:37:32.783545: step 2741, loss 0.0481113, acc 0.984375
2017-03-02T23:37:32.887989: step 2742, loss 0.00796003, acc 1
2017-03-02T23:37:32.989478: step 2743, loss 0.0256085, acc 0.984375
2017-03-02T23:37:33.094388: step 2744, loss 0.0234083, acc 1
2017-03-02T23:37:33.198784: step 2745, loss 0.0768222, acc 0.96875
2017-03-02T23:37:33.303544: step 2746, loss 0.018175, acc 1
2017-03-02T23:37:33.407974: step 2747, loss 0.0368196, acc 0.98
2017-03-02T23:37:33.502449: step 2748, loss 0.0191276, acc 0.984375
2017-03-02T23:37:33.623070: step 2749, loss 0.0138677, acc 1
2017-03-02T23:37:33.731861: step 2750, loss 0.0220798, acc 0.984375
2017-03-02T23:37:33.843147: step 2751, loss 0.0229151, acc 1
2017-03-02T23:37:33.945839: step 2752, loss 0.0564675, acc 0.984375
2017-03-02T23:37:34.051123: step 2753, loss 0.0259221, acc 1
2017-03-02T23:37:34.146284: step 2754, loss 0.0175142, acc 1
2017-03-02T23:37:34.235532: step 2755, loss 0.0489766, acc 0.984375
2017-03-02T23:37:34.339054: step 2756, loss 0.0162322, acc 1
2017-03-02T23:37:34.456458: step 2757, loss 0.0124235, acc 1
2017-03-02T23:37:34.557968: step 2758, loss 0.0258584, acc 1
2017-03-02T23:37:34.680785: step 2759, loss 0.0146792, acc 1
2017-03-02T23:37:34.782874: step 2760, loss 0.00703511, acc 1
2017-03-02T23:37:34.886379: step 2761, loss 0.0123658, acc 1
2017-03-02T23:37:34.975985: step 2762, loss 0.041212, acc 0.984375
2017-03-02T23:37:35.083376: step 2763, loss 0.024951, acc 0.984375
2017-03-02T23:37:35.191857: step 2764, loss 0.0105435, acc 1
2017-03-02T23:37:35.299182: step 2765, loss 0.0242971, acc 1
2017-03-02T23:37:35.408417: step 2766, loss 0.0246216, acc 1
2017-03-02T23:37:35.514485: step 2767, loss 0.0337672, acc 0.984375
2017-03-02T23:37:35.617684: step 2768, loss 0.00752467, acc 1
2017-03-02T23:37:35.709308: step 2769, loss 0.00243455, acc 1
2017-03-02T23:37:35.810272: step 2770, loss 0.0268046, acc 0.984375
2017-03-02T23:37:35.913741: step 2771, loss 0.0676937, acc 0.984375
2017-03-02T23:37:36.022132: step 2772, loss 0.0340341, acc 0.984375
2017-03-02T23:37:36.132093: step 2773, loss 0.0310457, acc 0.984375
2017-03-02T23:37:36.237590: step 2774, loss 0.0618656, acc 0.984375
2017-03-02T23:37:36.349777: step 2775, loss 0.0506309, acc 0.984375
2017-03-02T23:37:36.437948: step 2776, loss 0.0577363, acc 0.96875
2017-03-02T23:37:36.538666: step 2777, loss 0.00856648, acc 1
2017-03-02T23:37:36.644831: step 2778, loss 0.00404212, acc 1
2017-03-02T23:37:36.758360: step 2779, loss 0.0383659, acc 0.96875
2017-03-02T23:37:36.860158: step 2780, loss 0.039018, acc 0.984375
2017-03-02T23:37:36.968332: step 2781, loss 0.01566, acc 1
2017-03-02T23:37:37.081452: step 2782, loss 0.0170912, acc 1
2017-03-02T23:37:37.177155: step 2783, loss 0.0205603, acc 0.984375
2017-03-02T23:37:37.271792: step 2784, loss 0.0172381, acc 1
2017-03-02T23:37:37.378698: step 2785, loss 0.0239916, acc 0.984375
2017-03-02T23:37:37.479429: step 2786, loss 0.015001, acc 1
2017-03-02T23:37:37.582477: step 2787, loss 0.0204447, acc 0.984375
2017-03-02T23:37:37.676511: step 2788, loss 0.0292584, acc 0.98
2017-03-02T23:37:37.784389: step 2789, loss 0.0136214, acc 1
2017-03-02T23:37:37.889143: step 2790, loss 0.115069, acc 0.96875
2017-03-02T23:37:37.978319: step 2791, loss 0.0222491, acc 1
2017-03-02T23:37:38.076181: step 2792, loss 0.017209, acc 1
2017-03-02T23:37:38.179486: step 2793, loss 0.022481, acc 1
2017-03-02T23:37:38.284067: step 2794, loss 0.00639193, acc 1
2017-03-02T23:37:38.392006: step 2795, loss 0.0261694, acc 0.984375
2017-03-02T23:37:38.495592: step 2796, loss 0.0228306, acc 1
2017-03-02T23:37:38.601137: step 2797, loss 0.0120789, acc 1
2017-03-02T23:37:38.690950: step 2798, loss 0.0188971, acc 1
2017-03-02T23:37:38.781542: step 2799, loss 0.0215186, acc 1
2017-03-02T23:37:38.884383: step 2800, loss 0.0697013, acc 0.984375

Evaluation:
2017-03-02T23:37:38.940320: step 2800, loss 2.07774, acc 0.536332

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2800

2017-03-02T23:37:39.378558: step 2801, loss 0.0276963, acc 1
2017-03-02T23:37:39.467689: step 2802, loss 0.0168312, acc 0.984375
2017-03-02T23:37:39.572469: step 2803, loss 0.0226544, acc 1
2017-03-02T23:37:39.672796: step 2804, loss 0.0446054, acc 0.96875
2017-03-02T23:37:39.778125: step 2805, loss 0.0363558, acc 0.984375
2017-03-02T23:37:39.878754: step 2806, loss 0.00684478, acc 1
2017-03-02T23:37:39.981532: step 2807, loss 0.00297996, acc 1
2017-03-02T23:37:40.101894: step 2808, loss 0.0149206, acc 1
2017-03-02T23:37:40.195233: step 2809, loss 0.0363716, acc 0.984375
2017-03-02T23:37:40.298041: step 2810, loss 0.040453, acc 1
2017-03-02T23:37:40.405964: step 2811, loss 0.0269328, acc 0.984375
2017-03-02T23:37:40.508102: step 2812, loss 0.0513701, acc 0.984375
2017-03-02T23:37:40.620892: step 2813, loss 0.0306773, acc 0.984375
2017-03-02T23:37:40.725317: step 2814, loss 0.00829968, acc 1
2017-03-02T23:37:40.826531: step 2815, loss 0.0116051, acc 1
2017-03-02T23:37:40.921697: step 2816, loss 0.0540809, acc 0.984375
2017-03-02T23:37:41.020436: step 2817, loss 0.0166389, acc 1
2017-03-02T23:37:41.125881: step 2818, loss 0.00819749, acc 1
2017-03-02T23:37:41.233640: step 2819, loss 0.0326902, acc 0.984375
2017-03-02T23:37:41.349628: step 2820, loss 0.04064, acc 0.96875
2017-03-02T23:37:41.450830: step 2821, loss 0.0344161, acc 0.984375
2017-03-02T23:37:41.552778: step 2822, loss 0.017004, acc 1
2017-03-02T23:37:41.642393: step 2823, loss 0.00793992, acc 1
2017-03-02T23:37:41.725624: step 2824, loss 0.0288545, acc 0.984375
2017-03-02T23:37:41.829102: step 2825, loss 0.0165011, acc 1
2017-03-02T23:37:41.936933: step 2826, loss 0.133902, acc 0.96875
2017-03-02T23:37:42.049978: step 2827, loss 0.0494287, acc 0.984375
2017-03-02T23:37:42.154337: step 2828, loss 0.00519037, acc 1
2017-03-02T23:37:42.251016: step 2829, loss 0.0043639, acc 1
2017-03-02T23:37:42.355350: step 2830, loss 0.0350869, acc 1
2017-03-02T23:37:42.439688: step 2831, loss 0.0205814, acc 1
2017-03-02T23:37:42.544135: step 2832, loss 0.0136686, acc 1
2017-03-02T23:37:42.652082: step 2833, loss 0.00945475, acc 1
2017-03-02T23:37:42.752580: step 2834, loss 0.0278829, acc 0.984375
2017-03-02T23:37:42.870825: step 2835, loss 0.0270717, acc 0.984375
2017-03-02T23:37:42.983533: step 2836, loss 0.0259939, acc 1
2017-03-02T23:37:43.105250: step 2837, loss 0.0476757, acc 0.984375
2017-03-02T23:37:43.190007: step 2838, loss 0.0464285, acc 0.984375
2017-03-02T23:37:43.291863: step 2839, loss 0.0332963, acc 0.984375
2017-03-02T23:37:43.406564: step 2840, loss 0.0169672, acc 1
2017-03-02T23:37:43.515464: step 2841, loss 0.00730902, acc 1
2017-03-02T23:37:43.621049: step 2842, loss 0.0113384, acc 1
2017-03-02T23:37:43.723904: step 2843, loss 0.00595205, acc 1
2017-03-02T23:37:43.833885: step 2844, loss 0.0377136, acc 0.984375
2017-03-02T23:37:43.923449: step 2845, loss 0.0162965, acc 1
2017-03-02T23:37:44.030818: step 2846, loss 0.0134018, acc 1
2017-03-02T23:37:44.134142: step 2847, loss 0.0200405, acc 0.984375
2017-03-02T23:37:44.245830: step 2848, loss 0.0241282, acc 0.984375
2017-03-02T23:37:44.349176: step 2849, loss 0.0153081, acc 1
2017-03-02T23:37:44.448318: step 2850, loss 0.010596, acc 1
2017-03-02T23:37:44.543859: step 2851, loss 0.0069924, acc 1
2017-03-02T23:37:44.631729: step 2852, loss 0.0456974, acc 0.96875
2017-03-02T23:37:44.724726: step 2853, loss 0.0360919, acc 0.984375
2017-03-02T23:37:44.830335: step 2854, loss 0.0132346, acc 1
2017-03-02T23:37:44.934790: step 2855, loss 0.0359276, acc 0.984375
2017-03-02T23:37:45.043638: step 2856, loss 0.0308422, acc 0.984375
2017-03-02T23:37:45.151404: step 2857, loss 0.00396824, acc 1
2017-03-02T23:37:45.262483: step 2858, loss 0.00867632, acc 1
2017-03-02T23:37:45.361057: step 2859, loss 0.0224143, acc 1
2017-03-02T23:37:45.446833: step 2860, loss 0.0102055, acc 1
2017-03-02T23:37:45.547855: step 2861, loss 0.108578, acc 0.96875
2017-03-02T23:37:45.650820: step 2862, loss 0.0229783, acc 0.984375
2017-03-02T23:37:45.753010: step 2863, loss 0.0277918, acc 1
2017-03-02T23:37:45.859482: step 2864, loss 0.0105485, acc 1
2017-03-02T23:37:45.961208: step 2865, loss 0.00655924, acc 1
2017-03-02T23:37:46.066043: step 2866, loss 0.0121532, acc 1
2017-03-02T23:37:46.160327: step 2867, loss 0.0157745, acc 1
2017-03-02T23:37:46.260183: step 2868, loss 0.00403592, acc 1
2017-03-02T23:37:46.366162: step 2869, loss 0.00540779, acc 1
2017-03-02T23:37:46.453378: step 2870, loss 0.00506486, acc 1
2017-03-02T23:37:46.559440: step 2871, loss 0.00797881, acc 1
2017-03-02T23:37:46.665475: step 2872, loss 0.0321553, acc 0.984375
2017-03-02T23:37:46.775978: step 2873, loss 0.00305912, acc 1
2017-03-02T23:37:46.869201: step 2874, loss 0.0260111, acc 0.984375
2017-03-02T23:37:46.973177: step 2875, loss 0.0343445, acc 0.984375
2017-03-02T23:37:47.076431: step 2876, loss 0.0293812, acc 0.984375
2017-03-02T23:37:47.182131: step 2877, loss 0.0497323, acc 0.984375
2017-03-02T23:37:47.294689: step 2878, loss 0.00413374, acc 1
2017-03-02T23:37:47.388918: step 2879, loss 0.0265414, acc 1
2017-03-02T23:37:47.493862: step 2880, loss 0.0185993, acc 0.984375
2017-03-02T23:37:47.601178: step 2881, loss 0.0149212, acc 1
2017-03-02T23:37:47.696124: step 2882, loss 0.011762, acc 1
2017-03-02T23:37:47.797346: step 2883, loss 0.0186216, acc 1
2017-03-02T23:37:47.900095: step 2884, loss 0.0198647, acc 0.984375
2017-03-02T23:37:48.006710: step 2885, loss 0.104804, acc 0.953125
2017-03-02T23:37:48.111296: step 2886, loss 0.0221391, acc 1
2017-03-02T23:37:48.212108: step 2887, loss 0.0376761, acc 0.984375
2017-03-02T23:37:48.313956: step 2888, loss 0.00831193, acc 1
2017-03-02T23:37:48.408287: step 2889, loss 0.0179446, acc 1
2017-03-02T23:37:48.505985: step 2890, loss 0.0129616, acc 1
2017-03-02T23:37:48.607630: step 2891, loss 0.0199573, acc 1
2017-03-02T23:37:48.705076: step 2892, loss 0.013084, acc 1
2017-03-02T23:37:48.811723: step 2893, loss 0.0159972, acc 1
2017-03-02T23:37:48.922275: step 2894, loss 0.0165884, acc 0.984375
2017-03-02T23:37:49.027404: step 2895, loss 0.0046033, acc 1
2017-03-02T23:37:49.118696: step 2896, loss 0.0233759, acc 1
2017-03-02T23:37:49.201469: step 2897, loss 0.0475871, acc 0.984375
2017-03-02T23:37:49.311842: step 2898, loss 0.0170017, acc 0.984375
2017-03-02T23:37:49.415103: step 2899, loss 0.00959853, acc 1
2017-03-02T23:37:49.525356: step 2900, loss 0.00745521, acc 1

Evaluation:
2017-03-02T23:37:49.582818: step 2900, loss 1.99125, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-2900

2017-03-02T23:37:51.200007: step 2901, loss 0.00885214, acc 1
2017-03-02T23:37:51.306999: step 2902, loss 0.0207444, acc 1
2017-03-02T23:37:51.418469: step 2903, loss 0.0336976, acc 0.984375
2017-03-02T23:37:51.527651: step 2904, loss 0.013305, acc 1
2017-03-02T23:37:51.634335: step 2905, loss 0.0700541, acc 0.984375
2017-03-02T23:37:51.738943: step 2906, loss 0.0598986, acc 0.96875
2017-03-02T23:37:51.845945: step 2907, loss 0.00961914, acc 1
2017-03-02T23:37:51.936361: step 2908, loss 0.0181986, acc 1
2017-03-02T23:37:52.035726: step 2909, loss 0.00670873, acc 1
2017-03-02T23:37:52.154792: step 2910, loss 0.019207, acc 1
2017-03-02T23:37:52.254134: step 2911, loss 0.0308938, acc 0.98
2017-03-02T23:37:52.358921: step 2912, loss 0.0109155, acc 1
2017-03-02T23:37:52.462831: step 2913, loss 0.0147816, acc 1
2017-03-02T23:37:52.566797: step 2914, loss 0.0203835, acc 1
2017-03-02T23:37:52.656914: step 2915, loss 0.00474325, acc 1
2017-03-02T23:37:52.752835: step 2916, loss 0.0682683, acc 0.96875
2017-03-02T23:37:52.857657: step 2917, loss 0.00505297, acc 1
2017-03-02T23:37:52.964996: step 2918, loss 0.0778598, acc 0.96875
2017-03-02T23:37:53.072611: step 2919, loss 0.0787652, acc 0.96875
2017-03-02T23:37:53.192015: step 2920, loss 0.0157532, acc 1
2017-03-02T23:37:53.295915: step 2921, loss 0.0548446, acc 0.984375
2017-03-02T23:37:53.404906: step 2922, loss 0.0491755, acc 0.984375
2017-03-02T23:37:53.498777: step 2923, loss 0.0260746, acc 1
2017-03-02T23:37:53.600204: step 2924, loss 0.00709662, acc 1
2017-03-02T23:37:53.705279: step 2925, loss 0.0168974, acc 1
2017-03-02T23:37:53.820783: step 2926, loss 0.00820722, acc 1
2017-03-02T23:37:53.927753: step 2927, loss 0.00251863, acc 1
2017-03-02T23:37:54.031285: step 2928, loss 0.0308373, acc 0.96875
2017-03-02T23:37:54.133247: step 2929, loss 0.0227683, acc 0.984375
2017-03-02T23:37:54.230186: step 2930, loss 0.0161598, acc 1
2017-03-02T23:37:54.318565: step 2931, loss 0.00793636, acc 1
2017-03-02T23:37:54.425202: step 2932, loss 0.00523319, acc 1
2017-03-02T23:37:54.528395: step 2933, loss 0.0311428, acc 0.984375
2017-03-02T23:37:54.634830: step 2934, loss 0.0192271, acc 1
2017-03-02T23:37:54.737008: step 2935, loss 0.0151716, acc 1
2017-03-02T23:37:54.844869: step 2936, loss 0.0130847, acc 1
2017-03-02T23:37:54.948905: step 2937, loss 0.0241795, acc 0.984375
2017-03-02T23:37:55.037116: step 2938, loss 0.0211798, acc 0.984375
2017-03-02T23:37:55.139306: step 2939, loss 0.0112253, acc 1
2017-03-02T23:37:55.258057: step 2940, loss 0.0204664, acc 1
2017-03-02T23:37:55.365869: step 2941, loss 0.00576714, acc 1
2017-03-02T23:37:55.468931: step 2942, loss 0.0248826, acc 0.984375
2017-03-02T23:37:55.571068: step 2943, loss 0.0127467, acc 1
2017-03-02T23:37:55.680095: step 2944, loss 0.0170888, acc 1
2017-03-02T23:37:55.768187: step 2945, loss 0.0308501, acc 0.984375
2017-03-02T23:37:55.869967: step 2946, loss 0.0215435, acc 1
2017-03-02T23:37:55.983704: step 2947, loss 0.0180529, acc 1
2017-03-02T23:37:56.087791: step 2948, loss 0.0226409, acc 1
2017-03-02T23:37:56.195097: step 2949, loss 0.00498282, acc 1
2017-03-02T23:37:56.297773: step 2950, loss 0.0168156, acc 1
2017-03-02T23:37:56.407430: step 2951, loss 0.00563831, acc 1
2017-03-02T23:37:56.498177: step 2952, loss 0.0200753, acc 1
2017-03-02T23:37:56.615525: step 2953, loss 0.018167, acc 1
2017-03-02T23:37:56.727078: step 2954, loss 0.00533735, acc 1
2017-03-02T23:37:56.832202: step 2955, loss 0.0247308, acc 0.984375
2017-03-02T23:37:56.937363: step 2956, loss 0.00930769, acc 1
2017-03-02T23:37:57.045570: step 2957, loss 0.00435037, acc 1
2017-03-02T23:37:57.149744: step 2958, loss 0.0184735, acc 1
2017-03-02T23:37:57.237882: step 2959, loss 0.0735161, acc 0.96875
2017-03-02T23:37:57.329266: step 2960, loss 0.0115487, acc 1
2017-03-02T23:37:57.430031: step 2961, loss 0.0226802, acc 1
2017-03-02T23:37:57.531005: step 2962, loss 0.010865, acc 1
2017-03-02T23:37:57.636472: step 2963, loss 0.0129698, acc 1
2017-03-02T23:37:57.740696: step 2964, loss 0.0133935, acc 1
2017-03-02T23:37:57.841773: step 2965, loss 0.0356819, acc 0.984375
2017-03-02T23:37:57.946533: step 2966, loss 0.00924203, acc 1
2017-03-02T23:37:58.043166: step 2967, loss 0.0140441, acc 1
2017-03-02T23:37:58.147874: step 2968, loss 0.041061, acc 0.96875
2017-03-02T23:37:58.257572: step 2969, loss 0.107543, acc 0.984375
2017-03-02T23:37:58.360384: step 2970, loss 0.0231914, acc 0.984375
2017-03-02T23:37:58.468891: step 2971, loss 0.0298765, acc 0.984375
2017-03-02T23:37:58.570034: step 2972, loss 0.0246303, acc 1
2017-03-02T23:37:58.677537: step 2973, loss 0.0240305, acc 1
2017-03-02T23:37:58.765767: step 2974, loss 0.00331095, acc 1
2017-03-02T23:37:58.881966: step 2975, loss 0.00714784, acc 1
2017-03-02T23:37:58.992630: step 2976, loss 0.00299686, acc 1
2017-03-02T23:37:59.104388: step 2977, loss 0.0105522, acc 1
2017-03-02T23:37:59.221061: step 2978, loss 0.0107942, acc 1
2017-03-02T23:37:59.326011: step 2979, loss 0.0121377, acc 1
2017-03-02T23:37:59.439441: step 2980, loss 0.0419975, acc 0.984375
2017-03-02T23:37:59.545004: step 2981, loss 0.00592844, acc 1
2017-03-02T23:37:59.647777: step 2982, loss 0.0256052, acc 0.984375
2017-03-02T23:37:59.759560: step 2983, loss 0.0960971, acc 0.984375
2017-03-02T23:37:59.864764: step 2984, loss 0.00757591, acc 1
2017-03-02T23:37:59.981443: step 2985, loss 0.0117708, acc 1
2017-03-02T23:38:00.084816: step 2986, loss 0.0142245, acc 1
2017-03-02T23:38:00.191251: step 2987, loss 0.0124727, acc 1
2017-03-02T23:38:00.277319: step 2988, loss 0.00698967, acc 1
2017-03-02T23:38:00.381740: step 2989, loss 0.0205803, acc 1
2017-03-02T23:38:00.480455: step 2990, loss 0.0135188, acc 1
2017-03-02T23:38:00.583391: step 2991, loss 0.0349054, acc 0.984375
2017-03-02T23:38:00.685736: step 2992, loss 0.00875085, acc 1
2017-03-02T23:38:00.782578: step 2993, loss 0.0250726, acc 0.98
2017-03-02T23:38:00.884570: step 2994, loss 0.0254735, acc 1
2017-03-02T23:38:00.982330: step 2995, loss 0.00667536, acc 1
2017-03-02T23:38:01.071549: step 2996, loss 0.00669009, acc 1
2017-03-02T23:38:01.172807: step 2997, loss 0.00621741, acc 1
2017-03-02T23:38:01.277851: step 2998, loss 0.0125958, acc 1
2017-03-02T23:38:01.377087: step 2999, loss 0.0168544, acc 1
2017-03-02T23:38:01.484842: step 3000, loss 0.00327967, acc 1

Evaluation:
2017-03-02T23:38:01.548115: step 3000, loss 1.9854, acc 0.550173

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3000

2017-03-02T23:38:01.999379: step 3001, loss 0.00845125, acc 1
2017-03-02T23:38:02.120348: step 3002, loss 0.0393556, acc 0.984375
2017-03-02T23:38:02.223744: step 3003, loss 0.0172958, acc 1
2017-03-02T23:38:02.332596: step 3004, loss 0.00787794, acc 1
2017-03-02T23:38:02.435667: step 3005, loss 0.0033264, acc 1
2017-03-02T23:38:02.523589: step 3006, loss 0.0511911, acc 0.984375
2017-03-02T23:38:02.625005: step 3007, loss 0.0152516, acc 0.984375
2017-03-02T23:38:02.728386: step 3008, loss 0.0542771, acc 0.984375
2017-03-02T23:38:02.833762: step 3009, loss 0.0217356, acc 1
2017-03-02T23:38:02.941442: step 3010, loss 0.00887301, acc 1
2017-03-02T23:38:03.046571: step 3011, loss 0.122344, acc 0.96875
2017-03-02T23:38:03.156114: step 3012, loss 0.0212094, acc 0.984375
2017-03-02T23:38:03.258577: step 3013, loss 0.00514874, acc 1
2017-03-02T23:38:03.363957: step 3014, loss 0.00435268, acc 1
2017-03-02T23:38:03.467914: step 3015, loss 0.0123213, acc 1
2017-03-02T23:38:03.573154: step 3016, loss 0.0131693, acc 1
2017-03-02T23:38:03.693876: step 3017, loss 0.0248649, acc 0.984375
2017-03-02T23:38:03.798005: step 3018, loss 0.0221396, acc 1
2017-03-02T23:38:03.899176: step 3019, loss 0.0314833, acc 0.984375
2017-03-02T23:38:03.986175: step 3020, loss 0.00509291, acc 1
2017-03-02T23:38:04.088331: step 3021, loss 0.00535979, acc 1
2017-03-02T23:38:04.200840: step 3022, loss 0.00435849, acc 1
2017-03-02T23:38:04.313660: step 3023, loss 0.0101969, acc 1
2017-03-02T23:38:04.414177: step 3024, loss 0.0116729, acc 1
2017-03-02T23:38:04.523124: step 3025, loss 0.0128228, acc 1
2017-03-02T23:38:04.626087: step 3026, loss 0.00977106, acc 1
2017-03-02T23:38:04.720592: step 3027, loss 0.00343865, acc 1
2017-03-02T23:38:04.809008: step 3028, loss 0.00442369, acc 1
2017-03-02T23:38:04.914087: step 3029, loss 0.00278858, acc 1
2017-03-02T23:38:05.018454: step 3030, loss 0.0144139, acc 1
2017-03-02T23:38:05.123943: step 3031, loss 0.0394191, acc 0.984375
2017-03-02T23:38:05.227181: step 3032, loss 0.0142498, acc 1
2017-03-02T23:38:05.331751: step 3033, loss 0.0379397, acc 0.984375
2017-03-02T23:38:05.429119: step 3034, loss 0.00462724, acc 1
2017-03-02T23:38:05.521696: step 3035, loss 0.0205368, acc 1
2017-03-02T23:38:05.627123: step 3036, loss 0.00850574, acc 1
2017-03-02T23:38:05.732534: step 3037, loss 0.01169, acc 1
2017-03-02T23:38:05.839284: step 3038, loss 0.0038291, acc 1
2017-03-02T23:38:05.939623: step 3039, loss 0.00643791, acc 1
2017-03-02T23:38:06.046366: step 3040, loss 0.0315003, acc 0.984375
2017-03-02T23:38:06.152932: step 3041, loss 0.00587237, acc 1
2017-03-02T23:38:06.241924: step 3042, loss 0.00245943, acc 1
2017-03-02T23:38:06.342287: step 3043, loss 0.011228, acc 1
2017-03-02T23:38:06.453804: step 3044, loss 0.0369631, acc 1
2017-03-02T23:38:06.564720: step 3045, loss 0.00582607, acc 1
2017-03-02T23:38:06.671410: step 3046, loss 0.00648053, acc 1
2017-03-02T23:38:06.780174: step 3047, loss 0.0222122, acc 0.984375
2017-03-02T23:38:06.886414: step 3048, loss 0.015994, acc 1
2017-03-02T23:38:06.980653: step 3049, loss 0.0622179, acc 0.96875
2017-03-02T23:38:07.087316: step 3050, loss 0.00409694, acc 1
2017-03-02T23:38:07.204736: step 3051, loss 0.00774605, acc 1
2017-03-02T23:38:07.308719: step 3052, loss 0.00923807, acc 1
2017-03-02T23:38:07.413129: step 3053, loss 0.0296821, acc 1
2017-03-02T23:38:07.520706: step 3054, loss 0.00958179, acc 1
2017-03-02T23:38:07.627083: step 3055, loss 0.0418417, acc 0.984375
2017-03-02T23:38:07.742193: step 3056, loss 0.00708629, acc 1
2017-03-02T23:38:07.834400: step 3057, loss 0.0664039, acc 0.984375
2017-03-02T23:38:07.939492: step 3058, loss 0.0116296, acc 1
2017-03-02T23:38:08.048673: step 3059, loss 0.0180195, acc 1
2017-03-02T23:38:08.155801: step 3060, loss 0.00302819, acc 1
2017-03-02T23:38:08.267860: step 3061, loss 0.0486646, acc 0.96875
2017-03-02T23:38:08.379214: step 3062, loss 0.00921354, acc 1
2017-03-02T23:38:08.480710: step 3063, loss 0.0125016, acc 1
2017-03-02T23:38:08.585179: step 3064, loss 0.073366, acc 0.96875
2017-03-02T23:38:08.688516: step 3065, loss 0.0123138, acc 1
2017-03-02T23:38:08.806505: step 3066, loss 0.011783, acc 1
2017-03-02T23:38:08.911442: step 3067, loss 0.0125266, acc 1
2017-03-02T23:38:09.013871: step 3068, loss 0.0322911, acc 0.984375
2017-03-02T23:38:09.122210: step 3069, loss 0.0449203, acc 0.96875
2017-03-02T23:38:09.228824: step 3070, loss 0.0310397, acc 0.984375
2017-03-02T23:38:09.314432: step 3071, loss 0.0374643, acc 1
2017-03-02T23:38:09.424845: step 3072, loss 0.00711552, acc 1
2017-03-02T23:38:09.535640: step 3073, loss 0.0214453, acc 1
2017-03-02T23:38:09.638902: step 3074, loss 0.0451971, acc 0.984375
2017-03-02T23:38:09.736493: step 3075, loss 0.0271554, acc 1
2017-03-02T23:38:09.843789: step 3076, loss 0.0106485, acc 1
2017-03-02T23:38:09.957018: step 3077, loss 0.0047728, acc 1
2017-03-02T23:38:10.050648: step 3078, loss 0.00184048, acc 1
2017-03-02T23:38:10.155510: step 3079, loss 0.00314239, acc 1
2017-03-02T23:38:10.258899: step 3080, loss 0.00380569, acc 1
2017-03-02T23:38:10.371564: step 3081, loss 0.00708761, acc 1
2017-03-02T23:38:10.473256: step 3082, loss 0.00878054, acc 1
2017-03-02T23:38:10.575675: step 3083, loss 0.00959662, acc 1
2017-03-02T23:38:10.676630: step 3084, loss 0.0197823, acc 1
2017-03-02T23:38:10.771797: step 3085, loss 0.00960673, acc 1
2017-03-02T23:38:10.884188: step 3086, loss 0.0218821, acc 0.984375
2017-03-02T23:38:10.989446: step 3087, loss 0.0175232, acc 1
2017-03-02T23:38:11.095527: step 3088, loss 0.0155716, acc 1
2017-03-02T23:38:11.203555: step 3089, loss 0.0249138, acc 0.984375
2017-03-02T23:38:11.309503: step 3090, loss 0.0377238, acc 0.984375
2017-03-02T23:38:11.430849: step 3091, loss 0.0286776, acc 0.984375
2017-03-02T23:38:11.519688: step 3092, loss 0.0164033, acc 1
2017-03-02T23:38:11.611202: step 3093, loss 0.0421951, acc 0.984375
2017-03-02T23:38:11.723616: step 3094, loss 0.00297687, acc 1
2017-03-02T23:38:11.830439: step 3095, loss 0.00451663, acc 1
2017-03-02T23:38:11.936910: step 3096, loss 0.0426773, acc 0.96875
2017-03-02T23:38:12.047897: step 3097, loss 0.0290012, acc 0.984375
2017-03-02T23:38:12.146364: step 3098, loss 0.00489306, acc 1
2017-03-02T23:38:12.248539: step 3099, loss 0.0666842, acc 0.96875
2017-03-02T23:38:12.328333: step 3100, loss 0.00279636, acc 1

Evaluation:
2017-03-02T23:38:12.382006: step 3100, loss 1.99151, acc 0.550173

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3100

2017-03-02T23:38:12.854345: step 3101, loss 0.0186199, acc 1
2017-03-02T23:38:12.959829: step 3102, loss 0.0537218, acc 0.984375
2017-03-02T23:38:13.050941: step 3103, loss 0.00844664, acc 1
2017-03-02T23:38:13.168808: step 3104, loss 0.0094844, acc 1
2017-03-02T23:38:13.273882: step 3105, loss 0.00889156, acc 1
2017-03-02T23:38:13.376587: step 3106, loss 0.00460364, acc 1
2017-03-02T23:38:13.478764: step 3107, loss 0.00645588, acc 1
2017-03-02T23:38:13.586524: step 3108, loss 0.00560167, acc 1
2017-03-02T23:38:13.694765: step 3109, loss 0.0263194, acc 0.984375
2017-03-02T23:38:13.786626: step 3110, loss 0.00466434, acc 1
2017-03-02T23:38:13.895313: step 3111, loss 0.00792339, acc 1
2017-03-02T23:38:14.004333: step 3112, loss 0.016888, acc 1
2017-03-02T23:38:14.112517: step 3113, loss 0.00928083, acc 1
2017-03-02T23:38:14.219637: step 3114, loss 0.00752645, acc 1
2017-03-02T23:38:14.325840: step 3115, loss 0.0425806, acc 0.984375
2017-03-02T23:38:14.415293: step 3116, loss 0.00643403, acc 1
2017-03-02T23:38:14.507143: step 3117, loss 0.0101677, acc 1
2017-03-02T23:38:14.615937: step 3118, loss 0.0138865, acc 1
2017-03-02T23:38:14.739834: step 3119, loss 0.00627984, acc 1
2017-03-02T23:38:14.847645: step 3120, loss 0.0101092, acc 1
2017-03-02T23:38:14.951410: step 3121, loss 0.0151712, acc 1
2017-03-02T23:38:15.054435: step 3122, loss 0.0293657, acc 0.984375
2017-03-02T23:38:15.150355: step 3123, loss 0.024172, acc 0.984375
2017-03-02T23:38:15.242376: step 3124, loss 0.0251544, acc 0.984375
2017-03-02T23:38:15.348733: step 3125, loss 0.00717623, acc 1
2017-03-02T23:38:15.455893: step 3126, loss 0.00283277, acc 1
2017-03-02T23:38:15.559008: step 3127, loss 0.0409398, acc 0.984375
2017-03-02T23:38:15.670033: step 3128, loss 0.00851851, acc 1
2017-03-02T23:38:15.775433: step 3129, loss 0.00814633, acc 1
2017-03-02T23:38:15.882494: step 3130, loss 0.0194333, acc 0.984375
2017-03-02T23:38:15.979285: step 3131, loss 0.00632418, acc 1
2017-03-02T23:38:16.087129: step 3132, loss 0.0125711, acc 1
2017-03-02T23:38:16.199250: step 3133, loss 0.00864622, acc 1
2017-03-02T23:38:16.314806: step 3134, loss 0.00482548, acc 1
2017-03-02T23:38:16.424297: step 3135, loss 0.0088411, acc 1
2017-03-02T23:38:16.530366: step 3136, loss 0.00310444, acc 1
2017-03-02T23:38:16.644980: step 3137, loss 0.00662977, acc 1
2017-03-02T23:38:16.729308: step 3138, loss 0.0350313, acc 0.984375
2017-03-02T23:38:16.832296: step 3139, loss 0.00303429, acc 1
2017-03-02T23:38:16.940913: step 3140, loss 0.00950875, acc 1
2017-03-02T23:38:17.057132: step 3141, loss 0.0140621, acc 1
2017-03-02T23:38:17.160541: step 3142, loss 0.0195373, acc 0.984375
2017-03-02T23:38:17.264861: step 3143, loss 0.00801247, acc 1
2017-03-02T23:38:17.373500: step 3144, loss 0.00427182, acc 1
2017-03-02T23:38:17.488521: step 3145, loss 0.0305562, acc 0.984375
2017-03-02T23:38:17.582622: step 3146, loss 0.0128806, acc 1
2017-03-02T23:38:17.685318: step 3147, loss 0.00761644, acc 1
2017-03-02T23:38:17.791631: step 3148, loss 0.01059, acc 1
2017-03-02T23:38:17.899982: step 3149, loss 0.0275959, acc 0.984375
2017-03-02T23:38:18.005582: step 3150, loss 0.014585, acc 1
2017-03-02T23:38:18.104594: step 3151, loss 0.01518, acc 0.984375
2017-03-02T23:38:18.214744: step 3152, loss 0.0262281, acc 0.984375
2017-03-02T23:38:18.312065: step 3153, loss 0.0291905, acc 0.984375
2017-03-02T23:38:18.435798: step 3154, loss 0.00602757, acc 1
2017-03-02T23:38:18.540545: step 3155, loss 0.0186837, acc 0.984375
2017-03-02T23:38:18.646323: step 3156, loss 0.00291215, acc 1
2017-03-02T23:38:18.740430: step 3157, loss 0.00316936, acc 1
2017-03-02T23:38:18.846819: step 3158, loss 0.00761077, acc 1
2017-03-02T23:38:18.963279: step 3159, loss 0.00692001, acc 1
2017-03-02T23:38:19.054932: step 3160, loss 0.0188936, acc 1
2017-03-02T23:38:19.156403: step 3161, loss 0.00641472, acc 1
2017-03-02T23:38:19.263156: step 3162, loss 0.0109289, acc 1
2017-03-02T23:38:19.369313: step 3163, loss 0.00264446, acc 1
2017-03-02T23:38:19.472461: step 3164, loss 0.00521812, acc 1
2017-03-02T23:38:19.574060: step 3165, loss 0.00597446, acc 1
2017-03-02T23:38:19.677908: step 3166, loss 0.0456223, acc 0.984375
2017-03-02T23:38:19.778743: step 3167, loss 0.00741726, acc 1
2017-03-02T23:38:19.880375: step 3168, loss 0.0190337, acc 1
2017-03-02T23:38:19.981551: step 3169, loss 0.00355622, acc 1
2017-03-02T23:38:20.087146: step 3170, loss 0.00800111, acc 1
2017-03-02T23:38:20.195408: step 3171, loss 0.0144361, acc 1
2017-03-02T23:38:20.306440: step 3172, loss 0.0156845, acc 1
2017-03-02T23:38:20.418741: step 3173, loss 0.00604069, acc 1
2017-03-02T23:38:20.508086: step 3174, loss 0.00527801, acc 1
2017-03-02T23:38:20.610604: step 3175, loss 0.00487181, acc 1
2017-03-02T23:38:20.717876: step 3176, loss 0.0207934, acc 1
2017-03-02T23:38:20.830110: step 3177, loss 0.00884813, acc 1
2017-03-02T23:38:20.935447: step 3178, loss 0.0173064, acc 1
2017-03-02T23:38:21.036410: step 3179, loss 0.00322797, acc 1
2017-03-02T23:38:21.127954: step 3180, loss 0.0172982, acc 1
2017-03-02T23:38:21.224590: step 3181, loss 0.0107798, acc 1
2017-03-02T23:38:21.321355: step 3182, loss 0.0218596, acc 0.984375
2017-03-02T23:38:21.419916: step 3183, loss 0.0240558, acc 0.984375
2017-03-02T23:38:21.523932: step 3184, loss 0.0130117, acc 1
2017-03-02T23:38:21.636947: step 3185, loss 0.0133682, acc 1
2017-03-02T23:38:21.742836: step 3186, loss 0.0140689, acc 1
2017-03-02T23:38:21.849381: step 3187, loss 0.0027451, acc 1
2017-03-02T23:38:21.955925: step 3188, loss 0.00786406, acc 1
2017-03-02T23:38:22.049839: step 3189, loss 0.00648178, acc 1
2017-03-02T23:38:22.155951: step 3190, loss 0.015458, acc 1
2017-03-02T23:38:22.259382: step 3191, loss 0.0127797, acc 1
2017-03-02T23:38:22.367419: step 3192, loss 0.00188109, acc 1
2017-03-02T23:38:22.478672: step 3193, loss 0.002708, acc 1
2017-03-02T23:38:22.583221: step 3194, loss 0.0237144, acc 1
2017-03-02T23:38:22.691645: step 3195, loss 0.0276949, acc 0.984375
2017-03-02T23:38:22.786027: step 3196, loss 0.00815534, acc 1
2017-03-02T23:38:22.890067: step 3197, loss 0.0789929, acc 0.984375
2017-03-02T23:38:22.988307: step 3198, loss 0.00607015, acc 1
2017-03-02T23:38:23.095778: step 3199, loss 0.0196946, acc 1
2017-03-02T23:38:23.196623: step 3200, loss 0.00462944, acc 1

Evaluation:
2017-03-02T23:38:23.248994: step 3200, loss 1.94758, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3200

2017-03-02T23:38:23.677341: step 3201, loss 0.0195301, acc 1
2017-03-02T23:38:23.783836: step 3202, loss 0.0151468, acc 1
2017-03-02T23:38:23.897815: step 3203, loss 0.00669714, acc 1
2017-03-02T23:38:24.005550: step 3204, loss 0.00867695, acc 1
2017-03-02T23:38:24.118936: step 3205, loss 0.0155939, acc 1
2017-03-02T23:38:24.216101: step 3206, loss 0.0658686, acc 0.984375
2017-03-02T23:38:24.303125: step 3207, loss 0.0227096, acc 0.984375
2017-03-02T23:38:24.419363: step 3208, loss 0.0100293, acc 1
2017-03-02T23:38:24.525598: step 3209, loss 0.00764711, acc 1
2017-03-02T23:38:24.631087: step 3210, loss 0.0163906, acc 1
2017-03-02T23:38:24.729765: step 3211, loss 0.00179507, acc 1
2017-03-02T23:38:24.830200: step 3212, loss 0.0167597, acc 1
2017-03-02T23:38:24.934857: step 3213, loss 0.00950301, acc 1
2017-03-02T23:38:25.022536: step 3214, loss 0.0218815, acc 0.984375
2017-03-02T23:38:25.122804: step 3215, loss 0.03814, acc 0.984375
2017-03-02T23:38:25.230086: step 3216, loss 0.00236911, acc 1
2017-03-02T23:38:25.338882: step 3217, loss 0.0161474, acc 1
2017-03-02T23:38:25.446270: step 3218, loss 0.00516895, acc 1
2017-03-02T23:38:25.548841: step 3219, loss 0.0122534, acc 1
2017-03-02T23:38:25.671397: step 3220, loss 0.014438, acc 1
2017-03-02T23:38:25.759805: step 3221, loss 0.0163855, acc 1
2017-03-02T23:38:25.863226: step 3222, loss 0.00819392, acc 1
2017-03-02T23:38:25.968295: step 3223, loss 0.0032397, acc 1
2017-03-02T23:38:26.073976: step 3224, loss 0.00369451, acc 1
2017-03-02T23:38:26.182862: step 3225, loss 0.00618345, acc 1
2017-03-02T23:38:26.289247: step 3226, loss 0.0114033, acc 1
2017-03-02T23:38:26.396796: step 3227, loss 0.00691968, acc 1
2017-03-02T23:38:26.489180: step 3228, loss 0.00417863, acc 1
2017-03-02T23:38:26.586094: step 3229, loss 0.0021391, acc 1
2017-03-02T23:38:26.693986: step 3230, loss 0.0160041, acc 1
2017-03-02T23:38:26.795059: step 3231, loss 0.00530155, acc 1
2017-03-02T23:38:26.900379: step 3232, loss 0.0675822, acc 0.984375
2017-03-02T23:38:27.007039: step 3233, loss 0.00470074, acc 1
2017-03-02T23:38:27.118176: step 3234, loss 0.0276997, acc 0.984375
2017-03-02T23:38:27.228308: step 3235, loss 0.0184702, acc 1
2017-03-02T23:38:27.320071: step 3236, loss 0.0105845, acc 1
2017-03-02T23:38:27.427917: step 3237, loss 0.00620803, acc 1
2017-03-02T23:38:27.524300: step 3238, loss 0.017347, acc 1
2017-03-02T23:38:27.619729: step 3239, loss 0.0141439, acc 1
2017-03-02T23:38:27.735201: step 3240, loss 0.0152037, acc 1
2017-03-02T23:38:27.850731: step 3241, loss 0.0201976, acc 0.984375
2017-03-02T23:38:27.961756: step 3242, loss 0.00682658, acc 1
2017-03-02T23:38:28.065464: step 3243, loss 0.0136492, acc 1
2017-03-02T23:38:28.166063: step 3244, loss 0.0190955, acc 0.984375
2017-03-02T23:38:28.271432: step 3245, loss 0.0327548, acc 0.984375
2017-03-02T23:38:28.383926: step 3246, loss 0.0109157, acc 1
2017-03-02T23:38:28.488350: step 3247, loss 0.0189276, acc 0.984375
2017-03-02T23:38:28.598499: step 3248, loss 0.00688926, acc 1
2017-03-02T23:38:28.708088: step 3249, loss 0.00903856, acc 1
2017-03-02T23:38:28.802960: step 3250, loss 0.0101341, acc 1
2017-03-02T23:38:28.909762: step 3251, loss 0.0065925, acc 1
2017-03-02T23:38:29.015023: step 3252, loss 0.00681358, acc 1
2017-03-02T23:38:29.120672: step 3253, loss 0.00554827, acc 1
2017-03-02T23:38:29.221428: step 3254, loss 0.0253422, acc 1
2017-03-02T23:38:29.326892: step 3255, loss 0.076163, acc 0.984375
2017-03-02T23:38:29.434312: step 3256, loss 0.00635227, acc 1
2017-03-02T23:38:29.521485: step 3257, loss 0.00381138, acc 1
2017-03-02T23:38:29.612774: step 3258, loss 0.00777015, acc 1
2017-03-02T23:38:29.718573: step 3259, loss 0.0161176, acc 1
2017-03-02T23:38:29.821530: step 3260, loss 0.0495895, acc 0.984375
2017-03-02T23:38:29.930569: step 3261, loss 0.0209822, acc 1
2017-03-02T23:38:30.039865: step 3262, loss 0.00736301, acc 1
2017-03-02T23:38:30.142741: step 3263, loss 0.021926, acc 0.984375
2017-03-02T23:38:30.246488: step 3264, loss 0.039508, acc 0.984375
2017-03-02T23:38:30.337546: step 3265, loss 0.0104165, acc 1
2017-03-02T23:38:30.442557: step 3266, loss 0.0051038, acc 1
2017-03-02T23:38:30.545123: step 3267, loss 0.0174551, acc 0.984375
2017-03-02T23:38:30.650642: step 3268, loss 0.0105098, acc 1
2017-03-02T23:38:30.753654: step 3269, loss 0.0172465, acc 1
2017-03-02T23:38:30.852203: step 3270, loss 0.00842358, acc 1
2017-03-02T23:38:30.956146: step 3271, loss 0.0151519, acc 1
2017-03-02T23:38:31.049640: step 3272, loss 0.0157878, acc 1
2017-03-02T23:38:31.150796: step 3273, loss 0.0207216, acc 1
2017-03-02T23:38:31.250644: step 3274, loss 0.0232242, acc 0.984375
2017-03-02T23:38:31.355005: step 3275, loss 0.00105485, acc 1
2017-03-02T23:38:31.458781: step 3276, loss 0.00831623, acc 1
2017-03-02T23:38:31.572711: step 3277, loss 0.00285771, acc 1
2017-03-02T23:38:31.678134: step 3278, loss 0.0130445, acc 1
2017-03-02T23:38:31.772785: step 3279, loss 0.0181493, acc 0.984375
2017-03-02T23:38:31.855761: step 3280, loss 0.00215712, acc 1
2017-03-02T23:38:31.953315: step 3281, loss 0.0479997, acc 0.984375
2017-03-02T23:38:32.060611: step 3282, loss 0.00586518, acc 1
2017-03-02T23:38:32.167586: step 3283, loss 0.0277641, acc 0.984375
2017-03-02T23:38:32.283479: step 3284, loss 0.00541173, acc 1
2017-03-02T23:38:32.390131: step 3285, loss 0.00409286, acc 1
2017-03-02T23:38:32.505137: step 3286, loss 0.0028322, acc 1
2017-03-02T23:38:32.597369: step 3287, loss 0.00256542, acc 1
2017-03-02T23:38:32.696222: step 3288, loss 0.00699596, acc 1
2017-03-02T23:38:32.814231: step 3289, loss 0.0207848, acc 1
2017-03-02T23:38:32.922501: step 3290, loss 0.00260788, acc 1
2017-03-02T23:38:33.028203: step 3291, loss 0.00492282, acc 1
2017-03-02T23:38:33.134307: step 3292, loss 0.00582812, acc 1
2017-03-02T23:38:33.239132: step 3293, loss 0.00994642, acc 1
2017-03-02T23:38:33.319310: step 3294, loss 0.0246133, acc 1
2017-03-02T23:38:33.424658: step 3295, loss 0.00558561, acc 1
2017-03-02T23:38:33.526122: step 3296, loss 0.0231833, acc 0.984375
2017-03-02T23:38:33.628413: step 3297, loss 0.00265735, acc 1
2017-03-02T23:38:33.727217: step 3298, loss 0.00523791, acc 1
2017-03-02T23:38:33.830082: step 3299, loss 0.0197779, acc 1
2017-03-02T23:38:33.935104: step 3300, loss 0.00732163, acc 1

Evaluation:
2017-03-02T23:38:33.986543: step 3300, loss 2.11818, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3300

2017-03-02T23:38:34.457792: step 3301, loss 0.00591688, acc 1
2017-03-02T23:38:34.559507: step 3302, loss 0.0226345, acc 0.984375
2017-03-02T23:38:34.657062: step 3303, loss 0.00595697, acc 1
2017-03-02T23:38:34.752209: step 3304, loss 0.00281459, acc 1
2017-03-02T23:38:34.846728: step 3305, loss 0.00897308, acc 1
2017-03-02T23:38:34.952341: step 3306, loss 0.00360971, acc 1
2017-03-02T23:38:35.059936: step 3307, loss 0.00362701, acc 1
2017-03-02T23:38:35.167192: step 3308, loss 0.0913324, acc 0.984375
2017-03-02T23:38:35.269433: step 3309, loss 0.0128506, acc 1
2017-03-02T23:38:35.374713: step 3310, loss 0.00398675, acc 1
2017-03-02T23:38:35.464123: step 3311, loss 0.00751067, acc 1
2017-03-02T23:38:35.556709: step 3312, loss 0.0129649, acc 1
2017-03-02T23:38:35.664897: step 3313, loss 0.0120432, acc 1
2017-03-02T23:38:35.764497: step 3314, loss 0.0099602, acc 1
2017-03-02T23:38:35.868969: step 3315, loss 0.0289484, acc 0.984375
2017-03-02T23:38:35.977072: step 3316, loss 0.0025066, acc 1
2017-03-02T23:38:36.078412: step 3317, loss 0.00643265, acc 1
2017-03-02T23:38:36.186342: step 3318, loss 0.00963429, acc 1
2017-03-02T23:38:36.273762: step 3319, loss 0.0255495, acc 0.984375
2017-03-02T23:38:36.372719: step 3320, loss 0.0360172, acc 0.984375
2017-03-02T23:38:36.483130: step 3321, loss 0.00290862, acc 1
2017-03-02T23:38:36.596208: step 3322, loss 0.0113051, acc 1
2017-03-02T23:38:36.697849: step 3323, loss 0.000870643, acc 1
2017-03-02T23:38:36.800859: step 3324, loss 0.0158303, acc 1
2017-03-02T23:38:36.910733: step 3325, loss 0.00882708, acc 1
2017-03-02T23:38:36.996467: step 3326, loss 0.0057637, acc 1
2017-03-02T23:38:37.078657: step 3327, loss 0.0443327, acc 0.984375
2017-03-02T23:38:37.175256: step 3328, loss 0.00647847, acc 1
2017-03-02T23:38:37.279648: step 3329, loss 0.0202739, acc 1
2017-03-02T23:38:37.380216: step 3330, loss 0.0457481, acc 0.984375
2017-03-02T23:38:37.487150: step 3331, loss 0.00323036, acc 1
2017-03-02T23:38:37.592580: step 3332, loss 0.0196326, acc 0.984375
2017-03-02T23:38:37.695043: step 3333, loss 0.00425878, acc 1
2017-03-02T23:38:37.785007: step 3334, loss 0.00968067, acc 1
2017-03-02T23:38:37.885437: step 3335, loss 0.0107476, acc 1
2017-03-02T23:38:37.993822: step 3336, loss 0.00285232, acc 1
2017-03-02T23:38:38.112358: step 3337, loss 0.0329782, acc 0.984375
2017-03-02T23:38:38.217352: step 3338, loss 0.0394927, acc 0.984375
2017-03-02T23:38:38.317830: step 3339, loss 0.0526941, acc 0.984375
2017-03-02T23:38:38.423767: step 3340, loss 0.0040061, acc 1
2017-03-02T23:38:38.512818: step 3341, loss 0.0128248, acc 1
2017-03-02T23:38:38.604132: step 3342, loss 0.0169132, acc 0.984375
2017-03-02T23:38:38.698872: step 3343, loss 0.0151352, acc 0.984375
2017-03-02T23:38:38.804376: step 3344, loss 0.0151218, acc 1
2017-03-02T23:38:38.913027: step 3345, loss 0.00425851, acc 1
2017-03-02T23:38:39.018294: step 3346, loss 0.00526028, acc 1
2017-03-02T23:38:39.142841: step 3347, loss 0.00508382, acc 1
2017-03-02T23:38:39.244470: step 3348, loss 0.0071016, acc 1
2017-03-02T23:38:39.344891: step 3349, loss 0.0242618, acc 1
2017-03-02T23:38:39.447481: step 3350, loss 0.0135957, acc 1
2017-03-02T23:38:39.557279: step 3351, loss 0.00304581, acc 1
2017-03-02T23:38:39.684425: step 3352, loss 0.0127228, acc 1
2017-03-02T23:38:39.784599: step 3353, loss 0.0281183, acc 0.984375
2017-03-02T23:38:39.890014: step 3354, loss 0.0375212, acc 0.984375
2017-03-02T23:38:39.979065: step 3355, loss 0.0107662, acc 1
2017-03-02T23:38:40.077595: step 3356, loss 0.0276621, acc 0.984375
2017-03-02T23:38:40.182040: step 3357, loss 0.00632647, acc 1
2017-03-02T23:38:40.292065: step 3358, loss 0.00272871, acc 1
2017-03-02T23:38:40.400151: step 3359, loss 0.0106065, acc 1
2017-03-02T23:38:40.511264: step 3360, loss 0.00409219, acc 1
2017-03-02T23:38:40.615371: step 3361, loss 0.0228493, acc 0.984375
2017-03-02T23:38:40.716237: step 3362, loss 0.037936, acc 0.98
2017-03-02T23:38:40.812715: step 3363, loss 0.00574557, acc 1
2017-03-02T23:38:40.916580: step 3364, loss 0.00705996, acc 1
2017-03-02T23:38:41.021907: step 3365, loss 0.00579464, acc 1
2017-03-02T23:38:41.125097: step 3366, loss 0.00299437, acc 1
2017-03-02T23:38:41.231867: step 3367, loss 0.00624852, acc 1
2017-03-02T23:38:41.332598: step 3368, loss 0.00322924, acc 1
2017-03-02T23:38:41.437863: step 3369, loss 0.0118931, acc 1
2017-03-02T23:38:41.529042: step 3370, loss 0.00339517, acc 1
2017-03-02T23:38:41.632511: step 3371, loss 0.00422675, acc 1
2017-03-02T23:38:41.739126: step 3372, loss 0.00171944, acc 1
2017-03-02T23:38:41.844791: step 3373, loss 0.0331878, acc 0.984375
2017-03-02T23:38:41.950692: step 3374, loss 0.0418515, acc 0.984375
2017-03-02T23:38:42.062860: step 3375, loss 0.00483767, acc 1
2017-03-02T23:38:42.170279: step 3376, loss 0.00271705, acc 1
2017-03-02T23:38:42.264797: step 3377, loss 0.00319541, acc 1
2017-03-02T23:38:42.358554: step 3378, loss 0.0282982, acc 0.984375
2017-03-02T23:38:42.472024: step 3379, loss 0.0213903, acc 1
2017-03-02T23:38:42.577186: step 3380, loss 0.00818645, acc 1
2017-03-02T23:38:42.681980: step 3381, loss 0.00162391, acc 1
2017-03-02T23:38:42.788596: step 3382, loss 0.003363, acc 1
2017-03-02T23:38:42.896962: step 3383, loss 0.00540818, acc 1
2017-03-02T23:38:43.004298: step 3384, loss 0.0148031, acc 1
2017-03-02T23:38:43.098404: step 3385, loss 0.0471285, acc 0.984375
2017-03-02T23:38:43.205564: step 3386, loss 0.013764, acc 0.984375
2017-03-02T23:38:43.317482: step 3387, loss 0.0058254, acc 1
2017-03-02T23:38:43.430188: step 3388, loss 0.0317069, acc 0.984375
2017-03-02T23:38:43.535574: step 3389, loss 0.0127131, acc 1
2017-03-02T23:38:43.641685: step 3390, loss 0.0128253, acc 0.984375
2017-03-02T23:38:43.751990: step 3391, loss 0.0506935, acc 0.96875
2017-03-02T23:38:43.845315: step 3392, loss 0.00875737, acc 1
2017-03-02T23:38:43.946544: step 3393, loss 0.00314778, acc 1
2017-03-02T23:38:44.051326: step 3394, loss 0.00579994, acc 1
2017-03-02T23:38:44.147618: step 3395, loss 0.0128724, acc 0.984375
2017-03-02T23:38:44.251174: step 3396, loss 0.0340183, acc 0.984375
2017-03-02T23:38:44.354747: step 3397, loss 0.00529533, acc 1
2017-03-02T23:38:44.452873: step 3398, loss 0.00689372, acc 1
2017-03-02T23:38:44.543858: step 3399, loss 0.00226145, acc 1
2017-03-02T23:38:44.636248: step 3400, loss 0.0126043, acc 1

Evaluation:
2017-03-02T23:38:44.697654: step 3400, loss 2.18344, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3400

2017-03-02T23:38:45.152670: step 3401, loss 0.0225938, acc 0.984375
2017-03-02T23:38:45.252841: step 3402, loss 0.00103941, acc 1
2017-03-02T23:38:45.341668: step 3403, loss 0.00187985, acc 1
2017-03-02T23:38:45.443396: step 3404, loss 0.00414949, acc 1
2017-03-02T23:38:45.547773: step 3405, loss 0.00466265, acc 1
2017-03-02T23:38:45.654318: step 3406, loss 0.00715357, acc 1
2017-03-02T23:38:45.776563: step 3407, loss 0.034319, acc 0.984375
2017-03-02T23:38:45.881838: step 3408, loss 0.00524349, acc 1
2017-03-02T23:38:45.973127: step 3409, loss 0.00275432, acc 1
2017-03-02T23:38:46.066922: step 3410, loss 0.00102486, acc 1
2017-03-02T23:38:46.174278: step 3411, loss 0.00574119, acc 1
2017-03-02T23:38:46.278122: step 3412, loss 0.0422394, acc 0.984375
2017-03-02T23:38:46.383849: step 3413, loss 0.0169318, acc 0.984375
2017-03-02T23:38:46.491574: step 3414, loss 0.0153358, acc 1
2017-03-02T23:38:46.595802: step 3415, loss 0.00968178, acc 1
2017-03-02T23:38:46.707575: step 3416, loss 0.00231097, acc 1
2017-03-02T23:38:46.803722: step 3417, loss 0.0152251, acc 1
2017-03-02T23:38:46.907738: step 3418, loss 0.0180902, acc 0.984375
2017-03-02T23:38:47.015628: step 3419, loss 0.00280438, acc 1
2017-03-02T23:38:47.120546: step 3420, loss 0.00864403, acc 1
2017-03-02T23:38:47.237367: step 3421, loss 0.0428067, acc 0.96875
2017-03-02T23:38:47.347811: step 3422, loss 0.00407664, acc 1
2017-03-02T23:38:47.461339: step 3423, loss 0.00550316, acc 1
2017-03-02T23:38:47.554088: step 3424, loss 0.0039241, acc 1
2017-03-02T23:38:47.661287: step 3425, loss 0.00637793, acc 1
2017-03-02T23:38:47.767224: step 3426, loss 0.0044058, acc 1
2017-03-02T23:38:47.865001: step 3427, loss 0.0145704, acc 1
2017-03-02T23:38:47.956139: step 3428, loss 0.0188938, acc 1
2017-03-02T23:38:48.061260: step 3429, loss 0.00381274, acc 1
2017-03-02T23:38:48.166860: step 3430, loss 0.0246271, acc 1
2017-03-02T23:38:48.263231: step 3431, loss 0.0116921, acc 1
2017-03-02T23:38:48.371250: step 3432, loss 0.00231242, acc 1
2017-03-02T23:38:48.486037: step 3433, loss 0.00285005, acc 1
2017-03-02T23:38:48.591050: step 3434, loss 0.0204635, acc 1
2017-03-02T23:38:48.695770: step 3435, loss 0.0142081, acc 1
2017-03-02T23:38:48.800964: step 3436, loss 0.0176063, acc 1
2017-03-02T23:38:48.908554: step 3437, loss 0.00471241, acc 1
2017-03-02T23:38:48.997296: step 3438, loss 0.00328098, acc 1
2017-03-02T23:38:49.085053: step 3439, loss 0.010746, acc 1
2017-03-02T23:38:49.189328: step 3440, loss 0.0224415, acc 0.984375
2017-03-02T23:38:49.296637: step 3441, loss 0.0111655, acc 1
2017-03-02T23:38:49.410106: step 3442, loss 0.00474352, acc 1
2017-03-02T23:38:49.512146: step 3443, loss 0.003909, acc 1
2017-03-02T23:38:49.622474: step 3444, loss 0.0401836, acc 0.98
2017-03-02T23:38:49.728198: step 3445, loss 0.00868278, acc 1
2017-03-02T23:38:49.817681: step 3446, loss 0.00327438, acc 1
2017-03-02T23:38:49.922384: step 3447, loss 0.00375416, acc 1
2017-03-02T23:38:50.025572: step 3448, loss 0.00880362, acc 1
2017-03-02T23:38:50.129235: step 3449, loss 0.0113157, acc 1
2017-03-02T23:38:50.230547: step 3450, loss 0.014871, acc 1
2017-03-02T23:38:50.333354: step 3451, loss 0.0129417, acc 1
2017-03-02T23:38:50.432505: step 3452, loss 0.00206905, acc 1
2017-03-02T23:38:50.522090: step 3453, loss 0.00277513, acc 1
2017-03-02T23:38:50.625521: step 3454, loss 0.00755499, acc 1
2017-03-02T23:38:50.746947: step 3455, loss 0.00515998, acc 1
2017-03-02T23:38:50.851302: step 3456, loss 0.00493087, acc 1
2017-03-02T23:38:50.957513: step 3457, loss 0.00686661, acc 1
2017-03-02T23:38:51.067714: step 3458, loss 0.00857968, acc 1
2017-03-02T23:38:51.177184: step 3459, loss 0.00967411, acc 1
2017-03-02T23:38:51.276173: step 3460, loss 0.0236918, acc 0.984375
2017-03-02T23:38:51.376163: step 3461, loss 0.0063577, acc 1
2017-03-02T23:38:51.472244: step 3462, loss 0.0180309, acc 1
2017-03-02T23:38:51.576577: step 3463, loss 0.00425219, acc 1
2017-03-02T23:38:51.673025: step 3464, loss 0.00111377, acc 1
2017-03-02T23:38:51.778430: step 3465, loss 0.00332818, acc 1
2017-03-02T23:38:51.885276: step 3466, loss 0.0093247, acc 1
2017-03-02T23:38:51.979484: step 3467, loss 0.00812418, acc 1
2017-03-02T23:38:52.081641: step 3468, loss 0.00522221, acc 1
2017-03-02T23:38:52.190245: step 3469, loss 0.00924025, acc 1
2017-03-02T23:38:52.293699: step 3470, loss 0.0100171, acc 1
2017-03-02T23:38:52.398207: step 3471, loss 0.0142769, acc 0.984375
2017-03-02T23:38:52.506796: step 3472, loss 0.0107243, acc 1
2017-03-02T23:38:52.609593: step 3473, loss 0.0194377, acc 1
2017-03-02T23:38:52.719651: step 3474, loss 0.00516111, acc 1
2017-03-02T23:38:52.810053: step 3475, loss 0.00743543, acc 1
2017-03-02T23:38:52.916487: step 3476, loss 0.0179489, acc 0.984375
2017-03-02T23:38:53.030574: step 3477, loss 0.00472063, acc 1
2017-03-02T23:38:53.136165: step 3478, loss 0.00983566, acc 1
2017-03-02T23:38:53.245798: step 3479, loss 0.00555537, acc 1
2017-03-02T23:38:53.349281: step 3480, loss 0.00244386, acc 1
2017-03-02T23:38:53.463526: step 3481, loss 0.00441484, acc 1
2017-03-02T23:38:53.555038: step 3482, loss 0.0196295, acc 1
2017-03-02T23:38:53.657233: step 3483, loss 0.0386938, acc 0.984375
2017-03-02T23:38:53.762542: step 3484, loss 0.018126, acc 1
2017-03-02T23:38:53.860397: step 3485, loss 0.0162174, acc 1
2017-03-02T23:38:53.976374: step 3486, loss 0.000897849, acc 1
2017-03-02T23:38:54.080781: step 3487, loss 0.00390267, acc 1
2017-03-02T23:38:54.197884: step 3488, loss 0.00235194, acc 1
2017-03-02T23:38:54.288238: step 3489, loss 0.00604934, acc 1
2017-03-02T23:38:54.391644: step 3490, loss 0.00206452, acc 1
2017-03-02T23:38:54.498440: step 3491, loss 0.00655561, acc 1
2017-03-02T23:38:54.600019: step 3492, loss 0.00437589, acc 1
2017-03-02T23:38:54.704308: step 3493, loss 0.00285901, acc 1
2017-03-02T23:38:54.813327: step 3494, loss 0.00453797, acc 1
2017-03-02T23:38:54.922001: step 3495, loss 0.00653173, acc 1
2017-03-02T23:38:55.011182: step 3496, loss 0.000875447, acc 1
2017-03-02T23:38:55.103266: step 3497, loss 0.00106182, acc 1
2017-03-02T23:38:55.212374: step 3498, loss 0.00732567, acc 1
2017-03-02T23:38:55.318979: step 3499, loss 0.00448508, acc 1
2017-03-02T23:38:55.423802: step 3500, loss 0.00619675, acc 1

Evaluation:
2017-03-02T23:38:55.483590: step 3500, loss 2.15355, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3500

2017-03-02T23:38:55.965290: step 3501, loss 0.0107569, acc 1
2017-03-02T23:38:56.071032: step 3502, loss 0.00490611, acc 1
2017-03-02T23:38:56.182608: step 3503, loss 0.0106077, acc 1
2017-03-02T23:38:56.289335: step 3504, loss 0.00761685, acc 1
2017-03-02T23:38:56.401127: step 3505, loss 0.0154362, acc 0.984375
2017-03-02T23:38:56.493415: step 3506, loss 0.00262671, acc 1
2017-03-02T23:38:56.595537: step 3507, loss 0.023118, acc 0.984375
2017-03-02T23:38:56.698241: step 3508, loss 0.00209283, acc 1
2017-03-02T23:38:56.810445: step 3509, loss 0.00643622, acc 1
2017-03-02T23:38:56.913936: step 3510, loss 0.00617549, acc 1
2017-03-02T23:38:57.051756: step 3511, loss 0.00274199, acc 1
2017-03-02T23:38:57.171302: step 3512, loss 0.0156436, acc 1
2017-03-02T23:38:57.259602: step 3513, loss 0.00862623, acc 1
2017-03-02T23:38:57.359190: step 3514, loss 0.00532601, acc 1
2017-03-02T23:38:57.466315: step 3515, loss 0.0364095, acc 0.984375
2017-03-02T23:38:57.574368: step 3516, loss 0.0133213, acc 0.984375
2017-03-02T23:38:57.692591: step 3517, loss 0.00149613, acc 1
2017-03-02T23:38:57.802407: step 3518, loss 0.0091333, acc 1
2017-03-02T23:38:57.914577: step 3519, loss 0.00810885, acc 1
2017-03-02T23:38:58.005999: step 3520, loss 0.0133314, acc 1
2017-03-02T23:38:58.107856: step 3521, loss 0.00568741, acc 1
2017-03-02T23:38:58.220145: step 3522, loss 0.00908577, acc 1
2017-03-02T23:38:58.329004: step 3523, loss 0.014628, acc 0.984375
2017-03-02T23:38:58.425806: step 3524, loss 0.0109189, acc 1
2017-03-02T23:38:58.534827: step 3525, loss 0.00841532, acc 1
2017-03-02T23:38:58.630794: step 3526, loss 0.016349, acc 1
2017-03-02T23:38:58.730275: step 3527, loss 0.0449283, acc 0.984375
2017-03-02T23:38:58.832225: step 3528, loss 0.00581955, acc 1
2017-03-02T23:38:58.979513: step 3529, loss 0.0481915, acc 0.984375
2017-03-02T23:38:59.086096: step 3530, loss 0.00826786, acc 1
2017-03-02T23:38:59.191031: step 3531, loss 0.00455648, acc 1
2017-03-02T23:38:59.304513: step 3532, loss 0.00442408, acc 1
2017-03-02T23:38:59.408143: step 3533, loss 0.0137895, acc 1
2017-03-02T23:38:59.496563: step 3534, loss 0.0574041, acc 0.984375
2017-03-02T23:38:59.604990: step 3535, loss 0.0128912, acc 1
2017-03-02T23:38:59.715154: step 3536, loss 0.00697525, acc 1
2017-03-02T23:38:59.822520: step 3537, loss 0.0140072, acc 1
2017-03-02T23:38:59.926433: step 3538, loss 0.00715964, acc 1
2017-03-02T23:39:00.024829: step 3539, loss 0.00527731, acc 1
2017-03-02T23:39:00.130421: step 3540, loss 0.0100078, acc 1
2017-03-02T23:39:00.224226: step 3541, loss 0.00791959, acc 1
2017-03-02T23:39:00.318572: step 3542, loss 0.00294528, acc 1
2017-03-02T23:39:00.421891: step 3543, loss 0.00439474, acc 1
2017-03-02T23:39:00.528028: step 3544, loss 0.0121889, acc 1
2017-03-02T23:39:00.638305: step 3545, loss 0.0276549, acc 0.984375
2017-03-02T23:39:00.743864: step 3546, loss 0.0118013, acc 1
2017-03-02T23:39:00.848917: step 3547, loss 0.00493875, acc 1
2017-03-02T23:39:00.947423: step 3548, loss 0.00283408, acc 1
2017-03-02T23:39:01.035298: step 3549, loss 0.00700435, acc 1
2017-03-02T23:39:01.143149: step 3550, loss 0.0119914, acc 1
2017-03-02T23:39:01.271130: step 3551, loss 0.00141793, acc 1
2017-03-02T23:39:01.378767: step 3552, loss 0.00568226, acc 1
2017-03-02T23:39:01.477838: step 3553, loss 0.00298468, acc 1
2017-03-02T23:39:01.577987: step 3554, loss 0.00270355, acc 1
2017-03-02T23:39:01.687213: step 3555, loss 0.010097, acc 1
2017-03-02T23:39:01.775791: step 3556, loss 0.0044407, acc 1
2017-03-02T23:39:01.886683: step 3557, loss 0.00848065, acc 1
2017-03-02T23:39:02.003328: step 3558, loss 0.0015829, acc 1
2017-03-02T23:39:02.115317: step 3559, loss 0.00299257, acc 1
2017-03-02T23:39:02.225182: step 3560, loss 0.00175481, acc 1
2017-03-02T23:39:02.331111: step 3561, loss 0.00136287, acc 1
2017-03-02T23:39:02.439399: step 3562, loss 0.00588073, acc 1
2017-03-02T23:39:02.527381: step 3563, loss 0.00162145, acc 1
2017-03-02T23:39:02.641404: step 3564, loss 0.00180238, acc 1
2017-03-02T23:39:02.745850: step 3565, loss 0.0043075, acc 1
2017-03-02T23:39:02.841266: step 3566, loss 0.00326289, acc 1
2017-03-02T23:39:02.938574: step 3567, loss 0.00248777, acc 1
2017-03-02T23:39:03.043799: step 3568, loss 0.0174298, acc 0.984375
2017-03-02T23:39:03.152377: step 3569, loss 0.00162713, acc 1
2017-03-02T23:39:03.245622: step 3570, loss 0.00565882, acc 1
2017-03-02T23:39:03.348456: step 3571, loss 0.0080217, acc 1
2017-03-02T23:39:03.453094: step 3572, loss 0.0140675, acc 1
2017-03-02T23:39:03.556436: step 3573, loss 0.0123777, acc 1
2017-03-02T23:39:03.659897: step 3574, loss 0.00706919, acc 1
2017-03-02T23:39:03.757100: step 3575, loss 0.0187742, acc 1
2017-03-02T23:39:03.862329: step 3576, loss 0.00294329, acc 1
2017-03-02T23:39:03.969760: step 3577, loss 0.00326925, acc 1
2017-03-02T23:39:04.064375: step 3578, loss 0.00146867, acc 1
2017-03-02T23:39:04.169902: step 3579, loss 0.00333395, acc 1
2017-03-02T23:39:04.272340: step 3580, loss 0.00524406, acc 1
2017-03-02T23:39:04.374990: step 3581, loss 0.00371646, acc 1
2017-03-02T23:39:04.489846: step 3582, loss 0.00302084, acc 1
2017-03-02T23:39:04.606965: step 3583, loss 0.0225239, acc 0.984375
2017-03-02T23:39:04.715963: step 3584, loss 0.00159883, acc 1
2017-03-02T23:39:04.806220: step 3585, loss 0.00585945, acc 1
2017-03-02T23:39:04.909687: step 3586, loss 0.00085657, acc 1
2017-03-02T23:39:05.012983: step 3587, loss 0.00424492, acc 1
2017-03-02T23:39:05.123291: step 3588, loss 0.00780921, acc 1
2017-03-02T23:39:05.216683: step 3589, loss 0.02502, acc 0.984375
2017-03-02T23:39:05.326923: step 3590, loss 0.00558835, acc 1
2017-03-02T23:39:05.435726: step 3591, loss 0.0122932, acc 1
2017-03-02T23:39:05.531015: step 3592, loss 0.00277275, acc 1
2017-03-02T23:39:05.636080: step 3593, loss 0.0222577, acc 0.984375
2017-03-02T23:39:05.744416: step 3594, loss 0.010366, acc 1
2017-03-02T23:39:05.849254: step 3595, loss 0.00468388, acc 1
2017-03-02T23:39:05.952887: step 3596, loss 0.0024966, acc 1
2017-03-02T23:39:06.056000: step 3597, loss 0.00336241, acc 1
2017-03-02T23:39:06.163365: step 3598, loss 0.00708245, acc 1
2017-03-02T23:39:06.258014: step 3599, loss 0.0281993, acc 0.984375
2017-03-02T23:39:06.356127: step 3600, loss 0.00122475, acc 1

Evaluation:
2017-03-02T23:39:06.407589: step 3600, loss 2.25317, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3600

2017-03-02T23:39:06.866162: step 3601, loss 0.00870761, acc 1
2017-03-02T23:39:06.957511: step 3602, loss 0.00266528, acc 1
2017-03-02T23:39:07.055663: step 3603, loss 0.00434208, acc 1
2017-03-02T23:39:07.163717: step 3604, loss 0.0174542, acc 0.984375
2017-03-02T23:39:07.267242: step 3605, loss 0.0126834, acc 1
2017-03-02T23:39:07.370859: step 3606, loss 0.000592889, acc 1
2017-03-02T23:39:07.476427: step 3607, loss 0.0154239, acc 1
2017-03-02T23:39:07.584366: step 3608, loss 0.0194497, acc 1
2017-03-02T23:39:07.677310: step 3609, loss 0.00328074, acc 1
2017-03-02T23:39:07.774263: step 3610, loss 0.00347423, acc 1
2017-03-02T23:39:07.880813: step 3611, loss 0.00473045, acc 1
2017-03-02T23:39:07.986350: step 3612, loss 0.115133, acc 0.984375
2017-03-02T23:39:08.089033: step 3613, loss 0.00251789, acc 1
2017-03-02T23:39:08.192861: step 3614, loss 0.00255522, acc 1
2017-03-02T23:39:08.315971: step 3615, loss 0.00666855, acc 1
2017-03-02T23:39:08.411617: step 3616, loss 0.032512, acc 0.984375
2017-03-02T23:39:08.503521: step 3617, loss 0.00445842, acc 1
2017-03-02T23:39:08.619615: step 3618, loss 0.00340919, acc 1
2017-03-02T23:39:08.723415: step 3619, loss 0.00510039, acc 1
2017-03-02T23:39:08.842465: step 3620, loss 0.00435364, acc 1
2017-03-02T23:39:08.956869: step 3621, loss 0.0719312, acc 0.984375
2017-03-02T23:39:09.063432: step 3622, loss 0.00179847, acc 1
2017-03-02T23:39:09.181444: step 3623, loss 0.0176215, acc 1
2017-03-02T23:39:09.265118: step 3624, loss 0.0124378, acc 0.984375
2017-03-02T23:39:09.369112: step 3625, loss 0.0113934, acc 1
2017-03-02T23:39:09.475264: step 3626, loss 0.00287664, acc 1
2017-03-02T23:39:09.587090: step 3627, loss 0.00237615, acc 1
2017-03-02T23:39:09.706607: step 3628, loss 0.00734926, acc 1
2017-03-02T23:39:09.807070: step 3629, loss 0.00235573, acc 1
2017-03-02T23:39:09.923642: step 3630, loss 0.0238804, acc 0.984375
2017-03-02T23:39:10.018645: step 3631, loss 0.0108122, acc 1
2017-03-02T23:39:10.123785: step 3632, loss 0.00432449, acc 1
2017-03-02T23:39:10.231341: step 3633, loss 0.0304209, acc 0.984375
2017-03-02T23:39:10.337504: step 3634, loss 0.0284565, acc 0.984375
2017-03-02T23:39:10.446810: step 3635, loss 0.00277592, acc 1
2017-03-02T23:39:10.555968: step 3636, loss 0.00433813, acc 1
2017-03-02T23:39:10.665417: step 3637, loss 0.00468805, acc 1
2017-03-02T23:39:10.760088: step 3638, loss 0.00198253, acc 1
2017-03-02T23:39:10.871223: step 3639, loss 0.00143354, acc 1
2017-03-02T23:39:10.975754: step 3640, loss 0.00299001, acc 1
2017-03-02T23:39:11.083971: step 3641, loss 0.010654, acc 1
2017-03-02T23:39:11.185851: step 3642, loss 0.0355094, acc 0.984375
2017-03-02T23:39:11.296837: step 3643, loss 0.0158842, acc 1
2017-03-02T23:39:11.396607: step 3644, loss 0.0145303, acc 1
2017-03-02T23:39:11.491622: step 3645, loss 0.0094036, acc 1
2017-03-02T23:39:11.590323: step 3646, loss 0.00254815, acc 1
2017-03-02T23:39:11.694989: step 3647, loss 0.0403727, acc 0.984375
2017-03-02T23:39:11.814352: step 3648, loss 0.0259636, acc 0.984375
2017-03-02T23:39:11.916154: step 3649, loss 0.0243751, acc 1
2017-03-02T23:39:12.023062: step 3650, loss 0.00417771, acc 1
2017-03-02T23:39:12.137357: step 3651, loss 0.0224537, acc 0.984375
2017-03-02T23:39:12.228217: step 3652, loss 0.0859325, acc 0.984375
2017-03-02T23:39:12.321863: step 3653, loss 0.0099143, acc 1
2017-03-02T23:39:12.426625: step 3654, loss 0.0659704, acc 0.96875
2017-03-02T23:39:12.527218: step 3655, loss 0.00182983, acc 1
2017-03-02T23:39:12.631973: step 3656, loss 0.00682391, acc 1
2017-03-02T23:39:12.745261: step 3657, loss 0.00274268, acc 1
2017-03-02T23:39:12.842891: step 3658, loss 0.00828386, acc 1
2017-03-02T23:39:12.976073: step 3659, loss 0.00621818, acc 1
2017-03-02T23:39:13.065194: step 3660, loss 0.00203204, acc 1
2017-03-02T23:39:13.167101: step 3661, loss 0.00307709, acc 1
2017-03-02T23:39:13.274159: step 3662, loss 0.00356216, acc 1
2017-03-02T23:39:13.375398: step 3663, loss 0.0062957, acc 1
2017-03-02T23:39:13.481364: step 3664, loss 0.0119789, acc 1
2017-03-02T23:39:13.593108: step 3665, loss 0.00893153, acc 1
2017-03-02T23:39:13.699518: step 3666, loss 0.0190724, acc 0.984375
2017-03-02T23:39:13.804343: step 3667, loss 0.033631, acc 0.96875
2017-03-02T23:39:13.907179: step 3668, loss 0.00945164, acc 1
2017-03-02T23:39:14.010569: step 3669, loss 0.00350762, acc 1
2017-03-02T23:39:14.120214: step 3670, loss 0.00631047, acc 1
2017-03-02T23:39:14.232660: step 3671, loss 0.00336888, acc 1
2017-03-02T23:39:14.339043: step 3672, loss 0.00179447, acc 1
2017-03-02T23:39:14.449841: step 3673, loss 0.00857953, acc 1
2017-03-02T23:39:14.544482: step 3674, loss 0.0100147, acc 1
2017-03-02T23:39:14.643272: step 3675, loss 0.00840555, acc 1
2017-03-02T23:39:14.747335: step 3676, loss 0.00213551, acc 1
2017-03-02T23:39:14.859035: step 3677, loss 0.00190207, acc 1
2017-03-02T23:39:14.959975: step 3678, loss 0.000964598, acc 1
2017-03-02T23:39:15.067382: step 3679, loss 0.0209642, acc 0.984375
2017-03-02T23:39:15.168460: step 3680, loss 0.00850864, acc 1
2017-03-02T23:39:15.268495: step 3681, loss 0.0035176, acc 1
2017-03-02T23:39:15.359580: step 3682, loss 0.010412, acc 1
2017-03-02T23:39:15.467119: step 3683, loss 0.0065179, acc 1
2017-03-02T23:39:15.581785: step 3684, loss 0.0104615, acc 1
2017-03-02T23:39:15.682586: step 3685, loss 0.0358872, acc 0.984375
2017-03-02T23:39:15.783193: step 3686, loss 0.00622049, acc 1
2017-03-02T23:39:15.896386: step 3687, loss 0.00132097, acc 1
2017-03-02T23:39:16.011794: step 3688, loss 0.00403218, acc 1
2017-03-02T23:39:16.101751: step 3689, loss 0.0023171, acc 1
2017-03-02T23:39:16.195493: step 3690, loss 0.00181392, acc 1
2017-03-02T23:39:16.306432: step 3691, loss 0.00187483, acc 1
2017-03-02T23:39:16.415752: step 3692, loss 0.00684651, acc 1
2017-03-02T23:39:16.520536: step 3693, loss 0.0023704, acc 1
2017-03-02T23:39:16.619851: step 3694, loss 0.0122304, acc 0.984375
2017-03-02T23:39:16.729831: step 3695, loss 0.00444645, acc 1
2017-03-02T23:39:16.828036: step 3696, loss 0.020708, acc 0.984375
2017-03-02T23:39:16.937371: step 3697, loss 0.006082, acc 1
2017-03-02T23:39:17.030866: step 3698, loss 0.00127921, acc 1
2017-03-02T23:39:17.141761: step 3699, loss 0.00711525, acc 1
2017-03-02T23:39:17.265545: step 3700, loss 0.00194332, acc 1

Evaluation:
2017-03-02T23:39:17.331549: step 3700, loss 2.05807, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3700

2017-03-02T23:39:17.862836: step 3701, loss 0.0101652, acc 1
2017-03-02T23:39:17.976244: step 3702, loss 0.00390163, acc 1
2017-03-02T23:39:18.082256: step 3703, loss 0.00397836, acc 1
2017-03-02T23:39:18.191501: step 3704, loss 0.0448053, acc 0.984375
2017-03-02T23:39:18.285620: step 3705, loss 0.0112288, acc 1
2017-03-02T23:39:18.393757: step 3706, loss 0.002492, acc 1
2017-03-02T23:39:18.500644: step 3707, loss 0.0334492, acc 1
2017-03-02T23:39:18.616433: step 3708, loss 0.0214203, acc 0.984375
2017-03-02T23:39:18.724439: step 3709, loss 0.00950067, acc 1
2017-03-02T23:39:18.821987: step 3710, loss 0.00619102, acc 1
2017-03-02T23:39:18.922699: step 3711, loss 0.00131816, acc 1
2017-03-02T23:39:19.012617: step 3712, loss 0.0144987, acc 1
2017-03-02T23:39:19.102502: step 3713, loss 0.00285715, acc 1
2017-03-02T23:39:19.209217: step 3714, loss 0.00202233, acc 1
2017-03-02T23:39:19.326873: step 3715, loss 0.00491235, acc 1
2017-03-02T23:39:19.442193: step 3716, loss 0.00171497, acc 1
2017-03-02T23:39:19.545541: step 3717, loss 0.00645961, acc 1
2017-03-02T23:39:19.651756: step 3718, loss 0.00356149, acc 1
2017-03-02T23:39:19.751205: step 3719, loss 0.00781156, acc 1
2017-03-02T23:39:19.845628: step 3720, loss 0.00638668, acc 1
2017-03-02T23:39:19.960245: step 3721, loss 0.00652212, acc 1
2017-03-02T23:39:20.071756: step 3722, loss 0.0433716, acc 0.984375
2017-03-02T23:39:20.179375: step 3723, loss 0.0103922, acc 1
2017-03-02T23:39:20.295267: step 3724, loss 0.00685676, acc 1
2017-03-02T23:39:20.399162: step 3725, loss 0.00557484, acc 1
2017-03-02T23:39:20.495369: step 3726, loss 0.0074434, acc 1
2017-03-02T23:39:20.586205: step 3727, loss 0.0181956, acc 0.984375
2017-03-02T23:39:20.693058: step 3728, loss 0.0049218, acc 1
2017-03-02T23:39:20.803600: step 3729, loss 0.0174424, acc 0.984375
2017-03-02T23:39:20.923128: step 3730, loss 0.00169805, acc 1
2017-03-02T23:39:21.025226: step 3731, loss 0.00421837, acc 1
2017-03-02T23:39:21.128527: step 3732, loss 0.00458249, acc 1
2017-03-02T23:39:21.237990: step 3733, loss 0.00191958, acc 1
2017-03-02T23:39:21.334714: step 3734, loss 0.00790376, acc 1
2017-03-02T23:39:21.439994: step 3735, loss 0.00528859, acc 1
2017-03-02T23:39:21.546922: step 3736, loss 0.0116745, acc 1
2017-03-02T23:39:21.649224: step 3737, loss 0.00456426, acc 1
2017-03-02T23:39:21.756046: step 3738, loss 0.00921337, acc 1
2017-03-02T23:39:21.863183: step 3739, loss 0.00410611, acc 1
2017-03-02T23:39:21.975293: step 3740, loss 0.00879808, acc 1
2017-03-02T23:39:22.067578: step 3741, loss 0.00240594, acc 1
2017-03-02T23:39:22.175737: step 3742, loss 0.00171572, acc 1
2017-03-02T23:39:22.283605: step 3743, loss 0.00189729, acc 1
2017-03-02T23:39:22.387768: step 3744, loss 0.00510665, acc 1
2017-03-02T23:39:22.499523: step 3745, loss 0.00412334, acc 1
2017-03-02T23:39:22.600869: step 3746, loss 0.00758168, acc 1
2017-03-02T23:39:22.719221: step 3747, loss 0.00414619, acc 1
2017-03-02T23:39:22.814351: step 3748, loss 0.00341472, acc 1
2017-03-02T23:39:22.919939: step 3749, loss 0.0375818, acc 0.984375
2017-03-02T23:39:23.024699: step 3750, loss 0.00533754, acc 1
2017-03-02T23:39:23.136432: step 3751, loss 0.00391871, acc 1
2017-03-02T23:39:23.240372: step 3752, loss 0.00527782, acc 1
2017-03-02T23:39:23.355212: step 3753, loss 0.0230466, acc 0.984375
2017-03-02T23:39:23.471900: step 3754, loss 0.00222405, acc 1
2017-03-02T23:39:23.561937: step 3755, loss 0.00445505, acc 1
2017-03-02T23:39:23.677488: step 3756, loss 0.000606481, acc 1
2017-03-02T23:39:23.783239: step 3757, loss 0.00287409, acc 1
2017-03-02T23:39:23.887776: step 3758, loss 0.0324253, acc 0.984375
2017-03-02T23:39:23.994653: step 3759, loss 0.00844533, acc 1
2017-03-02T23:39:24.103464: step 3760, loss 0.00446753, acc 1
2017-03-02T23:39:24.206309: step 3761, loss 0.0210653, acc 0.984375
2017-03-02T23:39:24.299971: step 3762, loss 0.00304067, acc 1
2017-03-02T23:39:24.412876: step 3763, loss 0.00267074, acc 1
2017-03-02T23:39:24.521591: step 3764, loss 0.00839049, acc 1
2017-03-02T23:39:24.633104: step 3765, loss 0.0114717, acc 1
2017-03-02T23:39:24.739377: step 3766, loss 0.000958295, acc 1
2017-03-02T23:39:24.847481: step 3767, loss 0.00644601, acc 1
2017-03-02T23:39:24.958745: step 3768, loss 0.00174731, acc 1
2017-03-02T23:39:25.045420: step 3769, loss 0.0157561, acc 0.984375
2017-03-02T23:39:25.148542: step 3770, loss 0.00420204, acc 1
2017-03-02T23:39:25.241659: step 3771, loss 0.002133, acc 1
2017-03-02T23:39:25.337712: step 3772, loss 0.0116659, acc 1
2017-03-02T23:39:25.441561: step 3773, loss 0.00578197, acc 1
2017-03-02T23:39:25.552060: step 3774, loss 0.03018, acc 0.984375
2017-03-02T23:39:25.662166: step 3775, loss 0.00272067, acc 1
2017-03-02T23:39:25.764714: step 3776, loss 0.00483871, acc 1
2017-03-02T23:39:25.858069: step 3777, loss 0.00663659, acc 1
2017-03-02T23:39:25.969883: step 3778, loss 0.00194334, acc 1
2017-03-02T23:39:26.075516: step 3779, loss 0.00494717, acc 1
2017-03-02T23:39:26.181822: step 3780, loss 0.00671573, acc 1
2017-03-02T23:39:26.279759: step 3781, loss 0.00233049, acc 1
2017-03-02T23:39:26.378968: step 3782, loss 0.0070077, acc 1
2017-03-02T23:39:26.482943: step 3783, loss 0.00990029, acc 1
2017-03-02T23:39:26.576842: step 3784, loss 0.0246027, acc 0.984375
2017-03-02T23:39:26.688621: step 3785, loss 0.00751739, acc 1
2017-03-02T23:39:26.790052: step 3786, loss 0.00187839, acc 1
2017-03-02T23:39:26.891770: step 3787, loss 0.00393365, acc 1
2017-03-02T23:39:26.996780: step 3788, loss 0.0028073, acc 1
2017-03-02T23:39:27.102315: step 3789, loss 0.00205936, acc 1
2017-03-02T23:39:27.205420: step 3790, loss 0.00737482, acc 1
2017-03-02T23:39:27.295493: step 3791, loss 0.00142402, acc 1
2017-03-02T23:39:27.394686: step 3792, loss 0.00386477, acc 1
2017-03-02T23:39:27.503331: step 3793, loss 0.00105289, acc 1
2017-03-02T23:39:27.611613: step 3794, loss 0.00898195, acc 1
2017-03-02T23:39:27.712038: step 3795, loss 0.00139524, acc 1
2017-03-02T23:39:27.841504: step 3796, loss 0.0188185, acc 1
2017-03-02T23:39:27.946467: step 3797, loss 0.00264962, acc 1
2017-03-02T23:39:28.038585: step 3798, loss 0.00252963, acc 1
2017-03-02T23:39:28.145675: step 3799, loss 0.00663392, acc 1
2017-03-02T23:39:28.254981: step 3800, loss 0.00474299, acc 1

Evaluation:
2017-03-02T23:39:28.316935: step 3800, loss 2.29934, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3800

2017-03-02T23:39:28.757255: step 3801, loss 0.00160668, acc 1
2017-03-02T23:39:28.858879: step 3802, loss 0.0074944, acc 1
2017-03-02T23:39:28.965632: step 3803, loss 0.00145438, acc 1
2017-03-02T23:39:29.058495: step 3804, loss 0.00740737, acc 1
2017-03-02T23:39:29.157353: step 3805, loss 0.00974049, acc 1
2017-03-02T23:39:29.261722: step 3806, loss 0.00953038, acc 1
2017-03-02T23:39:29.354682: step 3807, loss 0.0131004, acc 1
2017-03-02T23:39:29.441433: step 3808, loss 0.0469873, acc 0.984375
2017-03-02T23:39:29.531031: step 3809, loss 0.0375613, acc 0.984375
2017-03-02T23:39:29.639462: step 3810, loss 0.00323788, acc 1
2017-03-02T23:39:29.741146: step 3811, loss 0.00710037, acc 1
2017-03-02T23:39:29.860483: step 3812, loss 0.0126148, acc 1
2017-03-02T23:39:29.950355: step 3813, loss 0.008578, acc 1
2017-03-02T23:39:30.054100: step 3814, loss 0.0030324, acc 1
2017-03-02T23:39:30.158698: step 3815, loss 0.0044427, acc 1
2017-03-02T23:39:30.248972: step 3816, loss 0.00412976, acc 1
2017-03-02T23:39:30.349027: step 3817, loss 0.0408013, acc 0.984375
2017-03-02T23:39:30.459093: step 3818, loss 0.00824298, acc 1
2017-03-02T23:39:30.572790: step 3819, loss 0.00227376, acc 1
2017-03-02T23:39:30.679554: step 3820, loss 0.00302421, acc 1
2017-03-02T23:39:30.783514: step 3821, loss 0.00579764, acc 1
2017-03-02T23:39:30.879454: step 3822, loss 0.00157305, acc 1
2017-03-02T23:39:30.972005: step 3823, loss 0.00127331, acc 1
2017-03-02T23:39:31.079144: step 3824, loss 0.00982102, acc 1
2017-03-02T23:39:31.183791: step 3825, loss 0.00193844, acc 1
2017-03-02T23:39:31.284954: step 3826, loss 0.00470959, acc 1
2017-03-02T23:39:31.394416: step 3827, loss 0.00224038, acc 1
2017-03-02T23:39:31.519884: step 3828, loss 0.00313799, acc 1
2017-03-02T23:39:31.629507: step 3829, loss 0.0101407, acc 1
2017-03-02T23:39:31.723902: step 3830, loss 0.00118198, acc 1
2017-03-02T23:39:31.825993: step 3831, loss 0.00483246, acc 1
2017-03-02T23:39:31.926751: step 3832, loss 0.00197503, acc 1
2017-03-02T23:39:32.032010: step 3833, loss 0.000941658, acc 1
2017-03-02T23:39:32.151026: step 3834, loss 0.00446706, acc 1
2017-03-02T23:39:32.255949: step 3835, loss 0.00148915, acc 1
2017-03-02T23:39:32.371689: step 3836, loss 0.00344909, acc 1
2017-03-02T23:39:32.475105: step 3837, loss 0.00360099, acc 1
2017-03-02T23:39:32.575915: step 3838, loss 0.0034158, acc 1
2017-03-02T23:39:32.680732: step 3839, loss 0.00111061, acc 1
2017-03-02T23:39:32.782766: step 3840, loss 0.00117687, acc 1
2017-03-02T23:39:32.882431: step 3841, loss 0.0020231, acc 1
2017-03-02T23:39:32.983377: step 3842, loss 0.0081488, acc 1
2017-03-02T23:39:33.082815: step 3843, loss 0.00321278, acc 1
2017-03-02T23:39:33.170897: step 3844, loss 0.00397519, acc 1
2017-03-02T23:39:33.265018: step 3845, loss 0.00196761, acc 1
2017-03-02T23:39:33.375832: step 3846, loss 0.0083272, acc 1
2017-03-02T23:39:33.476718: step 3847, loss 0.00131156, acc 1
2017-03-02T23:39:33.577132: step 3848, loss 0.0127117, acc 1
2017-03-02T23:39:33.684841: step 3849, loss 0.00199404, acc 1
2017-03-02T23:39:33.784837: step 3850, loss 0.0098941, acc 1
2017-03-02T23:39:33.901678: step 3851, loss 0.00260986, acc 1
2017-03-02T23:39:33.998650: step 3852, loss 0.00342933, acc 1
2017-03-02T23:39:34.102915: step 3853, loss 0.00482787, acc 1
2017-03-02T23:39:34.197254: step 3854, loss 0.00370776, acc 1
2017-03-02T23:39:34.311156: step 3855, loss 0.00414415, acc 1
2017-03-02T23:39:34.421256: step 3856, loss 0.0106007, acc 1
2017-03-02T23:39:34.523061: step 3857, loss 0.00474311, acc 1
2017-03-02T23:39:34.632982: step 3858, loss 0.00403095, acc 1
2017-03-02T23:39:34.738286: step 3859, loss 0.00536704, acc 1
2017-03-02T23:39:34.845786: step 3860, loss 0.00229941, acc 1
2017-03-02T23:39:34.952402: step 3861, loss 0.00270271, acc 1
2017-03-02T23:39:35.070428: step 3862, loss 0.00106115, acc 1
2017-03-02T23:39:35.182337: step 3863, loss 0.00100423, acc 1
2017-03-02T23:39:35.286184: step 3864, loss 0.013019, acc 1
2017-03-02T23:39:35.401213: step 3865, loss 0.00511407, acc 1
2017-03-02T23:39:35.497098: step 3866, loss 0.000627766, acc 1
2017-03-02T23:39:35.604309: step 3867, loss 0.00720075, acc 1
2017-03-02T23:39:35.704542: step 3868, loss 0.029362, acc 0.96875
2017-03-02T23:39:35.807301: step 3869, loss 0.00788534, acc 1
2017-03-02T23:39:35.916456: step 3870, loss 0.00696351, acc 1
2017-03-02T23:39:36.021991: step 3871, loss 0.00938469, acc 1
2017-03-02T23:39:36.124678: step 3872, loss 0.00481528, acc 1
2017-03-02T23:39:36.214157: step 3873, loss 0.00122015, acc 1
2017-03-02T23:39:36.313643: step 3874, loss 0.0299969, acc 0.984375
2017-03-02T23:39:36.425346: step 3875, loss 0.0049868, acc 1
2017-03-02T23:39:36.527588: step 3876, loss 0.00320659, acc 1
2017-03-02T23:39:36.638717: step 3877, loss 0.0109624, acc 1
2017-03-02T23:39:36.741162: step 3878, loss 0.0112303, acc 1
2017-03-02T23:39:36.844633: step 3879, loss 0.00935992, acc 1
2017-03-02T23:39:36.924928: step 3880, loss 0.0121721, acc 1
2017-03-02T23:39:37.016153: step 3881, loss 0.00214938, acc 1
2017-03-02T23:39:37.123476: step 3882, loss 0.0102795, acc 1
2017-03-02T23:39:37.224591: step 3883, loss 0.00283879, acc 1
2017-03-02T23:39:37.327809: step 3884, loss 0.00189293, acc 1
2017-03-02T23:39:37.440312: step 3885, loss 0.00323356, acc 1
2017-03-02T23:39:37.541037: step 3886, loss 0.00300189, acc 1
2017-03-02T23:39:37.652463: step 3887, loss 0.00684707, acc 1
2017-03-02T23:39:37.750474: step 3888, loss 0.00925442, acc 1
2017-03-02T23:39:37.855601: step 3889, loss 0.000721065, acc 1
2017-03-02T23:39:37.955013: step 3890, loss 0.00811459, acc 1
2017-03-02T23:39:38.063764: step 3891, loss 0.022298, acc 0.984375
2017-03-02T23:39:38.167776: step 3892, loss 0.00232593, acc 1
2017-03-02T23:39:38.270314: step 3893, loss 0.019296, acc 0.984375
2017-03-02T23:39:38.374523: step 3894, loss 0.00566062, acc 1
2017-03-02T23:39:38.459248: step 3895, loss 0.00252379, acc 1
2017-03-02T23:39:38.576118: step 3896, loss 0.00091205, acc 1
2017-03-02T23:39:38.686072: step 3897, loss 0.00186499, acc 1
2017-03-02T23:39:38.793527: step 3898, loss 0.0112726, acc 1
2017-03-02T23:39:38.899941: step 3899, loss 0.0029232, acc 1
2017-03-02T23:39:39.009569: step 3900, loss 0.00171799, acc 1

Evaluation:
2017-03-02T23:39:39.079328: step 3900, loss 2.36104, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-3900

2017-03-02T23:39:39.537516: step 3901, loss 0.00751929, acc 1
2017-03-02T23:39:39.646538: step 3902, loss 0.0056644, acc 1
2017-03-02T23:39:39.751569: step 3903, loss 0.00126186, acc 1
2017-03-02T23:39:39.860664: step 3904, loss 0.00303107, acc 1
2017-03-02T23:39:39.955064: step 3905, loss 0.00219177, acc 1
2017-03-02T23:39:40.060129: step 3906, loss 0.00335911, acc 1
2017-03-02T23:39:40.170035: step 3907, loss 0.00957233, acc 1
2017-03-02T23:39:40.285136: step 3908, loss 0.00803353, acc 1
2017-03-02T23:39:40.388711: step 3909, loss 0.00161906, acc 1
2017-03-02T23:39:40.489039: step 3910, loss 0.00653025, acc 1
2017-03-02T23:39:40.595485: step 3911, loss 0.0047577, acc 1
2017-03-02T23:39:40.687601: step 3912, loss 0.00244321, acc 1
2017-03-02T23:39:40.780934: step 3913, loss 0.00345004, acc 1
2017-03-02T23:39:40.882640: step 3914, loss 0.0126614, acc 1
2017-03-02T23:39:40.987121: step 3915, loss 0.020276, acc 0.984375
2017-03-02T23:39:41.102362: step 3916, loss 0.00389341, acc 1
2017-03-02T23:39:41.211653: step 3917, loss 0.0133869, acc 0.984375
2017-03-02T23:39:41.317398: step 3918, loss 0.00956275, acc 1
2017-03-02T23:39:41.405042: step 3919, loss 0.00376102, acc 1
2017-03-02T23:39:41.502754: step 3920, loss 0.00561521, acc 1
2017-03-02T23:39:41.612904: step 3921, loss 0.00340554, acc 1
2017-03-02T23:39:41.724944: step 3922, loss 0.00313696, acc 1
2017-03-02T23:39:41.826788: step 3923, loss 0.00193476, acc 1
2017-03-02T23:39:41.936826: step 3924, loss 0.0197981, acc 0.984375
2017-03-02T23:39:42.048970: step 3925, loss 0.00128387, acc 1
2017-03-02T23:39:42.153550: step 3926, loss 0.016003, acc 0.984375
2017-03-02T23:39:42.242892: step 3927, loss 0.00350135, acc 1
2017-03-02T23:39:42.347670: step 3928, loss 0.00147967, acc 1
2017-03-02T23:39:42.458136: step 3929, loss 0.00321033, acc 1
2017-03-02T23:39:42.571389: step 3930, loss 0.00265914, acc 1
2017-03-02T23:39:42.680070: step 3931, loss 0.00157492, acc 1
2017-03-02T23:39:42.790510: step 3932, loss 0.00583129, acc 1
2017-03-02T23:39:42.903761: step 3933, loss 0.00793705, acc 1
2017-03-02T23:39:42.996837: step 3934, loss 0.000927675, acc 1
2017-03-02T23:39:43.100455: step 3935, loss 0.0152368, acc 1
2017-03-02T23:39:43.196646: step 3936, loss 0.000731873, acc 1
2017-03-02T23:39:43.305631: step 3937, loss 0.00184671, acc 1
2017-03-02T23:39:43.407155: step 3938, loss 0.00223767, acc 1
2017-03-02T23:39:43.513404: step 3939, loss 0.00127038, acc 1
2017-03-02T23:39:43.618253: step 3940, loss 0.0129531, acc 1
2017-03-02T23:39:43.708933: step 3941, loss 0.00991819, acc 1
2017-03-02T23:39:43.815748: step 3942, loss 0.0045766, acc 1
2017-03-02T23:39:43.921573: step 3943, loss 0.0146855, acc 0.984375
2017-03-02T23:39:44.027727: step 3944, loss 0.00223441, acc 1
2017-03-02T23:39:44.133447: step 3945, loss 0.00411378, acc 1
2017-03-02T23:39:44.234336: step 3946, loss 0.0210723, acc 0.984375
2017-03-02T23:39:44.339848: step 3947, loss 0.00188748, acc 1
2017-03-02T23:39:44.433090: step 3948, loss 0.00245764, acc 1
2017-03-02T23:39:44.519144: step 3949, loss 0.00248346, acc 1
2017-03-02T23:39:44.625742: step 3950, loss 0.00782176, acc 1
2017-03-02T23:39:44.730744: step 3951, loss 0.00578341, acc 1
2017-03-02T23:39:44.826500: step 3952, loss 0.0024755, acc 1
2017-03-02T23:39:44.936334: step 3953, loss 0.00332135, acc 1
2017-03-02T23:39:45.043457: step 3954, loss 0.00210627, acc 1
2017-03-02T23:39:45.147399: step 3955, loss 0.00482851, acc 1
2017-03-02T23:39:45.244654: step 3956, loss 0.00249372, acc 1
2017-03-02T23:39:45.353178: step 3957, loss 0.00509463, acc 1
2017-03-02T23:39:45.456349: step 3958, loss 0.00194384, acc 1
2017-03-02T23:39:45.572451: step 3959, loss 0.00407682, acc 1
2017-03-02T23:39:45.672907: step 3960, loss 0.00314817, acc 1
2017-03-02T23:39:45.775601: step 3961, loss 0.00191674, acc 1
2017-03-02T23:39:45.882326: step 3962, loss 0.0106025, acc 1
2017-03-02T23:39:45.972541: step 3963, loss 0.00918514, acc 1
2017-03-02T23:39:46.065762: step 3964, loss 0.00139768, acc 1
2017-03-02T23:39:46.169763: step 3965, loss 0.00442507, acc 1
2017-03-02T23:39:46.281566: step 3966, loss 0.00151973, acc 1
2017-03-02T23:39:46.390057: step 3967, loss 0.0120478, acc 1
2017-03-02T23:39:46.503258: step 3968, loss 0.00212371, acc 1
2017-03-02T23:39:46.608458: step 3969, loss 0.0107679, acc 1
2017-03-02T23:39:46.708580: step 3970, loss 0.00495118, acc 1
2017-03-02T23:39:46.807318: step 3971, loss 0.00238459, acc 1
2017-03-02T23:39:46.909986: step 3972, loss 0.000844359, acc 1
2017-03-02T23:39:47.015878: step 3973, loss 0.00826872, acc 1
2017-03-02T23:39:47.124325: step 3974, loss 0.0151668, acc 1
2017-03-02T23:39:47.221776: step 3975, loss 0.00365208, acc 1
2017-03-02T23:39:47.332190: step 3976, loss 0.00850824, acc 1
2017-03-02T23:39:47.427066: step 3977, loss 0.0028547, acc 1
2017-03-02T23:39:47.515681: step 3978, loss 0.00512736, acc 1
2017-03-02T23:39:47.616798: step 3979, loss 0.0215323, acc 1
2017-03-02T23:39:47.721112: step 3980, loss 0.0258923, acc 0.984375
2017-03-02T23:39:47.832705: step 3981, loss 0.00355099, acc 1
2017-03-02T23:39:47.936538: step 3982, loss 0.00445639, acc 1
2017-03-02T23:39:48.035998: step 3983, loss 0.00121152, acc 1
2017-03-02T23:39:48.146749: step 3984, loss 0.00300208, acc 1
2017-03-02T23:39:48.238425: step 3985, loss 0.00341295, acc 1
2017-03-02T23:39:48.344847: step 3986, loss 0.00286619, acc 1
2017-03-02T23:39:48.453741: step 3987, loss 0.00070176, acc 1
2017-03-02T23:39:48.572132: step 3988, loss 0.0025635, acc 1
2017-03-02T23:39:48.678460: step 3989, loss 0.00794937, acc 1
2017-03-02T23:39:48.788522: step 3990, loss 0.00827658, acc 1
2017-03-02T23:39:48.898809: step 3991, loss 0.00491176, acc 1
2017-03-02T23:39:48.996473: step 3992, loss 0.0234802, acc 0.984375
2017-03-02T23:39:49.113476: step 3993, loss 0.000835467, acc 1
2017-03-02T23:39:49.233947: step 3994, loss 0.000567812, acc 1
2017-03-02T23:39:49.339012: step 3995, loss 0.00276372, acc 1
2017-03-02T23:39:49.448074: step 3996, loss 0.00453044, acc 1
2017-03-02T23:39:49.554721: step 3997, loss 0.0123985, acc 1
2017-03-02T23:39:49.676388: step 3998, loss 0.00149724, acc 1
2017-03-02T23:39:49.766637: step 3999, loss 0.00417396, acc 1
2017-03-02T23:39:49.869434: step 4000, loss 0.00506032, acc 1

Evaluation:
2017-03-02T23:39:49.923611: step 4000, loss 2.18938, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4000

2017-03-02T23:39:50.378535: step 4001, loss 0.006723, acc 1
2017-03-02T23:39:50.472908: step 4002, loss 0.000595248, acc 1
2017-03-02T23:39:50.575534: step 4003, loss 0.0611965, acc 0.984375
2017-03-02T23:39:50.685068: step 4004, loss 0.00411815, acc 1
2017-03-02T23:39:50.791176: step 4005, loss 0.00980168, acc 1
2017-03-02T23:39:50.892566: step 4006, loss 0.00145172, acc 1
2017-03-02T23:39:51.001203: step 4007, loss 0.00618141, acc 1
2017-03-02T23:39:51.106104: step 4008, loss 0.00428511, acc 1
2017-03-02T23:39:51.204419: step 4009, loss 0.00582117, acc 1
2017-03-02T23:39:51.303917: step 4010, loss 0.00448599, acc 1
2017-03-02T23:39:51.411156: step 4011, loss 0.030071, acc 1
2017-03-02T23:39:51.520131: step 4012, loss 0.0022578, acc 1
2017-03-02T23:39:51.624424: step 4013, loss 0.000507844, acc 1
2017-03-02T23:39:51.726612: step 4014, loss 0.0206542, acc 1
2017-03-02T23:39:51.834624: step 4015, loss 0.00774615, acc 1
2017-03-02T23:39:51.927604: step 4016, loss 0.00349154, acc 1
2017-03-02T23:39:52.028864: step 4017, loss 0.00472083, acc 1
2017-03-02T23:39:52.129507: step 4018, loss 0.0158204, acc 1
2017-03-02T23:39:52.242560: step 4019, loss 0.0109485, acc 1
2017-03-02T23:39:52.349043: step 4020, loss 0.0026775, acc 1
2017-03-02T23:39:52.463752: step 4021, loss 0.0114913, acc 1
2017-03-02T23:39:52.557930: step 4022, loss 0.00317157, acc 1
2017-03-02T23:39:52.660674: step 4023, loss 0.00499574, acc 1
2017-03-02T23:39:52.744352: step 4024, loss 0.00109752, acc 1
2017-03-02T23:39:52.864233: step 4025, loss 0.00253745, acc 1
2017-03-02T23:39:52.991403: step 4026, loss 0.0106282, acc 1
2017-03-02T23:39:53.118528: step 4027, loss 0.00162723, acc 1
2017-03-02T23:39:53.221033: step 4028, loss 0.00178956, acc 1
2017-03-02T23:39:53.324971: step 4029, loss 0.00069504, acc 1
2017-03-02T23:39:53.428456: step 4030, loss 0.00273212, acc 1
2017-03-02T23:39:53.520484: step 4031, loss 0.00484722, acc 1
2017-03-02T23:39:53.615304: step 4032, loss 0.00130353, acc 1
2017-03-02T23:39:53.720341: step 4033, loss 0.00377864, acc 1
2017-03-02T23:39:53.836864: step 4034, loss 0.000990969, acc 1
2017-03-02T23:39:53.932708: step 4035, loss 0.00124088, acc 1
2017-03-02T23:39:54.042812: step 4036, loss 0.00828548, acc 1
2017-03-02T23:39:54.149543: step 4037, loss 0.00232896, acc 1
2017-03-02T23:39:54.256422: step 4038, loss 0.0013243, acc 1
2017-03-02T23:39:54.362449: step 4039, loss 0.00712172, acc 1
2017-03-02T23:39:54.468724: step 4040, loss 0.00678393, acc 1
2017-03-02T23:39:54.571845: step 4041, loss 0.00158178, acc 1
2017-03-02T23:39:54.679478: step 4042, loss 0.000553399, acc 1
2017-03-02T23:39:54.789364: step 4043, loss 0.0040134, acc 1
2017-03-02T23:39:54.896109: step 4044, loss 0.00245025, acc 1
2017-03-02T23:39:55.010488: step 4045, loss 0.00440609, acc 1
2017-03-02T23:39:55.112495: step 4046, loss 0.00297485, acc 1
2017-03-02T23:39:55.220067: step 4047, loss 0.0218684, acc 0.984375
2017-03-02T23:39:55.329377: step 4048, loss 0.0043017, acc 1
2017-03-02T23:39:55.437008: step 4049, loss 0.00280215, acc 1
2017-03-02T23:39:55.547667: step 4050, loss 0.00211027, acc 1
2017-03-02T23:39:55.655320: step 4051, loss 0.00225193, acc 1
2017-03-02T23:39:55.752941: step 4052, loss 0.000949688, acc 1
2017-03-02T23:39:55.854723: step 4053, loss 0.00123917, acc 1
2017-03-02T23:39:55.961343: step 4054, loss 0.00249309, acc 1
2017-03-02T23:39:56.071897: step 4055, loss 0.0610502, acc 0.96875
2017-03-02T23:39:56.186366: step 4056, loss 0.00661983, acc 1
2017-03-02T23:39:56.291113: step 4057, loss 0.00262559, acc 1
2017-03-02T23:39:56.396540: step 4058, loss 0.00342179, acc 1
2017-03-02T23:39:56.493211: step 4059, loss 0.00400786, acc 1
2017-03-02T23:39:56.587815: step 4060, loss 0.0222426, acc 0.984375
2017-03-02T23:39:56.688674: step 4061, loss 0.00193909, acc 1
2017-03-02T23:39:56.791868: step 4062, loss 0.0282009, acc 0.984375
2017-03-02T23:39:56.900130: step 4063, loss 0.0021814, acc 1
2017-03-02T23:39:57.001063: step 4064, loss 0.00625584, acc 1
2017-03-02T23:39:57.105596: step 4065, loss 0.0136595, acc 1
2017-03-02T23:39:57.228166: step 4066, loss 0.00179242, acc 1
2017-03-02T23:39:57.324789: step 4067, loss 0.00392405, acc 1
2017-03-02T23:39:57.438729: step 4068, loss 0.00821159, acc 1
2017-03-02T23:39:57.548840: step 4069, loss 0.00100703, acc 1
2017-03-02T23:39:57.657725: step 4070, loss 0.00710242, acc 1
2017-03-02T23:39:57.766770: step 4071, loss 0.00354563, acc 1
2017-03-02T23:39:57.871283: step 4072, loss 0.00297655, acc 1
2017-03-02T23:39:57.969758: step 4073, loss 0.00139901, acc 1
2017-03-02T23:39:58.048822: step 4074, loss 0.0070462, acc 1
2017-03-02T23:39:58.151448: step 4075, loss 0.002207, acc 1
2017-03-02T23:39:58.257001: step 4076, loss 0.00092603, acc 1
2017-03-02T23:39:58.362140: step 4077, loss 0.00212824, acc 1
2017-03-02T23:39:58.476388: step 4078, loss 0.00578014, acc 1
2017-03-02T23:39:58.581689: step 4079, loss 0.0272343, acc 0.984375
2017-03-02T23:39:58.692203: step 4080, loss 0.0022817, acc 1
2017-03-02T23:39:58.790764: step 4081, loss 0.0025296, acc 1
2017-03-02T23:39:58.891599: step 4082, loss 0.00272527, acc 1
2017-03-02T23:39:58.993141: step 4083, loss 0.00134644, acc 1
2017-03-02T23:39:59.097376: step 4084, loss 0.0186491, acc 0.984375
2017-03-02T23:39:59.207720: step 4085, loss 0.00770482, acc 1
2017-03-02T23:39:59.313234: step 4086, loss 0.00146355, acc 1
2017-03-02T23:39:59.427243: step 4087, loss 0.00397408, acc 1
2017-03-02T23:39:59.518934: step 4088, loss 0.00179157, acc 1
2017-03-02T23:39:59.616366: step 4089, loss 0.00216706, acc 1
2017-03-02T23:39:59.720691: step 4090, loss 0.00421607, acc 1
2017-03-02T23:39:59.826725: step 4091, loss 0.00390961, acc 1
2017-03-02T23:39:59.924092: step 4092, loss 0.000816131, acc 1
2017-03-02T23:40:00.029256: step 4093, loss 0.00121179, acc 1
2017-03-02T23:40:00.123015: step 4094, loss 0.00458181, acc 1
2017-03-02T23:40:00.223165: step 4095, loss 0.00239438, acc 1
2017-03-02T23:40:00.310677: step 4096, loss 0.0078448, acc 1
2017-03-02T23:40:00.416922: step 4097, loss 0.000826054, acc 1
2017-03-02T23:40:00.523387: step 4098, loss 0.0028146, acc 1
2017-03-02T23:40:00.639435: step 4099, loss 0.003074, acc 1
2017-03-02T23:40:00.751100: step 4100, loss 0.0108997, acc 1

Evaluation:
2017-03-02T23:40:00.806670: step 4100, loss 2.54062, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4100

2017-03-02T23:40:01.263782: step 4101, loss 0.00878883, acc 1
2017-03-02T23:40:01.369115: step 4102, loss 0.00241182, acc 1
2017-03-02T23:40:01.476196: step 4103, loss 0.00128805, acc 1
2017-03-02T23:40:01.582325: step 4104, loss 0.00137394, acc 1
2017-03-02T23:40:01.690410: step 4105, loss 0.00733465, acc 1
2017-03-02T23:40:01.776908: step 4106, loss 0.000896299, acc 1
2017-03-02T23:40:01.880496: step 4107, loss 0.00531916, acc 1
2017-03-02T23:40:01.991438: step 4108, loss 0.00382428, acc 1
2017-03-02T23:40:02.103720: step 4109, loss 0.00147886, acc 1
2017-03-02T23:40:02.218069: step 4110, loss 0.00375884, acc 1
2017-03-02T23:40:02.324892: step 4111, loss 0.00457269, acc 1
2017-03-02T23:40:02.434639: step 4112, loss 0.00674189, acc 1
2017-03-02T23:40:02.520576: step 4113, loss 0.00343519, acc 1
2017-03-02T23:40:02.623991: step 4114, loss 0.00104143, acc 1
2017-03-02T23:40:02.738660: step 4115, loss 0.00190254, acc 1
2017-03-02T23:40:02.839666: step 4116, loss 0.00410049, acc 1
2017-03-02T23:40:02.943783: step 4117, loss 0.00271764, acc 1
2017-03-02T23:40:03.059160: step 4118, loss 0.00270589, acc 1
2017-03-02T23:40:03.164999: step 4119, loss 0.00169506, acc 1
2017-03-02T23:40:03.256136: step 4120, loss 0.00273489, acc 1
2017-03-02T23:40:03.354679: step 4121, loss 0.064549, acc 0.984375
2017-03-02T23:40:03.457233: step 4122, loss 0.00115135, acc 1
2017-03-02T23:40:03.566922: step 4123, loss 0.00719627, acc 1
2017-03-02T23:40:03.674675: step 4124, loss 0.00397519, acc 1
2017-03-02T23:40:03.787615: step 4125, loss 0.00078067, acc 1
2017-03-02T23:40:03.892648: step 4126, loss 0.00390348, acc 1
2017-03-02T23:40:03.990193: step 4127, loss 0.0110967, acc 1
2017-03-02T23:40:04.093142: step 4128, loss 0.00717421, acc 1
2017-03-02T23:40:04.198912: step 4129, loss 0.00677998, acc 1
2017-03-02T23:40:04.305559: step 4130, loss 0.00100978, acc 1
2017-03-02T23:40:04.412367: step 4131, loss 0.00167846, acc 1
2017-03-02T23:40:04.523702: step 4132, loss 0.00154228, acc 1
2017-03-02T23:40:04.634118: step 4133, loss 0.00908919, acc 1
2017-03-02T23:40:04.729680: step 4134, loss 0.00326176, acc 1
2017-03-02T23:40:04.831855: step 4135, loss 0.00341837, acc 1
2017-03-02T23:40:04.937127: step 4136, loss 0.00312805, acc 1
2017-03-02T23:40:05.052121: step 4137, loss 0.0196157, acc 0.984375
2017-03-02T23:40:05.165661: step 4138, loss 0.00672626, acc 1
2017-03-02T23:40:05.278028: step 4139, loss 0.000660954, acc 1
2017-03-02T23:40:05.385251: step 4140, loss 0.004528, acc 1
2017-03-02T23:40:05.467052: step 4141, loss 0.000500251, acc 1
2017-03-02T23:40:05.580044: step 4142, loss 0.00537454, acc 1
2017-03-02T23:40:05.683816: step 4143, loss 0.0314683, acc 0.984375
2017-03-02T23:40:05.791947: step 4144, loss 0.0123066, acc 1
2017-03-02T23:40:05.899685: step 4145, loss 0.00169021, acc 1
2017-03-02T23:40:06.008720: step 4146, loss 0.00841976, acc 1
2017-03-02T23:40:06.124827: step 4147, loss 0.00236158, acc 1
2017-03-02T23:40:06.216073: step 4148, loss 0.0104877, acc 1
2017-03-02T23:40:06.318958: step 4149, loss 0.00933533, acc 1
2017-03-02T23:40:06.430902: step 4150, loss 0.00614603, acc 1
2017-03-02T23:40:06.543122: step 4151, loss 0.0160159, acc 1
2017-03-02T23:40:06.647102: step 4152, loss 0.00238787, acc 1
2017-03-02T23:40:06.762504: step 4153, loss 0.00071326, acc 1
2017-03-02T23:40:06.871971: step 4154, loss 0.00271313, acc 1
2017-03-02T23:40:06.957522: step 4155, loss 0.00239981, acc 1
2017-03-02T23:40:07.057000: step 4156, loss 0.00211775, acc 1
2017-03-02T23:40:07.158471: step 4157, loss 0.00137042, acc 1
2017-03-02T23:40:07.259877: step 4158, loss 0.00170431, acc 1
2017-03-02T23:40:07.364972: step 4159, loss 0.00303512, acc 1
2017-03-02T23:40:07.466429: step 4160, loss 0.00371629, acc 1
2017-03-02T23:40:07.571520: step 4161, loss 0.000401646, acc 1
2017-03-02T23:40:07.669043: step 4162, loss 0.00241738, acc 1
2017-03-02T23:40:07.760604: step 4163, loss 0.00440578, acc 1
2017-03-02T23:40:07.881980: step 4164, loss 0.00131533, acc 1
2017-03-02T23:40:07.989575: step 4165, loss 0.00269608, acc 1
2017-03-02T23:40:08.112549: step 4166, loss 0.00397637, acc 1
2017-03-02T23:40:08.223147: step 4167, loss 0.00291232, acc 1
2017-03-02T23:40:08.320970: step 4168, loss 0.00105412, acc 1
2017-03-02T23:40:08.425456: step 4169, loss 0.000685384, acc 1
2017-03-02T23:40:08.518800: step 4170, loss 0.000438665, acc 1
2017-03-02T23:40:08.625164: step 4171, loss 0.00218352, acc 1
2017-03-02T23:40:08.737942: step 4172, loss 0.00730238, acc 1
2017-03-02T23:40:08.842778: step 4173, loss 0.00208117, acc 1
2017-03-02T23:40:08.952413: step 4174, loss 0.0193798, acc 0.984375
2017-03-02T23:40:09.057875: step 4175, loss 0.000435327, acc 1
2017-03-02T23:40:09.169092: step 4176, loss 0.0021916, acc 1
2017-03-02T23:40:09.255986: step 4177, loss 0.00367633, acc 1
2017-03-02T23:40:09.357752: step 4178, loss 0.0029189, acc 1
2017-03-02T23:40:09.481285: step 4179, loss 0.0122642, acc 1
2017-03-02T23:40:09.584897: step 4180, loss 0.00217779, acc 1
2017-03-02T23:40:09.678595: step 4181, loss 0.00084885, acc 1
2017-03-02T23:40:09.774384: step 4182, loss 0.000452285, acc 1
2017-03-02T23:40:09.882977: step 4183, loss 0.00292303, acc 1
2017-03-02T23:40:09.972852: step 4184, loss 0.0297302, acc 0.984375
2017-03-02T23:40:10.057365: step 4185, loss 0.0168489, acc 0.984375
2017-03-02T23:40:10.160326: step 4186, loss 0.00103736, acc 1
2017-03-02T23:40:10.270286: step 4187, loss 0.00171303, acc 1
2017-03-02T23:40:10.385005: step 4188, loss 0.00107707, acc 1
2017-03-02T23:40:10.489420: step 4189, loss 0.00395375, acc 1
2017-03-02T23:40:10.591750: step 4190, loss 0.00947848, acc 1
2017-03-02T23:40:10.693843: step 4191, loss 0.0144207, acc 0.984375
2017-03-02T23:40:10.785109: step 4192, loss 0.0096512, acc 1
2017-03-02T23:40:10.888943: step 4193, loss 0.00490917, acc 1
2017-03-02T23:40:10.993900: step 4194, loss 0.00244471, acc 1
2017-03-02T23:40:11.096183: step 4195, loss 0.00239342, acc 1
2017-03-02T23:40:11.204611: step 4196, loss 0.00183888, acc 1
2017-03-02T23:40:11.311318: step 4197, loss 0.00177448, acc 1
2017-03-02T23:40:11.419134: step 4198, loss 0.00508816, acc 1
2017-03-02T23:40:11.506329: step 4199, loss 0.00202996, acc 1
2017-03-02T23:40:11.603018: step 4200, loss 0.0243959, acc 0.984375

Evaluation:
2017-03-02T23:40:11.665915: step 4200, loss 2.31885, acc 0.543253

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4200

2017-03-02T23:40:12.136172: step 4201, loss 0.00046753, acc 1
2017-03-02T23:40:12.226546: step 4202, loss 0.00197836, acc 1
2017-03-02T23:40:12.335718: step 4203, loss 0.00173334, acc 1
2017-03-02T23:40:12.447493: step 4204, loss 0.00213996, acc 1
2017-03-02T23:40:12.554182: step 4205, loss 0.00162795, acc 1
2017-03-02T23:40:12.659419: step 4206, loss 0.0069093, acc 1
2017-03-02T23:40:12.761308: step 4207, loss 0.00137963, acc 1
2017-03-02T23:40:12.866499: step 4208, loss 0.00512697, acc 1
2017-03-02T23:40:12.958560: step 4209, loss 0.00171858, acc 1
2017-03-02T23:40:13.059482: step 4210, loss 0.00357143, acc 1
2017-03-02T23:40:13.163981: step 4211, loss 0.00999616, acc 1
2017-03-02T23:40:13.270967: step 4212, loss 0.00266359, acc 1
2017-03-02T23:40:13.376933: step 4213, loss 0.00367272, acc 1
2017-03-02T23:40:13.484861: step 4214, loss 0.00311066, acc 1
2017-03-02T23:40:13.592049: step 4215, loss 0.00210413, acc 1
2017-03-02T23:40:13.686332: step 4216, loss 0.00448428, acc 1
2017-03-02T23:40:13.772300: step 4217, loss 0.0094957, acc 1
2017-03-02T23:40:13.871937: step 4218, loss 0.00173975, acc 1
2017-03-02T23:40:13.974439: step 4219, loss 0.00263711, acc 1
2017-03-02T23:40:14.083258: step 4220, loss 0.00223879, acc 1
2017-03-02T23:40:14.191741: step 4221, loss 0.000773179, acc 1
2017-03-02T23:40:14.294930: step 4222, loss 0.0182741, acc 0.984375
2017-03-02T23:40:14.400010: step 4223, loss 0.0028292, acc 1
2017-03-02T23:40:14.495307: step 4224, loss 0.00209223, acc 1
2017-03-02T23:40:14.596738: step 4225, loss 0.000855845, acc 1
2017-03-02T23:40:14.700837: step 4226, loss 0.00249076, acc 1
2017-03-02T23:40:14.814785: step 4227, loss 0.0235737, acc 0.984375
2017-03-02T23:40:14.933986: step 4228, loss 0.00241171, acc 1
2017-03-02T23:40:15.036724: step 4229, loss 0.00265239, acc 1
2017-03-02T23:40:15.141022: step 4230, loss 0.00318154, acc 1
2017-03-02T23:40:15.246760: step 4231, loss 0.0226635, acc 0.984375
2017-03-02T23:40:15.348478: step 4232, loss 0.00168138, acc 1
2017-03-02T23:40:15.454764: step 4233, loss 0.0110384, acc 1
2017-03-02T23:40:15.574606: step 4234, loss 0.00396693, acc 1
2017-03-02T23:40:15.691028: step 4235, loss 0.00178416, acc 1
2017-03-02T23:40:15.800167: step 4236, loss 0.001198, acc 1
2017-03-02T23:40:15.909797: step 4237, loss 0.0376653, acc 0.984375
2017-03-02T23:40:15.998510: step 4238, loss 0.00234028, acc 1
2017-03-02T23:40:16.106986: step 4239, loss 0.000809942, acc 1
2017-03-02T23:40:16.212639: step 4240, loss 0.00243694, acc 1
2017-03-02T23:40:16.318377: step 4241, loss 0.000813517, acc 1
2017-03-02T23:40:16.428400: step 4242, loss 0.00713242, acc 1
2017-03-02T23:40:16.533598: step 4243, loss 0.00157306, acc 1
2017-03-02T23:40:16.641510: step 4244, loss 0.00614477, acc 1
2017-03-02T23:40:16.737148: step 4245, loss 0.00310516, acc 1
2017-03-02T23:40:16.856778: step 4246, loss 0.000999851, acc 1
2017-03-02T23:40:16.965694: step 4247, loss 0.000855095, acc 1
2017-03-02T23:40:17.073195: step 4248, loss 0.00111397, acc 1
2017-03-02T23:40:17.181243: step 4249, loss 0.00420581, acc 1
2017-03-02T23:40:17.287849: step 4250, loss 0.0015604, acc 1
2017-03-02T23:40:17.397796: step 4251, loss 0.00479543, acc 1
2017-03-02T23:40:17.484310: step 4252, loss 0.00147274, acc 1
2017-03-02T23:40:17.588159: step 4253, loss 0.00380957, acc 1
2017-03-02T23:40:17.699489: step 4254, loss 0.00288378, acc 1
2017-03-02T23:40:17.806768: step 4255, loss 0.000768178, acc 1
2017-03-02T23:40:17.913961: step 4256, loss 0.00349954, acc 1
2017-03-02T23:40:18.013709: step 4257, loss 0.00393938, acc 1
2017-03-02T23:40:18.130205: step 4258, loss 0.0056691, acc 1
2017-03-02T23:40:18.226268: step 4259, loss 0.0217978, acc 0.984375
2017-03-02T23:40:18.331062: step 4260, loss 0.00155948, acc 1
2017-03-02T23:40:18.442922: step 4261, loss 0.00127506, acc 1
2017-03-02T23:40:18.552798: step 4262, loss 0.0119713, acc 0.984375
2017-03-02T23:40:18.656379: step 4263, loss 0.0330601, acc 0.984375
2017-03-02T23:40:18.753173: step 4264, loss 0.000745401, acc 1
2017-03-02T23:40:18.869473: step 4265, loss 0.00635863, acc 1
2017-03-02T23:40:18.960924: step 4266, loss 0.00934908, acc 1
2017-03-02T23:40:19.061989: step 4267, loss 0.00825586, acc 1
2017-03-02T23:40:19.201375: step 4268, loss 0.000808934, acc 1
2017-03-02T23:40:19.305591: step 4269, loss 0.00911903, acc 1
2017-03-02T23:40:19.411695: step 4270, loss 0.000894871, acc 1
2017-03-02T23:40:19.521328: step 4271, loss 0.00410314, acc 1
2017-03-02T23:40:19.629806: step 4272, loss 0.00695243, acc 1
2017-03-02T23:40:19.717405: step 4273, loss 0.00154196, acc 1
2017-03-02T23:40:19.818202: step 4274, loss 0.00341152, acc 1
2017-03-02T23:40:19.924515: step 4275, loss 0.00199246, acc 1
2017-03-02T23:40:20.028191: step 4276, loss 0.00284026, acc 1
2017-03-02T23:40:20.134505: step 4277, loss 0.00441736, acc 1
2017-03-02T23:40:20.240902: step 4278, loss 0.00133874, acc 1
2017-03-02T23:40:20.345676: step 4279, loss 0.000798756, acc 1
2017-03-02T23:40:20.435498: step 4280, loss 0.00255464, acc 1
2017-03-02T23:40:20.530123: step 4281, loss 0.00220051, acc 1
2017-03-02T23:40:20.635128: step 4282, loss 0.0443236, acc 0.984375
2017-03-02T23:40:20.736407: step 4283, loss 0.000458568, acc 1
2017-03-02T23:40:20.853638: step 4284, loss 0.00188984, acc 1
2017-03-02T23:40:20.957691: step 4285, loss 0.00387004, acc 1
2017-03-02T23:40:21.063466: step 4286, loss 0.00729892, acc 1
2017-03-02T23:40:21.160053: step 4287, loss 0.00392091, acc 1
2017-03-02T23:40:21.251005: step 4288, loss 0.00279149, acc 1
2017-03-02T23:40:21.359954: step 4289, loss 0.00185267, acc 1
2017-03-02T23:40:21.476027: step 4290, loss 0.00455259, acc 1
2017-03-02T23:40:21.583500: step 4291, loss 0.00076502, acc 1
2017-03-02T23:40:21.689819: step 4292, loss 0.00159994, acc 1
2017-03-02T23:40:21.794865: step 4293, loss 0.0118577, acc 1
2017-03-02T23:40:21.897246: step 4294, loss 0.00210263, acc 1
2017-03-02T23:40:21.985478: step 4295, loss 0.000831232, acc 1
2017-03-02T23:40:22.101965: step 4296, loss 0.000881736, acc 1
2017-03-02T23:40:22.211327: step 4297, loss 0.00228688, acc 1
2017-03-02T23:40:22.320197: step 4298, loss 0.000603599, acc 1
2017-03-02T23:40:22.423328: step 4299, loss 0.00359546, acc 1
2017-03-02T23:40:22.533552: step 4300, loss 0.0633312, acc 0.953125

Evaluation:
2017-03-02T23:40:22.588849: step 4300, loss 2.4709, acc 0.536332

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4300

2017-03-02T23:40:23.039901: step 4301, loss 0.00853701, acc 1
2017-03-02T23:40:23.147638: step 4302, loss 0.00641883, acc 1
2017-03-02T23:40:23.256986: step 4303, loss 0.00104338, acc 1
2017-03-02T23:40:23.364947: step 4304, loss 0.00289192, acc 1
2017-03-02T23:40:23.455330: step 4305, loss 0.00169516, acc 1
2017-03-02T23:40:23.556061: step 4306, loss 0.00219758, acc 1
2017-03-02T23:40:23.663831: step 4307, loss 0.00333832, acc 1
2017-03-02T23:40:23.772674: step 4308, loss 0.00330013, acc 1
2017-03-02T23:40:23.882158: step 4309, loss 0.0075615, acc 1
2017-03-02T23:40:23.982671: step 4310, loss 0.00287596, acc 1
2017-03-02T23:40:24.088475: step 4311, loss 0.00419474, acc 1
2017-03-02T23:40:24.175684: step 4312, loss 0.00164742, acc 1
2017-03-02T23:40:24.274546: step 4313, loss 0.00166221, acc 1
2017-03-02T23:40:24.380907: step 4314, loss 0.0041878, acc 1
2017-03-02T23:40:24.487074: step 4315, loss 0.00328202, acc 1
2017-03-02T23:40:24.588914: step 4316, loss 0.00218277, acc 1
2017-03-02T23:40:24.696787: step 4317, loss 0.0012937, acc 1
2017-03-02T23:40:24.800500: step 4318, loss 0.00193771, acc 1
2017-03-02T23:40:24.916028: step 4319, loss 0.00512761, acc 1
2017-03-02T23:40:25.007925: step 4320, loss 0.00148323, acc 1
2017-03-02T23:40:25.112985: step 4321, loss 0.00278936, acc 1
2017-03-02T23:40:25.221176: step 4322, loss 0.00533099, acc 1
2017-03-02T23:40:25.330393: step 4323, loss 0.00215088, acc 1
2017-03-02T23:40:25.436485: step 4324, loss 0.00430832, acc 1
2017-03-02T23:40:25.551224: step 4325, loss 0.000348099, acc 1
2017-03-02T23:40:25.655228: step 4326, loss 0.00442405, acc 1
2017-03-02T23:40:25.748433: step 4327, loss 0.00239714, acc 1
2017-03-02T23:40:25.854916: step 4328, loss 0.00886304, acc 1
2017-03-02T23:40:25.949452: step 4329, loss 0.0031767, acc 1
2017-03-02T23:40:26.058184: step 4330, loss 0.00332698, acc 1
2017-03-02T23:40:26.168054: step 4331, loss 0.00372053, acc 1
2017-03-02T23:40:26.260087: step 4332, loss 0.00368375, acc 1
2017-03-02T23:40:26.365562: step 4333, loss 0.00743029, acc 1
2017-03-02T23:40:26.453362: step 4334, loss 0.00038092, acc 1
2017-03-02T23:40:26.553569: step 4335, loss 0.0187702, acc 1
2017-03-02T23:40:26.662964: step 4336, loss 0.000551819, acc 1
2017-03-02T23:40:26.768214: step 4337, loss 0.0082383, acc 1
2017-03-02T23:40:26.873150: step 4338, loss 0.0108277, acc 1
2017-03-02T23:40:26.975791: step 4339, loss 0.000414988, acc 1
2017-03-02T23:40:27.078773: step 4340, loss 0.00128926, acc 1
2017-03-02T23:40:27.168070: step 4341, loss 0.00355329, acc 1
2017-03-02T23:40:27.260959: step 4342, loss 0.00143278, acc 1
2017-03-02T23:40:27.368206: step 4343, loss 0.00174002, acc 1
2017-03-02T23:40:27.482938: step 4344, loss 0.0081837, acc 1
2017-03-02T23:40:27.590642: step 4345, loss 0.00825982, acc 1
2017-03-02T23:40:27.688959: step 4346, loss 0.00992944, acc 1
2017-03-02T23:40:27.792409: step 4347, loss 0.0798202, acc 0.984375
2017-03-02T23:40:27.900552: step 4348, loss 0.00327484, acc 1
2017-03-02T23:40:27.995524: step 4349, loss 0.00923616, acc 1
2017-03-02T23:40:28.101361: step 4350, loss 0.00168699, acc 1
2017-03-02T23:40:28.213455: step 4351, loss 0.00338794, acc 1
2017-03-02T23:40:28.318782: step 4352, loss 0.00175669, acc 1
2017-03-02T23:40:28.440395: step 4353, loss 0.00727206, acc 1
2017-03-02T23:40:28.557922: step 4354, loss 0.00162363, acc 1
2017-03-02T23:40:28.690775: step 4355, loss 0.0169404, acc 1
2017-03-02T23:40:28.787561: step 4356, loss 0.00383282, acc 1
2017-03-02T23:40:28.912414: step 4357, loss 0.00607333, acc 1
2017-03-02T23:40:29.014957: step 4358, loss 0.00130294, acc 1
2017-03-02T23:40:29.124068: step 4359, loss 0.00400931, acc 1
2017-03-02T23:40:29.242206: step 4360, loss 0.00576355, acc 1
2017-03-02T23:40:29.347215: step 4361, loss 0.00204344, acc 1
2017-03-02T23:40:29.441251: step 4362, loss 0.00160836, acc 1
2017-03-02T23:40:29.528744: step 4363, loss 0.006174, acc 1
2017-03-02T23:40:29.635290: step 4364, loss 0.00125648, acc 1
2017-03-02T23:40:29.738129: step 4365, loss 0.0109222, acc 1
2017-03-02T23:40:29.855412: step 4366, loss 0.00314052, acc 1
2017-03-02T23:40:29.959537: step 4367, loss 0.0012391, acc 1
2017-03-02T23:40:30.067098: step 4368, loss 0.0119816, acc 1
2017-03-02T23:40:30.177926: step 4369, loss 0.011685, acc 1
2017-03-02T23:40:30.271356: step 4370, loss 0.0359723, acc 0.984375
2017-03-02T23:40:30.377798: step 4371, loss 0.00708965, acc 1
2017-03-02T23:40:30.488020: step 4372, loss 0.00605371, acc 1
2017-03-02T23:40:30.593904: step 4373, loss 0.00266727, acc 1
2017-03-02T23:40:30.690683: step 4374, loss 0.00158447, acc 1
2017-03-02T23:40:30.791677: step 4375, loss 0.000724439, acc 1
2017-03-02T23:40:30.904288: step 4376, loss 0.0077619, acc 1
2017-03-02T23:40:30.996651: step 4377, loss 0.000885446, acc 1
2017-03-02T23:40:31.097856: step 4378, loss 0.000382546, acc 1
2017-03-02T23:40:31.200319: step 4379, loss 0.0031768, acc 1
2017-03-02T23:40:31.304098: step 4380, loss 0.00444442, acc 1
2017-03-02T23:40:31.409611: step 4381, loss 0.00238177, acc 1
2017-03-02T23:40:31.516124: step 4382, loss 0.0176014, acc 0.984375
2017-03-02T23:40:31.618995: step 4383, loss 0.00168891, acc 1
2017-03-02T23:40:31.715020: step 4384, loss 0.00288521, acc 1
2017-03-02T23:40:31.810145: step 4385, loss 0.00149081, acc 1
2017-03-02T23:40:31.932808: step 4386, loss 0.00239376, acc 1
2017-03-02T23:40:32.025664: step 4387, loss 0.00366821, acc 1
2017-03-02T23:40:32.136188: step 4388, loss 0.00338335, acc 1
2017-03-02T23:40:32.241415: step 4389, loss 0.00394058, acc 1
2017-03-02T23:40:32.349592: step 4390, loss 0.0209497, acc 0.984375
2017-03-02T23:40:32.453643: step 4391, loss 0.0157632, acc 0.984375
2017-03-02T23:40:32.548227: step 4392, loss 0.00139553, acc 1
2017-03-02T23:40:32.656124: step 4393, loss 0.00259878, acc 1
2017-03-02T23:40:32.759080: step 4394, loss 0.00300894, acc 1
2017-03-02T23:40:32.866492: step 4395, loss 0.00117211, acc 1
2017-03-02T23:40:32.980181: step 4396, loss 0.033521, acc 0.96875
2017-03-02T23:40:33.085876: step 4397, loss 0.000890145, acc 1
2017-03-02T23:40:33.194978: step 4398, loss 0.00573788, acc 1
2017-03-02T23:40:33.279125: step 4399, loss 0.00156533, acc 1
2017-03-02T23:40:33.376713: step 4400, loss 0.0101324, acc 1

Evaluation:
2017-03-02T23:40:33.432605: step 4400, loss 2.39667, acc 0.525952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4400

2017-03-02T23:40:33.891429: step 4401, loss 0.00380432, acc 1
2017-03-02T23:40:33.978762: step 4402, loss 0.00178357, acc 1
2017-03-02T23:40:34.093912: step 4403, loss 0.00149023, acc 1
2017-03-02T23:40:34.198386: step 4404, loss 0.00871846, acc 1
2017-03-02T23:40:34.305387: step 4405, loss 0.00228744, acc 1
2017-03-02T23:40:34.413740: step 4406, loss 0.00340652, acc 1
2017-03-02T23:40:34.519133: step 4407, loss 0.00694937, acc 1
2017-03-02T23:40:34.620088: step 4408, loss 0.00341526, acc 1
2017-03-02T23:40:34.706622: step 4409, loss 0.00164599, acc 1
2017-03-02T23:40:34.826824: step 4410, loss 0.00212054, acc 1
2017-03-02T23:40:34.929989: step 4411, loss 0.00567103, acc 1
2017-03-02T23:40:35.043410: step 4412, loss 0.00167178, acc 1
2017-03-02T23:40:35.152787: step 4413, loss 0.00236656, acc 1
2017-03-02T23:40:35.256925: step 4414, loss 0.00143851, acc 1
2017-03-02T23:40:35.367974: step 4415, loss 0.00161527, acc 1
2017-03-02T23:40:35.459351: step 4416, loss 0.014926, acc 0.984375
2017-03-02T23:40:35.554840: step 4417, loss 0.00141099, acc 1
2017-03-02T23:40:35.662546: step 4418, loss 0.0096829, acc 1
2017-03-02T23:40:35.772766: step 4419, loss 0.0035063, acc 1
2017-03-02T23:40:35.884780: step 4420, loss 0.00876908, acc 1
2017-03-02T23:40:35.986333: step 4421, loss 0.00829493, acc 1
2017-03-02T23:40:36.091482: step 4422, loss 0.03139, acc 0.984375
2017-03-02T23:40:36.192207: step 4423, loss 0.00118155, acc 1
2017-03-02T23:40:36.295735: step 4424, loss 0.00236961, acc 1
2017-03-02T23:40:36.396674: step 4425, loss 0.000543719, acc 1
2017-03-02T23:40:36.506921: step 4426, loss 0.00340436, acc 1
2017-03-02T23:40:36.619245: step 4427, loss 0.00903319, acc 1
2017-03-02T23:40:36.713885: step 4428, loss 0.00128296, acc 1
2017-03-02T23:40:36.827963: step 4429, loss 0.000624901, acc 1
2017-03-02T23:40:36.927214: step 4430, loss 0.0004466, acc 1
2017-03-02T23:40:37.028641: step 4431, loss 0.000676094, acc 1
2017-03-02T23:40:37.133512: step 4432, loss 0.00102742, acc 1
2017-03-02T23:40:37.238547: step 4433, loss 0.00164691, acc 1
2017-03-02T23:40:37.340966: step 4434, loss 0.00102253, acc 1
2017-03-02T23:40:37.446806: step 4435, loss 0.000790076, acc 1
2017-03-02T23:40:37.551956: step 4436, loss 0.00196117, acc 1
2017-03-02T23:40:37.639947: step 4437, loss 0.00199606, acc 1
2017-03-02T23:40:37.729891: step 4438, loss 0.00285681, acc 1
2017-03-02T23:40:37.828651: step 4439, loss 0.00975438, acc 1
2017-03-02T23:40:37.933422: step 4440, loss 0.00314701, acc 1
2017-03-02T23:40:38.045618: step 4441, loss 0.000642353, acc 1
2017-03-02T23:40:38.147464: step 4442, loss 0.00335873, acc 1
2017-03-02T23:40:38.256508: step 4443, loss 0.000649936, acc 1
2017-03-02T23:40:38.365512: step 4444, loss 0.00335634, acc 1
2017-03-02T23:40:38.456034: step 4445, loss 0.000854163, acc 1
2017-03-02T23:40:38.552201: step 4446, loss 0.00382545, acc 1
2017-03-02T23:40:38.661375: step 4447, loss 0.000867652, acc 1
2017-03-02T23:40:38.776003: step 4448, loss 0.0186551, acc 0.984375
2017-03-02T23:40:38.887718: step 4449, loss 0.00334928, acc 1
2017-03-02T23:40:38.990683: step 4450, loss 0.00145141, acc 1
2017-03-02T23:40:39.096007: step 4451, loss 0.00241959, acc 1
2017-03-02T23:40:39.191943: step 4452, loss 0.00327723, acc 1
2017-03-02T23:40:39.295504: step 4453, loss 0.00171614, acc 1
2017-03-02T23:40:39.404036: step 4454, loss 0.00931396, acc 1
2017-03-02T23:40:39.512091: step 4455, loss 0.00604599, acc 1
2017-03-02T23:40:39.626989: step 4456, loss 0.00307258, acc 1
2017-03-02T23:40:39.729785: step 4457, loss 0.000829432, acc 1
2017-03-02T23:40:39.828379: step 4458, loss 0.00112501, acc 1
2017-03-02T23:40:39.920140: step 4459, loss 0.00126324, acc 1
2017-03-02T23:40:40.014519: step 4460, loss 0.000544082, acc 1
2017-03-02T23:40:40.140349: step 4461, loss 0.00090694, acc 1
2017-03-02T23:40:40.245061: step 4462, loss 0.00156436, acc 1
2017-03-02T23:40:40.345252: step 4463, loss 0.00283051, acc 1
2017-03-02T23:40:40.443916: step 4464, loss 0.000639654, acc 1
2017-03-02T23:40:40.544569: step 4465, loss 0.00798398, acc 1
2017-03-02T23:40:40.659291: step 4466, loss 0.00145341, acc 1
2017-03-02T23:40:40.741518: step 4467, loss 0.00110363, acc 1
2017-03-02T23:40:40.844343: step 4468, loss 0.00249678, acc 1
2017-03-02T23:40:40.941955: step 4469, loss 0.0124406, acc 1
2017-03-02T23:40:41.040232: step 4470, loss 0.0089288, acc 1
2017-03-02T23:40:41.146517: step 4471, loss 0.00152282, acc 1
2017-03-02T23:40:41.254540: step 4472, loss 0.0114281, acc 1
2017-03-02T23:40:41.359787: step 4473, loss 0.00151234, acc 1
2017-03-02T23:40:41.448484: step 4474, loss 0.00883577, acc 1
2017-03-02T23:40:41.545488: step 4475, loss 0.00105751, acc 1
2017-03-02T23:40:41.653979: step 4476, loss 0.00271991, acc 1
2017-03-02T23:40:41.758133: step 4477, loss 0.0151132, acc 1
2017-03-02T23:40:41.855814: step 4478, loss 0.00354028, acc 1
2017-03-02T23:40:41.958949: step 4479, loss 0.00286642, acc 1
2017-03-02T23:40:42.069784: step 4480, loss 0.0107535, acc 1
2017-03-02T23:40:42.166983: step 4481, loss 0.0015436, acc 1
2017-03-02T23:40:42.259902: step 4482, loss 0.00229099, acc 1
2017-03-02T23:40:42.369718: step 4483, loss 0.0060446, acc 1
2017-03-02T23:40:42.474961: step 4484, loss 0.00130326, acc 1
2017-03-02T23:40:42.578394: step 4485, loss 0.00120016, acc 1
2017-03-02T23:40:42.685221: step 4486, loss 0.0011925, acc 1
2017-03-02T23:40:42.789682: step 4487, loss 0.00247115, acc 1
2017-03-02T23:40:42.895984: step 4488, loss 0.00118047, acc 1
2017-03-02T23:40:42.986888: step 4489, loss 0.00211475, acc 1
2017-03-02T23:40:43.083714: step 4490, loss 0.0032294, acc 1
2017-03-02T23:40:43.188993: step 4491, loss 0.00130341, acc 1
2017-03-02T23:40:43.292397: step 4492, loss 0.00278152, acc 1
2017-03-02T23:40:43.393989: step 4493, loss 0.000913362, acc 1
2017-03-02T23:40:43.497178: step 4494, loss 0.00135762, acc 1
2017-03-02T23:40:43.592749: step 4495, loss 0.000387884, acc 1
2017-03-02T23:40:43.682268: step 4496, loss 0.00168344, acc 1
2017-03-02T23:40:43.774034: step 4497, loss 0.0195714, acc 0.984375
2017-03-02T23:40:43.879059: step 4498, loss 0.00986136, acc 1
2017-03-02T23:40:43.980106: step 4499, loss 0.000843931, acc 1
2017-03-02T23:40:44.098748: step 4500, loss 0.011852, acc 1

Evaluation:
2017-03-02T23:40:44.150281: step 4500, loss 2.35796, acc 0.525952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4500

2017-03-02T23:40:44.602659: step 4501, loss 0.00105104, acc 1
2017-03-02T23:40:44.711528: step 4502, loss 0.00836704, acc 1
2017-03-02T23:40:44.830889: step 4503, loss 0.000667772, acc 1
2017-03-02T23:40:44.942586: step 4504, loss 0.0369845, acc 0.984375
2017-03-02T23:40:45.046620: step 4505, loss 0.0054086, acc 1
2017-03-02T23:40:45.143713: step 4506, loss 0.00116389, acc 1
2017-03-02T23:40:45.240439: step 4507, loss 0.0016443, acc 1
2017-03-02T23:40:45.350686: step 4508, loss 0.0015771, acc 1
2017-03-02T23:40:45.456174: step 4509, loss 0.00569077, acc 1
2017-03-02T23:40:45.550665: step 4510, loss 0.00179318, acc 1
2017-03-02T23:40:45.663651: step 4511, loss 0.000389533, acc 1
2017-03-02T23:40:45.777219: step 4512, loss 0.0589126, acc 0.984375
2017-03-02T23:40:45.876984: step 4513, loss 0.00107603, acc 1
2017-03-02T23:40:45.959095: step 4514, loss 0.00183198, acc 1
2017-03-02T23:40:46.078351: step 4515, loss 0.00221738, acc 1
2017-03-02T23:40:46.181751: step 4516, loss 0.00369776, acc 1
2017-03-02T23:40:46.293911: step 4517, loss 0.00304099, acc 1
2017-03-02T23:40:46.399649: step 4518, loss 0.000522433, acc 1
2017-03-02T23:40:46.500622: step 4519, loss 0.0016738, acc 1
2017-03-02T23:40:46.605630: step 4520, loss 0.0117009, acc 1
2017-03-02T23:40:46.695439: step 4521, loss 0.00206732, acc 1
2017-03-02T23:40:46.810160: step 4522, loss 0.000340081, acc 1
2017-03-02T23:40:46.918544: step 4523, loss 0.00781588, acc 1
2017-03-02T23:40:47.029381: step 4524, loss 0.0013655, acc 1
2017-03-02T23:40:47.137817: step 4525, loss 0.0075998, acc 1
2017-03-02T23:40:47.243116: step 4526, loss 0.00442729, acc 1
2017-03-02T23:40:47.354682: step 4527, loss 0.00628948, acc 1
2017-03-02T23:40:47.449110: step 4528, loss 0.000873417, acc 1
2017-03-02T23:40:47.560957: step 4529, loss 0.0356627, acc 0.984375
2017-03-02T23:40:47.674023: step 4530, loss 0.00640873, acc 1
2017-03-02T23:40:47.788883: step 4531, loss 0.0005799, acc 1
2017-03-02T23:40:47.887500: step 4532, loss 0.000542842, acc 1
2017-03-02T23:40:47.996949: step 4533, loss 0.000922871, acc 1
2017-03-02T23:40:48.106460: step 4534, loss 0.00799262, acc 1
2017-03-02T23:40:48.209725: step 4535, loss 0.00314167, acc 1
2017-03-02T23:40:48.316418: step 4536, loss 0.00180337, acc 1
2017-03-02T23:40:48.419122: step 4537, loss 0.00166484, acc 1
2017-03-02T23:40:48.521649: step 4538, loss 0.00223202, acc 1
2017-03-02T23:40:48.620870: step 4539, loss 0.000693202, acc 1
2017-03-02T23:40:48.726985: step 4540, loss 0.0070152, acc 1
2017-03-02T23:40:48.833894: step 4541, loss 0.00183536, acc 1
2017-03-02T23:40:48.923525: step 4542, loss 0.00230713, acc 1
2017-03-02T23:40:49.033653: step 4543, loss 0.00101324, acc 1
2017-03-02T23:40:49.142448: step 4544, loss 0.00116068, acc 1
2017-03-02T23:40:49.245244: step 4545, loss 0.00115634, acc 1
2017-03-02T23:40:49.351414: step 4546, loss 0.00383881, acc 1
2017-03-02T23:40:49.460695: step 4547, loss 0.00942692, acc 1
2017-03-02T23:40:49.565195: step 4548, loss 0.00130398, acc 1
2017-03-02T23:40:49.648607: step 4549, loss 0.00320486, acc 1
2017-03-02T23:40:49.748090: step 4550, loss 0.00451556, acc 1
2017-03-02T23:40:49.857917: step 4551, loss 0.0011115, acc 1
2017-03-02T23:40:49.959155: step 4552, loss 0.00389323, acc 1
2017-03-02T23:40:50.063260: step 4553, loss 0.00281513, acc 1
2017-03-02T23:40:50.167563: step 4554, loss 0.00147597, acc 1
2017-03-02T23:40:50.274545: step 4555, loss 0.00510088, acc 1
2017-03-02T23:40:50.377550: step 4556, loss 0.000447528, acc 1
2017-03-02T23:40:50.471205: step 4557, loss 0.00981005, acc 1
2017-03-02T23:40:50.573899: step 4558, loss 0.000923261, acc 1
2017-03-02T23:40:50.685746: step 4559, loss 0.016521, acc 0.984375
2017-03-02T23:40:50.795584: step 4560, loss 0.0234733, acc 0.984375
2017-03-02T23:40:50.912763: step 4561, loss 0.00243542, acc 1
2017-03-02T23:40:51.026872: step 4562, loss 0.000536909, acc 1
2017-03-02T23:40:51.138546: step 4563, loss 0.00274094, acc 1
2017-03-02T23:40:51.225835: step 4564, loss 0.00169328, acc 1
2017-03-02T23:40:51.330246: step 4565, loss 0.000704261, acc 1
2017-03-02T23:40:51.434771: step 4566, loss 0.00521497, acc 1
2017-03-02T23:40:51.538934: step 4567, loss 0.0057132, acc 1
2017-03-02T23:40:51.644142: step 4568, loss 0.00638105, acc 1
2017-03-02T23:40:51.752356: step 4569, loss 0.000474753, acc 1
2017-03-02T23:40:51.855121: step 4570, loss 0.00880886, acc 1
2017-03-02T23:40:51.949627: step 4571, loss 0.00208068, acc 1
2017-03-02T23:40:52.062325: step 4572, loss 0.000523453, acc 1
2017-03-02T23:40:52.167849: step 4573, loss 0.00295466, acc 1
2017-03-02T23:40:52.278328: step 4574, loss 0.00797113, acc 1
2017-03-02T23:40:52.384166: step 4575, loss 0.00348139, acc 1
2017-03-02T23:40:52.491598: step 4576, loss 0.00287812, acc 1
2017-03-02T23:40:52.602153: step 4577, loss 0.00060726, acc 1
2017-03-02T23:40:52.694710: step 4578, loss 0.00203357, acc 1
2017-03-02T23:40:52.805866: step 4579, loss 0.00102861, acc 1
2017-03-02T23:40:52.909445: step 4580, loss 0.00251264, acc 1
2017-03-02T23:40:53.019846: step 4581, loss 0.00296444, acc 1
2017-03-02T23:40:53.123256: step 4582, loss 0.00107588, acc 1
2017-03-02T23:40:53.228311: step 4583, loss 0.000867839, acc 1
2017-03-02T23:40:53.335795: step 4584, loss 0.00508754, acc 1
2017-03-02T23:40:53.421637: step 4585, loss 0.00202579, acc 1
2017-03-02T23:40:53.523387: step 4586, loss 0.00190095, acc 1
2017-03-02T23:40:53.627224: step 4587, loss 0.000879154, acc 1
2017-03-02T23:40:53.732908: step 4588, loss 0.00123736, acc 1
2017-03-02T23:40:53.845385: step 4589, loss 0.00260668, acc 1
2017-03-02T23:40:53.952651: step 4590, loss 0.00593312, acc 1
2017-03-02T23:40:54.057561: step 4591, loss 0.00055885, acc 1
2017-03-02T23:40:54.140751: step 4592, loss 0.000554367, acc 1
2017-03-02T23:40:54.238587: step 4593, loss 0.000210606, acc 1
2017-03-02T23:40:54.346025: step 4594, loss 0.000521884, acc 1
2017-03-02T23:40:54.451926: step 4595, loss 0.00268957, acc 1
2017-03-02T23:40:54.562040: step 4596, loss 0.00387233, acc 1
2017-03-02T23:40:54.675492: step 4597, loss 0.00107966, acc 1
2017-03-02T23:40:54.785064: step 4598, loss 0.00149487, acc 1
2017-03-02T23:40:54.884219: step 4599, loss 0.00116965, acc 1
2017-03-02T23:40:54.987794: step 4600, loss 0.00289348, acc 1

Evaluation:
2017-03-02T23:40:55.050573: step 4600, loss 2.5332, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4600

2017-03-02T23:40:55.501002: step 4601, loss 0.000910817, acc 1
2017-03-02T23:40:55.587196: step 4602, loss 0.000664887, acc 1
2017-03-02T23:40:55.701487: step 4603, loss 0.00581518, acc 1
2017-03-02T23:40:55.809807: step 4604, loss 0.0129298, acc 0.984375
2017-03-02T23:40:55.915746: step 4605, loss 0.0029577, acc 1
2017-03-02T23:40:56.044005: step 4606, loss 0.00162128, acc 1
2017-03-02T23:40:56.150800: step 4607, loss 0.000859286, acc 1
2017-03-02T23:40:56.254801: step 4608, loss 0.00158594, acc 1
2017-03-02T23:40:56.347085: step 4609, loss 0.00231449, acc 1
2017-03-02T23:40:56.450266: step 4610, loss 0.000848743, acc 1
2017-03-02T23:40:56.555219: step 4611, loss 0.002325, acc 1
2017-03-02T23:40:56.654437: step 4612, loss 0.0138899, acc 0.984375
2017-03-02T23:40:56.757243: step 4613, loss 0.00147224, acc 1
2017-03-02T23:40:56.863728: step 4614, loss 0.0172381, acc 0.984375
2017-03-02T23:40:56.964833: step 4615, loss 0.000937026, acc 1
2017-03-02T23:40:57.073165: step 4616, loss 0.000646449, acc 1
2017-03-02T23:40:57.181164: step 4617, loss 0.00355216, acc 1
2017-03-02T23:40:57.287317: step 4618, loss 0.00735932, acc 1
2017-03-02T23:40:57.396171: step 4619, loss 0.00452848, acc 1
2017-03-02T23:40:57.501433: step 4620, loss 0.00701772, acc 1
2017-03-02T23:40:57.613942: step 4621, loss 0.0377166, acc 0.984375
2017-03-02T23:40:57.719161: step 4622, loss 0.00698959, acc 1
2017-03-02T23:40:57.814824: step 4623, loss 0.000279734, acc 1
2017-03-02T23:40:57.911390: step 4624, loss 0.00173383, acc 1
2017-03-02T23:40:58.022692: step 4625, loss 0.00064346, acc 1
2017-03-02T23:40:58.116626: step 4626, loss 0.010345, acc 1
2017-03-02T23:40:58.230114: step 4627, loss 0.00057408, acc 1
2017-03-02T23:40:58.340069: step 4628, loss 0.00313629, acc 1
2017-03-02T23:40:58.445921: step 4629, loss 0.000895113, acc 1
2017-03-02T23:40:58.538898: step 4630, loss 0.000832519, acc 1
2017-03-02T23:40:58.634745: step 4631, loss 0.00290076, acc 1
2017-03-02T23:40:58.741878: step 4632, loss 0.00116376, acc 1
2017-03-02T23:40:58.841946: step 4633, loss 0.00115469, acc 1
2017-03-02T23:40:58.950874: step 4634, loss 0.00129433, acc 1
2017-03-02T23:40:59.052668: step 4635, loss 0.000931559, acc 1
2017-03-02T23:40:59.156480: step 4636, loss 0.00318374, acc 1
2017-03-02T23:40:59.265065: step 4637, loss 0.00102594, acc 1
2017-03-02T23:40:59.353847: step 4638, loss 0.000609668, acc 1
2017-03-02T23:40:59.450869: step 4639, loss 0.00743511, acc 1
2017-03-02T23:40:59.557585: step 4640, loss 0.01961, acc 0.984375
2017-03-02T23:40:59.661261: step 4641, loss 0.00103236, acc 1
2017-03-02T23:40:59.767086: step 4642, loss 0.000759977, acc 1
2017-03-02T23:40:59.877323: step 4643, loss 0.000464828, acc 1
2017-03-02T23:40:59.982758: step 4644, loss 0.00754137, acc 1
2017-03-02T23:41:00.067999: step 4645, loss 0.00120094, acc 1
2017-03-02T23:41:00.164026: step 4646, loss 0.00284741, acc 1
2017-03-02T23:41:00.264340: step 4647, loss 0.00528063, acc 1
2017-03-02T23:41:00.375207: step 4648, loss 0.000543355, acc 1
2017-03-02T23:41:00.484573: step 4649, loss 0.00864653, acc 1
2017-03-02T23:41:00.594027: step 4650, loss 0.000980775, acc 1
2017-03-02T23:41:00.701440: step 4651, loss 0.00415127, acc 1
2017-03-02T23:41:00.793304: step 4652, loss 0.00575427, acc 1
2017-03-02T23:41:00.893199: step 4653, loss 0.00182467, acc 1
2017-03-02T23:41:01.001112: step 4654, loss 0.00519199, acc 1
2017-03-02T23:41:01.112410: step 4655, loss 0.0119055, acc 1
2017-03-02T23:41:01.215223: step 4656, loss 0.00141016, acc 1
2017-03-02T23:41:01.320349: step 4657, loss 0.00320534, acc 1
2017-03-02T23:41:01.430736: step 4658, loss 0.0176294, acc 0.984375
2017-03-02T23:41:01.531060: step 4659, loss 0.00246144, acc 1
2017-03-02T23:41:01.621479: step 4660, loss 0.00314085, acc 1
2017-03-02T23:41:01.722827: step 4661, loss 0.00175136, acc 1
2017-03-02T23:41:01.831091: step 4662, loss 0.00373305, acc 1
2017-03-02T23:41:01.932627: step 4663, loss 0.00116887, acc 1
2017-03-02T23:41:02.040430: step 4664, loss 0.00575797, acc 1
2017-03-02T23:41:02.143087: step 4665, loss 0.00225639, acc 1
2017-03-02T23:41:02.244145: step 4666, loss 0.00213968, acc 1
2017-03-02T23:41:02.336095: step 4667, loss 0.000377343, acc 1
2017-03-02T23:41:02.432861: step 4668, loss 0.00162369, acc 1
2017-03-02T23:41:02.543399: step 4669, loss 0.00163994, acc 1
2017-03-02T23:41:02.648198: step 4670, loss 0.00143347, acc 1
2017-03-02T23:41:02.759190: step 4671, loss 0.00104623, acc 1
2017-03-02T23:41:02.867227: step 4672, loss 0.00153997, acc 1
2017-03-02T23:41:02.972763: step 4673, loss 0.00392613, acc 1
2017-03-02T23:41:03.062456: step 4674, loss 0.00104249, acc 1
2017-03-02T23:41:03.161988: step 4675, loss 0.00032119, acc 1
2017-03-02T23:41:03.267015: step 4676, loss 0.00524392, acc 1
2017-03-02T23:41:03.370257: step 4677, loss 0.000655973, acc 1
2017-03-02T23:41:03.478005: step 4678, loss 0.00048848, acc 1
2017-03-02T23:41:03.584373: step 4679, loss 0.00605676, acc 1
2017-03-02T23:41:03.685365: step 4680, loss 0.00045071, acc 1
2017-03-02T23:41:03.792114: step 4681, loss 0.00647213, acc 1
2017-03-02T23:41:03.882202: step 4682, loss 0.000940919, acc 1
2017-03-02T23:41:03.979793: step 4683, loss 0.00459068, acc 1
2017-03-02T23:41:04.083458: step 4684, loss 0.00125412, acc 1
2017-03-02T23:41:04.187458: step 4685, loss 0.0027493, acc 1
2017-03-02T23:41:04.299620: step 4686, loss 0.00218357, acc 1
2017-03-02T23:41:04.413471: step 4687, loss 0.00298553, acc 1
2017-03-02T23:41:04.520553: step 4688, loss 0.00300852, acc 1
2017-03-02T23:41:04.614773: step 4689, loss 0.00359678, acc 1
2017-03-02T23:41:04.711379: step 4690, loss 0.00265118, acc 1
2017-03-02T23:41:04.819271: step 4691, loss 0.00272488, acc 1
2017-03-02T23:41:04.921525: step 4692, loss 0.00497605, acc 1
2017-03-02T23:41:05.026370: step 4693, loss 0.00115201, acc 1
2017-03-02T23:41:05.122704: step 4694, loss 0.00133844, acc 1
2017-03-02T23:41:05.226320: step 4695, loss 0.00191004, acc 1
2017-03-02T23:41:05.333918: step 4696, loss 0.00538525, acc 1
2017-03-02T23:41:05.427832: step 4697, loss 0.000781667, acc 1
2017-03-02T23:41:05.528514: step 4698, loss 0.00178026, acc 1
2017-03-02T23:41:05.641232: step 4699, loss 0.00322542, acc 1
2017-03-02T23:41:05.743291: step 4700, loss 0.0013504, acc 1

Evaluation:
2017-03-02T23:41:05.796394: step 4700, loss 2.54362, acc 0.522491

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4700

2017-03-02T23:41:06.249094: step 4701, loss 0.0028308, acc 1
2017-03-02T23:41:06.356413: step 4702, loss 0.00120417, acc 1
2017-03-02T23:41:06.462047: step 4703, loss 0.0136991, acc 1
2017-03-02T23:41:06.574626: step 4704, loss 0.00143626, acc 1
2017-03-02T23:41:06.675333: step 4705, loss 0.000350339, acc 1
2017-03-02T23:41:06.779394: step 4706, loss 0.00114339, acc 1
2017-03-02T23:41:06.869358: step 4707, loss 0.0105556, acc 1
2017-03-02T23:41:06.970201: step 4708, loss 0.00431378, acc 1
2017-03-02T23:41:07.074623: step 4709, loss 0.00147761, acc 1
2017-03-02T23:41:07.174439: step 4710, loss 0.00223234, acc 1
2017-03-02T23:41:07.284137: step 4711, loss 0.00280222, acc 1
2017-03-02T23:41:07.399489: step 4712, loss 0.000834892, acc 1
2017-03-02T23:41:07.513793: step 4713, loss 0.00366594, acc 1
2017-03-02T23:41:07.606011: step 4714, loss 0.00808164, acc 1
2017-03-02T23:41:07.704317: step 4715, loss 0.000541229, acc 1
2017-03-02T23:41:07.817954: step 4716, loss 0.00144878, acc 1
2017-03-02T23:41:07.925445: step 4717, loss 0.000758328, acc 1
2017-03-02T23:41:08.025908: step 4718, loss 0.00290212, acc 1
2017-03-02T23:41:08.124592: step 4719, loss 0.00254071, acc 1
2017-03-02T23:41:08.231905: step 4720, loss 0.00207329, acc 1
2017-03-02T23:41:08.325576: step 4721, loss 0.00335875, acc 1
2017-03-02T23:41:08.416009: step 4722, loss 0.0264238, acc 0.984375
2017-03-02T23:41:08.524162: step 4723, loss 0.0009824, acc 1
2017-03-02T23:41:08.630159: step 4724, loss 0.0019147, acc 1
2017-03-02T23:41:08.736470: step 4725, loss 0.00141649, acc 1
2017-03-02T23:41:08.835220: step 4726, loss 0.00172396, acc 1
2017-03-02T23:41:08.941876: step 4727, loss 0.0018915, acc 1
2017-03-02T23:41:09.046753: step 4728, loss 0.00293055, acc 1
2017-03-02T23:41:09.138514: step 4729, loss 0.000383186, acc 1
2017-03-02T23:41:09.245909: step 4730, loss 0.00194795, acc 1
2017-03-02T23:41:09.348186: step 4731, loss 0.000645611, acc 1
2017-03-02T23:41:09.452866: step 4732, loss 0.0206485, acc 0.984375
2017-03-02T23:41:09.559679: step 4733, loss 0.00107838, acc 1
2017-03-02T23:41:09.667377: step 4734, loss 0.000310602, acc 1
2017-03-02T23:41:09.775192: step 4735, loss 0.000424697, acc 1
2017-03-02T23:41:09.867583: step 4736, loss 0.005205, acc 1
2017-03-02T23:41:09.969688: step 4737, loss 0.00030152, acc 1
2017-03-02T23:41:10.075758: step 4738, loss 0.00281416, acc 1
2017-03-02T23:41:10.183859: step 4739, loss 0.0128082, acc 0.984375
2017-03-02T23:41:10.290782: step 4740, loss 0.000760436, acc 1
2017-03-02T23:41:10.391632: step 4741, loss 0.00183927, acc 1
2017-03-02T23:41:10.495253: step 4742, loss 0.000906804, acc 1
2017-03-02T23:41:10.589687: step 4743, loss 0.00496181, acc 1
2017-03-02T23:41:10.681396: step 4744, loss 0.000848822, acc 1
2017-03-02T23:41:10.784775: step 4745, loss 0.00322809, acc 1
2017-03-02T23:41:10.886324: step 4746, loss 0.00455751, acc 1
2017-03-02T23:41:11.004170: step 4747, loss 0.00042717, acc 1
2017-03-02T23:41:11.110443: step 4748, loss 0.00160539, acc 1
2017-03-02T23:41:11.222119: step 4749, loss 0.000586623, acc 1
2017-03-02T23:41:11.333102: step 4750, loss 0.00140382, acc 1
2017-03-02T23:41:11.422124: step 4751, loss 0.00152196, acc 1
2017-03-02T23:41:11.526661: step 4752, loss 0.00112323, acc 1
2017-03-02T23:41:11.620752: step 4753, loss 0.00193942, acc 1
2017-03-02T23:41:11.716601: step 4754, loss 0.00331549, acc 1
2017-03-02T23:41:11.827316: step 4755, loss 0.000390909, acc 1
2017-03-02T23:41:11.926163: step 4756, loss 0.0850858, acc 0.98
2017-03-02T23:41:12.039346: step 4757, loss 0.00238771, acc 1
2017-03-02T23:41:12.131401: step 4758, loss 0.000889688, acc 1
2017-03-02T23:41:12.230036: step 4759, loss 0.0023284, acc 1
2017-03-02T23:41:12.337374: step 4760, loss 0.00253212, acc 1
2017-03-02T23:41:12.438643: step 4761, loss 0.000515857, acc 1
2017-03-02T23:41:12.545483: step 4762, loss 0.000904042, acc 1
2017-03-02T23:41:12.639940: step 4763, loss 0.00545517, acc 1
2017-03-02T23:41:12.745804: step 4764, loss 0.00104597, acc 1
2017-03-02T23:41:12.846919: step 4765, loss 0.00193254, acc 1
2017-03-02T23:41:12.935723: step 4766, loss 0.00188011, acc 1
2017-03-02T23:41:13.045906: step 4767, loss 0.000694138, acc 1
2017-03-02T23:41:13.152702: step 4768, loss 0.000625175, acc 1
2017-03-02T23:41:13.258726: step 4769, loss 0.00287023, acc 1
2017-03-02T23:41:13.367448: step 4770, loss 0.00216495, acc 1
2017-03-02T23:41:13.474415: step 4771, loss 0.0189994, acc 0.984375
2017-03-02T23:41:13.584648: step 4772, loss 0.003184, acc 1
2017-03-02T23:41:13.679513: step 4773, loss 0.00679723, acc 1
2017-03-02T23:41:13.784170: step 4774, loss 0.0184977, acc 1
2017-03-02T23:41:13.885576: step 4775, loss 0.00161429, acc 1
2017-03-02T23:41:13.992250: step 4776, loss 0.00200792, acc 1
2017-03-02T23:41:14.095749: step 4777, loss 0.00238701, acc 1
2017-03-02T23:41:14.203306: step 4778, loss 0.0203278, acc 0.984375
2017-03-02T23:41:14.323656: step 4779, loss 0.00412176, acc 1
2017-03-02T23:41:14.418179: step 4780, loss 0.00137682, acc 1
2017-03-02T23:41:14.520740: step 4781, loss 0.00397511, acc 1
2017-03-02T23:41:14.627532: step 4782, loss 0.00162393, acc 1
2017-03-02T23:41:14.739399: step 4783, loss 0.00136481, acc 1
2017-03-02T23:41:14.857608: step 4784, loss 0.00129601, acc 1
2017-03-02T23:41:14.960378: step 4785, loss 0.0019464, acc 1
2017-03-02T23:41:15.072751: step 4786, loss 0.00175272, acc 1
2017-03-02T23:41:15.165105: step 4787, loss 0.00208356, acc 1
2017-03-02T23:41:15.268760: step 4788, loss 0.000721193, acc 1
2017-03-02T23:41:15.377419: step 4789, loss 0.00723088, acc 1
2017-03-02T23:41:15.485573: step 4790, loss 0.00166552, acc 1
2017-03-02T23:41:15.588030: step 4791, loss 0.000707315, acc 1
2017-03-02T23:41:15.693223: step 4792, loss 0.00368866, acc 1
2017-03-02T23:41:15.798435: step 4793, loss 0.00143075, acc 1
2017-03-02T23:41:15.899647: step 4794, loss 0.000612735, acc 1
2017-03-02T23:41:15.997758: step 4795, loss 0.00110367, acc 1
2017-03-02T23:41:16.110908: step 4796, loss 0.00357714, acc 1
2017-03-02T23:41:16.206471: step 4797, loss 0.00120122, acc 1
2017-03-02T23:41:16.308853: step 4798, loss 0.000499676, acc 1
2017-03-02T23:41:16.412606: step 4799, loss 0.00128801, acc 1
2017-03-02T23:41:16.518012: step 4800, loss 0.0031089, acc 1

Evaluation:
2017-03-02T23:41:16.563657: step 4800, loss 2.63878, acc 0.522491

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4800

2017-03-02T23:41:17.007625: step 4801, loss 0.00756558, acc 1
2017-03-02T23:41:17.109588: step 4802, loss 0.000532584, acc 1
2017-03-02T23:41:17.200001: step 4803, loss 0.000288215, acc 1
2017-03-02T23:41:17.308880: step 4804, loss 0.0031513, acc 1
2017-03-02T23:41:17.404120: step 4805, loss 0.000922355, acc 1
2017-03-02T23:41:17.513150: step 4806, loss 0.00246913, acc 1
2017-03-02T23:41:17.623748: step 4807, loss 0.00528184, acc 1
2017-03-02T23:41:17.729659: step 4808, loss 0.00188063, acc 1
2017-03-02T23:41:17.831087: step 4809, loss 0.000827296, acc 1
2017-03-02T23:41:17.935734: step 4810, loss 0.00053461, acc 1
2017-03-02T23:41:18.046198: step 4811, loss 0.00599737, acc 1
2017-03-02T23:41:18.135022: step 4812, loss 0.000560905, acc 1
2017-03-02T23:41:18.244376: step 4813, loss 0.00185634, acc 1
2017-03-02T23:41:18.345796: step 4814, loss 0.0018138, acc 1
2017-03-02T23:41:18.444022: step 4815, loss 0.00250461, acc 1
2017-03-02T23:41:18.548707: step 4816, loss 0.0010276, acc 1
2017-03-02T23:41:18.663992: step 4817, loss 0.000761449, acc 1
2017-03-02T23:41:18.767720: step 4818, loss 0.00175215, acc 1
2017-03-02T23:41:18.865106: step 4819, loss 0.00116689, acc 1
2017-03-02T23:41:18.975488: step 4820, loss 0.0050069, acc 1
2017-03-02T23:41:19.076782: step 4821, loss 0.0015402, acc 1
2017-03-02T23:41:19.179413: step 4822, loss 0.00257486, acc 1
2017-03-02T23:41:19.276251: step 4823, loss 0.000399499, acc 1
2017-03-02T23:41:19.382344: step 4824, loss 0.000338229, acc 1
2017-03-02T23:41:19.486327: step 4825, loss 0.00307241, acc 1
2017-03-02T23:41:19.590477: step 4826, loss 0.00315476, acc 1
2017-03-02T23:41:19.678164: step 4827, loss 0.000458876, acc 1
2017-03-02T23:41:19.781453: step 4828, loss 0.0533397, acc 0.984375
2017-03-02T23:41:19.876824: step 4829, loss 0.0696074, acc 0.984375
2017-03-02T23:41:19.977586: step 4830, loss 0.00314129, acc 1
2017-03-02T23:41:20.084602: step 4831, loss 0.000499108, acc 1
2017-03-02T23:41:20.191604: step 4832, loss 0.0016212, acc 1
2017-03-02T23:41:20.304800: step 4833, loss 0.00121168, acc 1
2017-03-02T23:41:20.398423: step 4834, loss 0.00191366, acc 1
2017-03-02T23:41:20.502208: step 4835, loss 0.00465062, acc 1
2017-03-02T23:41:20.616826: step 4836, loss 0.00871524, acc 1
2017-03-02T23:41:20.719025: step 4837, loss 0.00145901, acc 1
2017-03-02T23:41:20.816142: step 4838, loss 0.00750293, acc 1
2017-03-02T23:41:20.921852: step 4839, loss 0.001877, acc 1
2017-03-02T23:41:21.027589: step 4840, loss 0.00243285, acc 1
2017-03-02T23:41:21.126757: step 4841, loss 0.00405698, acc 1
2017-03-02T23:41:21.233544: step 4842, loss 0.000899807, acc 1
2017-03-02T23:41:21.338329: step 4843, loss 0.000902565, acc 1
2017-03-02T23:41:21.442653: step 4844, loss 0.0321843, acc 0.984375
2017-03-02T23:41:21.542699: step 4845, loss 0.000861821, acc 1
2017-03-02T23:41:21.648774: step 4846, loss 0.000865456, acc 1
2017-03-02T23:41:21.754565: step 4847, loss 0.00182954, acc 1
2017-03-02T23:41:21.851654: step 4848, loss 0.00168867, acc 1
2017-03-02T23:41:21.946392: step 4849, loss 0.000669412, acc 1
2017-03-02T23:41:22.063990: step 4850, loss 0.00110285, acc 1
2017-03-02T23:41:22.164718: step 4851, loss 0.000655172, acc 1
2017-03-02T23:41:22.270562: step 4852, loss 0.00195215, acc 1
2017-03-02T23:41:22.377894: step 4853, loss 0.00128149, acc 1
2017-03-02T23:41:22.483168: step 4854, loss 0.00231685, acc 1
2017-03-02T23:41:22.588423: step 4855, loss 0.00223496, acc 1
2017-03-02T23:41:22.682449: step 4856, loss 0.00107877, acc 1
2017-03-02T23:41:22.787480: step 4857, loss 0.00191333, acc 1
2017-03-02T23:41:22.901054: step 4858, loss 0.0148683, acc 0.984375
2017-03-02T23:41:23.012336: step 4859, loss 0.00240229, acc 1
2017-03-02T23:41:23.135167: step 4860, loss 0.000487109, acc 1
2017-03-02T23:41:23.243318: step 4861, loss 0.000967538, acc 1
2017-03-02T23:41:23.365142: step 4862, loss 0.000793477, acc 1
2017-03-02T23:41:23.457569: step 4863, loss 0.00485279, acc 1
2017-03-02T23:41:23.571000: step 4864, loss 0.00656862, acc 1
2017-03-02T23:41:23.662796: step 4865, loss 0.00167854, acc 1
2017-03-02T23:41:23.767438: step 4866, loss 0.00202931, acc 1
2017-03-02T23:41:23.875733: step 4867, loss 0.00077968, acc 1
2017-03-02T23:41:23.984508: step 4868, loss 0.00390925, acc 1
2017-03-02T23:41:24.092536: step 4869, loss 0.000175594, acc 1
2017-03-02T23:41:24.185589: step 4870, loss 0.000772452, acc 1
2017-03-02T23:41:24.284828: step 4871, loss 0.000245607, acc 1
2017-03-02T23:41:24.391054: step 4872, loss 0.00935645, acc 1
2017-03-02T23:41:24.499818: step 4873, loss 0.0010998, acc 1
2017-03-02T23:41:24.598848: step 4874, loss 0.00238777, acc 1
2017-03-02T23:41:24.717185: step 4875, loss 0.000883157, acc 1
2017-03-02T23:41:24.827768: step 4876, loss 0.00472786, acc 1
2017-03-02T23:41:24.920348: step 4877, loss 0.000727307, acc 1
2017-03-02T23:41:25.024318: step 4878, loss 0.000224151, acc 1
2017-03-02T23:41:25.132791: step 4879, loss 0.00023303, acc 1
2017-03-02T23:41:25.249193: step 4880, loss 0.00110427, acc 1
2017-03-02T23:41:25.353681: step 4881, loss 0.019931, acc 0.984375
2017-03-02T23:41:25.459973: step 4882, loss 0.00756697, acc 1
2017-03-02T23:41:25.570999: step 4883, loss 0.000622207, acc 1
2017-03-02T23:41:25.660004: step 4884, loss 0.00284531, acc 1
2017-03-02T23:41:25.774784: step 4885, loss 0.00122933, acc 1
2017-03-02T23:41:25.881627: step 4886, loss 0.00260131, acc 1
2017-03-02T23:41:25.984966: step 4887, loss 0.000794347, acc 1
2017-03-02T23:41:26.093181: step 4888, loss 0.00375839, acc 1
2017-03-02T23:41:26.199508: step 4889, loss 0.000228642, acc 1
2017-03-02T23:41:26.304160: step 4890, loss 0.000216884, acc 1
2017-03-02T23:41:26.394898: step 4891, loss 0.000832305, acc 1
2017-03-02T23:41:26.496746: step 4892, loss 0.00645237, acc 1
2017-03-02T23:41:26.601492: step 4893, loss 0.00347018, acc 1
2017-03-02T23:41:26.708755: step 4894, loss 0.0266666, acc 0.984375
2017-03-02T23:41:26.810773: step 4895, loss 0.00231315, acc 1
2017-03-02T23:41:26.915493: step 4896, loss 0.00108055, acc 1
2017-03-02T23:41:27.020645: step 4897, loss 0.00412361, acc 1
2017-03-02T23:41:27.110883: step 4898, loss 0.000990462, acc 1
2017-03-02T23:41:27.201246: step 4899, loss 0.0010733, acc 1
2017-03-02T23:41:27.310714: step 4900, loss 0.00126395, acc 1

Evaluation:
2017-03-02T23:41:27.373108: step 4900, loss 2.65169, acc 0.525952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-4900

2017-03-02T23:41:27.839758: step 4901, loss 0.00270923, acc 1
2017-03-02T23:41:27.950425: step 4902, loss 0.00369229, acc 1
2017-03-02T23:41:28.061376: step 4903, loss 0.000945255, acc 1
2017-03-02T23:41:28.165954: step 4904, loss 0.000823829, acc 1
2017-03-02T23:41:28.263769: step 4905, loss 0.00419312, acc 1
2017-03-02T23:41:28.366108: step 4906, loss 0.00117997, acc 1
2017-03-02T23:41:28.468066: step 4907, loss 0.00181085, acc 1
2017-03-02T23:41:28.558151: step 4908, loss 0.00146348, acc 1
2017-03-02T23:41:28.651227: step 4909, loss 0.00364528, acc 1
2017-03-02T23:41:28.751389: step 4910, loss 0.0129587, acc 0.984375
2017-03-02T23:41:28.856567: step 4911, loss 0.00202116, acc 1
2017-03-02T23:41:28.955700: step 4912, loss 0.00530297, acc 1
2017-03-02T23:41:29.066718: step 4913, loss 0.000880597, acc 1
2017-03-02T23:41:29.171908: step 4914, loss 0.0107861, acc 1
2017-03-02T23:41:29.283750: step 4915, loss 0.00175314, acc 1
2017-03-02T23:41:29.373450: step 4916, loss 0.00750267, acc 1
2017-03-02T23:41:29.485864: step 4917, loss 0.0110522, acc 1
2017-03-02T23:41:29.580738: step 4918, loss 0.0016128, acc 1
2017-03-02T23:41:29.696641: step 4919, loss 0.000427103, acc 1
2017-03-02T23:41:29.802415: step 4920, loss 0.00974435, acc 1
2017-03-02T23:41:29.904081: step 4921, loss 0.00350122, acc 1
2017-03-02T23:41:30.012820: step 4922, loss 0.00176102, acc 1
2017-03-02T23:41:30.105483: step 4923, loss 0.00099698, acc 1
2017-03-02T23:41:30.201377: step 4924, loss 0.000907416, acc 1
2017-03-02T23:41:30.308369: step 4925, loss 0.00104584, acc 1
2017-03-02T23:41:30.412069: step 4926, loss 0.000287489, acc 1
2017-03-02T23:41:30.519867: step 4927, loss 0.000632682, acc 1
2017-03-02T23:41:30.623975: step 4928, loss 0.00218764, acc 1
2017-03-02T23:41:30.728606: step 4929, loss 0.00122928, acc 1
2017-03-02T23:41:30.841742: step 4930, loss 0.0102677, acc 1
2017-03-02T23:41:30.938737: step 4931, loss 0.00533834, acc 1
2017-03-02T23:41:31.042369: step 4932, loss 0.00242952, acc 1
2017-03-02T23:41:31.147706: step 4933, loss 0.00627697, acc 1
2017-03-02T23:41:31.261886: step 4934, loss 0.000567251, acc 1
2017-03-02T23:41:31.364666: step 4935, loss 0.000588453, acc 1
2017-03-02T23:41:31.465429: step 4936, loss 0.00125392, acc 1
2017-03-02T23:41:31.570181: step 4937, loss 0.00286476, acc 1
2017-03-02T23:41:31.662803: step 4938, loss 0.000693816, acc 1
2017-03-02T23:41:31.768706: step 4939, loss 0.0005537, acc 1
2017-03-02T23:41:31.888604: step 4940, loss 0.000840865, acc 1
2017-03-02T23:41:32.000417: step 4941, loss 0.00415366, acc 1
2017-03-02T23:41:32.104091: step 4942, loss 0.00306629, acc 1
2017-03-02T23:41:32.205228: step 4943, loss 0.00234119, acc 1
2017-03-02T23:41:32.312455: step 4944, loss 0.00049595, acc 1
2017-03-02T23:41:32.408024: step 4945, loss 0.0027613, acc 1
2017-03-02T23:41:32.506627: step 4946, loss 0.000421639, acc 1
2017-03-02T23:41:32.612023: step 4947, loss 0.00370875, acc 1
2017-03-02T23:41:32.722704: step 4948, loss 0.00364665, acc 1
2017-03-02T23:41:32.827883: step 4949, loss 0.00226266, acc 1
2017-03-02T23:41:32.928123: step 4950, loss 0.00239503, acc 1
2017-03-02T23:41:33.038320: step 4951, loss 0.000712039, acc 1
2017-03-02T23:41:33.140970: step 4952, loss 0.00112158, acc 1
2017-03-02T23:41:33.253621: step 4953, loss 0.00555841, acc 1
2017-03-02T23:41:33.364429: step 4954, loss 0.0116636, acc 0.984375
2017-03-02T23:41:33.478140: step 4955, loss 0.000216221, acc 1
2017-03-02T23:41:33.568187: step 4956, loss 0.000494796, acc 1
2017-03-02T23:41:33.669914: step 4957, loss 0.00138595, acc 1
2017-03-02T23:41:33.769034: step 4958, loss 0.0051708, acc 1
2017-03-02T23:41:33.867228: step 4959, loss 0.0150996, acc 0.984375
2017-03-02T23:41:33.962813: step 4960, loss 0.00220995, acc 1
2017-03-02T23:41:34.056916: step 4961, loss 0.00576417, acc 1
2017-03-02T23:41:34.160464: step 4962, loss 0.000543574, acc 1
2017-03-02T23:41:34.267223: step 4963, loss 0.000347793, acc 1
2017-03-02T23:41:34.374107: step 4964, loss 0.00290751, acc 1
2017-03-02T23:41:34.481491: step 4965, loss 0.000748954, acc 1
2017-03-02T23:41:34.584167: step 4966, loss 0.000688341, acc 1
2017-03-02T23:41:34.675604: step 4967, loss 0.00188952, acc 1
2017-03-02T23:41:34.781690: step 4968, loss 0.000729709, acc 1
2017-03-02T23:41:34.890705: step 4969, loss 0.00213862, acc 1
2017-03-02T23:41:35.000991: step 4970, loss 0.00676029, acc 1
2017-03-02T23:41:35.105691: step 4971, loss 0.00115052, acc 1
2017-03-02T23:41:35.218432: step 4972, loss 0.0031573, acc 1
2017-03-02T23:41:35.330669: step 4973, loss 0.000885524, acc 1
2017-03-02T23:41:35.424193: step 4974, loss 0.00415132, acc 1
2017-03-02T23:41:35.526815: step 4975, loss 0.00359242, acc 1
2017-03-02T23:41:35.633480: step 4976, loss 0.0010086, acc 1
2017-03-02T23:41:35.742439: step 4977, loss 0.00107336, acc 1
2017-03-02T23:41:35.852573: step 4978, loss 0.00219556, acc 1
2017-03-02T23:41:35.965115: step 4979, loss 0.000633798, acc 1
2017-03-02T23:41:36.069830: step 4980, loss 0.000616986, acc 1
2017-03-02T23:41:36.159023: step 4981, loss 0.00172769, acc 1
2017-03-02T23:41:36.262430: step 4982, loss 0.00424818, acc 1
2017-03-02T23:41:36.367638: step 4983, loss 0.000855164, acc 1
2017-03-02T23:41:36.470692: step 4984, loss 0.000131842, acc 1
2017-03-02T23:41:36.573757: step 4985, loss 0.00440831, acc 1
2017-03-02T23:41:36.682250: step 4986, loss 0.00176738, acc 1
2017-03-02T23:41:36.785352: step 4987, loss 0.000955836, acc 1
2017-03-02T23:41:36.881244: step 4988, loss 0.00245326, acc 1
2017-03-02T23:41:36.972843: step 4989, loss 0.000833573, acc 1
2017-03-02T23:41:37.077649: step 4990, loss 0.0005566, acc 1
2017-03-02T23:41:37.178731: step 4991, loss 0.00431613, acc 1
2017-03-02T23:41:37.280634: step 4992, loss 0.00217745, acc 1
2017-03-02T23:41:37.388824: step 4993, loss 0.00109511, acc 1
2017-03-02T23:41:37.499620: step 4994, loss 0.000563888, acc 1
2017-03-02T23:41:37.595694: step 4995, loss 0.00290006, acc 1
2017-03-02T23:41:37.687345: step 4996, loss 0.00191795, acc 1
2017-03-02T23:41:37.788712: step 4997, loss 0.0056545, acc 1
2017-03-02T23:41:37.899845: step 4998, loss 0.000549992, acc 1
2017-03-02T23:41:38.010591: step 4999, loss 0.0008346, acc 1
2017-03-02T23:41:38.121136: step 5000, loss 0.000454483, acc 1

Evaluation:
2017-03-02T23:41:38.182793: step 5000, loss 2.59985, acc 0.525952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5000

2017-03-02T23:41:38.612488: step 5001, loss 0.00173479, acc 1
2017-03-02T23:41:38.698498: step 5002, loss 0.000512573, acc 1
2017-03-02T23:41:38.802605: step 5003, loss 0.00571172, acc 1
2017-03-02T23:41:38.897398: step 5004, loss 0.00100187, acc 1
2017-03-02T23:41:38.999819: step 5005, loss 0.009898, acc 1
2017-03-02T23:41:39.085687: step 5006, loss 0.000771151, acc 1
2017-03-02T23:41:39.177614: step 5007, loss 0.00158529, acc 1
2017-03-02T23:41:39.282775: step 5008, loss 0.000404694, acc 1
2017-03-02T23:41:39.383350: step 5009, loss 0.00138895, acc 1
2017-03-02T23:41:39.488410: step 5010, loss 0.000313495, acc 1
2017-03-02T23:41:39.595058: step 5011, loss 0.00063207, acc 1
2017-03-02T23:41:39.701934: step 5012, loss 0.000276543, acc 1
2017-03-02T23:41:39.811908: step 5013, loss 0.00224932, acc 1
2017-03-02T23:41:39.906673: step 5014, loss 0.00241702, acc 1
2017-03-02T23:41:40.002156: step 5015, loss 0.000484694, acc 1
2017-03-02T23:41:40.115030: step 5016, loss 0.00223256, acc 1
2017-03-02T23:41:40.228558: step 5017, loss 0.000581232, acc 1
2017-03-02T23:41:40.329970: step 5018, loss 0.00158272, acc 1
2017-03-02T23:41:40.434281: step 5019, loss 0.000814155, acc 1
2017-03-02T23:41:40.544079: step 5020, loss 0.000515879, acc 1
2017-03-02T23:41:40.635691: step 5021, loss 0.000948531, acc 1
2017-03-02T23:41:40.739061: step 5022, loss 0.000536856, acc 1
2017-03-02T23:41:40.843342: step 5023, loss 0.000390282, acc 1
2017-03-02T23:41:40.952430: step 5024, loss 0.00111846, acc 1
2017-03-02T23:41:41.051278: step 5025, loss 0.000806659, acc 1
2017-03-02T23:41:41.157750: step 5026, loss 0.000458333, acc 1
2017-03-02T23:41:41.280343: step 5027, loss 0.00147638, acc 1
2017-03-02T23:41:41.372506: step 5028, loss 0.000195409, acc 1
2017-03-02T23:41:41.469280: step 5029, loss 0.000625563, acc 1
2017-03-02T23:41:41.571049: step 5030, loss 0.00112186, acc 1
2017-03-02T23:41:41.682403: step 5031, loss 0.00862219, acc 1
2017-03-02T23:41:41.779292: step 5032, loss 0.00137037, acc 1
2017-03-02T23:41:41.889566: step 5033, loss 0.000881008, acc 1
2017-03-02T23:41:42.000455: step 5034, loss 0.000469848, acc 1
2017-03-02T23:41:42.097012: step 5035, loss 0.0028475, acc 1
2017-03-02T23:41:42.189415: step 5036, loss 0.00112012, acc 1
2017-03-02T23:41:42.297021: step 5037, loss 0.000993032, acc 1
2017-03-02T23:41:42.410910: step 5038, loss 0.000607525, acc 1
2017-03-02T23:41:42.520760: step 5039, loss 0.00068499, acc 1
2017-03-02T23:41:42.629149: step 5040, loss 0.00480571, acc 1
2017-03-02T23:41:42.737769: step 5041, loss 0.000852296, acc 1
2017-03-02T23:41:42.856406: step 5042, loss 0.000854242, acc 1
2017-03-02T23:41:42.940991: step 5043, loss 0.00288348, acc 1
2017-03-02T23:41:43.061504: step 5044, loss 0.00136542, acc 1
2017-03-02T23:41:43.172173: step 5045, loss 0.0108415, acc 1
2017-03-02T23:41:43.281430: step 5046, loss 0.0015775, acc 1
2017-03-02T23:41:43.388252: step 5047, loss 0.000543708, acc 1
2017-03-02T23:41:43.489772: step 5048, loss 0.000606256, acc 1
2017-03-02T23:41:43.601384: step 5049, loss 0.000986068, acc 1
2017-03-02T23:41:43.692871: step 5050, loss 0.000608569, acc 1
2017-03-02T23:41:43.796384: step 5051, loss 0.00972794, acc 1
2017-03-02T23:41:43.901255: step 5052, loss 0.000672526, acc 1
2017-03-02T23:41:44.004698: step 5053, loss 0.000602112, acc 1
2017-03-02T23:41:44.115790: step 5054, loss 0.0044519, acc 1
2017-03-02T23:41:44.217093: step 5055, loss 0.000375375, acc 1
2017-03-02T23:41:44.324730: step 5056, loss 0.000414775, acc 1
2017-03-02T23:41:44.418201: step 5057, loss 0.000885856, acc 1
2017-03-02T23:41:44.520033: step 5058, loss 0.00135079, acc 1
2017-03-02T23:41:44.623617: step 5059, loss 0.00456205, acc 1
2017-03-02T23:41:44.727650: step 5060, loss 0.00021463, acc 1
2017-03-02T23:41:44.829354: step 5061, loss 0.00264207, acc 1
2017-03-02T23:41:44.936929: step 5062, loss 0.000270676, acc 1
2017-03-02T23:41:45.037459: step 5063, loss 0.000692448, acc 1
2017-03-02T23:41:45.143747: step 5064, loss 0.00186949, acc 1
2017-03-02T23:41:45.228240: step 5065, loss 0.00286732, acc 1
2017-03-02T23:41:45.337066: step 5066, loss 0.000271726, acc 1
2017-03-02T23:41:45.454583: step 5067, loss 0.000432133, acc 1
2017-03-02T23:41:45.559671: step 5068, loss 0.000457008, acc 1
2017-03-02T23:41:45.666948: step 5069, loss 0.00195032, acc 1
2017-03-02T23:41:45.773145: step 5070, loss 0.00439308, acc 1
2017-03-02T23:41:45.877224: step 5071, loss 0.000845171, acc 1
2017-03-02T23:41:45.967253: step 5072, loss 0.00210551, acc 1
2017-03-02T23:41:46.066229: step 5073, loss 0.000266407, acc 1
2017-03-02T23:41:46.171961: step 5074, loss 0.00184186, acc 1
2017-03-02T23:41:46.276803: step 5075, loss 0.00225046, acc 1
2017-03-02T23:41:46.378429: step 5076, loss 0.00069987, acc 1
2017-03-02T23:41:46.481709: step 5077, loss 0.0002803, acc 1
2017-03-02T23:41:46.594607: step 5078, loss 0.00746345, acc 1
2017-03-02T23:41:46.685067: step 5079, loss 0.00166111, acc 1
2017-03-02T23:41:46.783805: step 5080, loss 0.0257076, acc 0.984375
2017-03-02T23:41:46.885415: step 5081, loss 0.00123176, acc 1
2017-03-02T23:41:46.994577: step 5082, loss 0.000457589, acc 1
2017-03-02T23:41:47.104716: step 5083, loss 0.000549185, acc 1
2017-03-02T23:41:47.194217: step 5084, loss 0.0010972, acc 1
2017-03-02T23:41:47.295788: step 5085, loss 0.00142453, acc 1
2017-03-02T23:41:47.387889: step 5086, loss 0.000249457, acc 1
2017-03-02T23:41:47.489843: step 5087, loss 0.000458255, acc 1
2017-03-02T23:41:47.597311: step 5088, loss 0.00127577, acc 1
2017-03-02T23:41:47.702473: step 5089, loss 0.0170465, acc 1
2017-03-02T23:41:47.806251: step 5090, loss 0.00107278, acc 1
2017-03-02T23:41:47.912717: step 5091, loss 0.00255342, acc 1
2017-03-02T23:41:48.019406: step 5092, loss 0.00349123, acc 1
2017-03-02T23:41:48.123458: step 5093, loss 0.000866005, acc 1
2017-03-02T23:41:48.219383: step 5094, loss 0.00038363, acc 1
2017-03-02T23:41:48.325864: step 5095, loss 0.00117672, acc 1
2017-03-02T23:41:48.438600: step 5096, loss 0.000392773, acc 1
2017-03-02T23:41:48.553591: step 5097, loss 0.000741717, acc 1
2017-03-02T23:41:48.654028: step 5098, loss 0.00125458, acc 1
2017-03-02T23:41:48.758774: step 5099, loss 0.000912217, acc 1
2017-03-02T23:41:48.866194: step 5100, loss 0.000685356, acc 1

Evaluation:
2017-03-02T23:41:48.914546: step 5100, loss 2.6194, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5100

2017-03-02T23:41:49.365216: step 5101, loss 0.00552604, acc 1
2017-03-02T23:41:49.473924: step 5102, loss 0.00538046, acc 1
2017-03-02T23:41:49.580734: step 5103, loss 0.00581774, acc 1
2017-03-02T23:41:49.678093: step 5104, loss 0.0045074, acc 1
2017-03-02T23:41:49.778666: step 5105, loss 0.00111953, acc 1
2017-03-02T23:41:49.890016: step 5106, loss 0.000637798, acc 1
2017-03-02T23:41:49.999156: step 5107, loss 0.000816421, acc 1
2017-03-02T23:41:50.107931: step 5108, loss 0.000586193, acc 1
2017-03-02T23:41:50.215706: step 5109, loss 0.000872753, acc 1
2017-03-02T23:41:50.325854: step 5110, loss 0.00138544, acc 1
2017-03-02T23:41:50.419680: step 5111, loss 0.000829062, acc 1
2017-03-02T23:41:50.532351: step 5112, loss 0.000709121, acc 1
2017-03-02T23:41:50.642189: step 5113, loss 0.000634359, acc 1
2017-03-02T23:41:50.747399: step 5114, loss 0.00671203, acc 1
2017-03-02T23:41:50.856748: step 5115, loss 0.00459989, acc 1
2017-03-02T23:41:50.963780: step 5116, loss 0.00264637, acc 1
2017-03-02T23:41:51.085171: step 5117, loss 0.000753996, acc 1
2017-03-02T23:41:51.178148: step 5118, loss 0.000869981, acc 1
2017-03-02T23:41:51.283789: step 5119, loss 0.00451652, acc 1
2017-03-02T23:41:51.386428: step 5120, loss 0.0020838, acc 1
2017-03-02T23:41:51.492748: step 5121, loss 0.00208726, acc 1
2017-03-02T23:41:51.602093: step 5122, loss 0.0012317, acc 1
2017-03-02T23:41:51.707501: step 5123, loss 0.00128313, acc 1
2017-03-02T23:41:51.813501: step 5124, loss 0.000260519, acc 1
2017-03-02T23:41:51.903532: step 5125, loss 0.000401627, acc 1
2017-03-02T23:41:52.010601: step 5126, loss 0.000391373, acc 1
2017-03-02T23:41:52.131733: step 5127, loss 0.00104189, acc 1
2017-03-02T23:41:52.242638: step 5128, loss 0.00104719, acc 1
2017-03-02T23:41:52.343934: step 5129, loss 0.00111493, acc 1
2017-03-02T23:41:52.450158: step 5130, loss 0.000716077, acc 1
2017-03-02T23:41:52.559338: step 5131, loss 0.000531295, acc 1
2017-03-02T23:41:52.650371: step 5132, loss 0.000389314, acc 1
2017-03-02T23:41:52.756760: step 5133, loss 0.00531374, acc 1
2017-03-02T23:41:52.869865: step 5134, loss 0.000444111, acc 1
2017-03-02T23:41:52.973525: step 5135, loss 0.000385138, acc 1
2017-03-02T23:41:53.083744: step 5136, loss 0.000249306, acc 1
2017-03-02T23:41:53.191987: step 5137, loss 0.00335476, acc 1
2017-03-02T23:41:53.298724: step 5138, loss 0.000504286, acc 1
2017-03-02T23:41:53.381224: step 5139, loss 0.000438498, acc 1
2017-03-02T23:41:53.481713: step 5140, loss 0.0028622, acc 1
2017-03-02T23:41:53.599387: step 5141, loss 0.000429871, acc 1
2017-03-02T23:41:53.706906: step 5142, loss 0.000131714, acc 1
2017-03-02T23:41:53.811555: step 5143, loss 0.00448306, acc 1
2017-03-02T23:41:53.926134: step 5144, loss 0.00119507, acc 1
2017-03-02T23:41:54.031994: step 5145, loss 0.000807446, acc 1
2017-03-02T23:41:54.125021: step 5146, loss 0.00153379, acc 1
2017-03-02T23:41:54.215049: step 5147, loss 0.00338258, acc 1
2017-03-02T23:41:54.324926: step 5148, loss 0.000613903, acc 1
2017-03-02T23:41:54.421727: step 5149, loss 0.00535466, acc 1
2017-03-02T23:41:54.533219: step 5150, loss 0.00286365, acc 1
2017-03-02T23:41:54.643127: step 5151, loss 0.000673439, acc 1
2017-03-02T23:41:54.752062: step 5152, loss 0.000662236, acc 1
2017-03-02T23:41:54.871936: step 5153, loss 0.00151068, acc 1
2017-03-02T23:41:54.970409: step 5154, loss 0.00179246, acc 1
2017-03-02T23:41:55.069671: step 5155, loss 0.000293411, acc 1
2017-03-02T23:41:55.174868: step 5156, loss 0.000470413, acc 1
2017-03-02T23:41:55.300033: step 5157, loss 0.00144096, acc 1
2017-03-02T23:41:55.405606: step 5158, loss 0.00114373, acc 1
2017-03-02T23:41:55.508917: step 5159, loss 0.000363302, acc 1
2017-03-02T23:41:55.616527: step 5160, loss 0.00550874, acc 1
2017-03-02T23:41:55.710011: step 5161, loss 0.00125379, acc 1
2017-03-02T23:41:55.807345: step 5162, loss 0.000366413, acc 1
2017-03-02T23:41:55.914325: step 5163, loss 0.000491396, acc 1
2017-03-02T23:41:56.018087: step 5164, loss 0.00167841, acc 1
2017-03-02T23:41:56.117106: step 5165, loss 0.000944288, acc 1
2017-03-02T23:41:56.208999: step 5166, loss 0.00156439, acc 1
2017-03-02T23:41:56.314111: step 5167, loss 0.00028772, acc 1
2017-03-02T23:41:56.416546: step 5168, loss 0.000597602, acc 1
2017-03-02T23:41:56.506925: step 5169, loss 0.000739976, acc 1
2017-03-02T23:41:56.619490: step 5170, loss 0.000407019, acc 1
2017-03-02T23:41:56.723897: step 5171, loss 0.00018844, acc 1
2017-03-02T23:41:56.828711: step 5172, loss 0.000629415, acc 1
2017-03-02T23:41:56.938510: step 5173, loss 0.00174319, acc 1
2017-03-02T23:41:57.048638: step 5174, loss 0.000307715, acc 1
2017-03-02T23:41:57.155326: step 5175, loss 0.00788643, acc 1
2017-03-02T23:41:57.249256: step 5176, loss 0.000336848, acc 1
2017-03-02T23:41:57.351084: step 5177, loss 0.00488744, acc 1
2017-03-02T23:41:57.458586: step 5178, loss 0.000401815, acc 1
2017-03-02T23:41:57.565035: step 5179, loss 0.000997102, acc 1
2017-03-02T23:41:57.676395: step 5180, loss 0.00730062, acc 1
2017-03-02T23:41:57.788383: step 5181, loss 0.000883978, acc 1
2017-03-02T23:41:57.888632: step 5182, loss 0.000864432, acc 1
2017-03-02T23:41:57.985964: step 5183, loss 0.000397442, acc 1
2017-03-02T23:41:58.071127: step 5184, loss 0.000316281, acc 1
2017-03-02T23:41:58.173909: step 5185, loss 0.00518042, acc 1
2017-03-02T23:41:58.279246: step 5186, loss 0.000761686, acc 1
2017-03-02T23:41:58.387437: step 5187, loss 0.00169187, acc 1
2017-03-02T23:41:58.501107: step 5188, loss 0.000656105, acc 1
2017-03-02T23:41:58.619534: step 5189, loss 0.00101331, acc 1
2017-03-02T23:41:58.722817: step 5190, loss 0.0281282, acc 0.984375
2017-03-02T23:41:58.810840: step 5191, loss 0.000160377, acc 1
2017-03-02T23:41:58.917869: step 5192, loss 0.00016834, acc 1
2017-03-02T23:41:59.027897: step 5193, loss 0.000606215, acc 1
2017-03-02T23:41:59.136397: step 5194, loss 0.00350676, acc 1
2017-03-02T23:41:59.248215: step 5195, loss 0.000542316, acc 1
2017-03-02T23:41:59.356159: step 5196, loss 0.000946754, acc 1
2017-03-02T23:41:59.458047: step 5197, loss 0.000222537, acc 1
2017-03-02T23:41:59.551452: step 5198, loss 0.002645, acc 1
2017-03-02T23:41:59.653686: step 5199, loss 0.000290927, acc 1
2017-03-02T23:41:59.759020: step 5200, loss 0.00128683, acc 1

Evaluation:
2017-03-02T23:41:59.820996: step 5200, loss 2.67552, acc 0.522491

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5200

2017-03-02T23:42:00.276403: step 5201, loss 0.000822074, acc 1
2017-03-02T23:42:00.374629: step 5202, loss 0.00219012, acc 1
2017-03-02T23:42:00.481695: step 5203, loss 0.00137815, acc 1
2017-03-02T23:42:00.587880: step 5204, loss 0.000741433, acc 1
2017-03-02T23:42:00.706547: step 5205, loss 0.00083021, acc 1
2017-03-02T23:42:00.813809: step 5206, loss 0.00120633, acc 1
2017-03-02T23:42:00.913139: step 5207, loss 0.00177545, acc 1
2017-03-02T23:42:01.004404: step 5208, loss 0.000754089, acc 1
2017-03-02T23:42:01.111478: step 5209, loss 0.000255239, acc 1
2017-03-02T23:42:01.221782: step 5210, loss 0.00115962, acc 1
2017-03-02T23:42:01.333863: step 5211, loss 0.000537952, acc 1
2017-03-02T23:42:01.433972: step 5212, loss 0.00121922, acc 1
2017-03-02T23:42:01.540767: step 5213, loss 0.000390388, acc 1
2017-03-02T23:42:01.648557: step 5214, loss 0.000536006, acc 1
2017-03-02T23:42:01.736337: step 5215, loss 0.00791108, acc 1
2017-03-02T23:42:01.846298: step 5216, loss 0.00019318, acc 1
2017-03-02T23:42:01.952273: step 5217, loss 0.000696205, acc 1
2017-03-02T23:42:02.060565: step 5218, loss 0.00129382, acc 1
2017-03-02T23:42:02.169749: step 5219, loss 0.000256286, acc 1
2017-03-02T23:42:02.272711: step 5220, loss 0.00073215, acc 1
2017-03-02T23:42:02.379591: step 5221, loss 0.000929552, acc 1
2017-03-02T23:42:02.471559: step 5222, loss 0.00115967, acc 1
2017-03-02T23:42:02.580713: step 5223, loss 0.00128103, acc 1
2017-03-02T23:42:02.686043: step 5224, loss 0.00348859, acc 1
2017-03-02T23:42:02.791111: step 5225, loss 0.00141256, acc 1
2017-03-02T23:42:02.897697: step 5226, loss 0.000520234, acc 1
2017-03-02T23:42:03.006473: step 5227, loss 0.0250484, acc 0.984375
2017-03-02T23:42:03.119039: step 5228, loss 0.00292264, acc 1
2017-03-02T23:42:03.217360: step 5229, loss 0.000190962, acc 1
2017-03-02T23:42:03.321872: step 5230, loss 0.000613313, acc 1
2017-03-02T23:42:03.428426: step 5231, loss 0.000389516, acc 1
2017-03-02T23:42:03.555426: step 5232, loss 0.00342182, acc 1
2017-03-02T23:42:03.672940: step 5233, loss 0.0112451, acc 1
2017-03-02T23:42:03.780142: step 5234, loss 0.000721083, acc 1
2017-03-02T23:42:03.890317: step 5235, loss 0.00423824, acc 1
2017-03-02T23:42:03.981812: step 5236, loss 0.00087976, acc 1
2017-03-02T23:42:04.091498: step 5237, loss 0.00153093, acc 1
2017-03-02T23:42:04.206151: step 5238, loss 0.00133098, acc 1
2017-03-02T23:42:04.326817: step 5239, loss 0.000611649, acc 1
2017-03-02T23:42:04.429031: step 5240, loss 0.00907268, acc 1
2017-03-02T23:42:04.536485: step 5241, loss 0.00573255, acc 1
2017-03-02T23:42:04.644972: step 5242, loss 0.000284353, acc 1
2017-03-02T23:42:04.731013: step 5243, loss 0.00193676, acc 1
2017-03-02T23:42:04.833132: step 5244, loss 0.000344192, acc 1
2017-03-02T23:42:04.930465: step 5245, loss 0.00446139, acc 1
2017-03-02T23:42:05.036132: step 5246, loss 0.00140751, acc 1
2017-03-02T23:42:05.141153: step 5247, loss 0.00386539, acc 1
2017-03-02T23:42:05.236946: step 5248, loss 0.000980072, acc 1
2017-03-02T23:42:05.338291: step 5249, loss 0.000820148, acc 1
2017-03-02T23:42:05.435317: step 5250, loss 0.000367024, acc 1
2017-03-02T23:42:05.530803: step 5251, loss 0.0014597, acc 1
2017-03-02T23:42:05.633512: step 5252, loss 0.0222825, acc 0.984375
2017-03-02T23:42:05.733268: step 5253, loss 0.00180132, acc 1
2017-03-02T23:42:05.837438: step 5254, loss 0.000862912, acc 1
2017-03-02T23:42:05.940039: step 5255, loss 0.000158648, acc 1
2017-03-02T23:42:06.057947: step 5256, loss 0.0041927, acc 1
2017-03-02T23:42:06.165151: step 5257, loss 0.00188829, acc 1
2017-03-02T23:42:06.252452: step 5258, loss 0.000718989, acc 1
2017-03-02T23:42:06.357706: step 5259, loss 0.000431166, acc 1
2017-03-02T23:42:06.463410: step 5260, loss 0.00112571, acc 1
2017-03-02T23:42:06.564397: step 5261, loss 0.000901735, acc 1
2017-03-02T23:42:06.668803: step 5262, loss 0.000810558, acc 1
2017-03-02T23:42:06.780375: step 5263, loss 0.000383508, acc 1
2017-03-02T23:42:06.891287: step 5264, loss 0.00152229, acc 1
2017-03-02T23:42:06.977503: step 5265, loss 0.00544139, acc 1
2017-03-02T23:42:07.073582: step 5266, loss 0.000399, acc 1
2017-03-02T23:42:07.180887: step 5267, loss 0.00137697, acc 1
2017-03-02T23:42:07.282519: step 5268, loss 0.00082332, acc 1
2017-03-02T23:42:07.389967: step 5269, loss 0.00111155, acc 1
2017-03-02T23:42:07.496523: step 5270, loss 0.0124508, acc 1
2017-03-02T23:42:07.610967: step 5271, loss 0.0026088, acc 1
2017-03-02T23:42:07.711153: step 5272, loss 0.00405279, acc 1
2017-03-02T23:42:07.797382: step 5273, loss 0.00221248, acc 1
2017-03-02T23:42:07.911586: step 5274, loss 0.00223498, acc 1
2017-03-02T23:42:08.021455: step 5275, loss 0.000149792, acc 1
2017-03-02T23:42:08.126648: step 5276, loss 0.00234485, acc 1
2017-03-02T23:42:08.230142: step 5277, loss 0.00116671, acc 1
2017-03-02T23:42:08.343090: step 5278, loss 0.00240876, acc 1
2017-03-02T23:42:08.434444: step 5279, loss 0.00287628, acc 1
2017-03-02T23:42:08.527830: step 5280, loss 0.00160315, acc 1
2017-03-02T23:42:08.633063: step 5281, loss 0.000362382, acc 1
2017-03-02T23:42:08.741421: step 5282, loss 0.00140712, acc 1
2017-03-02T23:42:08.847318: step 5283, loss 0.00977042, acc 1
2017-03-02T23:42:08.956949: step 5284, loss 0.000843742, acc 1
2017-03-02T23:42:09.061014: step 5285, loss 0.0145134, acc 0.984375
2017-03-02T23:42:09.167167: step 5286, loss 0.000380986, acc 1
2017-03-02T23:42:09.258225: step 5287, loss 0.000794185, acc 1
2017-03-02T23:42:09.362813: step 5288, loss 0.000253091, acc 1
2017-03-02T23:42:09.455702: step 5289, loss 0.000861555, acc 1
2017-03-02T23:42:09.573080: step 5290, loss 0.000889508, acc 1
2017-03-02T23:42:09.686635: step 5291, loss 0.00202208, acc 1
2017-03-02T23:42:09.792737: step 5292, loss 0.00331282, acc 1
2017-03-02T23:42:09.896772: step 5293, loss 0.00135717, acc 1
2017-03-02T23:42:09.985841: step 5294, loss 0.00159098, acc 1
2017-03-02T23:42:10.088965: step 5295, loss 0.00048331, acc 1
2017-03-02T23:42:10.197905: step 5296, loss 0.000478061, acc 1
2017-03-02T23:42:10.321124: step 5297, loss 0.000584817, acc 1
2017-03-02T23:42:10.424571: step 5298, loss 0.000318828, acc 1
2017-03-02T23:42:10.526795: step 5299, loss 0.0244323, acc 0.984375
2017-03-02T23:42:10.636987: step 5300, loss 0.0487454, acc 0.984375

Evaluation:
2017-03-02T23:42:10.676866: step 5300, loss 2.61823, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5300

2017-03-02T23:42:11.143631: step 5301, loss 0.000588386, acc 1
2017-03-02T23:42:11.250000: step 5302, loss 0.00052218, acc 1
2017-03-02T23:42:11.356080: step 5303, loss 0.0113699, acc 1
2017-03-02T23:42:11.456204: step 5304, loss 0.00080122, acc 1
2017-03-02T23:42:11.561056: step 5305, loss 0.000766649, acc 1
2017-03-02T23:42:11.663524: step 5306, loss 0.00032885, acc 1
2017-03-02T23:42:11.769292: step 5307, loss 0.00153701, acc 1
2017-03-02T23:42:11.876450: step 5308, loss 0.0187735, acc 1
2017-03-02T23:42:11.983792: step 5309, loss 0.00494645, acc 1
2017-03-02T23:42:12.093444: step 5310, loss 0.00712039, acc 1
2017-03-02T23:42:12.181904: step 5311, loss 0.00737842, acc 1
2017-03-02T23:42:12.294489: step 5312, loss 0.000557738, acc 1
2017-03-02T23:42:12.404710: step 5313, loss 0.00182684, acc 1
2017-03-02T23:42:12.521724: step 5314, loss 0.000689674, acc 1
2017-03-02T23:42:12.628347: step 5315, loss 0.000493837, acc 1
2017-03-02T23:42:12.730746: step 5316, loss 0.00032579, acc 1
2017-03-02T23:42:12.836111: step 5317, loss 0.00211295, acc 1
2017-03-02T23:42:12.913473: step 5318, loss 0.00198424, acc 1
2017-03-02T23:42:13.013718: step 5319, loss 0.000703584, acc 1
2017-03-02T23:42:13.127804: step 5320, loss 0.00757765, acc 1
2017-03-02T23:42:13.233951: step 5321, loss 0.000439272, acc 1
2017-03-02T23:42:13.343255: step 5322, loss 0.000371633, acc 1
2017-03-02T23:42:13.449140: step 5323, loss 0.00188546, acc 1
2017-03-02T23:42:13.564302: step 5324, loss 0.000412959, acc 1
2017-03-02T23:42:13.653268: step 5325, loss 0.00137749, acc 1
2017-03-02T23:42:13.748885: step 5326, loss 0.00182225, acc 1
2017-03-02T23:42:13.854535: step 5327, loss 0.00126981, acc 1
2017-03-02T23:42:13.966935: step 5328, loss 0.000173426, acc 1
2017-03-02T23:42:14.075711: step 5329, loss 0.00148372, acc 1
2017-03-02T23:42:14.172751: step 5330, loss 0.00203691, acc 1
2017-03-02T23:42:14.277987: step 5331, loss 0.000704314, acc 1
2017-03-02T23:42:14.370469: step 5332, loss 0.000464292, acc 1
2017-03-02T23:42:14.463517: step 5333, loss 0.000407214, acc 1
2017-03-02T23:42:14.569813: step 5334, loss 0.000251082, acc 1
2017-03-02T23:42:14.679819: step 5335, loss 0.00088039, acc 1
2017-03-02T23:42:14.787414: step 5336, loss 0.000769163, acc 1
2017-03-02T23:42:14.900377: step 5337, loss 0.00013507, acc 1
2017-03-02T23:42:15.014171: step 5338, loss 0.00169577, acc 1
2017-03-02T23:42:15.117881: step 5339, loss 0.00027206, acc 1
2017-03-02T23:42:15.205190: step 5340, loss 0.000240315, acc 1
2017-03-02T23:42:15.307707: step 5341, loss 0.00110473, acc 1
2017-03-02T23:42:15.419002: step 5342, loss 0.00314088, acc 1
2017-03-02T23:42:15.523174: step 5343, loss 0.000700882, acc 1
2017-03-02T23:42:15.623440: step 5344, loss 0.000651987, acc 1
2017-03-02T23:42:15.735541: step 5345, loss 0.000585542, acc 1
2017-03-02T23:42:15.843875: step 5346, loss 0.000493011, acc 1
2017-03-02T23:42:15.937440: step 5347, loss 0.000930492, acc 1
2017-03-02T23:42:16.051409: step 5348, loss 0.000768759, acc 1
2017-03-02T23:42:16.157057: step 5349, loss 0.000445039, acc 1
2017-03-02T23:42:16.256623: step 5350, loss 0.000898602, acc 1
2017-03-02T23:42:16.364187: step 5351, loss 0.00132255, acc 1
2017-03-02T23:42:16.470666: step 5352, loss 0.00124082, acc 1
2017-03-02T23:42:16.577028: step 5353, loss 0.000872106, acc 1
2017-03-02T23:42:16.666808: step 5354, loss 0.00198679, acc 1
2017-03-02T23:42:16.770829: step 5355, loss 0.000766531, acc 1
2017-03-02T23:42:16.868622: step 5356, loss 0.00122466, acc 1
2017-03-02T23:42:16.984822: step 5357, loss 0.00141823, acc 1
2017-03-02T23:42:17.088923: step 5358, loss 0.000830773, acc 1
2017-03-02T23:42:17.192588: step 5359, loss 0.000843771, acc 1
2017-03-02T23:42:17.305641: step 5360, loss 0.00624932, acc 1
2017-03-02T23:42:17.403456: step 5361, loss 0.00110616, acc 1
2017-03-02T23:42:17.492456: step 5362, loss 0.00125962, acc 1
2017-03-02T23:42:17.599817: step 5363, loss 0.000276772, acc 1
2017-03-02T23:42:17.711603: step 5364, loss 0.00406512, acc 1
2017-03-02T23:42:17.816940: step 5365, loss 0.000596161, acc 1
2017-03-02T23:42:17.932195: step 5366, loss 0.00207363, acc 1
2017-03-02T23:42:18.034925: step 5367, loss 0.00874182, acc 1
2017-03-02T23:42:18.137942: step 5368, loss 0.00291684, acc 1
2017-03-02T23:42:18.226557: step 5369, loss 0.000163196, acc 1
2017-03-02T23:42:18.328969: step 5370, loss 0.000771284, acc 1
2017-03-02T23:42:18.428305: step 5371, loss 0.00546469, acc 1
2017-03-02T23:42:18.535705: step 5372, loss 0.00221774, acc 1
2017-03-02T23:42:18.639833: step 5373, loss 0.00200703, acc 1
2017-03-02T23:42:18.745628: step 5374, loss 0.00631422, acc 1
2017-03-02T23:42:18.853729: step 5375, loss 0.0041142, acc 1
2017-03-02T23:42:18.947020: step 5376, loss 0.000199744, acc 1
2017-03-02T23:42:19.044030: step 5377, loss 0.000953215, acc 1
2017-03-02T23:42:19.143323: step 5378, loss 0.000173325, acc 1
2017-03-02T23:42:19.237857: step 5379, loss 0.00196741, acc 1
2017-03-02T23:42:19.351634: step 5380, loss 0.00427512, acc 1
2017-03-02T23:42:19.459557: step 5381, loss 0.000405339, acc 1
2017-03-02T23:42:19.595889: step 5382, loss 0.00452625, acc 1
2017-03-02T23:42:19.702532: step 5383, loss 0.000838865, acc 1
2017-03-02T23:42:19.806201: step 5384, loss 0.00101402, acc 1
2017-03-02T23:42:19.908222: step 5385, loss 0.000189869, acc 1
2017-03-02T23:42:20.005025: step 5386, loss 0.000109512, acc 1
2017-03-02T23:42:20.103828: step 5387, loss 0.000989613, acc 1
2017-03-02T23:42:20.205230: step 5388, loss 0.00211162, acc 1
2017-03-02T23:42:20.310710: step 5389, loss 0.00487272, acc 1
2017-03-02T23:42:20.399457: step 5390, loss 0.000591891, acc 1
2017-03-02T23:42:20.512461: step 5391, loss 0.00315033, acc 1
2017-03-02T23:42:20.620848: step 5392, loss 0.00170249, acc 1
2017-03-02T23:42:20.741160: step 5393, loss 0.000474868, acc 1
2017-03-02T23:42:20.852202: step 5394, loss 0.00100677, acc 1
2017-03-02T23:42:20.959015: step 5395, loss 0.00130171, acc 1
2017-03-02T23:42:21.069233: step 5396, loss 0.000887615, acc 1
2017-03-02T23:42:21.166982: step 5397, loss 0.000503389, acc 1
2017-03-02T23:42:21.260123: step 5398, loss 0.000834466, acc 1
2017-03-02T23:42:21.379545: step 5399, loss 0.00290058, acc 1
2017-03-02T23:42:21.482276: step 5400, loss 0.000412254, acc 1

Evaluation:
2017-03-02T23:42:21.537794: step 5400, loss 2.62107, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5400

2017-03-02T23:42:21.974557: step 5401, loss 0.0032694, acc 1
2017-03-02T23:42:22.093403: step 5402, loss 0.000518896, acc 1
2017-03-02T23:42:22.200386: step 5403, loss 0.000573522, acc 1
2017-03-02T23:42:22.300942: step 5404, loss 0.000492622, acc 1
2017-03-02T23:42:22.403496: step 5405, loss 0.00160882, acc 1
2017-03-02T23:42:22.509490: step 5406, loss 0.000513809, acc 1
2017-03-02T23:42:22.596753: step 5407, loss 0.000522974, acc 1
2017-03-02T23:42:22.702776: step 5408, loss 0.0043236, acc 1
2017-03-02T23:42:22.812208: step 5409, loss 0.00258271, acc 1
2017-03-02T23:42:22.919859: step 5410, loss 0.000274297, acc 1
2017-03-02T23:42:23.032688: step 5411, loss 0.000894771, acc 1
2017-03-02T23:42:23.132377: step 5412, loss 0.00109917, acc 1
2017-03-02T23:42:23.244015: step 5413, loss 0.00111028, acc 1
2017-03-02T23:42:23.327924: step 5414, loss 0.000883567, acc 1
2017-03-02T23:42:23.419900: step 5415, loss 0.00119829, acc 1
2017-03-02T23:42:23.528369: step 5416, loss 0.000511601, acc 1
2017-03-02T23:42:23.635686: step 5417, loss 0.000616004, acc 1
2017-03-02T23:42:23.750646: step 5418, loss 0.000805836, acc 1
2017-03-02T23:42:23.861120: step 5419, loss 0.00508921, acc 1
2017-03-02T23:42:24.052174: step 5420, loss 0.000500263, acc 1
2017-03-02T23:42:24.146567: step 5421, loss 0.00063502, acc 1
2017-03-02T23:42:24.258562: step 5422, loss 0.000642665, acc 1
2017-03-02T23:42:24.374814: step 5423, loss 0.0034585, acc 1
2017-03-02T23:42:24.486192: step 5424, loss 0.000664569, acc 1
2017-03-02T23:42:24.601387: step 5425, loss 0.00185269, acc 1
2017-03-02T23:42:24.712150: step 5426, loss 0.000560339, acc 1
2017-03-02T23:42:24.808522: step 5427, loss 0.0012345, acc 1
2017-03-02T23:42:24.898757: step 5428, loss 0.00446186, acc 1
2017-03-02T23:42:25.009475: step 5429, loss 0.000866416, acc 1
2017-03-02T23:42:25.113540: step 5430, loss 0.00218988, acc 1
2017-03-02T23:42:25.221473: step 5431, loss 0.00198026, acc 1
2017-03-02T23:42:25.328209: step 5432, loss 0.00172292, acc 1
2017-03-02T23:42:25.435190: step 5433, loss 0.00694641, acc 1
2017-03-02T23:42:25.537247: step 5434, loss 0.00130136, acc 1
2017-03-02T23:42:25.630993: step 5435, loss 0.00501456, acc 1
2017-03-02T23:42:25.729302: step 5436, loss 0.00170715, acc 1
2017-03-02T23:42:25.837817: step 5437, loss 0.000852368, acc 1
2017-03-02T23:42:25.941914: step 5438, loss 0.000297058, acc 1
2017-03-02T23:42:26.047937: step 5439, loss 0.012045, acc 1
2017-03-02T23:42:26.155530: step 5440, loss 0.000846779, acc 1
2017-03-02T23:42:26.270978: step 5441, loss 0.00126291, acc 1
2017-03-02T23:42:26.361118: step 5442, loss 0.000445688, acc 1
2017-03-02T23:42:26.467533: step 5443, loss 0.00172345, acc 1
2017-03-02T23:42:26.572535: step 5444, loss 0.000539516, acc 1
2017-03-02T23:42:26.685655: step 5445, loss 0.000876286, acc 1
2017-03-02T23:42:26.790565: step 5446, loss 0.000623474, acc 1
2017-03-02T23:42:26.911826: step 5447, loss 0.000333693, acc 1
2017-03-02T23:42:27.021894: step 5448, loss 0.00135376, acc 1
2017-03-02T23:42:27.109177: step 5449, loss 0.00781333, acc 1
2017-03-02T23:42:27.214837: step 5450, loss 0.00160249, acc 1
2017-03-02T23:42:27.323512: step 5451, loss 0.00158063, acc 1
2017-03-02T23:42:27.428853: step 5452, loss 0.00172011, acc 1
2017-03-02T23:42:27.543362: step 5453, loss 0.00210458, acc 1
2017-03-02T23:42:27.652641: step 5454, loss 0.000724587, acc 1
2017-03-02T23:42:27.763195: step 5455, loss 0.000368861, acc 1
2017-03-02T23:42:27.851628: step 5456, loss 0.000743613, acc 1
2017-03-02T23:42:27.963652: step 5457, loss 0.000633811, acc 1
2017-03-02T23:42:28.072109: step 5458, loss 0.00130551, acc 1
2017-03-02T23:42:28.172031: step 5459, loss 0.000271583, acc 1
2017-03-02T23:42:28.277461: step 5460, loss 0.00151716, acc 1
2017-03-02T23:42:28.384666: step 5461, loss 0.00164927, acc 1
2017-03-02T23:42:28.487963: step 5462, loss 0.000529188, acc 1
2017-03-02T23:42:28.582648: step 5463, loss 0.0045127, acc 1
2017-03-02T23:42:28.688426: step 5464, loss 0.000523214, acc 1
2017-03-02T23:42:28.789705: step 5465, loss 0.00265807, acc 1
2017-03-02T23:42:28.886562: step 5466, loss 0.000869979, acc 1
2017-03-02T23:42:28.991287: step 5467, loss 0.000649971, acc 1
2017-03-02T23:42:29.098003: step 5468, loss 0.00046498, acc 1
2017-03-02T23:42:29.204926: step 5469, loss 0.000693556, acc 1
2017-03-02T23:42:29.306394: step 5470, loss 0.00514546, acc 1
2017-03-02T23:42:29.396518: step 5471, loss 0.000991118, acc 1
2017-03-02T23:42:29.506175: step 5472, loss 0.000585051, acc 1
2017-03-02T23:42:29.613430: step 5473, loss 0.000542141, acc 1
2017-03-02T23:42:29.721623: step 5474, loss 0.0021306, acc 1
2017-03-02T23:42:29.843683: step 5475, loss 0.000354288, acc 1
2017-03-02T23:42:29.959110: step 5476, loss 0.000468967, acc 1
2017-03-02T23:42:30.064529: step 5477, loss 0.000402498, acc 1
2017-03-02T23:42:30.157761: step 5478, loss 0.00283218, acc 1
2017-03-02T23:42:30.261722: step 5479, loss 0.00128185, acc 1
2017-03-02T23:42:30.356743: step 5480, loss 0.000302876, acc 1
2017-03-02T23:42:30.462252: step 5481, loss 0.000244991, acc 1
2017-03-02T23:42:30.573790: step 5482, loss 0.00115253, acc 1
2017-03-02T23:42:30.674053: step 5483, loss 0.000293262, acc 1
2017-03-02T23:42:30.780598: step 5484, loss 0.0189019, acc 0.984375
2017-03-02T23:42:30.873903: step 5485, loss 0.000225351, acc 1
2017-03-02T23:42:30.978418: step 5486, loss 0.000566083, acc 1
2017-03-02T23:42:31.089019: step 5487, loss 0.00962074, acc 1
2017-03-02T23:42:31.204303: step 5488, loss 0.000404204, acc 1
2017-03-02T23:42:31.309392: step 5489, loss 0.00358186, acc 1
2017-03-02T23:42:31.410128: step 5490, loss 0.00288918, acc 1
2017-03-02T23:42:31.517452: step 5491, loss 0.00343622, acc 1
2017-03-02T23:42:31.606815: step 5492, loss 0.00320782, acc 1
2017-03-02T23:42:31.702416: step 5493, loss 0.00330443, acc 1
2017-03-02T23:42:31.808915: step 5494, loss 0.000910188, acc 1
2017-03-02T23:42:31.917940: step 5495, loss 0.000195396, acc 1
2017-03-02T23:42:32.023405: step 5496, loss 0.00155911, acc 1
2017-03-02T23:42:32.134279: step 5497, loss 0.000899833, acc 1
2017-03-02T23:42:32.236702: step 5498, loss 0.000665147, acc 1
2017-03-02T23:42:32.327409: step 5499, loss 0.00474552, acc 1
2017-03-02T23:42:32.416568: step 5500, loss 0.00248712, acc 1

Evaluation:
2017-03-02T23:42:32.473930: step 5500, loss 2.80703, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5500

2017-03-02T23:42:32.915930: step 5501, loss 0.000786806, acc 1
2017-03-02T23:42:33.015778: step 5502, loss 0.000395268, acc 1
2017-03-02T23:42:33.106266: step 5503, loss 0.000515566, acc 1
2017-03-02T23:42:33.213601: step 5504, loss 0.0180887, acc 0.984375
2017-03-02T23:42:33.326845: step 5505, loss 0.00448439, acc 1
2017-03-02T23:42:33.432661: step 5506, loss 0.000313642, acc 1
2017-03-02T23:42:33.547887: step 5507, loss 0.000183041, acc 1
2017-03-02T23:42:33.651142: step 5508, loss 0.0012159, acc 1
2017-03-02T23:42:33.756305: step 5509, loss 0.000264576, acc 1
2017-03-02T23:42:33.845534: step 5510, loss 0.00486631, acc 1
2017-03-02T23:42:33.961138: step 5511, loss 0.00169832, acc 1
2017-03-02T23:42:34.069664: step 5512, loss 0.000337074, acc 1
2017-03-02T23:42:34.192939: step 5513, loss 0.00022802, acc 1
2017-03-02T23:42:34.298968: step 5514, loss 0.00154446, acc 1
2017-03-02T23:42:34.401510: step 5515, loss 0.000398823, acc 1
2017-03-02T23:42:34.503713: step 5516, loss 0.00199118, acc 1
2017-03-02T23:42:34.592131: step 5517, loss 0.000297514, acc 1
2017-03-02T23:42:34.692594: step 5518, loss 0.0013181, acc 1
2017-03-02T23:42:34.800343: step 5519, loss 0.0015122, acc 1
2017-03-02T23:42:34.912178: step 5520, loss 0.00307391, acc 1
2017-03-02T23:42:35.023557: step 5521, loss 0.0138634, acc 0.984375
2017-03-02T23:42:35.130540: step 5522, loss 0.0010689, acc 1
2017-03-02T23:42:35.242728: step 5523, loss 0.000510422, acc 1
2017-03-02T23:42:35.332799: step 5524, loss 0.00297597, acc 1
2017-03-02T23:42:35.431804: step 5525, loss 0.00171946, acc 1
2017-03-02T23:42:35.542278: step 5526, loss 0.000681533, acc 1
2017-03-02T23:42:35.650227: step 5527, loss 0.000828869, acc 1
2017-03-02T23:42:35.756872: step 5528, loss 0.000752477, acc 1
2017-03-02T23:42:35.858048: step 5529, loss 0.000837072, acc 1
2017-03-02T23:42:35.955201: step 5530, loss 0.000586822, acc 1
2017-03-02T23:42:36.046415: step 5531, loss 0.00220914, acc 1
2017-03-02T23:42:36.137324: step 5532, loss 0.000551221, acc 1
2017-03-02T23:42:36.237488: step 5533, loss 0.000515339, acc 1
2017-03-02T23:42:36.340872: step 5534, loss 0.000281667, acc 1
2017-03-02T23:42:36.433559: step 5535, loss 0.00304262, acc 1
2017-03-02T23:42:36.538718: step 5536, loss 0.00115307, acc 1
2017-03-02T23:42:36.643683: step 5537, loss 0.00118675, acc 1
2017-03-02T23:42:36.762975: step 5538, loss 0.000741449, acc 1
2017-03-02T23:42:36.860188: step 5539, loss 0.00103104, acc 1
2017-03-02T23:42:36.970461: step 5540, loss 0.00447202, acc 1
2017-03-02T23:42:37.067404: step 5541, loss 0.000393174, acc 1
2017-03-02T23:42:37.186548: step 5542, loss 0.0102497, acc 1
2017-03-02T23:42:37.290519: step 5543, loss 0.00125659, acc 1
2017-03-02T23:42:37.408897: step 5544, loss 0.000868911, acc 1
2017-03-02T23:42:37.511372: step 5545, loss 0.00263241, acc 1
2017-03-02T23:42:37.608949: step 5546, loss 0.00384484, acc 1
2017-03-02T23:42:37.728377: step 5547, loss 0.000733731, acc 1
2017-03-02T23:42:37.835907: step 5548, loss 0.00077959, acc 1
2017-03-02T23:42:37.943990: step 5549, loss 0.0015898, acc 1
2017-03-02T23:42:38.052674: step 5550, loss 0.00088727, acc 1
2017-03-02T23:42:38.162329: step 5551, loss 0.000814267, acc 1
2017-03-02T23:42:38.268763: step 5552, loss 0.00228847, acc 1
2017-03-02T23:42:38.357392: step 5553, loss 0.0872291, acc 0.984375
2017-03-02T23:42:38.466580: step 5554, loss 0.000628376, acc 1
2017-03-02T23:42:38.571025: step 5555, loss 0.000203302, acc 1
2017-03-02T23:42:38.687289: step 5556, loss 0.0027067, acc 1
2017-03-02T23:42:38.794226: step 5557, loss 0.00262552, acc 1
2017-03-02T23:42:38.893577: step 5558, loss 0.000925934, acc 1
2017-03-02T23:42:38.998206: step 5559, loss 0.00110785, acc 1
2017-03-02T23:42:39.101945: step 5560, loss 0.00243542, acc 1
2017-03-02T23:42:39.202692: step 5561, loss 0.000233233, acc 1
2017-03-02T23:42:39.303252: step 5562, loss 0.00145256, acc 1
2017-03-02T23:42:39.405411: step 5563, loss 0.00224049, acc 1
2017-03-02T23:42:39.524831: step 5564, loss 0.000406921, acc 1
2017-03-02T23:42:39.626080: step 5565, loss 0.00446584, acc 1
2017-03-02T23:42:39.728583: step 5566, loss 0.000970805, acc 1
2017-03-02T23:42:39.821717: step 5567, loss 0.0111328, acc 0.984375
2017-03-02T23:42:39.925211: step 5568, loss 0.0014474, acc 1
2017-03-02T23:42:40.025117: step 5569, loss 0.000503655, acc 1
2017-03-02T23:42:40.116776: step 5570, loss 0.000934861, acc 1
2017-03-02T23:42:40.224227: step 5571, loss 0.00106505, acc 1
2017-03-02T23:42:40.330918: step 5572, loss 0.00220631, acc 1
2017-03-02T23:42:40.443787: step 5573, loss 0.0016841, acc 1
2017-03-02T23:42:40.545168: step 5574, loss 0.0020048, acc 1
2017-03-02T23:42:40.636741: step 5575, loss 0.000789014, acc 1
2017-03-02T23:42:40.740022: step 5576, loss 0.000878103, acc 1
2017-03-02T23:42:40.847101: step 5577, loss 0.000481303, acc 1
2017-03-02T23:42:40.952746: step 5578, loss 0.00102292, acc 1
2017-03-02T23:42:41.060555: step 5579, loss 0.000828836, acc 1
2017-03-02T23:42:41.154411: step 5580, loss 0.000309851, acc 1
2017-03-02T23:42:41.260509: step 5581, loss 0.00017159, acc 1
2017-03-02T23:42:41.352645: step 5582, loss 0.00555001, acc 1
2017-03-02T23:42:41.454835: step 5583, loss 0.000338929, acc 1
2017-03-02T23:42:41.561269: step 5584, loss 0.00867702, acc 1
2017-03-02T23:42:41.669516: step 5585, loss 0.000576007, acc 1
2017-03-02T23:42:41.786883: step 5586, loss 0.000328485, acc 1
2017-03-02T23:42:41.890925: step 5587, loss 0.000878021, acc 1
2017-03-02T23:42:41.996169: step 5588, loss 0.00050137, acc 1
2017-03-02T23:42:42.092814: step 5589, loss 0.00117153, acc 1
2017-03-02T23:42:42.190292: step 5590, loss 0.00251808, acc 1
2017-03-02T23:42:42.295059: step 5591, loss 0.000958625, acc 1
2017-03-02T23:42:42.413762: step 5592, loss 0.00440765, acc 1
2017-03-02T23:42:42.515046: step 5593, loss 0.00231219, acc 1
2017-03-02T23:42:42.610426: step 5594, loss 0.000792387, acc 1
2017-03-02T23:42:42.715313: step 5595, loss 0.00182845, acc 1
2017-03-02T23:42:42.813724: step 5596, loss 0.0178766, acc 0.984375
2017-03-02T23:42:42.904436: step 5597, loss 0.000211811, acc 1
2017-03-02T23:42:43.011177: step 5598, loss 0.000788915, acc 1
2017-03-02T23:42:43.120808: step 5599, loss 0.000145131, acc 1
2017-03-02T23:42:43.229451: step 5600, loss 0.000329051, acc 1

Evaluation:
2017-03-02T23:42:43.293318: step 5600, loss 2.78622, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5600

2017-03-02T23:42:43.748133: step 5601, loss 0.00141786, acc 1
2017-03-02T23:42:43.863876: step 5602, loss 0.00042372, acc 1
2017-03-02T23:42:43.972591: step 5603, loss 0.00133413, acc 1
2017-03-02T23:42:44.079687: step 5604, loss 0.000374425, acc 1
2017-03-02T23:42:44.184133: step 5605, loss 0.00330127, acc 1
2017-03-02T23:42:44.283960: step 5606, loss 0.000821666, acc 1
2017-03-02T23:42:44.374930: step 5607, loss 0.000293607, acc 1
2017-03-02T23:42:44.479127: step 5608, loss 0.000551674, acc 1
2017-03-02T23:42:44.581763: step 5609, loss 0.000823203, acc 1
2017-03-02T23:42:44.683497: step 5610, loss 0.00290126, acc 1
2017-03-02T23:42:44.793997: step 5611, loss 0.00399254, acc 1
2017-03-02T23:42:44.897107: step 5612, loss 0.000957782, acc 1
2017-03-02T23:42:45.000399: step 5613, loss 0.00251672, acc 1
2017-03-02T23:42:45.095876: step 5614, loss 0.00108304, acc 1
2017-03-02T23:42:45.198190: step 5615, loss 0.000549614, acc 1
2017-03-02T23:42:45.305681: step 5616, loss 0.000348836, acc 1
2017-03-02T23:42:45.405414: step 5617, loss 0.000316201, acc 1
2017-03-02T23:42:45.512075: step 5618, loss 0.00107379, acc 1
2017-03-02T23:42:45.619729: step 5619, loss 0.000596159, acc 1
2017-03-02T23:42:45.729396: step 5620, loss 0.000158237, acc 1
2017-03-02T23:42:45.825808: step 5621, loss 0.000166788, acc 1
2017-03-02T23:42:45.924477: step 5622, loss 0.00255999, acc 1
2017-03-02T23:42:46.033241: step 5623, loss 0.022324, acc 0.984375
2017-03-02T23:42:46.137861: step 5624, loss 0.000603913, acc 1
2017-03-02T23:42:46.245277: step 5625, loss 0.0016284, acc 1
2017-03-02T23:42:46.357817: step 5626, loss 0.000492173, acc 1
2017-03-02T23:42:46.461683: step 5627, loss 0.000396563, acc 1
2017-03-02T23:42:46.551571: step 5628, loss 0.000366767, acc 1
2017-03-02T23:42:46.641016: step 5629, loss 0.00195956, acc 1
2017-03-02T23:42:46.741387: step 5630, loss 0.000367261, acc 1
2017-03-02T23:42:46.845880: step 5631, loss 0.00157622, acc 1
2017-03-02T23:42:46.951620: step 5632, loss 0.000441245, acc 1
2017-03-02T23:42:47.050013: step 5633, loss 0.000536259, acc 1
2017-03-02T23:42:47.143082: step 5634, loss 0.000297482, acc 1
2017-03-02T23:42:47.255452: step 5635, loss 0.000802939, acc 1
2017-03-02T23:42:47.348802: step 5636, loss 0.00250914, acc 1
2017-03-02T23:42:47.435058: step 5637, loss 0.00257123, acc 1
2017-03-02T23:42:47.540389: step 5638, loss 0.0147552, acc 0.984375
2017-03-02T23:42:47.664361: step 5639, loss 0.00205486, acc 1
2017-03-02T23:42:47.769206: step 5640, loss 0.00526938, acc 1
2017-03-02T23:42:47.868601: step 5641, loss 0.000455485, acc 1
2017-03-02T23:42:47.972575: step 5642, loss 0.0007605, acc 1
2017-03-02T23:42:48.105719: step 5643, loss 0.000605115, acc 1
2017-03-02T23:42:48.195457: step 5644, loss 0.000954702, acc 1
2017-03-02T23:42:48.313500: step 5645, loss 0.000482308, acc 1
2017-03-02T23:42:48.416419: step 5646, loss 0.00039032, acc 1
2017-03-02T23:42:48.524894: step 5647, loss 0.00392283, acc 1
2017-03-02T23:42:48.629251: step 5648, loss 0.000303761, acc 1
2017-03-02T23:42:48.742920: step 5649, loss 0.00105181, acc 1
2017-03-02T23:42:48.850074: step 5650, loss 0.00154558, acc 1
2017-03-02T23:42:48.934819: step 5651, loss 0.00110094, acc 1
2017-03-02T23:42:49.042060: step 5652, loss 0.00252448, acc 1
2017-03-02T23:42:49.157332: step 5653, loss 0.00301904, acc 1
2017-03-02T23:42:49.258613: step 5654, loss 0.000513223, acc 1
2017-03-02T23:42:49.362913: step 5655, loss 0.000743895, acc 1
2017-03-02T23:42:49.469442: step 5656, loss 0.00907791, acc 1
2017-03-02T23:42:49.566353: step 5657, loss 0.000879984, acc 1
2017-03-02T23:42:49.654659: step 5658, loss 0.00175741, acc 1
2017-03-02T23:42:49.759407: step 5659, loss 0.000805877, acc 1
2017-03-02T23:42:49.867044: step 5660, loss 0.0173231, acc 0.984375
2017-03-02T23:42:49.974166: step 5661, loss 0.000867659, acc 1
2017-03-02T23:42:50.075558: step 5662, loss 0.000256332, acc 1
2017-03-02T23:42:50.181987: step 5663, loss 0.000191933, acc 1
2017-03-02T23:42:50.289348: step 5664, loss 0.000317767, acc 1
2017-03-02T23:42:50.385483: step 5665, loss 0.000288549, acc 1
2017-03-02T23:42:50.475226: step 5666, loss 0.0012978, acc 1
2017-03-02T23:42:50.580215: step 5667, loss 0.000184728, acc 1
2017-03-02T23:42:50.687286: step 5668, loss 0.000469834, acc 1
2017-03-02T23:42:50.790723: step 5669, loss 0.000701455, acc 1
2017-03-02T23:42:50.899102: step 5670, loss 0.00435698, acc 1
2017-03-02T23:42:51.007105: step 5671, loss 0.000253836, acc 1
2017-03-02T23:42:51.110643: step 5672, loss 0.00071649, acc 1
2017-03-02T23:42:51.200421: step 5673, loss 0.000953873, acc 1
2017-03-02T23:42:51.305289: step 5674, loss 0.00136542, acc 1
2017-03-02T23:42:51.416504: step 5675, loss 0.00101185, acc 1
2017-03-02T23:42:51.522365: step 5676, loss 0.000414429, acc 1
2017-03-02T23:42:51.623040: step 5677, loss 0.000707924, acc 1
2017-03-02T23:42:51.717753: step 5678, loss 0.00266186, acc 1
2017-03-02T23:42:51.812679: step 5679, loss 0.000906417, acc 1
2017-03-02T23:42:51.911338: step 5680, loss 0.000374732, acc 1
2017-03-02T23:42:51.996871: step 5681, loss 0.00140359, acc 1
2017-03-02T23:42:52.100762: step 5682, loss 0.00147583, acc 1
2017-03-02T23:42:52.206589: step 5683, loss 0.000289649, acc 1
2017-03-02T23:42:52.315223: step 5684, loss 7.32097e-05, acc 1
2017-03-02T23:42:52.422060: step 5685, loss 0.00241949, acc 1
2017-03-02T23:42:52.514600: step 5686, loss 0.00134037, acc 1
2017-03-02T23:42:52.619252: step 5687, loss 0.000426739, acc 1
2017-03-02T23:42:52.714499: step 5688, loss 0.00174336, acc 1
2017-03-02T23:42:52.820356: step 5689, loss 0.000145315, acc 1
2017-03-02T23:42:52.928348: step 5690, loss 0.0019422, acc 1
2017-03-02T23:42:53.037663: step 5691, loss 0.00119522, acc 1
2017-03-02T23:42:53.137794: step 5692, loss 0.000260037, acc 1
2017-03-02T23:42:53.242008: step 5693, loss 0.0017676, acc 1
2017-03-02T23:42:53.347235: step 5694, loss 0.0112858, acc 1
2017-03-02T23:42:53.451531: step 5695, loss 0.00155126, acc 1
2017-03-02T23:42:53.540039: step 5696, loss 0.00140018, acc 1
2017-03-02T23:42:53.646164: step 5697, loss 0.00105238, acc 1
2017-03-02T23:42:53.758902: step 5698, loss 0.00158454, acc 1
2017-03-02T23:42:53.853022: step 5699, loss 0.00328687, acc 1
2017-03-02T23:42:53.972451: step 5700, loss 0.00211866, acc 1

Evaluation:
2017-03-02T23:42:54.025357: step 5700, loss 2.90441, acc 0.536332

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5700

2017-03-02T23:42:54.484709: step 5701, loss 0.0015812, acc 1
2017-03-02T23:42:54.578869: step 5702, loss 0.00563681, acc 1
2017-03-02T23:42:54.674813: step 5703, loss 0.00158565, acc 1
2017-03-02T23:42:54.778362: step 5704, loss 0.000588975, acc 1
2017-03-02T23:42:54.877218: step 5705, loss 0.000216675, acc 1
2017-03-02T23:42:54.969152: step 5706, loss 0.000137521, acc 1
2017-03-02T23:42:55.068098: step 5707, loss 0.00226114, acc 1
2017-03-02T23:42:55.163367: step 5708, loss 0.00145171, acc 1
2017-03-02T23:42:55.275388: step 5709, loss 0.000269613, acc 1
2017-03-02T23:42:55.378956: step 5710, loss 0.000974386, acc 1
2017-03-02T23:42:55.489111: step 5711, loss 0.00322812, acc 1
2017-03-02T23:42:55.591565: step 5712, loss 0.000707491, acc 1
2017-03-02T23:42:55.686963: step 5713, loss 0.000308678, acc 1
2017-03-02T23:42:55.768947: step 5714, loss 0.00113541, acc 1
2017-03-02T23:42:55.875059: step 5715, loss 0.00196489, acc 1
2017-03-02T23:42:55.981296: step 5716, loss 0.000308312, acc 1
2017-03-02T23:42:56.086439: step 5717, loss 0.0121928, acc 0.984375
2017-03-02T23:42:56.183220: step 5718, loss 0.000803239, acc 1
2017-03-02T23:42:56.286154: step 5719, loss 0.000241395, acc 1
2017-03-02T23:42:56.392087: step 5720, loss 0.00111099, acc 1
2017-03-02T23:42:56.485191: step 5721, loss 0.00118137, acc 1
2017-03-02T23:42:56.584308: step 5722, loss 0.000485165, acc 1
2017-03-02T23:42:56.687371: step 5723, loss 0.00205177, acc 1
2017-03-02T23:42:56.785187: step 5724, loss 0.000341157, acc 1
2017-03-02T23:42:56.892140: step 5725, loss 0.000653783, acc 1
2017-03-02T23:42:56.997494: step 5726, loss 0.00304014, acc 1
2017-03-02T23:42:57.105211: step 5727, loss 0.00149274, acc 1
2017-03-02T23:42:57.199399: step 5728, loss 0.002661, acc 1
2017-03-02T23:42:57.301919: step 5729, loss 0.0230283, acc 0.984375
2017-03-02T23:42:57.408989: step 5730, loss 0.0005694, acc 1
2017-03-02T23:42:57.514180: step 5731, loss 0.000282176, acc 1
2017-03-02T23:42:57.623056: step 5732, loss 0.00196702, acc 1
2017-03-02T23:42:57.719560: step 5733, loss 0.000991327, acc 1
2017-03-02T23:42:57.824044: step 5734, loss 0.000528799, acc 1
2017-03-02T23:42:57.931610: step 5735, loss 0.000259935, acc 1
2017-03-02T23:42:58.026521: step 5736, loss 0.000193237, acc 1
2017-03-02T23:42:58.129607: step 5737, loss 0.000331857, acc 1
2017-03-02T23:42:58.231298: step 5738, loss 0.000962256, acc 1
2017-03-02T23:42:58.346535: step 5739, loss 0.00130261, acc 1
2017-03-02T23:42:58.454768: step 5740, loss 0.00105897, acc 1
2017-03-02T23:42:58.559851: step 5741, loss 0.00287966, acc 1
2017-03-02T23:42:58.678373: step 5742, loss 0.00235417, acc 1
2017-03-02T23:42:58.772708: step 5743, loss 0.000417347, acc 1
2017-03-02T23:42:58.878001: step 5744, loss 0.000556247, acc 1
2017-03-02T23:42:58.983327: step 5745, loss 0.000344472, acc 1
2017-03-02T23:42:59.088298: step 5746, loss 0.000698725, acc 1
2017-03-02T23:42:59.194065: step 5747, loss 0.000345292, acc 1
2017-03-02T23:42:59.300625: step 5748, loss 0.000594459, acc 1
2017-03-02T23:42:59.410712: step 5749, loss 0.000256388, acc 1
2017-03-02T23:42:59.501980: step 5750, loss 0.00661047, acc 1
2017-03-02T23:42:59.597274: step 5751, loss 0.000186556, acc 1
2017-03-02T23:42:59.703677: step 5752, loss 0.00134704, acc 1
2017-03-02T23:42:59.807287: step 5753, loss 0.0023933, acc 1
2017-03-02T23:42:59.910361: step 5754, loss 0.00174668, acc 1
2017-03-02T23:43:00.012978: step 5755, loss 0.000385029, acc 1
2017-03-02T23:43:00.115400: step 5756, loss 0.000518074, acc 1
2017-03-02T23:43:00.222816: step 5757, loss 0.00203239, acc 1
2017-03-02T23:43:00.310266: step 5758, loss 0.000914973, acc 1
2017-03-02T23:43:00.410221: step 5759, loss 0.000612923, acc 1
2017-03-02T23:43:00.510300: step 5760, loss 0.00249436, acc 1
2017-03-02T23:43:00.603513: step 5761, loss 0.000362062, acc 1
2017-03-02T23:43:00.707565: step 5762, loss 0.000867829, acc 1
2017-03-02T23:43:00.813377: step 5763, loss 0.000631224, acc 1
2017-03-02T23:43:00.921315: step 5764, loss 0.000840593, acc 1
2017-03-02T23:43:01.017620: step 5765, loss 0.00135478, acc 1
2017-03-02T23:43:01.128903: step 5766, loss 0.000977619, acc 1
2017-03-02T23:43:01.236846: step 5767, loss 0.000392106, acc 1
2017-03-02T23:43:01.341874: step 5768, loss 0.00615059, acc 1
2017-03-02T23:43:01.451259: step 5769, loss 0.000444974, acc 1
2017-03-02T23:43:01.556418: step 5770, loss 0.00271961, acc 1
2017-03-02T23:43:01.667676: step 5771, loss 0.000662585, acc 1
2017-03-02T23:43:01.760605: step 5772, loss 0.00020635, acc 1
2017-03-02T23:43:01.859228: step 5773, loss 0.00268673, acc 1
2017-03-02T23:43:01.965598: step 5774, loss 0.000242566, acc 1
2017-03-02T23:43:02.071764: step 5775, loss 0.00138806, acc 1
2017-03-02T23:43:02.176875: step 5776, loss 0.000390411, acc 1
2017-03-02T23:43:02.280193: step 5777, loss 0.00375521, acc 1
2017-03-02T23:43:02.385844: step 5778, loss 0.00040317, acc 1
2017-03-02T23:43:02.472201: step 5779, loss 0.00033486, acc 1
2017-03-02T23:43:02.563010: step 5780, loss 0.00132845, acc 1
2017-03-02T23:43:02.662917: step 5781, loss 0.000343603, acc 1
2017-03-02T23:43:02.766508: step 5782, loss 0.000566119, acc 1
2017-03-02T23:43:02.873455: step 5783, loss 0.000694855, acc 1
2017-03-02T23:43:02.973585: step 5784, loss 0.000504992, acc 1
2017-03-02T23:43:03.074239: step 5785, loss 0.00035857, acc 1
2017-03-02T23:43:03.184819: step 5786, loss 0.000843009, acc 1
2017-03-02T23:43:03.274687: step 5787, loss 0.00238036, acc 1
2017-03-02T23:43:03.375242: step 5788, loss 0.0009366, acc 1
2017-03-02T23:43:03.470239: step 5789, loss 0.000603646, acc 1
2017-03-02T23:43:03.571145: step 5790, loss 0.000768941, acc 1
2017-03-02T23:43:03.678819: step 5791, loss 0.00120261, acc 1
2017-03-02T23:43:03.779945: step 5792, loss 0.000578535, acc 1
2017-03-02T23:43:03.888600: step 5793, loss 0.00340158, acc 1
2017-03-02T23:43:03.992916: step 5794, loss 0.0010897, acc 1
2017-03-02T23:43:04.086932: step 5795, loss 0.000360388, acc 1
2017-03-02T23:43:04.188158: step 5796, loss 0.000747147, acc 1
2017-03-02T23:43:04.292576: step 5797, loss 0.000760357, acc 1
2017-03-02T23:43:04.400331: step 5798, loss 0.000463372, acc 1
2017-03-02T23:43:04.509880: step 5799, loss 0.000347672, acc 1
2017-03-02T23:43:04.610089: step 5800, loss 0.013236, acc 1

Evaluation:
2017-03-02T23:43:04.668797: step 5800, loss 2.74209, acc 0.515571

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5800

2017-03-02T23:43:05.138160: step 5801, loss 0.000463355, acc 1
2017-03-02T23:43:05.253437: step 5802, loss 0.000184815, acc 1
2017-03-02T23:43:05.362538: step 5803, loss 0.000242987, acc 1
2017-03-02T23:43:05.467403: step 5804, loss 0.000324628, acc 1
2017-03-02T23:43:05.568825: step 5805, loss 0.0010023, acc 1
2017-03-02T23:43:05.681196: step 5806, loss 0.000805695, acc 1
2017-03-02T23:43:05.785135: step 5807, loss 0.000315356, acc 1
2017-03-02T23:43:05.893574: step 5808, loss 0.000433363, acc 1
2017-03-02T23:43:05.997567: step 5809, loss 0.00056921, acc 1
2017-03-02T23:43:06.107492: step 5810, loss 0.00130699, acc 1
2017-03-02T23:43:06.207914: step 5811, loss 0.000527697, acc 1
2017-03-02T23:43:06.300840: step 5812, loss 0.000646773, acc 1
2017-03-02T23:43:06.409426: step 5813, loss 0.000653271, acc 1
2017-03-02T23:43:06.517086: step 5814, loss 0.0157757, acc 0.984375
2017-03-02T23:43:06.622742: step 5815, loss 0.000795879, acc 1
2017-03-02T23:43:06.733544: step 5816, loss 0.000492143, acc 1
2017-03-02T23:43:06.850429: step 5817, loss 0.000522115, acc 1
2017-03-02T23:43:06.958767: step 5818, loss 0.00184048, acc 1
2017-03-02T23:43:07.052620: step 5819, loss 0.00279614, acc 1
2017-03-02T23:43:07.155079: step 5820, loss 0.00522726, acc 1
2017-03-02T23:43:07.263140: step 5821, loss 0.000256247, acc 1
2017-03-02T23:43:07.362425: step 5822, loss 0.00144556, acc 1
2017-03-02T23:43:07.480857: step 5823, loss 0.000577606, acc 1
2017-03-02T23:43:07.579722: step 5824, loss 0.000764247, acc 1
2017-03-02T23:43:07.686675: step 5825, loss 0.000888394, acc 1
2017-03-02T23:43:07.775219: step 5826, loss 0.00360538, acc 1
2017-03-02T23:43:07.879591: step 5827, loss 0.00131809, acc 1
2017-03-02T23:43:07.991802: step 5828, loss 0.00160824, acc 1
2017-03-02T23:43:08.098642: step 5829, loss 0.00104472, acc 1
2017-03-02T23:43:08.197793: step 5830, loss 0.00294286, acc 1
2017-03-02T23:43:08.309326: step 5831, loss 0.000477008, acc 1
2017-03-02T23:43:08.426237: step 5832, loss 0.00259223, acc 1
2017-03-02T23:43:08.525609: step 5833, loss 0.000267358, acc 1
2017-03-02T23:43:08.618316: step 5834, loss 5.87188e-05, acc 1
2017-03-02T23:43:08.720222: step 5835, loss 0.000287102, acc 1
2017-03-02T23:43:08.828360: step 5836, loss 0.000955662, acc 1
2017-03-02T23:43:08.941660: step 5837, loss 0.00191503, acc 1
2017-03-02T23:43:09.046712: step 5838, loss 0.00107636, acc 1
2017-03-02T23:43:09.150808: step 5839, loss 0.000546348, acc 1
2017-03-02T23:43:09.243561: step 5840, loss 0.0061578, acc 1
2017-03-02T23:43:09.333201: step 5841, loss 0.00196109, acc 1
2017-03-02T23:43:09.439448: step 5842, loss 0.000147862, acc 1
2017-03-02T23:43:09.540179: step 5843, loss 0.00166096, acc 1
2017-03-02T23:43:09.644988: step 5844, loss 0.020062, acc 0.984375
2017-03-02T23:43:09.753003: step 5845, loss 0.000702518, acc 1
2017-03-02T23:43:09.861411: step 5846, loss 0.000536371, acc 1
2017-03-02T23:43:09.967964: step 5847, loss 0.000633977, acc 1
2017-03-02T23:43:10.054791: step 5848, loss 0.000480702, acc 1
2017-03-02T23:43:10.151322: step 5849, loss 0.000221345, acc 1
2017-03-02T23:43:10.257170: step 5850, loss 0.000559221, acc 1
2017-03-02T23:43:10.368639: step 5851, loss 0.000423914, acc 1
2017-03-02T23:43:10.476846: step 5852, loss 0.000196771, acc 1
2017-03-02T23:43:10.588100: step 5853, loss 0.000574763, acc 1
2017-03-02T23:43:10.688666: step 5854, loss 0.000597828, acc 1
2017-03-02T23:43:10.782591: step 5855, loss 0.00810063, acc 1
2017-03-02T23:43:10.897835: step 5856, loss 0.00114088, acc 1
2017-03-02T23:43:11.023235: step 5857, loss 0.00121705, acc 1
2017-03-02T23:43:11.126810: step 5858, loss 0.00522188, acc 1
2017-03-02T23:43:11.248075: step 5859, loss 0.000408006, acc 1
2017-03-02T23:43:11.356483: step 5860, loss 0.000414743, acc 1
2017-03-02T23:43:11.476993: step 5861, loss 0.00145162, acc 1
2017-03-02T23:43:11.573726: step 5862, loss 0.0401112, acc 0.984375
2017-03-02T23:43:11.673271: step 5863, loss 0.000634397, acc 1
2017-03-02T23:43:11.772747: step 5864, loss 0.000718071, acc 1
2017-03-02T23:43:11.877979: step 5865, loss 0.00226109, acc 1
2017-03-02T23:43:11.983129: step 5866, loss 0.00033685, acc 1
2017-03-02T23:43:12.099986: step 5867, loss 0.000686252, acc 1
2017-03-02T23:43:12.211456: step 5868, loss 0.00626824, acc 1
2017-03-02T23:43:12.297924: step 5869, loss 0.000953948, acc 1
2017-03-02T23:43:12.412147: step 5870, loss 0.000280109, acc 1
2017-03-02T23:43:12.514602: step 5871, loss 0.000953802, acc 1
2017-03-02T23:43:12.620827: step 5872, loss 0.000434141, acc 1
2017-03-02T23:43:12.727108: step 5873, loss 0.000562273, acc 1
2017-03-02T23:43:12.836404: step 5874, loss 0.000906474, acc 1
2017-03-02T23:43:12.929590: step 5875, loss 0.00368501, acc 1
2017-03-02T23:43:13.020438: step 5876, loss 0.00106617, acc 1
2017-03-02T23:43:13.114129: step 5877, loss 0.000426922, acc 1
2017-03-02T23:43:13.216080: step 5878, loss 9.17274e-05, acc 1
2017-03-02T23:43:13.325486: step 5879, loss 0.000362773, acc 1
2017-03-02T23:43:13.427039: step 5880, loss 0.00105186, acc 1
2017-03-02T23:43:13.534325: step 5881, loss 0.00214546, acc 1
2017-03-02T23:43:13.638709: step 5882, loss 0.000174101, acc 1
2017-03-02T23:43:13.747977: step 5883, loss 0.000471538, acc 1
2017-03-02T23:43:13.842063: step 5884, loss 0.00146066, acc 1
2017-03-02T23:43:13.957987: step 5885, loss 0.00126553, acc 1
2017-03-02T23:43:14.077979: step 5886, loss 0.000355989, acc 1
2017-03-02T23:43:14.187125: step 5887, loss 0.00148053, acc 1
2017-03-02T23:43:14.289949: step 5888, loss 0.00190883, acc 1
2017-03-02T23:43:14.393792: step 5889, loss 0.00869845, acc 1
2017-03-02T23:43:14.502538: step 5890, loss 0.000497707, acc 1
2017-03-02T23:43:14.587007: step 5891, loss 0.00198648, acc 1
2017-03-02T23:43:14.694135: step 5892, loss 0.000235132, acc 1
2017-03-02T23:43:14.798310: step 5893, loss 0.000167991, acc 1
2017-03-02T23:43:14.903123: step 5894, loss 0.00130349, acc 1
2017-03-02T23:43:15.013982: step 5895, loss 0.000941543, acc 1
2017-03-02T23:43:15.116144: step 5896, loss 0.00026828, acc 1
2017-03-02T23:43:15.225180: step 5897, loss 0.00113711, acc 1
2017-03-02T23:43:15.313314: step 5898, loss 0.00108719, acc 1
2017-03-02T23:43:15.422676: step 5899, loss 0.0002242, acc 1
2017-03-02T23:43:15.530130: step 5900, loss 0.000303502, acc 1

Evaluation:
2017-03-02T23:43:15.594655: step 5900, loss 2.89137, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-5900

2017-03-02T23:43:16.040608: step 5901, loss 0.000101859, acc 1
2017-03-02T23:43:16.138657: step 5902, loss 0.00144983, acc 1
2017-03-02T23:43:16.239299: step 5903, loss 0.000480016, acc 1
2017-03-02T23:43:16.347303: step 5904, loss 0.00037206, acc 1
2017-03-02T23:43:16.448105: step 5905, loss 0.000279486, acc 1
2017-03-02T23:43:16.557519: step 5906, loss 0.000648036, acc 1
2017-03-02T23:43:16.660128: step 5907, loss 0.00672613, acc 1
2017-03-02T23:43:16.760127: step 5908, loss 0.000309117, acc 1
2017-03-02T23:43:16.885625: step 5909, loss 0.000211711, acc 1
2017-03-02T23:43:16.998359: step 5910, loss 0.000637231, acc 1
2017-03-02T23:43:17.108948: step 5911, loss 0.000753534, acc 1
2017-03-02T23:43:17.209403: step 5912, loss 0.00159117, acc 1
2017-03-02T23:43:17.339687: step 5913, loss 0.000383485, acc 1
2017-03-02T23:43:17.439574: step 5914, loss 0.000154513, acc 1
2017-03-02T23:43:17.532728: step 5915, loss 0.000940993, acc 1
2017-03-02T23:43:17.645029: step 5916, loss 0.0010519, acc 1
2017-03-02T23:43:17.751998: step 5917, loss 0.000180337, acc 1
2017-03-02T23:43:17.861779: step 5918, loss 0.000785528, acc 1
2017-03-02T23:43:17.974188: step 5919, loss 0.000533985, acc 1
2017-03-02T23:43:18.081417: step 5920, loss 0.000399198, acc 1
2017-03-02T23:43:18.189651: step 5921, loss 0.00419549, acc 1
2017-03-02T23:43:18.278796: step 5922, loss 0.000470129, acc 1
2017-03-02T23:43:18.388673: step 5923, loss 0.000299918, acc 1
2017-03-02T23:43:18.495653: step 5924, loss 0.000351, acc 1
2017-03-02T23:43:18.603074: step 5925, loss 0.000556792, acc 1
2017-03-02T23:43:18.707828: step 5926, loss 0.000569295, acc 1
2017-03-02T23:43:18.817619: step 5927, loss 0.00137306, acc 1
2017-03-02T23:43:18.929792: step 5928, loss 0.000201431, acc 1
2017-03-02T23:43:19.022963: step 5929, loss 0.000121497, acc 1
2017-03-02T23:43:19.124025: step 5930, loss 0.00200896, acc 1
2017-03-02T23:43:19.232665: step 5931, loss 9.43294e-05, acc 1
2017-03-02T23:43:19.339431: step 5932, loss 0.00118231, acc 1
2017-03-02T23:43:19.445556: step 5933, loss 0.00271866, acc 1
2017-03-02T23:43:19.554186: step 5934, loss 0.000516405, acc 1
2017-03-02T23:43:19.658460: step 5935, loss 0.000252026, acc 1
2017-03-02T23:43:19.755603: step 5936, loss 0.0054854, acc 1
2017-03-02T23:43:19.847262: step 5937, loss 0.000360735, acc 1
2017-03-02T23:43:19.954506: step 5938, loss 0.000993676, acc 1
2017-03-02T23:43:20.060385: step 5939, loss 0.000556873, acc 1
2017-03-02T23:43:20.163711: step 5940, loss 0.000207306, acc 1
2017-03-02T23:43:20.270412: step 5941, loss 0.000416455, acc 1
2017-03-02T23:43:20.375187: step 5942, loss 0.00161412, acc 1
2017-03-02T23:43:20.483635: step 5943, loss 0.000361932, acc 1
2017-03-02T23:43:20.579408: step 5944, loss 0.000906353, acc 1
2017-03-02T23:43:20.677015: step 5945, loss 0.000870906, acc 1
2017-03-02T23:43:20.780923: step 5946, loss 0.011015, acc 1
2017-03-02T23:43:20.884087: step 5947, loss 0.0103004, acc 1
2017-03-02T23:43:20.984529: step 5948, loss 0.000264814, acc 1
2017-03-02T23:43:21.094268: step 5949, loss 0.00217245, acc 1
2017-03-02T23:43:21.207930: step 5950, loss 0.00764203, acc 1
2017-03-02T23:43:21.297486: step 5951, loss 0.00058204, acc 1
2017-03-02T23:43:21.399677: step 5952, loss 0.000748255, acc 1
2017-03-02T23:43:21.508116: step 5953, loss 0.000369879, acc 1
2017-03-02T23:43:21.610495: step 5954, loss 0.000306145, acc 1
2017-03-02T23:43:21.701412: step 5955, loss 0.000746763, acc 1
2017-03-02T23:43:21.806748: step 5956, loss 0.00178106, acc 1
2017-03-02T23:43:21.911646: step 5957, loss 0.000610708, acc 1
2017-03-02T23:43:22.000582: step 5958, loss 0.000587822, acc 1
2017-03-02T23:43:22.088152: step 5959, loss 0.000446076, acc 1
2017-03-02T23:43:22.194648: step 5960, loss 0.0124821, acc 1
2017-03-02T23:43:22.307308: step 5961, loss 0.000232789, acc 1
2017-03-02T23:43:22.411420: step 5962, loss 0.00687319, acc 1
2017-03-02T23:43:22.522337: step 5963, loss 0.000825514, acc 1
2017-03-02T23:43:22.622341: step 5964, loss 0.000250733, acc 1
2017-03-02T23:43:22.728905: step 5965, loss 0.000889584, acc 1
2017-03-02T23:43:22.819964: step 5966, loss 0.00132719, acc 1
2017-03-02T23:43:22.921234: step 5967, loss 0.000336136, acc 1
2017-03-02T23:43:23.026111: step 5968, loss 0.000622239, acc 1
2017-03-02T23:43:23.135646: step 5969, loss 0.00832331, acc 1
2017-03-02T23:43:23.246105: step 5970, loss 0.00252721, acc 1
2017-03-02T23:43:23.347340: step 5971, loss 0.000642594, acc 1
2017-03-02T23:43:23.449800: step 5972, loss 0.0037456, acc 1
2017-03-02T23:43:23.545569: step 5973, loss 0.000819911, acc 1
2017-03-02T23:43:23.639825: step 5974, loss 0.00112105, acc 1
2017-03-02T23:43:23.744875: step 5975, loss 0.000134542, acc 1
2017-03-02T23:43:23.851494: step 5976, loss 0.00103475, acc 1
2017-03-02T23:43:23.959750: step 5977, loss 0.00188132, acc 1
2017-03-02T23:43:24.073399: step 5978, loss 0.00189741, acc 1
2017-03-02T23:43:24.184054: step 5979, loss 0.000171129, acc 1
2017-03-02T23:43:24.284356: step 5980, loss 0.000618152, acc 1
2017-03-02T23:43:24.372579: step 5981, loss 0.000580761, acc 1
2017-03-02T23:43:24.476443: step 5982, loss 0.00140536, acc 1
2017-03-02T23:43:24.582575: step 5983, loss 0.00103228, acc 1
2017-03-02T23:43:24.684899: step 5984, loss 0.000398617, acc 1
2017-03-02T23:43:24.780750: step 5985, loss 0.000690119, acc 1
2017-03-02T23:43:24.880026: step 5986, loss 0.00012715, acc 1
2017-03-02T23:43:24.981761: step 5987, loss 0.000166834, acc 1
2017-03-02T23:43:25.079976: step 5988, loss 0.000265004, acc 1
2017-03-02T23:43:25.181716: step 5989, loss 0.0003601, acc 1
2017-03-02T23:43:25.298216: step 5990, loss 0.00137664, acc 1
2017-03-02T23:43:25.409469: step 5991, loss 0.000144054, acc 1
2017-03-02T23:43:25.515846: step 5992, loss 0.000217606, acc 1
2017-03-02T23:43:25.624248: step 5993, loss 0.000319543, acc 1
2017-03-02T23:43:25.751754: step 5994, loss 0.00269802, acc 1
2017-03-02T23:43:25.843044: step 5995, loss 0.000287623, acc 1
2017-03-02T23:43:25.953067: step 5996, loss 0.000435778, acc 1
2017-03-02T23:43:26.059404: step 5997, loss 0.000937326, acc 1
2017-03-02T23:43:26.184599: step 5998, loss 0.00135776, acc 1
2017-03-02T23:43:26.281438: step 5999, loss 0.000302257, acc 1
2017-03-02T23:43:26.384399: step 6000, loss 0.00289585, acc 1

Evaluation:
2017-03-02T23:43:26.435739: step 6000, loss 2.97576, acc 0.532872

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6000

2017-03-02T23:43:26.887747: step 6001, loss 0.000214296, acc 1
2017-03-02T23:43:27.004241: step 6002, loss 0.00103613, acc 1
2017-03-02T23:43:27.109035: step 6003, loss 0.000847602, acc 1
2017-03-02T23:43:27.215374: step 6004, loss 0.000545266, acc 1
2017-03-02T23:43:27.312846: step 6005, loss 0.000420949, acc 1
2017-03-02T23:43:27.417393: step 6006, loss 0.00311776, acc 1
2017-03-02T23:43:27.525182: step 6007, loss 0.000817123, acc 1
2017-03-02T23:43:27.632746: step 6008, loss 0.00238121, acc 1
2017-03-02T23:43:27.736111: step 6009, loss 0.00224395, acc 1
2017-03-02T23:43:27.828095: step 6010, loss 0.00447181, acc 1
2017-03-02T23:43:27.933641: step 6011, loss 0.000654881, acc 1
2017-03-02T23:43:28.030176: step 6012, loss 0.000900723, acc 1
2017-03-02T23:43:28.145053: step 6013, loss 0.000432231, acc 1
2017-03-02T23:43:28.256118: step 6014, loss 6.77715e-05, acc 1
2017-03-02T23:43:28.364867: step 6015, loss 0.000197588, acc 1
2017-03-02T23:43:28.472378: step 6016, loss 7.32418e-05, acc 1
2017-03-02T23:43:28.580688: step 6017, loss 0.000866252, acc 1
2017-03-02T23:43:28.694897: step 6018, loss 0.000352507, acc 1
2017-03-02T23:43:28.787877: step 6019, loss 0.0013011, acc 1
2017-03-02T23:43:28.880536: step 6020, loss 0.000339368, acc 1
2017-03-02T23:43:28.986758: step 6021, loss 0.000439148, acc 1
2017-03-02T23:43:29.087270: step 6022, loss 0.00165514, acc 1
2017-03-02T23:43:29.193059: step 6023, loss 0.00105041, acc 1
2017-03-02T23:43:29.325587: step 6024, loss 0.000543604, acc 1
2017-03-02T23:43:29.437332: step 6025, loss 0.00047429, acc 1
2017-03-02T23:43:29.527406: step 6026, loss 0.00580485, acc 1
2017-03-02T23:43:29.609435: step 6027, loss 0.000272303, acc 1
2017-03-02T23:43:29.732930: step 6028, loss 0.000591562, acc 1
2017-03-02T23:43:29.843904: step 6029, loss 0.00022919, acc 1
2017-03-02T23:43:29.948008: step 6030, loss 0.000276896, acc 1
2017-03-02T23:43:30.066531: step 6031, loss 0.000569369, acc 1
2017-03-02T23:43:30.169956: step 6032, loss 0.000193354, acc 1
2017-03-02T23:43:30.270348: step 6033, loss 0.00281519, acc 1
2017-03-02T23:43:30.358341: step 6034, loss 0.00136054, acc 1
2017-03-02T23:43:30.472893: step 6035, loss 0.000317807, acc 1
2017-03-02T23:43:30.583086: step 6036, loss 0.00073004, acc 1
2017-03-02T23:43:30.690310: step 6037, loss 0.00159539, acc 1
2017-03-02T23:43:30.793271: step 6038, loss 0.000423013, acc 1
2017-03-02T23:43:30.900894: step 6039, loss 0.0028648, acc 1
2017-03-02T23:43:31.001674: step 6040, loss 0.000312532, acc 1
2017-03-02T23:43:31.091693: step 6041, loss 0.00114899, acc 1
2017-03-02T23:43:31.193857: step 6042, loss 0.000453133, acc 1
2017-03-02T23:43:31.303440: step 6043, loss 0.000743319, acc 1
2017-03-02T23:43:31.413286: step 6044, loss 0.000295576, acc 1
2017-03-02T23:43:31.519877: step 6045, loss 0.000380899, acc 1
2017-03-02T23:43:31.624215: step 6046, loss 0.00211774, acc 1
2017-03-02T23:43:31.722560: step 6047, loss 0.000291214, acc 1
2017-03-02T23:43:31.817655: step 6048, loss 9.55381e-05, acc 1
2017-03-02T23:43:31.916986: step 6049, loss 0.00135738, acc 1
2017-03-02T23:43:32.018269: step 6050, loss 0.000990465, acc 1
2017-03-02T23:43:32.122891: step 6051, loss 0.000347684, acc 1
2017-03-02T23:43:32.217600: step 6052, loss 0.000117953, acc 1
2017-03-02T23:43:32.331332: step 6053, loss 0.00248576, acc 1
2017-03-02T23:43:32.437190: step 6054, loss 0.010022, acc 1
2017-03-02T23:43:32.546954: step 6055, loss 0.000664034, acc 1
2017-03-02T23:43:32.638035: step 6056, loss 0.000770898, acc 1
2017-03-02T23:43:32.745890: step 6057, loss 0.000230796, acc 1
2017-03-02T23:43:32.853102: step 6058, loss 0.000285466, acc 1
2017-03-02T23:43:32.963038: step 6059, loss 0.00433647, acc 1
2017-03-02T23:43:33.072040: step 6060, loss 0.000660319, acc 1
2017-03-02T23:43:33.186933: step 6061, loss 0.00224742, acc 1
2017-03-02T23:43:33.282379: step 6062, loss 0.000541264, acc 1
2017-03-02T23:43:33.370204: step 6063, loss 0.0290284, acc 0.984375
2017-03-02T23:43:33.476702: step 6064, loss 0.000430872, acc 1
2017-03-02T23:43:33.578553: step 6065, loss 0.00101918, acc 1
2017-03-02T23:43:33.684567: step 6066, loss 0.0013276, acc 1
2017-03-02T23:43:33.786113: step 6067, loss 0.000352483, acc 1
2017-03-02T23:43:33.881407: step 6068, loss 0.000403536, acc 1
2017-03-02T23:43:33.997112: step 6069, loss 9.84204e-05, acc 1
2017-03-02T23:43:34.090662: step 6070, loss 0.000252961, acc 1
2017-03-02T23:43:34.183415: step 6071, loss 0.000415963, acc 1
2017-03-02T23:43:34.296961: step 6072, loss 0.00157241, acc 1
2017-03-02T23:43:34.400963: step 6073, loss 0.000142393, acc 1
2017-03-02T23:43:34.510509: step 6074, loss 0.000223503, acc 1
2017-03-02T23:43:34.613441: step 6075, loss 0.00558048, acc 1
2017-03-02T23:43:34.718090: step 6076, loss 0.000388339, acc 1
2017-03-02T23:43:34.819821: step 6077, loss 0.00968931, acc 1
2017-03-02T23:43:34.940232: step 6078, loss 0.00283323, acc 1
2017-03-02T23:43:35.043694: step 6079, loss 0.000270239, acc 1
2017-03-02T23:43:35.148754: step 6080, loss 0.000958309, acc 1
2017-03-02T23:43:35.253287: step 6081, loss 0.00313381, acc 1
2017-03-02T23:43:35.362869: step 6082, loss 0.00129215, acc 1
2017-03-02T23:43:35.477184: step 6083, loss 0.000314145, acc 1
2017-03-02T23:43:35.569745: step 6084, loss 0.000186901, acc 1
2017-03-02T23:43:35.665831: step 6085, loss 0.000709683, acc 1
2017-03-02T23:43:35.764657: step 6086, loss 0.00706835, acc 1
2017-03-02T23:43:35.880725: step 6087, loss 0.000927073, acc 1
2017-03-02T23:43:35.988142: step 6088, loss 0.00200231, acc 1
2017-03-02T23:43:36.094324: step 6089, loss 0.00221565, acc 1
2017-03-02T23:43:36.201695: step 6090, loss 0.000524342, acc 1
2017-03-02T23:43:36.288839: step 6091, loss 0.000859049, acc 1
2017-03-02T23:43:36.382398: step 6092, loss 0.000493485, acc 1
2017-03-02T23:43:36.481849: step 6093, loss 0.00056558, acc 1
2017-03-02T23:43:36.589930: step 6094, loss 0.000123249, acc 1
2017-03-02T23:43:36.701409: step 6095, loss 0.000193968, acc 1
2017-03-02T23:43:36.813143: step 6096, loss 0.000371473, acc 1
2017-03-02T23:43:36.913432: step 6097, loss 0.000754604, acc 1
2017-03-02T23:43:37.030279: step 6098, loss 0.000460435, acc 1
2017-03-02T23:43:37.124756: step 6099, loss 0.000203765, acc 1
2017-03-02T23:43:37.223100: step 6100, loss 0.00132729, acc 1

Evaluation:
2017-03-02T23:43:37.277179: step 6100, loss 2.86844, acc 0.519031

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6100

2017-03-02T23:43:37.778315: step 6101, loss 0.000257734, acc 1
2017-03-02T23:43:37.885579: step 6102, loss 0.0010945, acc 1
2017-03-02T23:43:37.995970: step 6103, loss 0.000409689, acc 1
2017-03-02T23:43:38.115025: step 6104, loss 0.000971042, acc 1
2017-03-02T23:43:38.224990: step 6105, loss 0.00175955, acc 1
2017-03-02T23:43:38.334446: step 6106, loss 0.00120833, acc 1
2017-03-02T23:43:38.435082: step 6107, loss 0.000260508, acc 1
2017-03-02T23:43:38.521098: step 6108, loss 0.000461014, acc 1
2017-03-02T23:43:38.607714: step 6109, loss 0.000266565, acc 1
2017-03-02T23:43:38.711201: step 6110, loss 0.000145625, acc 1
2017-03-02T23:43:38.820533: step 6111, loss 0.00977002, acc 1
2017-03-02T23:43:38.932463: step 6112, loss 0.000377776, acc 1
2017-03-02T23:43:39.038832: step 6113, loss 0.000560415, acc 1
2017-03-02T23:43:39.150944: step 6114, loss 0.00079819, acc 1
2017-03-02T23:43:39.266022: step 6115, loss 0.00018845, acc 1
2017-03-02T23:43:39.364431: step 6116, loss 0.000101811, acc 1
2017-03-02T23:43:39.469883: step 6117, loss 0.00093073, acc 1
2017-03-02T23:43:39.581917: step 6118, loss 0.00514517, acc 1
2017-03-02T23:43:39.688752: step 6119, loss 0.000250123, acc 1
2017-03-02T23:43:39.795159: step 6120, loss 0.00153181, acc 1
2017-03-02T23:43:39.901054: step 6121, loss 0.00154006, acc 1
2017-03-02T23:43:40.009603: step 6122, loss 0.00292515, acc 1
2017-03-02T23:43:40.098726: step 6123, loss 0.00208826, acc 1
2017-03-02T23:43:40.202158: step 6124, loss 0.000567869, acc 1
2017-03-02T23:43:40.303736: step 6125, loss 0.00138333, acc 1
2017-03-02T23:43:40.414229: step 6126, loss 0.00351346, acc 1
2017-03-02T23:43:40.524800: step 6127, loss 0.000906676, acc 1
2017-03-02T23:43:40.628681: step 6128, loss 0.000341453, acc 1
2017-03-02T23:43:40.739912: step 6129, loss 0.00117144, acc 1
2017-03-02T23:43:40.830604: step 6130, loss 0.000709419, acc 1
2017-03-02T23:43:40.933392: step 6131, loss 0.000415942, acc 1
2017-03-02T23:43:41.044957: step 6132, loss 0.0127402, acc 1
2017-03-02T23:43:41.157958: step 6133, loss 0.000562432, acc 1
2017-03-02T23:43:41.274920: step 6134, loss 0.000287797, acc 1
2017-03-02T23:43:41.374694: step 6135, loss 0.0023109, acc 1
2017-03-02T23:43:41.490083: step 6136, loss 0.000218451, acc 1
2017-03-02T23:43:41.579065: step 6137, loss 0.000756858, acc 1
2017-03-02T23:43:41.682155: step 6138, loss 0.000484992, acc 1
2017-03-02T23:43:41.790178: step 6139, loss 0.00865009, acc 1
2017-03-02T23:43:41.900677: step 6140, loss 0.000544947, acc 1
2017-03-02T23:43:42.009929: step 6141, loss 0.000901501, acc 1
2017-03-02T23:43:42.119738: step 6142, loss 0.000199915, acc 1
2017-03-02T23:43:42.223428: step 6143, loss 0.000628015, acc 1
2017-03-02T23:43:42.312643: step 6144, loss 0.000230118, acc 1
2017-03-02T23:43:42.409184: step 6145, loss 0.00125747, acc 1
2017-03-02T23:43:42.513610: step 6146, loss 0.00188979, acc 1
2017-03-02T23:43:42.621719: step 6147, loss 0.0005563, acc 1
2017-03-02T23:43:42.728011: step 6148, loss 0.000389157, acc 1
2017-03-02T23:43:42.838791: step 6149, loss 0.000828456, acc 1
2017-03-02T23:43:42.933799: step 6150, loss 0.000568373, acc 1
2017-03-02T23:43:43.030879: step 6151, loss 0.000213762, acc 1
2017-03-02T23:43:43.121469: step 6152, loss 0.000183433, acc 1
2017-03-02T23:43:43.230951: step 6153, loss 0.000559366, acc 1
2017-03-02T23:43:43.331429: step 6154, loss 0.000405098, acc 1
2017-03-02T23:43:43.440773: step 6155, loss 0.0126613, acc 0.984375
2017-03-02T23:43:43.543670: step 6156, loss 0.00264745, acc 1
2017-03-02T23:43:43.641306: step 6157, loss 0.000157742, acc 1
2017-03-02T23:43:43.753646: step 6158, loss 0.000265468, acc 1
2017-03-02T23:43:43.848130: step 6159, loss 0.00177816, acc 1
2017-03-02T23:43:43.946732: step 6160, loss 0.00143047, acc 1
2017-03-02T23:43:44.043500: step 6161, loss 0.000132793, acc 1
2017-03-02T23:43:44.150151: step 6162, loss 0.000431423, acc 1
2017-03-02T23:43:44.256668: step 6163, loss 0.00313553, acc 1
2017-03-02T23:43:44.362745: step 6164, loss 0.000194355, acc 1
2017-03-02T23:43:44.465183: step 6165, loss 0.000310052, acc 1
2017-03-02T23:43:44.562627: step 6166, loss 0.000425443, acc 1
2017-03-02T23:43:44.663526: step 6167, loss 0.000818516, acc 1
2017-03-02T23:43:44.765940: step 6168, loss 0.0111597, acc 1
2017-03-02T23:43:44.866937: step 6169, loss 0.000301576, acc 1
2017-03-02T23:43:44.970648: step 6170, loss 0.00160032, acc 1
2017-03-02T23:43:45.070788: step 6171, loss 0.000529658, acc 1
2017-03-02T23:43:45.173657: step 6172, loss 0.000842553, acc 1
2017-03-02T23:43:45.276178: step 6173, loss 0.000487085, acc 1
2017-03-02T23:43:45.369056: step 6174, loss 0.000369583, acc 1
2017-03-02T23:43:45.485762: step 6175, loss 0.00142046, acc 1
2017-03-02T23:43:45.594528: step 6176, loss 0.00100659, acc 1
2017-03-02T23:43:45.694845: step 6177, loss 0.00531008, acc 1
2017-03-02T23:43:45.802873: step 6178, loss 0.000264493, acc 1
2017-03-02T23:43:45.908860: step 6179, loss 0.00138298, acc 1
2017-03-02T23:43:46.018580: step 6180, loss 0.000485529, acc 1
2017-03-02T23:43:46.109638: step 6181, loss 0.000375873, acc 1
2017-03-02T23:43:46.215695: step 6182, loss 0.000689262, acc 1
2017-03-02T23:43:46.322099: step 6183, loss 0.000425413, acc 1
2017-03-02T23:43:46.426278: step 6184, loss 0.0025, acc 1
2017-03-02T23:43:46.537093: step 6185, loss 0.00323225, acc 1
2017-03-02T23:43:46.642549: step 6186, loss 0.000803573, acc 1
2017-03-02T23:43:46.749094: step 6187, loss 0.000989135, acc 1
2017-03-02T23:43:46.845003: step 6188, loss 0.00547432, acc 1
2017-03-02T23:43:46.933592: step 6189, loss 0.000511625, acc 1
2017-03-02T23:43:47.035183: step 6190, loss 0.000323826, acc 1
2017-03-02T23:43:47.146631: step 6191, loss 0.000226817, acc 1
2017-03-02T23:43:47.253522: step 6192, loss 0.00873127, acc 1
2017-03-02T23:43:47.358306: step 6193, loss 0.000957983, acc 1
2017-03-02T23:43:47.472648: step 6194, loss 0.00189728, acc 1
2017-03-02T23:43:47.579301: step 6195, loss 0.000331115, acc 1
2017-03-02T23:43:47.666213: step 6196, loss 0.000955084, acc 1
2017-03-02T23:43:47.769975: step 6197, loss 0.000654954, acc 1
2017-03-02T23:43:47.872996: step 6198, loss 0.000839554, acc 1
2017-03-02T23:43:47.972921: step 6199, loss 0.000424136, acc 1
2017-03-02T23:43:48.087125: step 6200, loss 0.0071982, acc 1

Evaluation:
2017-03-02T23:43:48.149279: step 6200, loss 2.97815, acc 0.522491

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6200

2017-03-02T23:43:48.619773: step 6201, loss 0.000167571, acc 1
2017-03-02T23:43:48.738128: step 6202, loss 0.000117266, acc 1
2017-03-02T23:43:48.843224: step 6203, loss 0.000378189, acc 1
2017-03-02T23:43:48.948546: step 6204, loss 0.000822698, acc 1
2017-03-02T23:43:49.057886: step 6205, loss 0.000408028, acc 1
2017-03-02T23:43:49.149487: step 6206, loss 0.00116718, acc 1
2017-03-02T23:43:49.255004: step 6207, loss 0.000546741, acc 1
2017-03-02T23:43:49.357906: step 6208, loss 0.000591073, acc 1
2017-03-02T23:43:49.473825: step 6209, loss 0.000553356, acc 1
2017-03-02T23:43:49.585940: step 6210, loss 0.000275695, acc 1
2017-03-02T23:43:49.698762: step 6211, loss 0.0055237, acc 1
2017-03-02T23:43:49.811310: step 6212, loss 0.000306732, acc 1
2017-03-02T23:43:49.910735: step 6213, loss 0.000463865, acc 1
2017-03-02T23:43:50.014209: step 6214, loss 0.000123842, acc 1
2017-03-02T23:43:50.126888: step 6215, loss 0.00116446, acc 1
2017-03-02T23:43:50.238471: step 6216, loss 8.40035e-05, acc 1
2017-03-02T23:43:50.353666: step 6217, loss 7.79204e-05, acc 1
2017-03-02T23:43:50.457845: step 6218, loss 0.000642944, acc 1
2017-03-02T23:43:50.563296: step 6219, loss 0.000220674, acc 1
2017-03-02T23:43:50.654697: step 6220, loss 0.000216877, acc 1
2017-03-02T23:43:50.757032: step 6221, loss 0.00017667, acc 1
2017-03-02T23:43:50.863052: step 6222, loss 0.00102441, acc 1
2017-03-02T23:43:50.966682: step 6223, loss 0.000110279, acc 1
2017-03-02T23:43:51.076578: step 6224, loss 0.000570427, acc 1
2017-03-02T23:43:51.184931: step 6225, loss 0.0189451, acc 0.984375
2017-03-02T23:43:51.299941: step 6226, loss 0.000683397, acc 1
2017-03-02T23:43:51.395021: step 6227, loss 0.000509597, acc 1
2017-03-02T23:43:51.505940: step 6228, loss 0.000183961, acc 1
2017-03-02T23:43:51.611910: step 6229, loss 0.000543925, acc 1
2017-03-02T23:43:51.715814: step 6230, loss 0.000234044, acc 1
2017-03-02T23:43:51.821546: step 6231, loss 0.00143426, acc 1
2017-03-02T23:43:51.918454: step 6232, loss 0.00287651, acc 1
2017-03-02T23:43:52.014342: step 6233, loss 0.000939588, acc 1
2017-03-02T23:43:52.111529: step 6234, loss 0.00120845, acc 1
2017-03-02T23:43:52.201377: step 6235, loss 0.00128787, acc 1
2017-03-02T23:43:52.303507: step 6236, loss 0.0031832, acc 1
2017-03-02T23:43:52.413464: step 6237, loss 0.0116061, acc 1
2017-03-02T23:43:52.529835: step 6238, loss 0.000861033, acc 1
2017-03-02T23:43:52.640521: step 6239, loss 0.000424389, acc 1
2017-03-02T23:43:52.745032: step 6240, loss 0.000149986, acc 1
2017-03-02T23:43:52.849436: step 6241, loss 0.00192155, acc 1
2017-03-02T23:43:52.946746: step 6242, loss 0.0043737, acc 1
2017-03-02T23:43:53.054647: step 6243, loss 0.000495994, acc 1
2017-03-02T23:43:53.151349: step 6244, loss 0.000168117, acc 1
2017-03-02T23:43:53.253612: step 6245, loss 0.000158147, acc 1
2017-03-02T23:43:53.360946: step 6246, loss 0.00133908, acc 1
2017-03-02T23:43:53.465273: step 6247, loss 0.000234065, acc 1
2017-03-02T23:43:53.561614: step 6248, loss 0.00067359, acc 1
2017-03-02T23:43:53.655067: step 6249, loss 0.000199049, acc 1
2017-03-02T23:43:53.761473: step 6250, loss 0.000806422, acc 1
2017-03-02T23:43:53.864940: step 6251, loss 8.44811e-05, acc 1
2017-03-02T23:43:53.962838: step 6252, loss 0.000993186, acc 1
2017-03-02T23:43:54.071168: step 6253, loss 0.000728076, acc 1
2017-03-02T23:43:54.174754: step 6254, loss 0.000809682, acc 1
2017-03-02T23:43:54.280747: step 6255, loss 0.000403231, acc 1
2017-03-02T23:43:54.374270: step 6256, loss 0.00126502, acc 1
2017-03-02T23:43:54.465659: step 6257, loss 0.00128164, acc 1
2017-03-02T23:43:54.569681: step 6258, loss 0.000426769, acc 1
2017-03-02T23:43:54.685113: step 6259, loss 0.00620034, acc 1
2017-03-02T23:43:54.791070: step 6260, loss 0.000418319, acc 1
2017-03-02T23:43:54.894727: step 6261, loss 0.000556523, acc 1
2017-03-02T23:43:54.994943: step 6262, loss 0.00062607, acc 1
2017-03-02T23:43:55.102131: step 6263, loss 0.013962, acc 0.984375
2017-03-02T23:43:55.196913: step 6264, loss 0.000179519, acc 1
2017-03-02T23:43:55.303623: step 6265, loss 0.000854982, acc 1
2017-03-02T23:43:55.421685: step 6266, loss 0.000318031, acc 1
2017-03-02T23:43:55.530623: step 6267, loss 0.000568769, acc 1
2017-03-02T23:43:55.639472: step 6268, loss 0.00133277, acc 1
2017-03-02T23:43:55.739505: step 6269, loss 0.000246148, acc 1
2017-03-02T23:43:55.838788: step 6270, loss 0.000199812, acc 1
2017-03-02T23:43:55.921360: step 6271, loss 0.000132028, acc 1
2017-03-02T23:43:56.030962: step 6272, loss 0.000834976, acc 1
2017-03-02T23:43:56.125403: step 6273, loss 0.000448846, acc 1
2017-03-02T23:43:56.233633: step 6274, loss 0.000646934, acc 1
2017-03-02T23:43:56.338672: step 6275, loss 0.000218732, acc 1
2017-03-02T23:43:56.437470: step 6276, loss 0.000905371, acc 1
2017-03-02T23:43:56.540380: step 6277, loss 0.000492734, acc 1
2017-03-02T23:43:56.639380: step 6278, loss 8.24719e-05, acc 1
2017-03-02T23:43:56.725702: step 6279, loss 0.000312823, acc 1
2017-03-02T23:43:56.839994: step 6280, loss 0.00942821, acc 1
2017-03-02T23:43:56.945708: step 6281, loss 0.00251104, acc 1
2017-03-02T23:43:57.052973: step 6282, loss 0.0143872, acc 0.984375
2017-03-02T23:43:57.156066: step 6283, loss 0.00189896, acc 1
2017-03-02T23:43:57.263402: step 6284, loss 0.011014, acc 1
2017-03-02T23:43:57.368654: step 6285, loss 0.0202666, acc 0.984375
2017-03-02T23:43:57.456573: step 6286, loss 0.000717494, acc 1
2017-03-02T23:43:57.552774: step 6287, loss 0.0018507, acc 1
2017-03-02T23:43:57.660273: step 6288, loss 0.00159196, acc 1
2017-03-02T23:43:57.760875: step 6289, loss 0.000691407, acc 1
2017-03-02T23:43:57.865903: step 6290, loss 0.00106857, acc 1
2017-03-02T23:43:57.975390: step 6291, loss 0.00106489, acc 1
2017-03-02T23:43:58.079734: step 6292, loss 0.00021083, acc 1
2017-03-02T23:43:58.168216: step 6293, loss 0.0116859, acc 1
2017-03-02T23:43:58.264339: step 6294, loss 0.000831524, acc 1
2017-03-02T23:43:58.364995: step 6295, loss 0.000555548, acc 1
2017-03-02T23:43:58.463509: step 6296, loss 0.0035579, acc 1
2017-03-02T23:43:58.569760: step 6297, loss 0.00194137, acc 1
2017-03-02T23:43:58.673693: step 6298, loss 0.000889952, acc 1
2017-03-02T23:43:58.783619: step 6299, loss 0.00129525, acc 1
2017-03-02T23:43:58.891471: step 6300, loss 0.00109263, acc 1

Evaluation:
2017-03-02T23:43:58.938371: step 6300, loss 3.27019, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6300

2017-03-02T23:43:59.476107: step 6301, loss 0.00103714, acc 1
2017-03-02T23:43:59.579971: step 6302, loss 0.0015574, acc 1
2017-03-02T23:43:59.674771: step 6303, loss 0.000404658, acc 1
2017-03-02T23:43:59.779584: step 6304, loss 0.000328405, acc 1
2017-03-02T23:43:59.890096: step 6305, loss 0.000105096, acc 1
2017-03-02T23:43:59.994086: step 6306, loss 0.00075139, acc 1
2017-03-02T23:44:00.093813: step 6307, loss 0.000801062, acc 1
2017-03-02T23:44:00.200667: step 6308, loss 0.000795341, acc 1
2017-03-02T23:44:00.306649: step 6309, loss 0.000139434, acc 1
2017-03-02T23:44:00.395402: step 6310, loss 0.000303202, acc 1
2017-03-02T23:44:00.500189: step 6311, loss 0.000124157, acc 1
2017-03-02T23:44:00.614327: step 6312, loss 0.00177724, acc 1
2017-03-02T23:44:00.718526: step 6313, loss 0.000999087, acc 1
2017-03-02T23:44:00.817203: step 6314, loss 0.000973225, acc 1
2017-03-02T23:44:00.914799: step 6315, loss 0.00136783, acc 1
2017-03-02T23:44:01.022959: step 6316, loss 0.00388053, acc 1
2017-03-02T23:44:01.115193: step 6317, loss 0.000192001, acc 1
2017-03-02T23:44:01.199700: step 6318, loss 0.00143297, acc 1
2017-03-02T23:44:01.306268: step 6319, loss 0.00031493, acc 1
2017-03-02T23:44:01.418383: step 6320, loss 0.000207853, acc 1
2017-03-02T23:44:01.523267: step 6321, loss 0.000229109, acc 1
2017-03-02T23:44:01.628655: step 6322, loss 0.000941492, acc 1
2017-03-02T23:44:01.734919: step 6323, loss 0.00135236, acc 1
2017-03-02T23:44:01.844856: step 6324, loss 0.000218187, acc 1
2017-03-02T23:44:01.936420: step 6325, loss 0.000470825, acc 1
2017-03-02T23:44:02.043643: step 6326, loss 0.00330501, acc 1
2017-03-02T23:44:02.147946: step 6327, loss 0.0235391, acc 0.984375
2017-03-02T23:44:02.248871: step 6328, loss 0.000417815, acc 1
2017-03-02T23:44:02.356728: step 6329, loss 0.000310724, acc 1
2017-03-02T23:44:02.470221: step 6330, loss 0.00128677, acc 1
2017-03-02T23:44:02.576698: step 6331, loss 0.00084001, acc 1
2017-03-02T23:44:02.664832: step 6332, loss 0.000627436, acc 1
2017-03-02T23:44:02.761177: step 6333, loss 0.00034664, acc 1
2017-03-02T23:44:02.878430: step 6334, loss 0.00137867, acc 1
2017-03-02T23:44:02.985442: step 6335, loss 0.00104687, acc 1
2017-03-02T23:44:03.090619: step 6336, loss 0.00110369, acc 1
2017-03-02T23:44:03.198809: step 6337, loss 0.000402945, acc 1
2017-03-02T23:44:03.302233: step 6338, loss 0.00018231, acc 1
2017-03-02T23:44:03.386920: step 6339, loss 0.000157445, acc 1
2017-03-02T23:44:03.475240: step 6340, loss 0.000194587, acc 1
2017-03-02T23:44:03.583286: step 6341, loss 0.000293856, acc 1
2017-03-02T23:44:03.691308: step 6342, loss 0.000200872, acc 1
2017-03-02T23:44:03.791325: step 6343, loss 0.000715474, acc 1
2017-03-02T23:44:03.898961: step 6344, loss 0.000190315, acc 1
2017-03-02T23:44:03.999137: step 6345, loss 0.000391025, acc 1
2017-03-02T23:44:04.109321: step 6346, loss 0.00160843, acc 1
2017-03-02T23:44:04.205511: step 6347, loss 0.000127353, acc 1
2017-03-02T23:44:04.308359: step 6348, loss 0.000482521, acc 1
2017-03-02T23:44:04.409410: step 6349, loss 0.000901125, acc 1
2017-03-02T23:44:04.517845: step 6350, loss 0.0297435, acc 0.984375
2017-03-02T23:44:04.625617: step 6351, loss 0.00303485, acc 1
2017-03-02T23:44:04.728042: step 6352, loss 0.00041965, acc 1
2017-03-02T23:44:04.834060: step 6353, loss 0.00158543, acc 1
2017-03-02T23:44:04.937277: step 6354, loss 0.000246324, acc 1
2017-03-02T23:44:05.029509: step 6355, loss 0.000169418, acc 1
2017-03-02T23:44:05.133948: step 6356, loss 0.000823969, acc 1
2017-03-02T23:44:05.241515: step 6357, loss 0.000790642, acc 1
2017-03-02T23:44:05.354401: step 6358, loss 0.00038844, acc 1
2017-03-02T23:44:05.458371: step 6359, loss 0.00267981, acc 1
2017-03-02T23:44:05.571335: step 6360, loss 0.00132665, acc 1
2017-03-02T23:44:05.664126: step 6361, loss 0.00206782, acc 1
2017-03-02T23:44:05.763441: step 6362, loss 0.0016773, acc 1
2017-03-02T23:44:05.875595: step 6363, loss 0.00346191, acc 1
2017-03-02T23:44:05.976613: step 6364, loss 0.00110607, acc 1
2017-03-02T23:44:06.080495: step 6365, loss 0.000725475, acc 1
2017-03-02T23:44:06.184856: step 6366, loss 0.00398032, acc 1
2017-03-02T23:44:06.287156: step 6367, loss 0.00176702, acc 1
2017-03-02T23:44:06.380069: step 6368, loss 0.00916468, acc 1
2017-03-02T23:44:06.474340: step 6369, loss 0.000169952, acc 1
2017-03-02T23:44:06.582816: step 6370, loss 0.00249646, acc 1
2017-03-02T23:44:06.692574: step 6371, loss 0.000133441, acc 1
2017-03-02T23:44:06.803423: step 6372, loss 0.00327406, acc 1
2017-03-02T23:44:06.909109: step 6373, loss 0.000460542, acc 1
2017-03-02T23:44:07.019023: step 6374, loss 0.000583539, acc 1
2017-03-02T23:44:07.129606: step 6375, loss 0.000576073, acc 1
2017-03-02T23:44:07.221653: step 6376, loss 0.000339076, acc 1
2017-03-02T23:44:07.328719: step 6377, loss 0.000363689, acc 1
2017-03-02T23:44:07.440870: step 6378, loss 0.000357738, acc 1
2017-03-02T23:44:07.545836: step 6379, loss 0.00272551, acc 1
2017-03-02T23:44:07.652146: step 6380, loss 0.00031465, acc 1
2017-03-02T23:44:07.758212: step 6381, loss 0.00641491, acc 1
2017-03-02T23:44:07.862943: step 6382, loss 0.00272938, acc 1
2017-03-02T23:44:07.949508: step 6383, loss 0.000402734, acc 1
2017-03-02T23:44:08.053792: step 6384, loss 0.000666104, acc 1
2017-03-02T23:44:08.157662: step 6385, loss 9.53501e-05, acc 1
2017-03-02T23:44:08.262592: step 6386, loss 0.000447633, acc 1
2017-03-02T23:44:08.370092: step 6387, loss 0.000296572, acc 1
2017-03-02T23:44:08.471051: step 6388, loss 0.00148711, acc 1
2017-03-02T23:44:08.572882: step 6389, loss 0.00024558, acc 1
2017-03-02T23:44:08.662864: step 6390, loss 0.000616842, acc 1
2017-03-02T23:44:08.749240: step 6391, loss 0.000156565, acc 1
2017-03-02T23:44:08.853423: step 6392, loss 0.000184839, acc 1
2017-03-02T23:44:08.966462: step 6393, loss 0.000196581, acc 1
2017-03-02T23:44:09.077751: step 6394, loss 0.00177887, acc 1
2017-03-02T23:44:09.189536: step 6395, loss 0.000757587, acc 1
2017-03-02T23:44:09.287664: step 6396, loss 0.000385704, acc 1
2017-03-02T23:44:09.390435: step 6397, loss 0.000611249, acc 1
2017-03-02T23:44:09.480357: step 6398, loss 0.00139668, acc 1
2017-03-02T23:44:09.592362: step 6399, loss 0.000670662, acc 1
2017-03-02T23:44:09.700494: step 6400, loss 0.000199734, acc 1

Evaluation:
2017-03-02T23:44:09.756914: step 6400, loss 2.88464, acc 0.546713

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6400

2017-03-02T23:44:10.230576: step 6401, loss 0.00144203, acc 1
2017-03-02T23:44:10.337109: step 6402, loss 0.00682994, acc 1
2017-03-02T23:44:10.453289: step 6403, loss 0.000226493, acc 1
2017-03-02T23:44:10.556564: step 6404, loss 0.000240647, acc 1
2017-03-02T23:44:10.659177: step 6405, loss 0.001803, acc 1
2017-03-02T23:44:10.754782: step 6406, loss 0.000191246, acc 1
2017-03-02T23:44:10.850429: step 6407, loss 0.000338215, acc 1
2017-03-02T23:44:10.937268: step 6408, loss 0.00101517, acc 1
2017-03-02T23:44:11.038036: step 6409, loss 0.000402702, acc 1
2017-03-02T23:44:11.135300: step 6410, loss 0.000246944, acc 1
2017-03-02T23:44:11.236542: step 6411, loss 0.0280764, acc 0.984375
2017-03-02T23:44:11.366791: step 6412, loss 0.000279943, acc 1
2017-03-02T23:44:11.483723: step 6413, loss 0.000164913, acc 1
2017-03-02T23:44:11.593241: step 6414, loss 0.000303113, acc 1
2017-03-02T23:44:11.689566: step 6415, loss 0.00026298, acc 1
2017-03-02T23:44:11.806117: step 6416, loss 0.00716256, acc 1
2017-03-02T23:44:11.911791: step 6417, loss 0.000648346, acc 1
2017-03-02T23:44:12.021893: step 6418, loss 0.00114038, acc 1
2017-03-02T23:44:12.122696: step 6419, loss 0.000637591, acc 1
2017-03-02T23:44:12.222930: step 6420, loss 0.000250852, acc 1
2017-03-02T23:44:12.328019: step 6421, loss 0.00167519, acc 1
2017-03-02T23:44:12.429525: step 6422, loss 0.000468207, acc 1
2017-03-02T23:44:12.538437: step 6423, loss 0.000223213, acc 1
2017-03-02T23:44:12.644694: step 6424, loss 0.000352736, acc 1
2017-03-02T23:44:12.747176: step 6425, loss 0.000423323, acc 1
2017-03-02T23:44:12.852173: step 6426, loss 0.000286667, acc 1
2017-03-02T23:44:12.961179: step 6427, loss 0.0014751, acc 1
2017-03-02T23:44:13.064712: step 6428, loss 0.000517947, acc 1
2017-03-02T23:44:13.153724: step 6429, loss 0.000748782, acc 1
2017-03-02T23:44:13.248700: step 6430, loss 0.00044853, acc 1
2017-03-02T23:44:13.353105: step 6431, loss 0.00205647, acc 1
2017-03-02T23:44:13.461579: step 6432, loss 0.0011777, acc 1
2017-03-02T23:44:13.567217: step 6433, loss 0.000266787, acc 1
2017-03-02T23:44:13.672980: step 6434, loss 0.000755932, acc 1
2017-03-02T23:44:13.781534: step 6435, loss 0.00831029, acc 1
2017-03-02T23:44:13.884483: step 6436, loss 0.00111955, acc 1
2017-03-02T23:44:13.968737: step 6437, loss 0.000231475, acc 1
2017-03-02T23:44:14.066297: step 6438, loss 0.0017846, acc 1
2017-03-02T23:44:14.169476: step 6439, loss 0.000348704, acc 1
2017-03-02T23:44:14.273900: step 6440, loss 0.00022342, acc 1
2017-03-02T23:44:14.383169: step 6441, loss 0.00066511, acc 1
2017-03-02T23:44:14.489858: step 6442, loss 0.000180272, acc 1
2017-03-02T23:44:14.589678: step 6443, loss 0.00120794, acc 1
2017-03-02T23:44:14.685491: step 6444, loss 0.000140757, acc 1
2017-03-02T23:44:14.777246: step 6445, loss 0.000101992, acc 1
2017-03-02T23:44:14.882596: step 6446, loss 0.000223247, acc 1
2017-03-02T23:44:14.985720: step 6447, loss 0.000367376, acc 1
2017-03-02T23:44:15.098210: step 6448, loss 0.000919686, acc 1
2017-03-02T23:44:15.205456: step 6449, loss 0.000156405, acc 1
2017-03-02T23:44:15.307763: step 6450, loss 0.000132041, acc 1
2017-03-02T23:44:15.412747: step 6451, loss 0.00418215, acc 1
2017-03-02T23:44:15.502004: step 6452, loss 0.00133503, acc 1
2017-03-02T23:44:15.611939: step 6453, loss 0.00116699, acc 1
2017-03-02T23:44:15.723180: step 6454, loss 0.000452884, acc 1
2017-03-02T23:44:15.831802: step 6455, loss 0.000802867, acc 1
2017-03-02T23:44:15.938773: step 6456, loss 0.000883319, acc 1
2017-03-02T23:44:16.043784: step 6457, loss 0.000133904, acc 1
2017-03-02T23:44:16.153012: step 6458, loss 0.000130156, acc 1
2017-03-02T23:44:16.249022: step 6459, loss 0.0231893, acc 0.984375
2017-03-02T23:44:16.353568: step 6460, loss 0.000340333, acc 1
2017-03-02T23:44:16.461446: step 6461, loss 0.000289324, acc 1
2017-03-02T23:44:16.569336: step 6462, loss 0.00225397, acc 1
2017-03-02T23:44:16.688423: step 6463, loss 0.000658856, acc 1
2017-03-02T23:44:16.795452: step 6464, loss 0.00104371, acc 1
2017-03-02T23:44:16.917498: step 6465, loss 0.000689036, acc 1
2017-03-02T23:44:17.008885: step 6466, loss 0.0006114, acc 1
2017-03-02T23:44:17.118659: step 6467, loss 0.000995005, acc 1
2017-03-02T23:44:17.220992: step 6468, loss 0.00032884, acc 1
2017-03-02T23:44:17.324764: step 6469, loss 0.00162248, acc 1
2017-03-02T23:44:17.423949: step 6470, loss 0.000895975, acc 1
2017-03-02T23:44:17.533816: step 6471, loss 0.000777609, acc 1
2017-03-02T23:44:17.638801: step 6472, loss 0.000944813, acc 1
2017-03-02T23:44:17.737767: step 6473, loss 0.000530341, acc 1
2017-03-02T23:44:17.837158: step 6474, loss 0.000237415, acc 1
2017-03-02T23:44:17.941459: step 6475, loss 0.0033534, acc 1
2017-03-02T23:44:18.045104: step 6476, loss 0.00581664, acc 1
2017-03-02T23:44:18.155056: step 6477, loss 0.000321733, acc 1
2017-03-02T23:44:18.237309: step 6478, loss 0.000961337, acc 1
2017-03-02T23:44:18.342309: step 6479, loss 0.000332275, acc 1
2017-03-02T23:44:18.445080: step 6480, loss 0.000811788, acc 1
2017-03-02T23:44:18.535147: step 6481, loss 9.56073e-05, acc 1
2017-03-02T23:44:18.634825: step 6482, loss 0.000227743, acc 1
2017-03-02T23:44:18.740963: step 6483, loss 0.000298939, acc 1
2017-03-02T23:44:18.842866: step 6484, loss 0.000430945, acc 1
2017-03-02T23:44:18.960538: step 6485, loss 0.00199587, acc 1
2017-03-02T23:44:19.064436: step 6486, loss 0.000533031, acc 1
2017-03-02T23:44:19.172065: step 6487, loss 0.000147394, acc 1
2017-03-02T23:44:19.270176: step 6488, loss 0.000243007, acc 1
2017-03-02T23:44:19.365725: step 6489, loss 0.00251516, acc 1
2017-03-02T23:44:19.467441: step 6490, loss 0.00145637, acc 1
2017-03-02T23:44:19.571307: step 6491, loss 0.00042196, acc 1
2017-03-02T23:44:19.677694: step 6492, loss 0.00584088, acc 1
2017-03-02T23:44:19.784185: step 6493, loss 0.000711807, acc 1
2017-03-02T23:44:19.883756: step 6494, loss 0.000119697, acc 1
2017-03-02T23:44:19.995806: step 6495, loss 0.0010714, acc 1
2017-03-02T23:44:20.086351: step 6496, loss 0.000341157, acc 1
2017-03-02T23:44:20.203298: step 6497, loss 0.000498062, acc 1
2017-03-02T23:44:20.311060: step 6498, loss 0.00060926, acc 1
2017-03-02T23:44:20.425739: step 6499, loss 0.000150565, acc 1
2017-03-02T23:44:20.535796: step 6500, loss 0.000263047, acc 1

Evaluation:
2017-03-02T23:44:20.591403: step 6500, loss 3.00346, acc 0.536332

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6500

2017-03-02T23:44:21.043545: step 6501, loss 0.00117092, acc 1
2017-03-02T23:44:21.152343: step 6502, loss 0.000122502, acc 1
2017-03-02T23:44:21.253847: step 6503, loss 0.000144045, acc 1
2017-03-02T23:44:21.354684: step 6504, loss 0.000209119, acc 1
2017-03-02T23:44:21.458696: step 6505, loss 0.00106962, acc 1
2017-03-02T23:44:21.542406: step 6506, loss 0.00022712, acc 1
2017-03-02T23:44:21.635185: step 6507, loss 0.000624923, acc 1
2017-03-02T23:44:21.765180: step 6508, loss 0.00494497, acc 1
2017-03-02T23:44:21.869069: step 6509, loss 0.00182717, acc 1
2017-03-02T23:44:21.981732: step 6510, loss 0.000547424, acc 1
2017-03-02T23:44:22.093103: step 6511, loss 0.000225569, acc 1
2017-03-02T23:44:22.197137: step 6512, loss 0.00027663, acc 1
2017-03-02T23:44:22.289268: step 6513, loss 0.000230013, acc 1
2017-03-02T23:44:22.387344: step 6514, loss 0.00864623, acc 1
2017-03-02T23:44:22.490582: step 6515, loss 0.00030164, acc 1
2017-03-02T23:44:22.595500: step 6516, loss 0.000107954, acc 1
2017-03-02T23:44:22.701988: step 6517, loss 0.000197053, acc 1
2017-03-02T23:44:22.809716: step 6518, loss 0.000445548, acc 1
2017-03-02T23:44:22.903604: step 6519, loss 0.00218615, acc 1
2017-03-02T23:44:23.015829: step 6520, loss 0.00111155, acc 1
2017-03-02T23:44:23.113047: step 6521, loss 0.000269001, acc 1
2017-03-02T23:44:23.216424: step 6522, loss 0.000919217, acc 1
2017-03-02T23:44:23.322122: step 6523, loss 0.00484755, acc 1
2017-03-02T23:44:23.429090: step 6524, loss 0.000389627, acc 1
2017-03-02T23:44:23.538836: step 6525, loss 0.00203424, acc 1
2017-03-02T23:44:23.640193: step 6526, loss 0.000321732, acc 1
2017-03-02T23:44:23.744037: step 6527, loss 0.000190124, acc 1
2017-03-02T23:44:23.840109: step 6528, loss 0.0139341, acc 0.984375
2017-03-02T23:44:23.943749: step 6529, loss 0.00159366, acc 1
2017-03-02T23:44:24.063413: step 6530, loss 0.00063777, acc 1
2017-03-02T23:44:24.166753: step 6531, loss 0.000165841, acc 1
2017-03-02T23:44:24.263068: step 6532, loss 0.00217746, acc 1
2017-03-02T23:44:24.371263: step 6533, loss 0.000314602, acc 1
2017-03-02T23:44:24.478167: step 6534, loss 0.00159625, acc 1
2017-03-02T23:44:24.571721: step 6535, loss 0.000954604, acc 1
2017-03-02T23:44:24.673841: step 6536, loss 0.000139272, acc 1
2017-03-02T23:44:24.778329: step 6537, loss 0.000859546, acc 1
2017-03-02T23:44:24.881184: step 6538, loss 0.000782312, acc 1
2017-03-02T23:44:24.985006: step 6539, loss 0.00174888, acc 1
2017-03-02T23:44:25.088586: step 6540, loss 0.000608897, acc 1
2017-03-02T23:44:25.179364: step 6541, loss 0.00103477, acc 1
2017-03-02T23:44:25.281458: step 6542, loss 0.000749379, acc 1
2017-03-02T23:44:25.375439: step 6543, loss 0.00109048, acc 1
2017-03-02T23:44:25.478295: step 6544, loss 0.00022569, acc 1
2017-03-02T23:44:25.581495: step 6545, loss 0.00538015, acc 1
2017-03-02T23:44:25.686179: step 6546, loss 0.000696466, acc 1
2017-03-02T23:44:25.787240: step 6547, loss 0.0219609, acc 0.984375
2017-03-02T23:44:25.909992: step 6548, loss 0.00131645, acc 1
2017-03-02T23:44:26.012433: step 6549, loss 0.00070879, acc 1
2017-03-02T23:44:26.111803: step 6550, loss 0.000153836, acc 1
2017-03-02T23:44:26.218261: step 6551, loss 7.26231e-05, acc 1
2017-03-02T23:44:26.315135: step 6552, loss 0.00326223, acc 1
2017-03-02T23:44:26.422640: step 6553, loss 0.00143875, acc 1
2017-03-02T23:44:26.525993: step 6554, loss 0.000266565, acc 1
2017-03-02T23:44:26.630998: step 6555, loss 0.00571567, acc 1
2017-03-02T23:44:26.736503: step 6556, loss 0.000264902, acc 1
2017-03-02T23:44:26.830529: step 6557, loss 0.000403674, acc 1
2017-03-02T23:44:26.927247: step 6558, loss 0.000306756, acc 1
2017-03-02T23:44:27.037931: step 6559, loss 0.000165193, acc 1
2017-03-02T23:44:27.131248: step 6560, loss 0.00105732, acc 1
2017-03-02T23:44:27.236821: step 6561, loss 0.00493143, acc 1
2017-03-02T23:44:27.342554: step 6562, loss 8.7812e-05, acc 1
2017-03-02T23:44:27.447199: step 6563, loss 0.000225505, acc 1
2017-03-02T23:44:27.557305: step 6564, loss 0.000241538, acc 1
2017-03-02T23:44:27.643930: step 6565, loss 0.000469263, acc 1
2017-03-02T23:44:27.746344: step 6566, loss 9.01398e-05, acc 1
2017-03-02T23:44:27.849005: step 6567, loss 0.000395215, acc 1
2017-03-02T23:44:27.968667: step 6568, loss 0.00125458, acc 1
2017-03-02T23:44:28.076517: step 6569, loss 0.000376686, acc 1
2017-03-02T23:44:28.182655: step 6570, loss 0.000789931, acc 1
2017-03-02T23:44:28.283435: step 6571, loss 0.000454057, acc 1
2017-03-02T23:44:28.374656: step 6572, loss 0.000191804, acc 1
2017-03-02T23:44:28.474572: step 6573, loss 0.000452852, acc 1
2017-03-02T23:44:28.591361: step 6574, loss 0.000497265, acc 1
2017-03-02T23:44:28.712527: step 6575, loss 0.00022613, acc 1
2017-03-02T23:44:28.813207: step 6576, loss 0.000804733, acc 1
2017-03-02T23:44:28.916333: step 6577, loss 0.000124344, acc 1
2017-03-02T23:44:29.023883: step 6578, loss 0.000294545, acc 1
2017-03-02T23:44:29.118106: step 6579, loss 0.000854349, acc 1
2017-03-02T23:44:29.219411: step 6580, loss 0.000556883, acc 1
2017-03-02T23:44:29.324417: step 6581, loss 0.00205781, acc 1
2017-03-02T23:44:29.447505: step 6582, loss 0.00339042, acc 1
2017-03-02T23:44:29.553209: step 6583, loss 0.00420597, acc 1
2017-03-02T23:44:29.665319: step 6584, loss 0.00039763, acc 1
2017-03-02T23:44:29.769839: step 6585, loss 0.000196273, acc 1
2017-03-02T23:44:29.862020: step 6586, loss 0.0111894, acc 1
2017-03-02T23:44:29.965420: step 6587, loss 0.00083367, acc 1
2017-03-02T23:44:30.068945: step 6588, loss 0.000346999, acc 1
2017-03-02T23:44:30.182462: step 6589, loss 0.000318023, acc 1
2017-03-02T23:44:30.286476: step 6590, loss 0.000468678, acc 1
2017-03-02T23:44:30.396358: step 6591, loss 0.000516808, acc 1
2017-03-02T23:44:30.492443: step 6592, loss 0.000731264, acc 1
2017-03-02T23:44:30.586011: step 6593, loss 0.00149674, acc 1
2017-03-02T23:44:30.673789: step 6594, loss 0.000436933, acc 1
2017-03-02T23:44:30.778460: step 6595, loss 0.000567658, acc 1
2017-03-02T23:44:30.883675: step 6596, loss 0.00050212, acc 1
2017-03-02T23:44:30.993856: step 6597, loss 0.000175199, acc 1
2017-03-02T23:44:31.098767: step 6598, loss 0.00378786, acc 1
2017-03-02T23:44:31.210274: step 6599, loss 0.00183722, acc 1
2017-03-02T23:44:31.304616: step 6600, loss 0.00656756, acc 1

Evaluation:
2017-03-02T23:44:31.359170: step 6600, loss 2.79469, acc 0.50519

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6600

2017-03-02T23:44:31.803026: step 6601, loss 0.000105883, acc 1
2017-03-02T23:44:31.920619: step 6602, loss 0.000584402, acc 1
2017-03-02T23:44:32.016019: step 6603, loss 0.00143623, acc 1
2017-03-02T23:44:32.102720: step 6604, loss 0.000438788, acc 1
2017-03-02T23:44:32.204823: step 6605, loss 0.000826176, acc 1
2017-03-02T23:44:32.311226: step 6606, loss 0.00327739, acc 1
2017-03-02T23:44:32.418730: step 6607, loss 0.000411681, acc 1
2017-03-02T23:44:32.526693: step 6608, loss 0.00182439, acc 1
2017-03-02T23:44:32.629762: step 6609, loss 0.00219333, acc 1
2017-03-02T23:44:32.732606: step 6610, loss 0.000605413, acc 1
2017-03-02T23:44:32.825041: step 6611, loss 0.000611602, acc 1
2017-03-02T23:44:32.925045: step 6612, loss 0.000270465, acc 1
2017-03-02T23:44:33.041152: step 6613, loss 0.000413522, acc 1
2017-03-02T23:44:33.148425: step 6614, loss 0.000338806, acc 1
2017-03-02T23:44:33.245258: step 6615, loss 0.00199659, acc 1
2017-03-02T23:44:33.355045: step 6616, loss 0.0035978, acc 1
2017-03-02T23:44:33.470784: step 6617, loss 0.102048, acc 0.984375
2017-03-02T23:44:33.556854: step 6618, loss 0.00114621, acc 1
2017-03-02T23:44:33.654446: step 6619, loss 0.000103255, acc 1
2017-03-02T23:44:33.758730: step 6620, loss 0.00235831, acc 1
2017-03-02T23:44:33.866008: step 6621, loss 0.0211993, acc 0.984375
2017-03-02T23:44:33.970968: step 6622, loss 0.000121956, acc 1
2017-03-02T23:44:34.074200: step 6623, loss 0.000863717, acc 1
2017-03-02T23:44:34.175129: step 6624, loss 0.00528449, acc 1
2017-03-02T23:44:34.272038: step 6625, loss 0.00070265, acc 1
2017-03-02T23:44:34.358297: step 6626, loss 0.00167159, acc 1
2017-03-02T23:44:34.463161: step 6627, loss 0.002071, acc 1
2017-03-02T23:44:34.568531: step 6628, loss 0.000804084, acc 1
2017-03-02T23:44:34.677907: step 6629, loss 0.000267996, acc 1
2017-03-02T23:44:34.783206: step 6630, loss 0.00122763, acc 1
2017-03-02T23:44:34.888978: step 6631, loss 0.00753276, acc 1
2017-03-02T23:44:34.994001: step 6632, loss 0.0207113, acc 0.984375
2017-03-02T23:44:35.085240: step 6633, loss 0.00024776, acc 1
2017-03-02T23:44:35.189221: step 6634, loss 0.000642831, acc 1
2017-03-02T23:44:35.290942: step 6635, loss 0.000566693, acc 1
2017-03-02T23:44:35.397078: step 6636, loss 0.000199741, acc 1
2017-03-02T23:44:35.493493: step 6637, loss 0.0170766, acc 0.984375
2017-03-02T23:44:35.615225: step 6638, loss 0.0255726, acc 0.984375
2017-03-02T23:44:35.722161: step 6639, loss 0.00020895, acc 1
2017-03-02T23:44:35.812070: step 6640, loss 0.000804714, acc 1
2017-03-02T23:44:35.912552: step 6641, loss 0.00480477, acc 1
2017-03-02T23:44:36.007460: step 6642, loss 0.000233073, acc 1
2017-03-02T23:44:36.116564: step 6643, loss 0.000484071, acc 1
2017-03-02T23:44:36.227296: step 6644, loss 0.000890045, acc 1
2017-03-02T23:44:36.337170: step 6645, loss 0.000769522, acc 1
2017-03-02T23:44:36.453571: step 6646, loss 0.00420445, acc 1
2017-03-02T23:44:36.544854: step 6647, loss 0.000302789, acc 1
2017-03-02T23:44:36.657022: step 6648, loss 0.00394694, acc 1
2017-03-02T23:44:36.768847: step 6649, loss 0.00022095, acc 1
2017-03-02T23:44:36.872968: step 6650, loss 0.000676188, acc 1
2017-03-02T23:44:36.976839: step 6651, loss 0.000467462, acc 1
2017-03-02T23:44:37.087581: step 6652, loss 0.00345458, acc 1
2017-03-02T23:44:37.191441: step 6653, loss 0.00015197, acc 1
2017-03-02T23:44:37.283364: step 6654, loss 0.000357497, acc 1
2017-03-02T23:44:37.377266: step 6655, loss 0.00106327, acc 1
2017-03-02T23:44:37.493728: step 6656, loss 0.00025999, acc 1
2017-03-02T23:44:37.603677: step 6657, loss 0.000101253, acc 1
2017-03-02T23:44:37.712834: step 6658, loss 0.00180815, acc 1
2017-03-02T23:44:37.821159: step 6659, loss 0.000262979, acc 1
2017-03-02T23:44:37.926293: step 6660, loss 0.000378265, acc 1
2017-03-02T23:44:38.027900: step 6661, loss 0.000395871, acc 1
2017-03-02T23:44:38.122284: step 6662, loss 0.000341852, acc 1
2017-03-02T23:44:38.231985: step 6663, loss 0.00381962, acc 1
2017-03-02T23:44:38.337512: step 6664, loss 0.00106289, acc 1
2017-03-02T23:44:38.444971: step 6665, loss 4.16128e-05, acc 1
2017-03-02T23:44:38.569144: step 6666, loss 0.000375001, acc 1
2017-03-02T23:44:38.671505: step 6667, loss 0.000212959, acc 1
2017-03-02T23:44:38.774754: step 6668, loss 0.000164643, acc 1
2017-03-02T23:44:38.863964: step 6669, loss 0.000621034, acc 1
2017-03-02T23:44:38.964799: step 6670, loss 0.000153075, acc 1
2017-03-02T23:44:39.069438: step 6671, loss 0.000480957, acc 1
2017-03-02T23:44:39.189715: step 6672, loss 0.00326831, acc 1
2017-03-02T23:44:39.293718: step 6673, loss 0.00374309, acc 1
2017-03-02T23:44:39.398135: step 6674, loss 0.00478316, acc 1
2017-03-02T23:44:39.510767: step 6675, loss 0.00030033, acc 1
2017-03-02T23:44:39.603227: step 6676, loss 0.000131976, acc 1
2017-03-02T23:44:39.699254: step 6677, loss 0.000143186, acc 1
2017-03-02T23:44:39.819603: step 6678, loss 0.00044804, acc 1
2017-03-02T23:44:39.925444: step 6679, loss 0.000254128, acc 1
2017-03-02T23:44:40.028501: step 6680, loss 0.00213277, acc 1
2017-03-02T23:44:40.135792: step 6681, loss 0.0100838, acc 1
2017-03-02T23:44:40.244974: step 6682, loss 0.0180255, acc 0.984375
2017-03-02T23:44:40.334337: step 6683, loss 0.000356614, acc 1
2017-03-02T23:44:40.426622: step 6684, loss 0.00370125, acc 1
2017-03-02T23:44:40.533308: step 6685, loss 0.000906121, acc 1
2017-03-02T23:44:40.643430: step 6686, loss 0.000101642, acc 1
2017-03-02T23:44:40.751977: step 6687, loss 0.000463389, acc 1
2017-03-02T23:44:40.860183: step 6688, loss 0.00286381, acc 1
2017-03-02T23:44:40.955204: step 6689, loss 0.00046166, acc 1
2017-03-02T23:44:41.067160: step 6690, loss 8.4561e-05, acc 1
2017-03-02T23:44:41.155792: step 6691, loss 0.000507842, acc 1
2017-03-02T23:44:41.255561: step 6692, loss 0.00152358, acc 1
2017-03-02T23:44:41.363915: step 6693, loss 0.00167841, acc 1
2017-03-02T23:44:41.459675: step 6694, loss 0.00156915, acc 1
2017-03-02T23:44:41.566861: step 6695, loss 0.000539738, acc 1
2017-03-02T23:44:41.675905: step 6696, loss 0.000693888, acc 1
2017-03-02T23:44:41.780299: step 6697, loss 0.00108837, acc 1
2017-03-02T23:44:41.870844: step 6698, loss 0.000333624, acc 1
2017-03-02T23:44:41.963595: step 6699, loss 0.000298717, acc 1
2017-03-02T23:44:42.058177: step 6700, loss 0.00310683, acc 1

Evaluation:
2017-03-02T23:44:42.110631: step 6700, loss 3.32986, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6700

2017-03-02T23:44:42.561157: step 6701, loss 0.00190617, acc 1
2017-03-02T23:44:42.660707: step 6702, loss 0.000582071, acc 1
2017-03-02T23:44:42.766802: step 6703, loss 0.00282772, acc 1
2017-03-02T23:44:42.870980: step 6704, loss 0.00017327, acc 1
2017-03-02T23:44:42.977966: step 6705, loss 0.00920695, acc 1
2017-03-02T23:44:43.082221: step 6706, loss 0.000322277, acc 1
2017-03-02T23:44:43.187994: step 6707, loss 0.000221866, acc 1
2017-03-02T23:44:43.280996: step 6708, loss 0.000249776, acc 1
2017-03-02T23:44:43.379005: step 6709, loss 0.00024438, acc 1
2017-03-02T23:44:43.482845: step 6710, loss 0.00286303, acc 1
2017-03-02T23:44:43.592423: step 6711, loss 0.00116733, acc 1
2017-03-02T23:44:43.699901: step 6712, loss 0.00164172, acc 1
2017-03-02T23:44:43.806116: step 6713, loss 0.00014994, acc 1
2017-03-02T23:44:43.915915: step 6714, loss 0.000285034, acc 1
2017-03-02T23:44:44.030189: step 6715, loss 0.0121098, acc 0.984375
2017-03-02T23:44:44.123489: step 6716, loss 0.000899324, acc 1
2017-03-02T23:44:44.233994: step 6717, loss 0.000352153, acc 1
2017-03-02T23:44:44.343265: step 6718, loss 0.00185284, acc 1
2017-03-02T23:44:44.459714: step 6719, loss 0.000674837, acc 1
2017-03-02T23:44:44.565804: step 6720, loss 0.000257654, acc 1
2017-03-02T23:44:44.671527: step 6721, loss 0.000951016, acc 1
2017-03-02T23:44:44.780655: step 6722, loss 0.000517712, acc 1
2017-03-02T23:44:44.877227: step 6723, loss 0.00171116, acc 1
2017-03-02T23:44:44.969680: step 6724, loss 0.00607396, acc 1
2017-03-02T23:44:45.085874: step 6725, loss 0.000755827, acc 1
2017-03-02T23:44:45.196553: step 6726, loss 0.000669289, acc 1
2017-03-02T23:44:45.303855: step 6727, loss 0.000689316, acc 1
2017-03-02T23:44:45.406274: step 6728, loss 0.000672447, acc 1
2017-03-02T23:44:45.523653: step 6729, loss 0.000732707, acc 1
2017-03-02T23:44:45.612677: step 6730, loss 0.00168335, acc 1
2017-03-02T23:44:45.709168: step 6731, loss 0.000931324, acc 1
2017-03-02T23:44:45.829854: step 6732, loss 0.000853804, acc 1
2017-03-02T23:44:45.933111: step 6733, loss 0.0130774, acc 0.984375
2017-03-02T23:44:46.041529: step 6734, loss 0.000311212, acc 1
2017-03-02T23:44:46.143498: step 6735, loss 0.000447198, acc 1
2017-03-02T23:44:46.243895: step 6736, loss 0.000533669, acc 1
2017-03-02T23:44:46.334248: step 6737, loss 0.000341612, acc 1
2017-03-02T23:44:46.429287: step 6738, loss 0.0013461, acc 1
2017-03-02T23:44:46.534953: step 6739, loss 0.000529911, acc 1
2017-03-02T23:44:46.640619: step 6740, loss 0.00102054, acc 1
2017-03-02T23:44:46.748712: step 6741, loss 0.00109814, acc 1
2017-03-02T23:44:46.855981: step 6742, loss 0.00041637, acc 1
2017-03-02T23:44:46.961417: step 6743, loss 0.000795044, acc 1
2017-03-02T23:44:47.068057: step 6744, loss 0.00344544, acc 1
2017-03-02T23:44:47.168678: step 6745, loss 0.0011655, acc 1
2017-03-02T23:44:47.278660: step 6746, loss 0.000701926, acc 1
2017-03-02T23:44:47.385512: step 6747, loss 0.00132746, acc 1
2017-03-02T23:44:47.498091: step 6748, loss 0.000663281, acc 1
2017-03-02T23:44:47.615096: step 6749, loss 0.0010187, acc 1
2017-03-02T23:44:47.731823: step 6750, loss 0.00142356, acc 1
2017-03-02T23:44:47.820221: step 6751, loss 0.00123688, acc 1
2017-03-02T23:44:47.920139: step 6752, loss 0.000486446, acc 1
2017-03-02T23:44:48.029223: step 6753, loss 0.00143961, acc 1
2017-03-02T23:44:48.127539: step 6754, loss 0.000876018, acc 1
2017-03-02T23:44:48.228614: step 6755, loss 0.00357587, acc 1
2017-03-02T23:44:48.332720: step 6756, loss 0.000555388, acc 1
2017-03-02T23:44:48.436976: step 6757, loss 0.00112857, acc 1
2017-03-02T23:44:48.535343: step 6758, loss 0.00108826, acc 1
2017-03-02T23:44:48.629460: step 6759, loss 0.000224484, acc 1
2017-03-02T23:44:48.734502: step 6760, loss 0.00262297, acc 1
2017-03-02T23:44:48.835846: step 6761, loss 0.000210404, acc 1
2017-03-02T23:44:48.950220: step 6762, loss 0.000337295, acc 1
2017-03-02T23:44:49.054758: step 6763, loss 0.000533606, acc 1
2017-03-02T23:44:49.159382: step 6764, loss 0.000433796, acc 1
2017-03-02T23:44:49.256713: step 6765, loss 0.000270233, acc 1
2017-03-02T23:44:49.344701: step 6766, loss 0.000346877, acc 1
2017-03-02T23:44:49.436192: step 6767, loss 0.000117119, acc 1
2017-03-02T23:44:49.541691: step 6768, loss 0.000241247, acc 1
2017-03-02T23:44:49.650247: step 6769, loss 9.69125e-05, acc 1
2017-03-02T23:44:49.754494: step 6770, loss 0.00059093, acc 1
2017-03-02T23:44:49.863469: step 6771, loss 0.000918003, acc 1
2017-03-02T23:44:49.964442: step 6772, loss 9.58104e-05, acc 1
2017-03-02T23:44:50.073596: step 6773, loss 0.001224, acc 1
2017-03-02T23:44:50.171614: step 6774, loss 0.000542151, acc 1
2017-03-02T23:44:50.273027: step 6775, loss 0.000499354, acc 1
2017-03-02T23:44:50.380396: step 6776, loss 0.00214495, acc 1
2017-03-02T23:44:50.487924: step 6777, loss 0.000729853, acc 1
2017-03-02T23:44:50.590613: step 6778, loss 0.00041508, acc 1
2017-03-02T23:44:50.695615: step 6779, loss 0.00018349, acc 1
2017-03-02T23:44:50.803634: step 6780, loss 0.000990135, acc 1
2017-03-02T23:44:50.896780: step 6781, loss 0.000357328, acc 1
2017-03-02T23:44:51.014492: step 6782, loss 0.00251499, acc 1
2017-03-02T23:44:51.119129: step 6783, loss 0.000148909, acc 1
2017-03-02T23:44:51.223857: step 6784, loss 0.00216223, acc 1
2017-03-02T23:44:51.331804: step 6785, loss 0.00101724, acc 1
2017-03-02T23:44:51.442772: step 6786, loss 0.0938191, acc 0.984375
2017-03-02T23:44:51.560943: step 6787, loss 0.000548915, acc 1
2017-03-02T23:44:51.653841: step 6788, loss 0.00438411, acc 1
2017-03-02T23:44:51.753800: step 6789, loss 0.000361609, acc 1
2017-03-02T23:44:51.851838: step 6790, loss 0.000282721, acc 1
2017-03-02T23:44:51.959054: step 6791, loss 0.00145674, acc 1
2017-03-02T23:44:52.070208: step 6792, loss 0.00120808, acc 1
2017-03-02T23:44:52.181940: step 6793, loss 0.00146093, acc 1
2017-03-02T23:44:52.289268: step 6794, loss 0.00232282, acc 1
2017-03-02T23:44:52.381661: step 6795, loss 0.00152704, acc 1
2017-03-02T23:44:52.498543: step 6796, loss 0.00812467, acc 1
2017-03-02T23:44:52.605161: step 6797, loss 0.00338224, acc 1
2017-03-02T23:44:52.711968: step 6798, loss 0.00765842, acc 1
2017-03-02T23:44:52.814598: step 6799, loss 0.00921504, acc 1
2017-03-02T23:44:52.916561: step 6800, loss 0.000415209, acc 1

Evaluation:
2017-03-02T23:44:52.969244: step 6800, loss 2.86427, acc 0.50519

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6800

2017-03-02T23:44:53.441846: step 6801, loss 0.000438349, acc 1
2017-03-02T23:44:53.546414: step 6802, loss 0.000844351, acc 1
2017-03-02T23:44:53.650795: step 6803, loss 0.00348419, acc 1
2017-03-02T23:44:53.759128: step 6804, loss 0.000188568, acc 1
2017-03-02T23:44:53.850791: step 6805, loss 0.000204228, acc 1
2017-03-02T23:44:53.945036: step 6806, loss 0.000240104, acc 1
2017-03-02T23:44:54.068335: step 6807, loss 0.000490349, acc 1
2017-03-02T23:44:54.175551: step 6808, loss 0.000417694, acc 1
2017-03-02T23:44:54.281254: step 6809, loss 0.000328956, acc 1
2017-03-02T23:44:54.396450: step 6810, loss 0.000458395, acc 1
2017-03-02T23:44:54.503490: step 6811, loss 0.00220593, acc 1
2017-03-02T23:44:54.583540: step 6812, loss 0.00108387, acc 1
2017-03-02T23:44:54.686385: step 6813, loss 0.00151881, acc 1
2017-03-02T23:44:54.796381: step 6814, loss 0.000633077, acc 1
2017-03-02T23:44:54.895766: step 6815, loss 0.000446852, acc 1
2017-03-02T23:44:55.000569: step 6816, loss 0.000358017, acc 1
2017-03-02T23:44:55.097584: step 6817, loss 0.000397395, acc 1
2017-03-02T23:44:55.205506: step 6818, loss 9.2295e-05, acc 1
2017-03-02T23:44:55.317802: step 6819, loss 0.000432933, acc 1
2017-03-02T23:44:55.406067: step 6820, loss 0.0036662, acc 1
2017-03-02T23:44:55.509562: step 6821, loss 5.6197e-05, acc 1
2017-03-02T23:44:55.614816: step 6822, loss 0.00105379, acc 1
2017-03-02T23:44:55.722155: step 6823, loss 0.00783165, acc 1
2017-03-02T23:44:55.832538: step 6824, loss 0.000867464, acc 1
2017-03-02T23:44:55.946122: step 6825, loss 0.000332727, acc 1
2017-03-02T23:44:56.054216: step 6826, loss 0.00133328, acc 1
2017-03-02T23:44:56.147716: step 6827, loss 0.00223963, acc 1
2017-03-02T23:44:56.267797: step 6828, loss 0.00421744, acc 1
2017-03-02T23:44:56.371737: step 6829, loss 0.000361959, acc 1
2017-03-02T23:44:56.479228: step 6830, loss 0.0018448, acc 1
2017-03-02T23:44:56.585782: step 6831, loss 0.000223026, acc 1
2017-03-02T23:44:56.695492: step 6832, loss 0.0012145, acc 1
2017-03-02T23:44:56.802813: step 6833, loss 0.000199954, acc 1
2017-03-02T23:44:56.893718: step 6834, loss 0.00429438, acc 1
2017-03-02T23:44:57.000238: step 6835, loss 0.000220198, acc 1
2017-03-02T23:44:57.105951: step 6836, loss 0.000242109, acc 1
2017-03-02T23:44:57.212833: step 6837, loss 0.00315154, acc 1
2017-03-02T23:44:57.318182: step 6838, loss 0.000285167, acc 1
2017-03-02T23:44:57.417866: step 6839, loss 0.000957704, acc 1
2017-03-02T23:44:57.518902: step 6840, loss 0.000508653, acc 1
2017-03-02T23:44:57.605207: step 6841, loss 0.000840648, acc 1
2017-03-02T23:44:57.693206: step 6842, loss 0.00301291, acc 1
2017-03-02T23:44:57.807377: step 6843, loss 0.00180016, acc 1
2017-03-02T23:44:57.918240: step 6844, loss 0.00233959, acc 1
2017-03-02T23:44:58.025121: step 6845, loss 0.000416723, acc 1
2017-03-02T23:44:58.133447: step 6846, loss 0.000176875, acc 1
2017-03-02T23:44:58.227833: step 6847, loss 0.000127569, acc 1
2017-03-02T23:44:58.335118: step 6848, loss 0.000696864, acc 1
2017-03-02T23:44:58.425239: step 6849, loss 0.000773555, acc 1
2017-03-02T23:44:58.529249: step 6850, loss 6.3332e-05, acc 1
2017-03-02T23:44:58.627953: step 6851, loss 0.000559411, acc 1
2017-03-02T23:44:58.739190: step 6852, loss 0.000434556, acc 1
2017-03-02T23:44:58.837146: step 6853, loss 0.000440641, acc 1
2017-03-02T23:44:58.943329: step 6854, loss 0.00578344, acc 1
2017-03-02T23:44:59.050856: step 6855, loss 0.000221183, acc 1
2017-03-02T23:44:59.145785: step 6856, loss 0.000653179, acc 1
2017-03-02T23:44:59.244055: step 6857, loss 0.000200184, acc 1
2017-03-02T23:44:59.347785: step 6858, loss 0.000414314, acc 1
2017-03-02T23:44:59.454553: step 6859, loss 0.0253577, acc 0.984375
2017-03-02T23:44:59.557881: step 6860, loss 0.000222395, acc 1
2017-03-02T23:44:59.661507: step 6861, loss 0.000151025, acc 1
2017-03-02T23:44:59.765288: step 6862, loss 0.00047131, acc 1
2017-03-02T23:44:59.855179: step 6863, loss 0.00086704, acc 1
2017-03-02T23:44:59.945831: step 6864, loss 0.00072388, acc 1
2017-03-02T23:45:00.054362: step 6865, loss 0.000160761, acc 1
2017-03-02T23:45:00.173924: step 6866, loss 0.00064709, acc 1
2017-03-02T23:45:00.278372: step 6867, loss 0.000123627, acc 1
2017-03-02T23:45:00.391916: step 6868, loss 0.000255391, acc 1
2017-03-02T23:45:00.505803: step 6869, loss 0.000298253, acc 1
2017-03-02T23:45:00.601770: step 6870, loss 0.000308475, acc 1
2017-03-02T23:45:00.692732: step 6871, loss 0.00145295, acc 1
2017-03-02T23:45:00.793237: step 6872, loss 0.000147714, acc 1
2017-03-02T23:45:00.909858: step 6873, loss 0.000733755, acc 1
2017-03-02T23:45:01.015766: step 6874, loss 0.000823826, acc 1
2017-03-02T23:45:01.129220: step 6875, loss 0.00127852, acc 1
2017-03-02T23:45:01.231247: step 6876, loss 0.00076073, acc 1
2017-03-02T23:45:01.356611: step 6877, loss 0.000757748, acc 1
2017-03-02T23:45:01.444903: step 6878, loss 0.000496374, acc 1
2017-03-02T23:45:01.563767: step 6879, loss 5.49641e-05, acc 1
2017-03-02T23:45:01.661334: step 6880, loss 0.000643273, acc 1
2017-03-02T23:45:01.760894: step 6881, loss 0.00386644, acc 1
2017-03-02T23:45:01.865388: step 6882, loss 0.000639256, acc 1
2017-03-02T23:45:01.972021: step 6883, loss 0.000487853, acc 1
2017-03-02T23:45:02.078678: step 6884, loss 9.88618e-05, acc 1
2017-03-02T23:45:02.172298: step 6885, loss 0.000275952, acc 1
2017-03-02T23:45:02.283092: step 6886, loss 0.000103865, acc 1
2017-03-02T23:45:02.393065: step 6887, loss 0.000451846, acc 1
2017-03-02T23:45:02.487137: step 6888, loss 0.000395064, acc 1
2017-03-02T23:45:02.590986: step 6889, loss 9.16187e-05, acc 1
2017-03-02T23:45:02.699674: step 6890, loss 0.016722, acc 0.984375
2017-03-02T23:45:02.805285: step 6891, loss 0.000388459, acc 1
2017-03-02T23:45:02.900020: step 6892, loss 0.000529781, acc 1
2017-03-02T23:45:03.006795: step 6893, loss 0.00231019, acc 1
2017-03-02T23:45:03.107173: step 6894, loss 0.000193695, acc 1
2017-03-02T23:45:03.211621: step 6895, loss 0.00180059, acc 1
2017-03-02T23:45:03.317686: step 6896, loss 0.000938791, acc 1
2017-03-02T23:45:03.417241: step 6897, loss 0.000641551, acc 1
2017-03-02T23:45:03.512732: step 6898, loss 0.000176363, acc 1
2017-03-02T23:45:03.629529: step 6899, loss 0.00045034, acc 1
2017-03-02T23:45:03.717398: step 6900, loss 0.00132848, acc 1

Evaluation:
2017-03-02T23:45:03.772682: step 6900, loss 3.4976, acc 0.543253

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-6900

2017-03-02T23:45:04.230669: step 6901, loss 9.44218e-05, acc 1
2017-03-02T23:45:04.321671: step 6902, loss 0.00523431, acc 1
2017-03-02T23:45:04.420639: step 6903, loss 0.00119263, acc 1
2017-03-02T23:45:04.540045: step 6904, loss 0.00553123, acc 1
2017-03-02T23:45:04.651459: step 6905, loss 0.000447152, acc 1
2017-03-02T23:45:04.760091: step 6906, loss 0.000224226, acc 1
2017-03-02T23:45:04.866608: step 6907, loss 0.000113383, acc 1
2017-03-02T23:45:04.974568: step 6908, loss 0.00258343, acc 1
2017-03-02T23:45:05.068597: step 6909, loss 0.00712832, acc 1
2017-03-02T23:45:05.165942: step 6910, loss 0.000367932, acc 1
2017-03-02T23:45:05.292962: step 6911, loss 0.000218252, acc 1
2017-03-02T23:45:05.395714: step 6912, loss 0.0020927, acc 1
2017-03-02T23:45:05.496223: step 6913, loss 0.000845755, acc 1
2017-03-02T23:45:05.603863: step 6914, loss 0.00222611, acc 1
2017-03-02T23:45:05.714317: step 6915, loss 0.000386828, acc 1
2017-03-02T23:45:05.803759: step 6916, loss 0.000334937, acc 1
2017-03-02T23:45:05.892867: step 6917, loss 0.000490588, acc 1
2017-03-02T23:45:05.998429: step 6918, loss 0.00443134, acc 1
2017-03-02T23:45:06.087390: step 6919, loss 0.000980612, acc 1
2017-03-02T23:45:06.188852: step 6920, loss 0.000108649, acc 1
2017-03-02T23:45:06.288894: step 6921, loss 0.00205082, acc 1
2017-03-02T23:45:06.392385: step 6922, loss 0.00130589, acc 1
2017-03-02T23:45:06.499752: step 6923, loss 0.000403627, acc 1
2017-03-02T23:45:06.593868: step 6924, loss 0.00205726, acc 1
2017-03-02T23:45:06.690831: step 6925, loss 0.00128933, acc 1
2017-03-02T23:45:06.792114: step 6926, loss 0.00210427, acc 1
2017-03-02T23:45:06.895946: step 6927, loss 9.81546e-05, acc 1
2017-03-02T23:45:07.003245: step 6928, loss 0.000892532, acc 1
2017-03-02T23:45:07.100094: step 6929, loss 0.000493803, acc 1
2017-03-02T23:45:07.206843: step 6930, loss 0.0486459, acc 0.984375
2017-03-02T23:45:07.315005: step 6931, loss 0.00108068, acc 1
2017-03-02T23:45:07.406311: step 6932, loss 0.00523428, acc 1
2017-03-02T23:45:07.508946: step 6933, loss 0.000156051, acc 1
2017-03-02T23:45:07.615728: step 6934, loss 0.000976308, acc 1
2017-03-02T23:45:07.717111: step 6935, loss 0.000922711, acc 1
2017-03-02T23:45:07.825792: step 6936, loss 0.00148861, acc 1
2017-03-02T23:45:07.929941: step 6937, loss 0.000750362, acc 1
2017-03-02T23:45:08.034944: step 6938, loss 0.00461692, acc 1
2017-03-02T23:45:08.134326: step 6939, loss 0.000593441, acc 1
2017-03-02T23:45:08.231467: step 6940, loss 0.00273339, acc 1
2017-03-02T23:45:08.335053: step 6941, loss 0.000495367, acc 1
2017-03-02T23:45:08.437464: step 6942, loss 0.000266974, acc 1
2017-03-02T23:45:08.542086: step 6943, loss 0.000556442, acc 1
2017-03-02T23:45:08.646620: step 6944, loss 0.00103798, acc 1
2017-03-02T23:45:08.746789: step 6945, loss 0.00118533, acc 1
2017-03-02T23:45:08.857802: step 6946, loss 0.0154285, acc 0.984375
2017-03-02T23:45:08.947684: step 6947, loss 0.000324402, acc 1
2017-03-02T23:45:09.053317: step 6948, loss 0.000735187, acc 1
2017-03-02T23:45:09.153604: step 6949, loss 0.000206778, acc 1
2017-03-02T23:45:09.262125: step 6950, loss 0.00922963, acc 1
2017-03-02T23:45:09.372400: step 6951, loss 0.00561716, acc 1
2017-03-02T23:45:09.475087: step 6952, loss 0.00883776, acc 1
2017-03-02T23:45:09.589086: step 6953, loss 0.000811854, acc 1
2017-03-02T23:45:09.675536: step 6954, loss 0.00136278, acc 1
2017-03-02T23:45:09.772419: step 6955, loss 0.000671833, acc 1
2017-03-02T23:45:09.883642: step 6956, loss 0.000875443, acc 1
2017-03-02T23:45:09.989114: step 6957, loss 0.136534, acc 0.984375
2017-03-02T23:45:10.107546: step 6958, loss 0.052926, acc 0.984375
2017-03-02T23:45:10.204413: step 6959, loss 0.00214203, acc 1
2017-03-02T23:45:10.315879: step 6960, loss 0.00049363, acc 1
2017-03-02T23:45:10.413289: step 6961, loss 0.00608248, acc 1
2017-03-02T23:45:10.509153: step 6962, loss 0.00181062, acc 1
2017-03-02T23:45:10.621783: step 6963, loss 0.00443427, acc 1
2017-03-02T23:45:10.727175: step 6964, loss 0.00269594, acc 1
2017-03-02T23:45:10.836848: step 6965, loss 0.00377497, acc 1
2017-03-02T23:45:10.943415: step 6966, loss 0.00862812, acc 1
2017-03-02T23:45:11.048572: step 6967, loss 0.00120532, acc 1
2017-03-02T23:45:11.144954: step 6968, loss 0.00742245, acc 1
2017-03-02T23:45:11.230229: step 6969, loss 0.00779453, acc 1
2017-03-02T23:45:11.343729: step 6970, loss 0.000461167, acc 1
2017-03-02T23:45:11.453762: step 6971, loss 0.000864638, acc 1
2017-03-02T23:45:11.575198: step 6972, loss 0.000282609, acc 1
2017-03-02T23:45:11.682403: step 6973, loss 0.00100704, acc 1
2017-03-02T23:45:11.776503: step 6974, loss 0.00035882, acc 1
2017-03-02T23:45:11.872014: step 6975, loss 0.000162008, acc 1
2017-03-02T23:45:11.961151: step 6976, loss 0.000251999, acc 1
2017-03-02T23:45:12.064009: step 6977, loss 0.00201239, acc 1
2017-03-02T23:45:12.164641: step 6978, loss 0.00041533, acc 1
2017-03-02T23:45:12.267273: step 6979, loss 0.000220672, acc 1
2017-03-02T23:45:12.373899: step 6980, loss 0.00336433, acc 1
2017-03-02T23:45:12.487148: step 6981, loss 0.00071243, acc 1
2017-03-02T23:45:12.603752: step 6982, loss 0.000763917, acc 1
2017-03-02T23:45:12.699718: step 6983, loss 0.000193709, acc 1
2017-03-02T23:45:12.800914: step 6984, loss 8.02999e-05, acc 1
2017-03-02T23:45:12.907589: step 6985, loss 0.000801996, acc 1
2017-03-02T23:45:13.004528: step 6986, loss 0.00860557, acc 1
2017-03-02T23:45:13.114485: step 6987, loss 0.00606814, acc 1
2017-03-02T23:45:13.220113: step 6988, loss 0.00026088, acc 1
2017-03-02T23:45:13.335198: step 6989, loss 0.000424605, acc 1
2017-03-02T23:45:13.438149: step 6990, loss 0.000446797, acc 1
2017-03-02T23:45:13.537257: step 6991, loss 0.000653908, acc 1
2017-03-02T23:45:13.639545: step 6992, loss 0.000105537, acc 1
2017-03-02T23:45:13.738859: step 6993, loss 0.00125537, acc 1
2017-03-02T23:45:13.843200: step 6994, loss 0.00247825, acc 1
2017-03-02T23:45:13.944374: step 6995, loss 0.000209421, acc 1
2017-03-02T23:45:14.054016: step 6996, loss 0.000752478, acc 1
2017-03-02T23:45:14.147167: step 6997, loss 0.000773825, acc 1
2017-03-02T23:45:14.235743: step 6998, loss 0.00421617, acc 1
2017-03-02T23:45:14.342919: step 6999, loss 0.000430936, acc 1
2017-03-02T23:45:14.442595: step 7000, loss 0.0018325, acc 1

Evaluation:
2017-03-02T23:45:14.493816: step 7000, loss 3.08165, acc 0.512111

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7000

2017-03-02T23:45:14.933808: step 7001, loss 0.00244199, acc 1
2017-03-02T23:45:15.040285: step 7002, loss 0.00174694, acc 1
2017-03-02T23:45:15.152687: step 7003, loss 0.000961026, acc 1
2017-03-02T23:45:15.261843: step 7004, loss 0.00121187, acc 1
2017-03-02T23:45:15.373919: step 7005, loss 0.00031538, acc 1
2017-03-02T23:45:15.482987: step 7006, loss 0.000592856, acc 1
2017-03-02T23:45:15.585148: step 7007, loss 0.000280105, acc 1
2017-03-02T23:45:15.678517: step 7008, loss 0.00530372, acc 1
2017-03-02T23:45:15.789770: step 7009, loss 0.000161034, acc 1
2017-03-02T23:45:15.896093: step 7010, loss 0.000612627, acc 1
2017-03-02T23:45:16.003593: step 7011, loss 0.000807093, acc 1
2017-03-02T23:45:16.111256: step 7012, loss 0.000186123, acc 1
2017-03-02T23:45:16.231700: step 7013, loss 0.000801071, acc 1
2017-03-02T23:45:16.339514: step 7014, loss 0.00198399, acc 1
2017-03-02T23:45:16.438358: step 7015, loss 0.000831061, acc 1
2017-03-02T23:45:16.538573: step 7016, loss 0.000166195, acc 1
2017-03-02T23:45:16.641791: step 7017, loss 0.000405488, acc 1
2017-03-02T23:45:16.746999: step 7018, loss 0.000166552, acc 1
2017-03-02T23:45:16.851920: step 7019, loss 0.00289179, acc 1
2017-03-02T23:45:16.960948: step 7020, loss 0.00167637, acc 1
2017-03-02T23:45:17.078601: step 7021, loss 0.000618245, acc 1
2017-03-02T23:45:17.173160: step 7022, loss 0.00699346, acc 1
2017-03-02T23:45:17.286640: step 7023, loss 0.000966832, acc 1
2017-03-02T23:45:17.393825: step 7024, loss 0.000685118, acc 1
2017-03-02T23:45:17.499678: step 7025, loss 0.000837686, acc 1
2017-03-02T23:45:17.597785: step 7026, loss 0.000768213, acc 1
2017-03-02T23:45:17.706095: step 7027, loss 0.000914912, acc 1
2017-03-02T23:45:17.812158: step 7028, loss 0.000289616, acc 1
2017-03-02T23:45:17.901009: step 7029, loss 0.000143949, acc 1
2017-03-02T23:45:18.007802: step 7030, loss 0.00146604, acc 1
2017-03-02T23:45:18.117244: step 7031, loss 0.000452097, acc 1
2017-03-02T23:45:18.217478: step 7032, loss 3.95693e-05, acc 1
2017-03-02T23:45:18.324967: step 7033, loss 0.000491217, acc 1
2017-03-02T23:45:18.428598: step 7034, loss 0.00202469, acc 1
2017-03-02T23:45:18.535311: step 7035, loss 0.00122168, acc 1
2017-03-02T23:45:18.623017: step 7036, loss 0.000604641, acc 1
2017-03-02T23:45:18.724488: step 7037, loss 0.000798948, acc 1
2017-03-02T23:45:18.827615: step 7038, loss 0.000122327, acc 1
2017-03-02T23:45:18.932444: step 7039, loss 0.00279429, acc 1
2017-03-02T23:45:19.033302: step 7040, loss 0.000132908, acc 1
2017-03-02T23:45:19.137742: step 7041, loss 0.000286076, acc 1
2017-03-02T23:45:19.248566: step 7042, loss 0.00125256, acc 1
2017-03-02T23:45:19.349327: step 7043, loss 0.0004226, acc 1
2017-03-02T23:45:19.438801: step 7044, loss 0.000420386, acc 1
2017-03-02T23:45:19.544197: step 7045, loss 0.000151229, acc 1
2017-03-02T23:45:19.649093: step 7046, loss 0.000362451, acc 1
2017-03-02T23:45:19.751810: step 7047, loss 0.00131265, acc 1
2017-03-02T23:45:19.854979: step 7048, loss 0.00019211, acc 1
2017-03-02T23:45:19.954617: step 7049, loss 0.000706044, acc 1
2017-03-02T23:45:20.054827: step 7050, loss 0.000359409, acc 1
2017-03-02T23:45:20.142732: step 7051, loss 0.000149378, acc 1
2017-03-02T23:45:20.226479: step 7052, loss 0.000280681, acc 1
2017-03-02T23:45:20.336652: step 7053, loss 0.000659698, acc 1
2017-03-02T23:45:20.440385: step 7054, loss 0.00320301, acc 1
2017-03-02T23:45:20.551282: step 7055, loss 0.00108902, acc 1
2017-03-02T23:45:20.659685: step 7056, loss 0.000253939, acc 1
2017-03-02T23:45:20.772252: step 7057, loss 0.00159419, acc 1
2017-03-02T23:45:20.877235: step 7058, loss 0.00199179, acc 1
2017-03-02T23:45:20.970216: step 7059, loss 0.000711314, acc 1
2017-03-02T23:45:21.067477: step 7060, loss 0.000122039, acc 1
2017-03-02T23:45:21.180734: step 7061, loss 0.00068996, acc 1
2017-03-02T23:45:21.288095: step 7062, loss 0.00329065, acc 1
2017-03-02T23:45:21.393982: step 7063, loss 0.00057148, acc 1
2017-03-02T23:45:21.497822: step 7064, loss 8.08237e-05, acc 1
2017-03-02T23:45:21.608022: step 7065, loss 0.00842843, acc 1
2017-03-02T23:45:21.691394: step 7066, loss 0.000734593, acc 1
2017-03-02T23:45:21.785214: step 7067, loss 0.000458531, acc 1
2017-03-02T23:45:21.893575: step 7068, loss 7.4961e-05, acc 1
2017-03-02T23:45:21.997735: step 7069, loss 0.000767976, acc 1
2017-03-02T23:45:22.100459: step 7070, loss 0.000644071, acc 1
2017-03-02T23:45:22.206852: step 7071, loss 0.00247703, acc 1
2017-03-02T23:45:22.310642: step 7072, loss 0.000493003, acc 1
2017-03-02T23:45:22.413653: step 7073, loss 0.00202246, acc 1
2017-03-02T23:45:22.507358: step 7074, loss 0.00840273, acc 1
2017-03-02T23:45:22.612101: step 7075, loss 0.000308439, acc 1
2017-03-02T23:45:22.715478: step 7076, loss 0.000161549, acc 1
2017-03-02T23:45:22.834922: step 7077, loss 0.000261818, acc 1
2017-03-02T23:45:22.946387: step 7078, loss 0.0583, acc 0.984375
2017-03-02T23:45:23.051968: step 7079, loss 0.00164872, acc 1
2017-03-02T23:45:23.161507: step 7080, loss 0.000375911, acc 1
2017-03-02T23:45:23.276811: step 7081, loss 0.000827849, acc 1
2017-03-02T23:45:23.392097: step 7082, loss 0.00224051, acc 1
2017-03-02T23:45:23.496220: step 7083, loss 0.00954987, acc 1
2017-03-02T23:45:23.599231: step 7084, loss 0.000293631, acc 1
2017-03-02T23:45:23.712566: step 7085, loss 0.0132852, acc 0.984375
2017-03-02T23:45:23.814901: step 7086, loss 0.00125295, acc 1
2017-03-02T23:45:23.928715: step 7087, loss 0.000266046, acc 1
2017-03-02T23:45:24.016843: step 7088, loss 0.000356339, acc 1
2017-03-02T23:45:24.123566: step 7089, loss 0.0002515, acc 1
2017-03-02T23:45:24.227600: step 7090, loss 0.000276889, acc 1
2017-03-02T23:45:24.340945: step 7091, loss 0.000242183, acc 1
2017-03-02T23:45:24.441984: step 7092, loss 0.0013268, acc 1
2017-03-02T23:45:24.536637: step 7093, loss 0.000306772, acc 1
2017-03-02T23:45:24.644984: step 7094, loss 0.00105251, acc 1
2017-03-02T23:45:24.734009: step 7095, loss 0.00703712, acc 1
2017-03-02T23:45:24.822650: step 7096, loss 0.000218739, acc 1
2017-03-02T23:45:24.926782: step 7097, loss 0.000141252, acc 1
2017-03-02T23:45:25.036215: step 7098, loss 0.000173983, acc 1
2017-03-02T23:45:25.137050: step 7099, loss 0.0011247, acc 1
2017-03-02T23:45:25.242653: step 7100, loss 0.000289428, acc 1

Evaluation:
2017-03-02T23:45:25.299065: step 7100, loss 3.00267, acc 0.50173

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7100

2017-03-02T23:45:27.388010: step 7101, loss 0.000574063, acc 1
2017-03-02T23:45:27.475984: step 7102, loss 0.000522681, acc 1
2017-03-02T23:45:27.582562: step 7103, loss 0.000639412, acc 1
2017-03-02T23:45:27.694608: step 7104, loss 0.000652332, acc 1
2017-03-02T23:45:27.799346: step 7105, loss 0.00056723, acc 1
2017-03-02T23:45:27.911167: step 7106, loss 0.00724782, acc 1
2017-03-02T23:45:28.016240: step 7107, loss 0.00024579, acc 1
2017-03-02T23:45:28.122739: step 7108, loss 0.0112775, acc 1
2017-03-02T23:45:28.213897: step 7109, loss 8.38427e-05, acc 1
2017-03-02T23:45:28.315805: step 7110, loss 0.000272846, acc 1
2017-03-02T23:45:28.421694: step 7111, loss 0.000160869, acc 1
2017-03-02T23:45:28.524645: step 7112, loss 0.000381182, acc 1
2017-03-02T23:45:28.630353: step 7113, loss 0.0009644, acc 1
2017-03-02T23:45:28.737190: step 7114, loss 0.00104324, acc 1
2017-03-02T23:45:28.842892: step 7115, loss 0.00533672, acc 1
2017-03-02T23:45:28.938921: step 7116, loss 0.00266725, acc 1
2017-03-02T23:45:29.026733: step 7117, loss 0.00204754, acc 1
2017-03-02T23:45:29.133214: step 7118, loss 0.000777676, acc 1
2017-03-02T23:45:29.239366: step 7119, loss 0.00363779, acc 1
2017-03-02T23:45:29.341248: step 7120, loss 0.000749567, acc 1
2017-03-02T23:45:29.448841: step 7121, loss 0.000302737, acc 1
2017-03-02T23:45:29.552845: step 7122, loss 0.000880691, acc 1
2017-03-02T23:45:29.659414: step 7123, loss 0.00460666, acc 1
2017-03-02T23:45:29.749645: step 7124, loss 0.000105015, acc 1
2017-03-02T23:45:29.853266: step 7125, loss 7.10587e-05, acc 1
2017-03-02T23:45:29.962826: step 7126, loss 0.000175207, acc 1
2017-03-02T23:45:30.082760: step 7127, loss 0.000582921, acc 1
2017-03-02T23:45:30.191431: step 7128, loss 0.00128405, acc 1
2017-03-02T23:45:30.295288: step 7129, loss 0.000253657, acc 1
2017-03-02T23:45:30.396714: step 7130, loss 0.00264171, acc 1
2017-03-02T23:45:30.487137: step 7131, loss 0.0142825, acc 0.984375
2017-03-02T23:45:30.592502: step 7132, loss 0.000603731, acc 1
2017-03-02T23:45:30.699430: step 7133, loss 0.00064765, acc 1
2017-03-02T23:45:30.789690: step 7134, loss 0.000125699, acc 1
2017-03-02T23:45:30.890192: step 7135, loss 0.00170894, acc 1
2017-03-02T23:45:30.990788: step 7136, loss 0.000508276, acc 1
2017-03-02T23:45:31.092014: step 7137, loss 0.000790634, acc 1
2017-03-02T23:45:31.198549: step 7138, loss 0.00039564, acc 1
2017-03-02T23:45:31.293405: step 7139, loss 0.000475258, acc 1
2017-03-02T23:45:31.400637: step 7140, loss 0.000164514, acc 1
2017-03-02T23:45:31.511078: step 7141, loss 0.0103533, acc 1
2017-03-02T23:45:31.618060: step 7142, loss 0.000754964, acc 1
2017-03-02T23:45:31.725569: step 7143, loss 0.000662282, acc 1
2017-03-02T23:45:31.831972: step 7144, loss 0.000163016, acc 1
2017-03-02T23:45:31.939310: step 7145, loss 0.000720227, acc 1
2017-03-02T23:45:32.032789: step 7146, loss 0.000402919, acc 1
2017-03-02T23:45:32.138093: step 7147, loss 0.000350862, acc 1
2017-03-02T23:45:32.244551: step 7148, loss 0.001697, acc 1
2017-03-02T23:45:32.346648: step 7149, loss 0.00031537, acc 1
2017-03-02T23:45:32.463049: step 7150, loss 0.00105663, acc 1
2017-03-02T23:45:32.566572: step 7151, loss 0.000354765, acc 1
2017-03-02T23:45:32.684806: step 7152, loss 0.0111942, acc 1
2017-03-02T23:45:32.772006: step 7153, loss 0.00046478, acc 1
2017-03-02T23:45:32.870377: step 7154, loss 0.000251478, acc 1
2017-03-02T23:45:32.974243: step 7155, loss 0.00609039, acc 1
2017-03-02T23:45:33.074309: step 7156, loss 0.00375271, acc 1
2017-03-02T23:45:33.179770: step 7157, loss 0.000395683, acc 1
2017-03-02T23:45:33.277290: step 7158, loss 0.000794063, acc 1
2017-03-02T23:45:33.380047: step 7159, loss 0.00182335, acc 1
2017-03-02T23:45:33.476733: step 7160, loss 0.00180225, acc 1
2017-03-02T23:45:33.565069: step 7161, loss 0.00120171, acc 1
2017-03-02T23:45:33.670980: step 7162, loss 0.00106287, acc 1
2017-03-02T23:45:33.779810: step 7163, loss 0.000567627, acc 1
2017-03-02T23:45:33.884762: step 7164, loss 0.0187009, acc 0.984375
2017-03-02T23:45:34.002604: step 7165, loss 0.000150471, acc 1
2017-03-02T23:45:34.110978: step 7166, loss 0.000427803, acc 1
2017-03-02T23:45:34.212915: step 7167, loss 0.000173938, acc 1
2017-03-02T23:45:34.306346: step 7168, loss 0.0014884, acc 1
2017-03-02T23:45:34.412609: step 7169, loss 0.000137675, acc 1
2017-03-02T23:45:34.519281: step 7170, loss 0.00603599, acc 1
2017-03-02T23:45:34.622264: step 7171, loss 0.000471212, acc 1
2017-03-02T23:45:34.732797: step 7172, loss 0.000462956, acc 1
2017-03-02T23:45:34.837192: step 7173, loss 0.00132078, acc 1
2017-03-02T23:45:34.942302: step 7174, loss 0.000260952, acc 1
2017-03-02T23:45:35.023171: step 7175, loss 0.000722097, acc 1
2017-03-02T23:45:35.123059: step 7176, loss 0.0019712, acc 1
2017-03-02T23:45:35.242140: step 7177, loss 0.000879164, acc 1
2017-03-02T23:45:35.349733: step 7178, loss 0.000682874, acc 1
2017-03-02T23:45:35.453225: step 7179, loss 0.00273134, acc 1
2017-03-02T23:45:35.563593: step 7180, loss 0.000139433, acc 1
2017-03-02T23:45:35.666263: step 7181, loss 0.000617741, acc 1
2017-03-02T23:45:35.762086: step 7182, loss 0.000658477, acc 1
2017-03-02T23:45:35.861481: step 7183, loss 0.0012927, acc 1
2017-03-02T23:45:35.965128: step 7184, loss 0.00344202, acc 1
2017-03-02T23:45:36.066377: step 7185, loss 0.00436639, acc 1
2017-03-02T23:45:36.167157: step 7186, loss 0.000768014, acc 1
2017-03-02T23:45:36.270371: step 7187, loss 0.000255482, acc 1
2017-03-02T23:45:36.377037: step 7188, loss 0.000368882, acc 1
2017-03-02T23:45:36.482258: step 7189, loss 0.00101607, acc 1
2017-03-02T23:45:36.573963: step 7190, loss 0.00848766, acc 1
2017-03-02T23:45:36.679389: step 7191, loss 0.00083801, acc 1
2017-03-02T23:45:36.779114: step 7192, loss 0.00167247, acc 1
2017-03-02T23:45:36.884631: step 7193, loss 5.33195e-05, acc 1
2017-03-02T23:45:36.992548: step 7194, loss 0.000829024, acc 1
2017-03-02T23:45:37.100903: step 7195, loss 0.000757413, acc 1
2017-03-02T23:45:37.209644: step 7196, loss 0.000328428, acc 1
2017-03-02T23:45:37.300718: step 7197, loss 0.000283485, acc 1
2017-03-02T23:45:37.400715: step 7198, loss 0.000103498, acc 1
2017-03-02T23:45:37.514599: step 7199, loss 0.00394993, acc 1
2017-03-02T23:45:37.618440: step 7200, loss 0.000198741, acc 1

Evaluation:
2017-03-02T23:45:37.670769: step 7200, loss 3.0514, acc 0.512111

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7200

2017-03-02T23:45:38.122776: step 7201, loss 9.44077e-05, acc 1
2017-03-02T23:45:38.228790: step 7202, loss 7.18111e-05, acc 1
2017-03-02T23:45:38.337022: step 7203, loss 0.000195679, acc 1
2017-03-02T23:45:38.446944: step 7204, loss 0.00142886, acc 1
2017-03-02T23:45:38.551772: step 7205, loss 0.000392344, acc 1
2017-03-02T23:45:38.663680: step 7206, loss 0.000300142, acc 1
2017-03-02T23:45:38.751643: step 7207, loss 0.00184352, acc 1
2017-03-02T23:45:38.865701: step 7208, loss 0.000379844, acc 1
2017-03-02T23:45:38.970091: step 7209, loss 0.00028297, acc 1
2017-03-02T23:45:39.070761: step 7210, loss 0.000153475, acc 1
2017-03-02T23:45:39.179686: step 7211, loss 0.000703335, acc 1
2017-03-02T23:45:39.279544: step 7212, loss 0.00014408, acc 1
2017-03-02T23:45:39.381383: step 7213, loss 0.000308948, acc 1
2017-03-02T23:45:39.473702: step 7214, loss 0.00408391, acc 1
2017-03-02T23:45:39.575396: step 7215, loss 0.000609557, acc 1
2017-03-02T23:45:39.670513: step 7216, loss 0.020957, acc 0.98
2017-03-02T23:45:39.776855: step 7217, loss 0.000473583, acc 1
2017-03-02T23:45:39.883338: step 7218, loss 0.000254918, acc 1
2017-03-02T23:45:39.983952: step 7219, loss 0.00148603, acc 1
2017-03-02T23:45:40.087659: step 7220, loss 0.00206706, acc 1
2017-03-02T23:45:40.199239: step 7221, loss 0.00106449, acc 1
2017-03-02T23:45:40.293291: step 7222, loss 0.00127913, acc 1
2017-03-02T23:45:40.392452: step 7223, loss 0.00123367, acc 1
2017-03-02T23:45:40.495316: step 7224, loss 0.0110733, acc 1
2017-03-02T23:45:40.601303: step 7225, loss 0.000300831, acc 1
2017-03-02T23:45:40.704493: step 7226, loss 0.00188077, acc 1
2017-03-02T23:45:40.810548: step 7227, loss 0.00284322, acc 1
2017-03-02T23:45:40.922642: step 7228, loss 0.00261217, acc 1
2017-03-02T23:45:41.018464: step 7229, loss 0.000138708, acc 1
2017-03-02T23:45:41.120100: step 7230, loss 0.00174508, acc 1
2017-03-02T23:45:41.220424: step 7231, loss 0.000459163, acc 1
2017-03-02T23:45:41.344594: step 7232, loss 8.88625e-05, acc 1
2017-03-02T23:45:41.446801: step 7233, loss 0.00132279, acc 1
2017-03-02T23:45:41.551518: step 7234, loss 0.0121183, acc 0.984375
2017-03-02T23:45:41.662187: step 7235, loss 0.00369286, acc 1
2017-03-02T23:45:41.754369: step 7236, loss 0.00031158, acc 1
2017-03-02T23:45:41.856687: step 7237, loss 0.000315311, acc 1
2017-03-02T23:45:41.965011: step 7238, loss 0.00258942, acc 1
2017-03-02T23:45:42.069339: step 7239, loss 0.000430075, acc 1
2017-03-02T23:45:42.173648: step 7240, loss 0.000475067, acc 1
2017-03-02T23:45:42.280421: step 7241, loss 0.00159057, acc 1
2017-03-02T23:45:42.378241: step 7242, loss 0.00141523, acc 1
2017-03-02T23:45:42.458543: step 7243, loss 0.0134906, acc 0.984375
2017-03-02T23:45:42.553518: step 7244, loss 0.00071552, acc 1
2017-03-02T23:45:42.658189: step 7245, loss 0.00133637, acc 1
2017-03-02T23:45:42.757514: step 7246, loss 0.000160589, acc 1
2017-03-02T23:45:42.860795: step 7247, loss 0.000849099, acc 1
2017-03-02T23:45:42.960313: step 7248, loss 0.00105171, acc 1
2017-03-02T23:45:43.063111: step 7249, loss 0.00271778, acc 1
2017-03-02T23:45:43.165092: step 7250, loss 0.000791904, acc 1
2017-03-02T23:45:43.255699: step 7251, loss 0.00136639, acc 1
2017-03-02T23:45:43.352900: step 7252, loss 0.00172942, acc 1
2017-03-02T23:45:43.458904: step 7253, loss 0.000350017, acc 1
2017-03-02T23:45:43.578584: step 7254, loss 0.000369644, acc 1
2017-03-02T23:45:43.683157: step 7255, loss 0.000182554, acc 1
2017-03-02T23:45:43.793696: step 7256, loss 0.00098624, acc 1
2017-03-02T23:45:43.893836: step 7257, loss 0.00018817, acc 1
2017-03-02T23:45:43.979042: step 7258, loss 0.000539092, acc 1
2017-03-02T23:45:44.078672: step 7259, loss 0.00125468, acc 1
2017-03-02T23:45:44.182774: step 7260, loss 0.00101211, acc 1
2017-03-02T23:45:44.287516: step 7261, loss 0.000739437, acc 1
2017-03-02T23:45:44.403645: step 7262, loss 0.000492538, acc 1
2017-03-02T23:45:44.508842: step 7263, loss 0.0033832, acc 1
2017-03-02T23:45:44.612127: step 7264, loss 0.000274125, acc 1
2017-03-02T23:45:44.714883: step 7265, loss 0.000135692, acc 1
2017-03-02T23:45:44.807202: step 7266, loss 0.000317185, acc 1
2017-03-02T23:45:44.917439: step 7267, loss 0.000184043, acc 1
2017-03-02T23:45:45.022414: step 7268, loss 0.001793, acc 1
2017-03-02T23:45:45.126923: step 7269, loss 0.0120889, acc 0.984375
2017-03-02T23:45:45.235687: step 7270, loss 0.000357088, acc 1
2017-03-02T23:45:45.342064: step 7271, loss 0.000131217, acc 1
2017-03-02T23:45:45.450636: step 7272, loss 0.00326967, acc 1
2017-03-02T23:45:45.536290: step 7273, loss 0.000399663, acc 1
2017-03-02T23:45:45.647857: step 7274, loss 3.17569e-05, acc 1
2017-03-02T23:45:45.753263: step 7275, loss 0.000528939, acc 1
2017-03-02T23:45:45.860126: step 7276, loss 0.000256274, acc 1
2017-03-02T23:45:45.964707: step 7277, loss 0.00119874, acc 1
2017-03-02T23:45:46.076705: step 7278, loss 0.000402112, acc 1
2017-03-02T23:45:46.184149: step 7279, loss 6.56592e-05, acc 1
2017-03-02T23:45:46.272390: step 7280, loss 0.00117177, acc 1
2017-03-02T23:45:46.368812: step 7281, loss 0.00172255, acc 1
2017-03-02T23:45:46.473884: step 7282, loss 0.000263995, acc 1
2017-03-02T23:45:46.575440: step 7283, loss 0.000522751, acc 1
2017-03-02T23:45:46.690984: step 7284, loss 0.00138487, acc 1
2017-03-02T23:45:46.797384: step 7285, loss 0.00323196, acc 1
2017-03-02T23:45:46.906498: step 7286, loss 0.000207999, acc 1
2017-03-02T23:45:47.003098: step 7287, loss 0.000669987, acc 1
2017-03-02T23:45:47.093934: step 7288, loss 0.000700815, acc 1
2017-03-02T23:45:47.202495: step 7289, loss 0.0254391, acc 0.984375
2017-03-02T23:45:47.318503: step 7290, loss 0.00840635, acc 1
2017-03-02T23:45:47.421887: step 7291, loss 0.000186318, acc 1
2017-03-02T23:45:47.525894: step 7292, loss 0.000876407, acc 1
2017-03-02T23:45:47.627238: step 7293, loss 0.000558528, acc 1
2017-03-02T23:45:47.732247: step 7294, loss 0.000226738, acc 1
2017-03-02T23:45:47.820571: step 7295, loss 0.000516511, acc 1
2017-03-02T23:45:47.928164: step 7296, loss 0.000732556, acc 1
2017-03-02T23:45:48.033436: step 7297, loss 0.000254929, acc 1
2017-03-02T23:45:48.134034: step 7298, loss 0.0119489, acc 1
2017-03-02T23:45:48.231765: step 7299, loss 0.00225434, acc 1
2017-03-02T23:45:48.336864: step 7300, loss 0.00446257, acc 1

Evaluation:
2017-03-02T23:45:48.398241: step 7300, loss 3.24664, acc 0.525952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7300

2017-03-02T23:45:48.894994: step 7301, loss 8.75551e-05, acc 1
2017-03-02T23:45:49.001580: step 7302, loss 0.000339108, acc 1
2017-03-02T23:45:49.102271: step 7303, loss 0.0014553, acc 1
2017-03-02T23:45:49.213979: step 7304, loss 0.00011916, acc 1
2017-03-02T23:45:49.305575: step 7305, loss 0.000408113, acc 1
2017-03-02T23:45:49.411283: step 7306, loss 0.000237043, acc 1
2017-03-02T23:45:49.519250: step 7307, loss 0.000214777, acc 1
2017-03-02T23:45:49.623952: step 7308, loss 0.000400847, acc 1
2017-03-02T23:45:49.725344: step 7309, loss 0.0220992, acc 0.984375
2017-03-02T23:45:49.833544: step 7310, loss 0.00359306, acc 1
2017-03-02T23:45:49.943149: step 7311, loss 0.00033879, acc 1
2017-03-02T23:45:50.028033: step 7312, loss 0.00888891, acc 1
2017-03-02T23:45:50.122670: step 7313, loss 0.000349069, acc 1
2017-03-02T23:45:50.229853: step 7314, loss 0.000785456, acc 1
2017-03-02T23:45:50.333163: step 7315, loss 0.0012273, acc 1
2017-03-02T23:45:50.441428: step 7316, loss 0.00130932, acc 1
2017-03-02T23:45:50.542265: step 7317, loss 0.00307283, acc 1
2017-03-02T23:45:50.644647: step 7318, loss 0.00107057, acc 1
2017-03-02T23:45:50.749389: step 7319, loss 0.000921641, acc 1
2017-03-02T23:45:50.845293: step 7320, loss 0.000790022, acc 1
2017-03-02T23:45:50.949441: step 7321, loss 0.000314002, acc 1
2017-03-02T23:45:51.064529: step 7322, loss 0.00885223, acc 1
2017-03-02T23:45:51.167650: step 7323, loss 0.0178557, acc 0.984375
2017-03-02T23:45:51.268365: step 7324, loss 0.000130567, acc 1
2017-03-02T23:45:51.370370: step 7325, loss 0.00134503, acc 1
2017-03-02T23:45:51.477119: step 7326, loss 0.000298201, acc 1
2017-03-02T23:45:51.564518: step 7327, loss 6.60593e-05, acc 1
2017-03-02T23:45:51.666561: step 7328, loss 0.000728317, acc 1
2017-03-02T23:45:51.768783: step 7329, loss 0.00448273, acc 1
2017-03-02T23:45:51.868873: step 7330, loss 0.00147057, acc 1
2017-03-02T23:45:51.979240: step 7331, loss 0.00297712, acc 1
2017-03-02T23:45:52.084095: step 7332, loss 0.000131772, acc 1
2017-03-02T23:45:52.190550: step 7333, loss 0.000546079, acc 1
2017-03-02T23:45:52.279536: step 7334, loss 0.00325214, acc 1
2017-03-02T23:45:52.376572: step 7335, loss 0.000151569, acc 1
2017-03-02T23:45:52.483014: step 7336, loss 0.000162359, acc 1
2017-03-02T23:45:52.586911: step 7337, loss 0.00151665, acc 1
2017-03-02T23:45:52.688051: step 7338, loss 0.00051648, acc 1
2017-03-02T23:45:52.783368: step 7339, loss 0.00104681, acc 1
2017-03-02T23:45:52.892395: step 7340, loss 0.000193222, acc 1
2017-03-02T23:45:52.997428: step 7341, loss 0.000732169, acc 1
2017-03-02T23:45:53.086523: step 7342, loss 0.00090369, acc 1
2017-03-02T23:45:53.194522: step 7343, loss 0.0041769, acc 1
2017-03-02T23:45:53.308326: step 7344, loss 0.000192381, acc 1
2017-03-02T23:45:53.411285: step 7345, loss 0.0439976, acc 0.984375
2017-03-02T23:45:53.517123: step 7346, loss 0.000234271, acc 1
2017-03-02T23:45:53.624369: step 7347, loss 0.00106145, acc 1
2017-03-02T23:45:53.733918: step 7348, loss 0.000377222, acc 1
2017-03-02T23:45:53.823790: step 7349, loss 0.00194811, acc 1
2017-03-02T23:45:53.924848: step 7350, loss 0.000166481, acc 1
2017-03-02T23:45:54.032240: step 7351, loss 0.000504266, acc 1
2017-03-02T23:45:54.133714: step 7352, loss 0.00522255, acc 1
2017-03-02T23:45:54.239075: step 7353, loss 0.00154903, acc 1
2017-03-02T23:45:54.351602: step 7354, loss 0.000587994, acc 1
2017-03-02T23:45:54.457634: step 7355, loss 0.00250154, acc 1
2017-03-02T23:45:54.549125: step 7356, loss 0.00909803, acc 1
2017-03-02T23:45:54.642511: step 7357, loss 0.000305979, acc 1
2017-03-02T23:45:54.742237: step 7358, loss 0.000128042, acc 1
2017-03-02T23:45:54.839611: step 7359, loss 0.000576618, acc 1
2017-03-02T23:45:54.947846: step 7360, loss 3.14625e-05, acc 1
2017-03-02T23:45:55.052222: step 7361, loss 0.000414196, acc 1
2017-03-02T23:45:55.153018: step 7362, loss 0.000647512, acc 1
2017-03-02T23:45:55.256630: step 7363, loss 0.000413577, acc 1
2017-03-02T23:45:55.346936: step 7364, loss 0.000116352, acc 1
2017-03-02T23:45:55.440688: step 7365, loss 0.000171327, acc 1
2017-03-02T23:45:55.547586: step 7366, loss 0.000220898, acc 1
2017-03-02T23:45:55.652726: step 7367, loss 0.000556744, acc 1
2017-03-02T23:45:55.756492: step 7368, loss 0.0007535, acc 1
2017-03-02T23:45:55.859240: step 7369, loss 0.000254793, acc 1
2017-03-02T23:45:55.963686: step 7370, loss 0.00116296, acc 1
2017-03-02T23:45:56.049547: step 7371, loss 0.000221311, acc 1
2017-03-02T23:45:56.145727: step 7372, loss 0.000175375, acc 1
2017-03-02T23:45:56.251931: step 7373, loss 0.000462568, acc 1
2017-03-02T23:45:56.364614: step 7374, loss 0.000275809, acc 1
2017-03-02T23:45:56.465643: step 7375, loss 0.00288417, acc 1
2017-03-02T23:45:56.567350: step 7376, loss 0.000279053, acc 1
2017-03-02T23:45:56.675083: step 7377, loss 0.000157645, acc 1
2017-03-02T23:45:56.784331: step 7378, loss 0.00258521, acc 1
2017-03-02T23:45:56.870163: step 7379, loss 0.000142026, acc 1
2017-03-02T23:45:56.968809: step 7380, loss 0.000809138, acc 1
2017-03-02T23:45:57.069695: step 7381, loss 8.73238e-05, acc 1
2017-03-02T23:45:57.173779: step 7382, loss 0.000126097, acc 1
2017-03-02T23:45:57.285728: step 7383, loss 0.000273404, acc 1
2017-03-02T23:45:57.392562: step 7384, loss 0.000304151, acc 1
2017-03-02T23:45:57.510706: step 7385, loss 0.0005784, acc 1
2017-03-02T23:45:57.600096: step 7386, loss 0.0016335, acc 1
2017-03-02T23:45:57.702031: step 7387, loss 0.00442663, acc 1
2017-03-02T23:45:57.800303: step 7388, loss 0.00071641, acc 1
2017-03-02T23:45:57.905880: step 7389, loss 0.000655423, acc 1
2017-03-02T23:45:58.020902: step 7390, loss 0.000113053, acc 1
2017-03-02T23:45:58.125025: step 7391, loss 0.00153761, acc 1
2017-03-02T23:45:58.237812: step 7392, loss 0.000326865, acc 1
2017-03-02T23:45:58.333235: step 7393, loss 0.000893995, acc 1
2017-03-02T23:45:58.435407: step 7394, loss 0.000166075, acc 1
2017-03-02T23:45:58.545832: step 7395, loss 0.00043746, acc 1
2017-03-02T23:45:58.659377: step 7396, loss 0.000279944, acc 1
2017-03-02T23:45:58.777141: step 7397, loss 0.0007641, acc 1
2017-03-02T23:45:58.881645: step 7398, loss 0.00158133, acc 1
2017-03-02T23:45:58.991635: step 7399, loss 0.00827341, acc 1
2017-03-02T23:45:59.083460: step 7400, loss 0.000258197, acc 1

Evaluation:
2017-03-02T23:45:59.147590: step 7400, loss 3.14089, acc 0.508651

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7400

2017-03-02T23:45:59.938199: step 7401, loss 0.000702312, acc 1
2017-03-02T23:46:00.047184: step 7402, loss 0.000729106, acc 1
2017-03-02T23:46:00.153131: step 7403, loss 0.00258465, acc 1
2017-03-02T23:46:00.267559: step 7404, loss 0.000417095, acc 1
2017-03-02T23:46:00.380627: step 7405, loss 0.000189538, acc 1
2017-03-02T23:46:00.470832: step 7406, loss 0.000183257, acc 1
2017-03-02T23:46:00.563941: step 7407, loss 0.000186734, acc 1
2017-03-02T23:46:00.668939: step 7408, loss 0.00126949, acc 1
2017-03-02T23:46:00.780231: step 7409, loss 0.000245367, acc 1
2017-03-02T23:46:00.885062: step 7410, loss 0.000860168, acc 1
2017-03-02T23:46:00.992905: step 7411, loss 0.000165552, acc 1
2017-03-02T23:46:01.099708: step 7412, loss 0.000575728, acc 1
2017-03-02T23:46:01.220869: step 7413, loss 0.000138205, acc 1
2017-03-02T23:46:01.304938: step 7414, loss 0.00231259, acc 1
2017-03-02T23:46:01.411865: step 7415, loss 0.00172987, acc 1
2017-03-02T23:46:01.523235: step 7416, loss 0.000185997, acc 1
2017-03-02T23:46:01.628504: step 7417, loss 0.000155948, acc 1
2017-03-02T23:46:01.732847: step 7418, loss 0.000610806, acc 1
2017-03-02T23:46:01.830059: step 7419, loss 0.000301508, acc 1
2017-03-02T23:46:01.933712: step 7420, loss 0.000232891, acc 1
2017-03-02T23:46:02.024122: step 7421, loss 0.000292056, acc 1
2017-03-02T23:46:02.116034: step 7422, loss 0.000211682, acc 1
2017-03-02T23:46:02.217482: step 7423, loss 0.000478342, acc 1
2017-03-02T23:46:02.321657: step 7424, loss 0.000420827, acc 1
2017-03-02T23:46:02.427058: step 7425, loss 0.00119226, acc 1
2017-03-02T23:46:02.537356: step 7426, loss 0.00126333, acc 1
2017-03-02T23:46:02.631011: step 7427, loss 0.000508404, acc 1
2017-03-02T23:46:02.733861: step 7428, loss 0.00118933, acc 1
2017-03-02T23:46:02.818764: step 7429, loss 0.00072113, acc 1
2017-03-02T23:46:02.930049: step 7430, loss 0.000176055, acc 1
2017-03-02T23:46:03.031002: step 7431, loss 0.000598584, acc 1
2017-03-02T23:46:03.143291: step 7432, loss 0.000171547, acc 1
2017-03-02T23:46:03.247009: step 7433, loss 0.000438954, acc 1
2017-03-02T23:46:03.351186: step 7434, loss 0.00849924, acc 1
2017-03-02T23:46:03.459583: step 7435, loss 0.000232037, acc 1
2017-03-02T23:46:03.549011: step 7436, loss 0.00163448, acc 1
2017-03-02T23:46:03.647931: step 7437, loss 0.000128049, acc 1
2017-03-02T23:46:03.749168: step 7438, loss 0.000122937, acc 1
2017-03-02T23:46:03.858416: step 7439, loss 8.84079e-05, acc 1
2017-03-02T23:46:03.976852: step 7440, loss 0.00830355, acc 1
2017-03-02T23:46:04.083621: step 7441, loss 0.00019617, acc 1
2017-03-02T23:46:04.185823: step 7442, loss 0.00299744, acc 1
2017-03-02T23:46:04.279606: step 7443, loss 0.00114068, acc 1
2017-03-02T23:46:04.392923: step 7444, loss 0.000501516, acc 1
2017-03-02T23:46:04.496475: step 7445, loss 4.22722e-05, acc 1
2017-03-02T23:46:04.602025: step 7446, loss 0.018015, acc 0.984375
2017-03-02T23:46:04.711326: step 7447, loss 0.000105436, acc 1
2017-03-02T23:46:04.811633: step 7448, loss 0.000187834, acc 1
2017-03-02T23:46:04.919676: step 7449, loss 0.000227379, acc 1
2017-03-02T23:46:05.001973: step 7450, loss 0.000851647, acc 1
2017-03-02T23:46:05.093362: step 7451, loss 0.00688843, acc 1
2017-03-02T23:46:05.196564: step 7452, loss 0.00049185, acc 1
2017-03-02T23:46:05.300669: step 7453, loss 0.000329777, acc 1
2017-03-02T23:46:05.405810: step 7454, loss 0.00181522, acc 1
2017-03-02T23:46:05.510379: step 7455, loss 0.00677286, acc 1
2017-03-02T23:46:05.611588: step 7456, loss 0.000718044, acc 1
2017-03-02T23:46:05.718003: step 7457, loss 0.000202841, acc 1
2017-03-02T23:46:05.823489: step 7458, loss 0.000204162, acc 1
2017-03-02T23:46:05.925520: step 7459, loss 0.000565459, acc 1
2017-03-02T23:46:06.032249: step 7460, loss 0.00120109, acc 1
2017-03-02T23:46:06.139799: step 7461, loss 0.00384752, acc 1
2017-03-02T23:46:06.235645: step 7462, loss 3.46443e-05, acc 1
2017-03-02T23:46:06.344613: step 7463, loss 0.000473549, acc 1
2017-03-02T23:46:06.465813: step 7464, loss 0.000398977, acc 1
2017-03-02T23:46:06.559199: step 7465, loss 0.000652031, acc 1
2017-03-02T23:46:06.675367: step 7466, loss 0.000637632, acc 1
2017-03-02T23:46:06.776610: step 7467, loss 0.00235249, acc 1
2017-03-02T23:46:06.898266: step 7468, loss 0.000826477, acc 1
2017-03-02T23:46:06.999641: step 7469, loss 0.000209519, acc 1
2017-03-02T23:46:07.106998: step 7470, loss 0.000372028, acc 1
2017-03-02T23:46:07.216292: step 7471, loss 0.00040822, acc 1
2017-03-02T23:46:07.306000: step 7472, loss 0.000244679, acc 1
2017-03-02T23:46:07.406595: step 7473, loss 0.000293063, acc 1
2017-03-02T23:46:07.503097: step 7474, loss 0.00123819, acc 1
2017-03-02T23:46:07.605091: step 7475, loss 0.000473739, acc 1
2017-03-02T23:46:07.716890: step 7476, loss 0.000230206, acc 1
2017-03-02T23:46:07.822716: step 7477, loss 0.000368335, acc 1
2017-03-02T23:46:07.927518: step 7478, loss 0.000182237, acc 1
2017-03-02T23:46:08.037127: step 7479, loss 0.00853644, acc 1
2017-03-02T23:46:08.141426: step 7480, loss 0.011615, acc 1
2017-03-02T23:46:08.242260: step 7481, loss 0.000950398, acc 1
2017-03-02T23:46:08.356647: step 7482, loss 6.00172e-05, acc 1
2017-03-02T23:46:08.458256: step 7483, loss 0.00338328, acc 1
2017-03-02T23:46:08.562096: step 7484, loss 0.000484671, acc 1
2017-03-02T23:46:08.668780: step 7485, loss 0.000128339, acc 1
2017-03-02T23:46:08.764144: step 7486, loss 0.0011537, acc 1
2017-03-02T23:46:08.858373: step 7487, loss 0.000301949, acc 1
2017-03-02T23:46:08.963507: step 7488, loss 0.00114195, acc 1
2017-03-02T23:46:09.071094: step 7489, loss 0.00534343, acc 1
2017-03-02T23:46:09.174573: step 7490, loss 0.00035696, acc 1
2017-03-02T23:46:09.278430: step 7491, loss 0.000623578, acc 1
2017-03-02T23:46:09.402284: step 7492, loss 0.00380104, acc 1
2017-03-02T23:46:09.494458: step 7493, loss 0.000261311, acc 1
2017-03-02T23:46:09.589697: step 7494, loss 0.00014333, acc 1
2017-03-02T23:46:09.692036: step 7495, loss 0.000241037, acc 1
2017-03-02T23:46:09.796434: step 7496, loss 0.00445868, acc 1
2017-03-02T23:46:09.910259: step 7497, loss 0.000701879, acc 1
2017-03-02T23:46:10.017289: step 7498, loss 0.000684264, acc 1
2017-03-02T23:46:10.120080: step 7499, loss 0.000437685, acc 1
2017-03-02T23:46:10.222736: step 7500, loss 0.000222899, acc 1

Evaluation:
2017-03-02T23:46:10.274522: step 7500, loss 3.20249, acc 0.512111

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7500

2017-03-02T23:46:10.869099: step 7501, loss 0.000138754, acc 1
2017-03-02T23:46:10.967405: step 7502, loss 0.000813958, acc 1
2017-03-02T23:46:11.060364: step 7503, loss 0.000459487, acc 1
2017-03-02T23:46:11.175685: step 7504, loss 0.000214804, acc 1
2017-03-02T23:46:11.279483: step 7505, loss 0.00127378, acc 1
2017-03-02T23:46:11.388644: step 7506, loss 4.69767e-05, acc 1
2017-03-02T23:46:11.492874: step 7507, loss 0.0137675, acc 1
2017-03-02T23:46:11.597894: step 7508, loss 0.000385874, acc 1
2017-03-02T23:46:11.684412: step 7509, loss 0.000394421, acc 1
2017-03-02T23:46:11.780562: step 7510, loss 0.000460043, acc 1
2017-03-02T23:46:11.889016: step 7511, loss 0.000125554, acc 1
2017-03-02T23:46:11.992896: step 7512, loss 0.000717315, acc 1
2017-03-02T23:46:12.101793: step 7513, loss 8.91114e-05, acc 1
2017-03-02T23:46:12.208742: step 7514, loss 0.00135384, acc 1
2017-03-02T23:46:12.311727: step 7515, loss 0.00764286, acc 1
2017-03-02T23:46:12.417672: step 7516, loss 5.07131e-05, acc 1
2017-03-02T23:46:12.509149: step 7517, loss 0.000466889, acc 1
2017-03-02T23:46:12.611041: step 7518, loss 0.000419668, acc 1
2017-03-02T23:46:12.717578: step 7519, loss 0.00220779, acc 1
2017-03-02T23:46:12.825622: step 7520, loss 0.000771849, acc 1
2017-03-02T23:46:12.929084: step 7521, loss 0.000255838, acc 1
2017-03-02T23:46:13.045649: step 7522, loss 0.000339991, acc 1
2017-03-02T23:46:13.141968: step 7523, loss 0.000973523, acc 1
2017-03-02T23:46:13.238968: step 7524, loss 0.000182394, acc 1
2017-03-02T23:46:13.363706: step 7525, loss 0.000402367, acc 1
2017-03-02T23:46:13.470140: step 7526, loss 0.00018047, acc 1
2017-03-02T23:46:13.572836: step 7527, loss 0.000517121, acc 1
2017-03-02T23:46:13.670033: step 7528, loss 0.00010581, acc 1
2017-03-02T23:46:13.776742: step 7529, loss 5.34411e-05, acc 1
2017-03-02T23:46:13.881747: step 7530, loss 0.000238647, acc 1
2017-03-02T23:46:13.982656: step 7531, loss 0.00156958, acc 1
2017-03-02T23:46:14.077336: step 7532, loss 7.64162e-05, acc 1
2017-03-02T23:46:14.195696: step 7533, loss 0.000376059, acc 1
2017-03-02T23:46:14.314732: step 7534, loss 0.000543755, acc 1
2017-03-02T23:46:14.428331: step 7535, loss 0.000193666, acc 1
2017-03-02T23:46:14.534591: step 7536, loss 0.000441173, acc 1
2017-03-02T23:46:14.657038: step 7537, loss 0.000110874, acc 1
2017-03-02T23:46:14.756439: step 7538, loss 0.00113297, acc 1
2017-03-02T23:46:14.855647: step 7539, loss 0.000131838, acc 1
2017-03-02T23:46:14.964903: step 7540, loss 7.69126e-05, acc 1
2017-03-02T23:46:15.067700: step 7541, loss 0.000135958, acc 1
2017-03-02T23:46:15.185476: step 7542, loss 0.000221869, acc 1
2017-03-02T23:46:15.285513: step 7543, loss 0.000560404, acc 1
2017-03-02T23:46:15.384123: step 7544, loss 0.000449557, acc 1
2017-03-02T23:46:15.476617: step 7545, loss 0.000240216, acc 1
2017-03-02T23:46:15.570550: step 7546, loss 0.000225728, acc 1
2017-03-02T23:46:15.671009: step 7547, loss 0.000155717, acc 1
2017-03-02T23:46:15.776627: step 7548, loss 0.000160863, acc 1
2017-03-02T23:46:15.879116: step 7549, loss 0.000273638, acc 1
2017-03-02T23:46:15.984548: step 7550, loss 0.000769946, acc 1
2017-03-02T23:46:16.087586: step 7551, loss 0.000299174, acc 1
2017-03-02T23:46:16.196793: step 7552, loss 0.00043839, acc 1
2017-03-02T23:46:16.284156: step 7553, loss 0.000686895, acc 1
2017-03-02T23:46:16.398003: step 7554, loss 0.000424878, acc 1
2017-03-02T23:46:16.504602: step 7555, loss 0.000836012, acc 1
2017-03-02T23:46:16.607201: step 7556, loss 0.000265303, acc 1
2017-03-02T23:46:16.709165: step 7557, loss 9.91005e-05, acc 1
2017-03-02T23:46:16.809843: step 7558, loss 0.00104883, acc 1
2017-03-02T23:46:16.917572: step 7559, loss 0.000431321, acc 1
2017-03-02T23:46:17.004553: step 7560, loss 0.00275186, acc 1
2017-03-02T23:46:17.113298: step 7561, loss 0.000389849, acc 1
2017-03-02T23:46:17.223958: step 7562, loss 0.000126274, acc 1
2017-03-02T23:46:17.336119: step 7563, loss 0.00096539, acc 1
2017-03-02T23:46:17.435898: step 7564, loss 0.000599535, acc 1
2017-03-02T23:46:17.540165: step 7565, loss 0.00311061, acc 1
2017-03-02T23:46:17.650082: step 7566, loss 0.00215253, acc 1
2017-03-02T23:46:17.747379: step 7567, loss 0.00832188, acc 1
2017-03-02T23:46:17.847698: step 7568, loss 4.60076e-05, acc 1
2017-03-02T23:46:17.950629: step 7569, loss 0.000105613, acc 1
2017-03-02T23:46:18.058272: step 7570, loss 0.00108571, acc 1
2017-03-02T23:46:18.163020: step 7571, loss 0.00290094, acc 1
2017-03-02T23:46:18.281948: step 7572, loss 0.00117131, acc 1
2017-03-02T23:46:18.384847: step 7573, loss 0.000246974, acc 1
2017-03-02T23:46:18.480884: step 7574, loss 0.000787175, acc 1
2017-03-02T23:46:18.573128: step 7575, loss 0.0181455, acc 0.984375
2017-03-02T23:46:18.686634: step 7576, loss 0.000595132, acc 1
2017-03-02T23:46:18.787681: step 7577, loss 0.000924251, acc 1
2017-03-02T23:46:18.902538: step 7578, loss 0.000933734, acc 1
2017-03-02T23:46:19.006196: step 7579, loss 0.000164556, acc 1
2017-03-02T23:46:19.095602: step 7580, loss 0.0035224, acc 1
2017-03-02T23:46:19.198220: step 7581, loss 0.00075873, acc 1
2017-03-02T23:46:19.292314: step 7582, loss 0.000722942, acc 1
2017-03-02T23:46:19.392004: step 7583, loss 0.00220569, acc 1
2017-03-02T23:46:19.496399: step 7584, loss 0.000150012, acc 1
2017-03-02T23:46:19.593135: step 7585, loss 0.00037215, acc 1
2017-03-02T23:46:19.699367: step 7586, loss 0.000400322, acc 1
2017-03-02T23:46:19.804574: step 7587, loss 0.000207726, acc 1
2017-03-02T23:46:19.916640: step 7588, loss 0.0014089, acc 1
2017-03-02T23:46:20.003720: step 7589, loss 5.45077e-05, acc 1
2017-03-02T23:46:20.104657: step 7590, loss 0.000575923, acc 1
2017-03-02T23:46:20.207602: step 7591, loss 0.000391278, acc 1
2017-03-02T23:46:20.312063: step 7592, loss 0.000470091, acc 1
2017-03-02T23:46:20.411542: step 7593, loss 0.000255148, acc 1
2017-03-02T23:46:20.517796: step 7594, loss 0.00178425, acc 1
2017-03-02T23:46:20.623549: step 7595, loss 0.00118549, acc 1
2017-03-02T23:46:20.733165: step 7596, loss 0.000425263, acc 1
2017-03-02T23:46:20.830660: step 7597, loss 0.00115515, acc 1
2017-03-02T23:46:20.937519: step 7598, loss 0.000225312, acc 1
2017-03-02T23:46:21.036213: step 7599, loss 0.000201686, acc 1
2017-03-02T23:46:21.140948: step 7600, loss 0.00350215, acc 1

Evaluation:
2017-03-02T23:46:21.197254: step 7600, loss 3.25723, acc 0.49827

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7600

2017-03-02T23:46:21.909515: step 7601, loss 6.86196e-05, acc 1
2017-03-02T23:46:22.013864: step 7602, loss 0.00023813, acc 1
2017-03-02T23:46:22.123557: step 7603, loss 0.000207295, acc 1
2017-03-02T23:46:22.222476: step 7604, loss 7.63824e-05, acc 1
2017-03-02T23:46:22.325824: step 7605, loss 4.42688e-05, acc 1
2017-03-02T23:46:22.434953: step 7606, loss 0.000205758, acc 1
2017-03-02T23:46:22.539414: step 7607, loss 0.000554542, acc 1
2017-03-02T23:46:22.642498: step 7608, loss 0.000308571, acc 1
2017-03-02T23:46:22.746445: step 7609, loss 0.000322179, acc 1
2017-03-02T23:46:22.850055: step 7610, loss 0.000123214, acc 1
2017-03-02T23:46:22.941716: step 7611, loss 0.0148768, acc 0.984375
2017-03-02T23:46:23.032823: step 7612, loss 2.16331e-05, acc 1
2017-03-02T23:46:23.150694: step 7613, loss 0.000784746, acc 1
2017-03-02T23:46:23.253181: step 7614, loss 0.000249313, acc 1
2017-03-02T23:46:23.371301: step 7615, loss 0.000174176, acc 1
2017-03-02T23:46:23.476371: step 7616, loss 0.00240045, acc 1
2017-03-02T23:46:23.579840: step 7617, loss 0.000593691, acc 1
2017-03-02T23:46:23.698540: step 7618, loss 0.000635984, acc 1
2017-03-02T23:46:23.782466: step 7619, loss 0.00373836, acc 1
2017-03-02T23:46:23.880071: step 7620, loss 0.000287797, acc 1
2017-03-02T23:46:23.983197: step 7621, loss 0.000321778, acc 1
2017-03-02T23:46:24.089656: step 7622, loss 0.00282241, acc 1
2017-03-02T23:46:24.192479: step 7623, loss 0.00024413, acc 1
2017-03-02T23:46:24.295770: step 7624, loss 0.000488637, acc 1
2017-03-02T23:46:24.408070: step 7625, loss 0.000452796, acc 1
2017-03-02T23:46:24.492563: step 7626, loss 0.000208859, acc 1
2017-03-02T23:46:24.594837: step 7627, loss 0.00390851, acc 1
2017-03-02T23:46:24.700926: step 7628, loss 0.00802103, acc 1
2017-03-02T23:46:24.814961: step 7629, loss 0.000817846, acc 1
2017-03-02T23:46:24.921115: step 7630, loss 0.000222059, acc 1
2017-03-02T23:46:25.029875: step 7631, loss 5.4641e-05, acc 1
2017-03-02T23:46:25.119297: step 7632, loss 0.000411833, acc 1
2017-03-02T23:46:25.207127: step 7633, loss 0.000112607, acc 1
2017-03-02T23:46:25.291528: step 7634, loss 0.000144892, acc 1
2017-03-02T23:46:25.392949: step 7635, loss 4.49805e-05, acc 1
2017-03-02T23:46:25.494563: step 7636, loss 0.00697994, acc 1
2017-03-02T23:46:25.611276: step 7637, loss 0.000315601, acc 1
2017-03-02T23:46:25.725820: step 7638, loss 0.000367791, acc 1
2017-03-02T23:46:25.848213: step 7639, loss 0.00025944, acc 1
2017-03-02T23:46:25.956308: step 7640, loss 0.00027623, acc 1
2017-03-02T23:46:26.066872: step 7641, loss 0.00029814, acc 1
2017-03-02T23:46:26.172105: step 7642, loss 6.81246e-05, acc 1
2017-03-02T23:46:26.284707: step 7643, loss 0.000870283, acc 1
2017-03-02T23:46:26.393990: step 7644, loss 0.00311272, acc 1
2017-03-02T23:46:26.496873: step 7645, loss 0.000546974, acc 1
2017-03-02T23:46:26.601377: step 7646, loss 0.000353, acc 1
2017-03-02T23:46:26.689743: step 7647, loss 0.000487868, acc 1
2017-03-02T23:46:26.777567: step 7648, loss 5.73395e-05, acc 1
2017-03-02T23:46:26.913559: step 7649, loss 8.612e-05, acc 1
2017-03-02T23:46:27.020531: step 7650, loss 0.00180263, acc 1
2017-03-02T23:46:27.128783: step 7651, loss 0.0174436, acc 0.984375
2017-03-02T23:46:27.232830: step 7652, loss 9.78885e-05, acc 1
2017-03-02T23:46:27.334713: step 7653, loss 0.002503, acc 1
2017-03-02T23:46:27.424306: step 7654, loss 0.0114877, acc 0.984375
2017-03-02T23:46:27.524584: step 7655, loss 0.000130515, acc 1
2017-03-02T23:46:27.630751: step 7656, loss 0.000513659, acc 1
2017-03-02T23:46:27.739373: step 7657, loss 0.000825038, acc 1
2017-03-02T23:46:27.845978: step 7658, loss 0.000187228, acc 1
2017-03-02T23:46:27.956622: step 7659, loss 0.00158876, acc 1
2017-03-02T23:46:28.068248: step 7660, loss 6.61405e-05, acc 1
2017-03-02T23:46:28.161395: step 7661, loss 0.00110106, acc 1
2017-03-02T23:46:28.252576: step 7662, loss 0.0011622, acc 1
2017-03-02T23:46:28.360699: step 7663, loss 0.000267255, acc 1
2017-03-02T23:46:28.466358: step 7664, loss 0.00272295, acc 1
2017-03-02T23:46:28.571317: step 7665, loss 0.00122583, acc 1
2017-03-02T23:46:28.669106: step 7666, loss 0.00116518, acc 1
2017-03-02T23:46:28.771036: step 7667, loss 0.0001299, acc 1
2017-03-02T23:46:28.884461: step 7668, loss 0.0032815, acc 1
2017-03-02T23:46:28.975985: step 7669, loss 0.000407416, acc 1
2017-03-02T23:46:29.086297: step 7670, loss 0.000683398, acc 1
2017-03-02T23:46:29.190073: step 7671, loss 0.000674649, acc 1
2017-03-02T23:46:29.301049: step 7672, loss 0.000679239, acc 1
2017-03-02T23:46:29.409990: step 7673, loss 9.13206e-05, acc 1
2017-03-02T23:46:29.518927: step 7674, loss 0.0101022, acc 1
2017-03-02T23:46:29.629215: step 7675, loss 0.000489615, acc 1
2017-03-02T23:46:29.719542: step 7676, loss 6.65737e-05, acc 1
2017-03-02T23:46:29.819244: step 7677, loss 0.001696, acc 1
2017-03-02T23:46:29.923996: step 7678, loss 0.00171795, acc 1
2017-03-02T23:46:30.035108: step 7679, loss 0.0374386, acc 0.984375
2017-03-02T23:46:30.137156: step 7680, loss 0.000273656, acc 1
2017-03-02T23:46:30.241546: step 7681, loss 0.00034791, acc 1
2017-03-02T23:46:30.355167: step 7682, loss 0.00173239, acc 1
2017-03-02T23:46:30.454034: step 7683, loss 0.000138972, acc 1
2017-03-02T23:46:30.558243: step 7684, loss 0.00683132, acc 1
2017-03-02T23:46:30.669476: step 7685, loss 0.00077139, acc 1
2017-03-02T23:46:30.776852: step 7686, loss 0.00459157, acc 1
2017-03-02T23:46:30.881348: step 7687, loss 0.00111766, acc 1
2017-03-02T23:46:30.989879: step 7688, loss 0.00016659, acc 1
2017-03-02T23:46:31.092713: step 7689, loss 0.000189906, acc 1
2017-03-02T23:46:31.190543: step 7690, loss 0.000618147, acc 1
2017-03-02T23:46:31.284400: step 7691, loss 0.000458943, acc 1
2017-03-02T23:46:31.406677: step 7692, loss 0.00257697, acc 1
2017-03-02T23:46:31.516429: step 7693, loss 0.000127811, acc 1
2017-03-02T23:46:31.625292: step 7694, loss 0.00128283, acc 1
2017-03-02T23:46:31.719364: step 7695, loss 0.000408636, acc 1
2017-03-02T23:46:31.828895: step 7696, loss 0.000811491, acc 1
2017-03-02T23:46:31.935325: step 7697, loss 0.0105591, acc 1
2017-03-02T23:46:32.027562: step 7698, loss 0.000951974, acc 1
2017-03-02T23:46:32.140388: step 7699, loss 7.4673e-05, acc 1
2017-03-02T23:46:32.256213: step 7700, loss 0.000241131, acc 1

Evaluation:
2017-03-02T23:46:32.307920: step 7700, loss 3.31668, acc 0.50519

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7700

2017-03-02T23:46:32.869467: step 7701, loss 0.000206044, acc 1
2017-03-02T23:46:32.973631: step 7702, loss 0.000876235, acc 1
2017-03-02T23:46:33.079958: step 7703, loss 0.000526575, acc 1
2017-03-02T23:46:33.182311: step 7704, loss 0.0112619, acc 0.984375
2017-03-02T23:46:33.285716: step 7705, loss 0.000348855, acc 1
2017-03-02T23:46:33.387693: step 7706, loss 0.000188381, acc 1
2017-03-02T23:46:33.475636: step 7707, loss 0.000551069, acc 1
2017-03-02T23:46:33.577220: step 7708, loss 0.000342767, acc 1
2017-03-02T23:46:33.685245: step 7709, loss 0.000220979, acc 1
2017-03-02T23:46:33.792800: step 7710, loss 0.00107422, acc 1
2017-03-02T23:46:33.911770: step 7711, loss 0.000521051, acc 1
2017-03-02T23:46:34.019482: step 7712, loss 4.96214e-05, acc 1
2017-03-02T23:46:34.130251: step 7713, loss 7.05391e-05, acc 1
2017-03-02T23:46:34.221003: step 7714, loss 0.00105521, acc 1
2017-03-02T23:46:34.326223: step 7715, loss 0.000278483, acc 1
2017-03-02T23:46:34.456373: step 7716, loss 0.000307208, acc 1
2017-03-02T23:46:34.564144: step 7717, loss 0.000164355, acc 1
2017-03-02T23:46:34.669135: step 7718, loss 0.00327307, acc 1
2017-03-02T23:46:34.773625: step 7719, loss 0.00162179, acc 1
2017-03-02T23:46:34.893395: step 7720, loss 0.000310744, acc 1
2017-03-02T23:46:34.987600: step 7721, loss 0.000112712, acc 1
2017-03-02T23:46:35.096067: step 7722, loss 0.000708379, acc 1
2017-03-02T23:46:35.210625: step 7723, loss 0.000184517, acc 1
2017-03-02T23:46:35.323900: step 7724, loss 0.00169718, acc 1
2017-03-02T23:46:35.432218: step 7725, loss 0.00184489, acc 1
2017-03-02T23:46:35.540508: step 7726, loss 0.00143661, acc 1
2017-03-02T23:46:35.656992: step 7727, loss 0.00139505, acc 1
2017-03-02T23:46:35.748243: step 7728, loss 0.000685304, acc 1
2017-03-02T23:46:35.854559: step 7729, loss 0.00406232, acc 1
2017-03-02T23:46:35.960890: step 7730, loss 0.000346844, acc 1
2017-03-02T23:46:36.076560: step 7731, loss 0.000537734, acc 1
2017-03-02T23:46:36.185064: step 7732, loss 0.000942781, acc 1
2017-03-02T23:46:36.285442: step 7733, loss 0.00299635, acc 1
2017-03-02T23:46:36.396407: step 7734, loss 0.00170989, acc 1
2017-03-02T23:46:36.488083: step 7735, loss 0.00038329, acc 1
2017-03-02T23:46:36.592366: step 7736, loss 0.00524484, acc 1
2017-03-02T23:46:36.695434: step 7737, loss 0.00127265, acc 1
2017-03-02T23:46:36.802685: step 7738, loss 0.000971565, acc 1
2017-03-02T23:46:36.903176: step 7739, loss 0.000117326, acc 1
2017-03-02T23:46:37.005619: step 7740, loss 0.000251762, acc 1
2017-03-02T23:46:37.113101: step 7741, loss 0.000923446, acc 1
2017-03-02T23:46:37.205573: step 7742, loss 0.000359451, acc 1
2017-03-02T23:46:37.301783: step 7743, loss 0.000548463, acc 1
2017-03-02T23:46:37.403126: step 7744, loss 0.00115102, acc 1
2017-03-02T23:46:37.507833: step 7745, loss 0.000675869, acc 1
2017-03-02T23:46:37.616735: step 7746, loss 0.00328478, acc 1
2017-03-02T23:46:37.721559: step 7747, loss 0.000157395, acc 1
2017-03-02T23:46:37.827752: step 7748, loss 0.000183891, acc 1
2017-03-02T23:46:37.920300: step 7749, loss 0.00114893, acc 1
2017-03-02T23:46:38.011047: step 7750, loss 4.54723e-05, acc 1
2017-03-02T23:46:38.118891: step 7751, loss 0.00169263, acc 1
2017-03-02T23:46:38.227801: step 7752, loss 0.000235229, acc 1
2017-03-02T23:46:38.323968: step 7753, loss 0.00297551, acc 1
2017-03-02T23:46:38.430712: step 7754, loss 0.00020874, acc 1
2017-03-02T23:46:38.522229: step 7755, loss 0.00536607, acc 1
2017-03-02T23:46:38.622005: step 7756, loss 4.81801e-05, acc 1
2017-03-02T23:46:38.729594: step 7757, loss 0.000112442, acc 1
2017-03-02T23:46:38.834983: step 7758, loss 0.00040668, acc 1
2017-03-02T23:46:38.939200: step 7759, loss 0.000121892, acc 1
2017-03-02T23:46:39.045253: step 7760, loss 9.15442e-05, acc 1
2017-03-02T23:46:39.141542: step 7761, loss 0.000233191, acc 1
2017-03-02T23:46:39.243951: step 7762, loss 0.000241499, acc 1
2017-03-02T23:46:39.369903: step 7763, loss 0.000581014, acc 1
2017-03-02T23:46:39.460180: step 7764, loss 0.00137279, acc 1
2017-03-02T23:46:39.565111: step 7765, loss 0.000519006, acc 1
2017-03-02T23:46:39.668054: step 7766, loss 0.00182811, acc 1
2017-03-02T23:46:39.769187: step 7767, loss 0.000146874, acc 1
2017-03-02T23:46:39.873777: step 7768, loss 0.00038869, acc 1
2017-03-02T23:46:39.976760: step 7769, loss 0.000356252, acc 1
2017-03-02T23:46:40.079300: step 7770, loss 0.000176838, acc 1
2017-03-02T23:46:40.170254: step 7771, loss 0.00245739, acc 1
2017-03-02T23:46:40.264925: step 7772, loss 0.000571149, acc 1
2017-03-02T23:46:40.393587: step 7773, loss 0.000191632, acc 1
2017-03-02T23:46:40.498356: step 7774, loss 0.000519011, acc 1
2017-03-02T23:46:40.600582: step 7775, loss 0.00065361, acc 1
2017-03-02T23:46:40.708696: step 7776, loss 0.00683431, acc 1
2017-03-02T23:46:40.813998: step 7777, loss 0.00028258, acc 1
2017-03-02T23:46:40.906242: step 7778, loss 0.000121657, acc 1
2017-03-02T23:46:40.996012: step 7779, loss 0.000472392, acc 1
2017-03-02T23:46:41.097466: step 7780, loss 0.000163075, acc 1
2017-03-02T23:46:41.206513: step 7781, loss 0.000392552, acc 1
2017-03-02T23:46:41.310668: step 7782, loss 0.0256554, acc 0.984375
2017-03-02T23:46:41.419095: step 7783, loss 0.00629695, acc 1
2017-03-02T23:46:41.520577: step 7784, loss 0.000484362, acc 1
2017-03-02T23:46:41.628206: step 7785, loss 0.000342014, acc 1
2017-03-02T23:46:41.720025: step 7786, loss 0.000719205, acc 1
2017-03-02T23:46:41.816209: step 7787, loss 8.90804e-05, acc 1
2017-03-02T23:46:41.920388: step 7788, loss 0.000351108, acc 1
2017-03-02T23:46:42.025557: step 7789, loss 0.00083529, acc 1
2017-03-02T23:46:42.112765: step 7790, loss 0.000351871, acc 1
2017-03-02T23:46:42.218368: step 7791, loss 4.80595e-05, acc 1
2017-03-02T23:46:42.320196: step 7792, loss 6.48556e-05, acc 1
2017-03-02T23:46:42.431619: step 7793, loss 0.00631907, acc 1
2017-03-02T23:46:42.518399: step 7794, loss 0.000402617, acc 1
2017-03-02T23:46:42.615632: step 7795, loss 0.000192705, acc 1
2017-03-02T23:46:42.717576: step 7796, loss 0.00030457, acc 1
2017-03-02T23:46:42.824108: step 7797, loss 7.33218e-05, acc 1
2017-03-02T23:46:42.931881: step 7798, loss 0.000731291, acc 1
2017-03-02T23:46:43.039616: step 7799, loss 0.000382415, acc 1
2017-03-02T23:46:43.144299: step 7800, loss 0.000458711, acc 1

Evaluation:
2017-03-02T23:46:43.193918: step 7800, loss 3.58722, acc 0.519031

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7800

2017-03-02T23:46:43.717908: step 7801, loss 0.00022648, acc 1
2017-03-02T23:46:43.824205: step 7802, loss 0.00022969, acc 1
2017-03-02T23:46:43.916454: step 7803, loss 0.0197634, acc 0.984375
2017-03-02T23:46:44.006531: step 7804, loss 0.00833784, acc 1
2017-03-02T23:46:44.106332: step 7805, loss 0.00064163, acc 1
2017-03-02T23:46:44.216526: step 7806, loss 0.00882192, acc 1
2017-03-02T23:46:44.339354: step 7807, loss 0.0010614, acc 1
2017-03-02T23:46:44.440716: step 7808, loss 0.00385812, acc 1
2017-03-02T23:46:44.545862: step 7809, loss 8.93581e-05, acc 1
2017-03-02T23:46:44.648801: step 7810, loss 0.000207853, acc 1
2017-03-02T23:46:44.745657: step 7811, loss 0.0025275, acc 1
2017-03-02T23:46:44.856029: step 7812, loss 5.29146e-05, acc 1
2017-03-02T23:46:44.961544: step 7813, loss 0.00396257, acc 1
2017-03-02T23:46:45.066808: step 7814, loss 0.000670961, acc 1
2017-03-02T23:46:45.168481: step 7815, loss 2.51482e-05, acc 1
2017-03-02T23:46:45.272126: step 7816, loss 0.0011945, acc 1
2017-03-02T23:46:45.381530: step 7817, loss 0.00590481, acc 1
2017-03-02T23:46:45.469570: step 7818, loss 0.000118621, acc 1
2017-03-02T23:46:45.564886: step 7819, loss 0.000750863, acc 1
2017-03-02T23:46:45.672849: step 7820, loss 0.000237077, acc 1
2017-03-02T23:46:45.779069: step 7821, loss 0.000219704, acc 1
2017-03-02T23:46:45.887059: step 7822, loss 0.000429165, acc 1
2017-03-02T23:46:45.990868: step 7823, loss 0.00037189, acc 1
2017-03-02T23:46:46.099694: step 7824, loss 0.0013177, acc 1
2017-03-02T23:46:46.200408: step 7825, loss 0.000747385, acc 1
2017-03-02T23:46:46.299398: step 7826, loss 0.0171158, acc 0.984375
2017-03-02T23:46:46.397221: step 7827, loss 0.00180014, acc 1
2017-03-02T23:46:46.506340: step 7828, loss 0.000872123, acc 1
2017-03-02T23:46:46.614770: step 7829, loss 0.00144581, acc 1
2017-03-02T23:46:46.721292: step 7830, loss 0.000189405, acc 1
2017-03-02T23:46:46.822496: step 7831, loss 0.000352993, acc 1
2017-03-02T23:46:46.923794: step 7832, loss 0.000314519, acc 1
2017-03-02T23:46:47.030424: step 7833, loss 0.0238895, acc 0.984375
2017-03-02T23:46:47.135203: step 7834, loss 0.000502351, acc 1
2017-03-02T23:46:47.231887: step 7835, loss 0.000708681, acc 1
2017-03-02T23:46:47.354543: step 7836, loss 0.000259423, acc 1
2017-03-02T23:46:47.449558: step 7837, loss 7.42116e-05, acc 1
2017-03-02T23:46:47.544132: step 7838, loss 0.000311245, acc 1
2017-03-02T23:46:47.645666: step 7839, loss 0.000100361, acc 1
2017-03-02T23:46:47.770472: step 7840, loss 0.00210336, acc 1
2017-03-02T23:46:47.864806: step 7841, loss 0.000627503, acc 1
2017-03-02T23:46:47.977290: step 7842, loss 4.01654e-05, acc 1
2017-03-02T23:46:48.078893: step 7843, loss 0.0109755, acc 1
2017-03-02T23:46:48.178350: step 7844, loss 0.000173972, acc 1
2017-03-02T23:46:48.275022: step 7845, loss 0.00256873, acc 1
2017-03-02T23:46:48.368380: step 7846, loss 7.98905e-05, acc 1
2017-03-02T23:46:48.467814: step 7847, loss 0.000431974, acc 1
2017-03-02T23:46:48.573971: step 7848, loss 0.000103584, acc 1
2017-03-02T23:46:48.669994: step 7849, loss 0.00296759, acc 1
2017-03-02T23:46:48.775773: step 7850, loss 0.000281558, acc 1
2017-03-02T23:46:48.871147: step 7851, loss 0.000816882, acc 1
2017-03-02T23:46:48.982656: step 7852, loss 0.000387767, acc 1
2017-03-02T23:46:49.075165: step 7853, loss 0.00287979, acc 1
2017-03-02T23:46:49.172624: step 7854, loss 0.000267465, acc 1
2017-03-02T23:46:49.275104: step 7855, loss 0.000897586, acc 1
2017-03-02T23:46:49.363944: step 7856, loss 0.00164154, acc 1
2017-03-02T23:46:49.467985: step 7857, loss 0.000485289, acc 1
2017-03-02T23:46:49.559732: step 7858, loss 0.000208158, acc 1
2017-03-02T23:46:49.661730: step 7859, loss 0.000201627, acc 1
2017-03-02T23:46:49.763476: step 7860, loss 0.000626107, acc 1
2017-03-02T23:46:49.861609: step 7861, loss 0.00112544, acc 1
2017-03-02T23:46:49.966323: step 7862, loss 0.000577914, acc 1
2017-03-02T23:46:50.062710: step 7863, loss 0.000243719, acc 1
2017-03-02T23:46:50.167338: step 7864, loss 1.79518e-05, acc 1
2017-03-02T23:46:50.267176: step 7865, loss 0.00132975, acc 1
2017-03-02T23:46:50.369837: step 7866, loss 7.68231e-05, acc 1
2017-03-02T23:46:50.477032: step 7867, loss 0.000115109, acc 1
2017-03-02T23:46:50.585622: step 7868, loss 0.000730543, acc 1
2017-03-02T23:46:50.699830: step 7869, loss 0.00617009, acc 1
2017-03-02T23:46:50.787338: step 7870, loss 0.000112812, acc 1
2017-03-02T23:46:50.891996: step 7871, loss 0.000213753, acc 1
2017-03-02T23:46:50.972533: step 7872, loss 0.0215013, acc 0.98
2017-03-02T23:46:51.087005: step 7873, loss 0.00178228, acc 1
2017-03-02T23:46:51.189054: step 7874, loss 0.000300394, acc 1
2017-03-02T23:46:51.288802: step 7875, loss 0.00682197, acc 1
2017-03-02T23:46:51.395020: step 7876, loss 0.000428683, acc 1
2017-03-02T23:46:51.489522: step 7877, loss 0.000116117, acc 1
2017-03-02T23:46:51.594507: step 7878, loss 0.00770215, acc 1
2017-03-02T23:46:51.699337: step 7879, loss 0.000135318, acc 1
2017-03-02T23:46:51.800968: step 7880, loss 0.000442937, acc 1
2017-03-02T23:46:51.906040: step 7881, loss 0.0101575, acc 1
2017-03-02T23:46:52.002232: step 7882, loss 0.00016373, acc 1
2017-03-02T23:46:52.102099: step 7883, loss 0.00409471, acc 1
2017-03-02T23:46:52.189783: step 7884, loss 0.000664802, acc 1
2017-03-02T23:46:52.293988: step 7885, loss 0.00160709, acc 1
2017-03-02T23:46:52.400145: step 7886, loss 0.000195369, acc 1
2017-03-02T23:46:52.506918: step 7887, loss 0.000391276, acc 1
2017-03-02T23:46:52.612740: step 7888, loss 0.000949514, acc 1
2017-03-02T23:46:52.711765: step 7889, loss 0.000629631, acc 1
2017-03-02T23:46:52.819339: step 7890, loss 0.000119649, acc 1
2017-03-02T23:46:52.902210: step 7891, loss 0.00768923, acc 1
2017-03-02T23:46:53.004985: step 7892, loss 0.000165729, acc 1
2017-03-02T23:46:53.108664: step 7893, loss 0.000368929, acc 1
2017-03-02T23:46:53.214898: step 7894, loss 0.000314488, acc 1
2017-03-02T23:46:53.327737: step 7895, loss 0.0446228, acc 0.984375
2017-03-02T23:46:53.421549: step 7896, loss 0.000942494, acc 1
2017-03-02T23:46:53.527651: step 7897, loss 0.00126813, acc 1
2017-03-02T23:46:53.620877: step 7898, loss 0.00161391, acc 1
2017-03-02T23:46:53.725698: step 7899, loss 0.000424543, acc 1
2017-03-02T23:46:53.831613: step 7900, loss 0.0157194, acc 0.984375

Evaluation:
2017-03-02T23:46:53.888791: step 7900, loss 3.38285, acc 0.49481

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-7900

2017-03-02T23:46:54.371510: step 7901, loss 0.0139379, acc 0.984375
2017-03-02T23:46:54.473695: step 7902, loss 0.00943325, acc 1
2017-03-02T23:46:54.552652: step 7903, loss 0.000702905, acc 1
2017-03-02T23:46:54.657849: step 7904, loss 0.000764893, acc 1
2017-03-02T23:46:54.764739: step 7905, loss 8.87974e-05, acc 1
2017-03-02T23:46:54.870425: step 7906, loss 0.000191319, acc 1
2017-03-02T23:46:54.968023: step 7907, loss 0.000109117, acc 1
2017-03-02T23:46:55.057819: step 7908, loss 4.16901e-05, acc 1
2017-03-02T23:46:55.161119: step 7909, loss 0.000842967, acc 1
2017-03-02T23:46:55.262533: step 7910, loss 0.000426389, acc 1
2017-03-02T23:46:55.369870: step 7911, loss 0.00267458, acc 1
2017-03-02T23:46:55.473682: step 7912, loss 0.00299864, acc 1
2017-03-02T23:46:55.561037: step 7913, loss 0.00010676, acc 1
2017-03-02T23:46:55.677849: step 7914, loss 0.000584508, acc 1
2017-03-02T23:46:55.763777: step 7915, loss 0.000151093, acc 1
2017-03-02T23:46:55.867398: step 7916, loss 0.00035823, acc 1
2017-03-02T23:46:55.966422: step 7917, loss 0.000921398, acc 1
2017-03-02T23:46:56.084680: step 7918, loss 8.31641e-05, acc 1
2017-03-02T23:46:56.189763: step 7919, loss 0.00512098, acc 1
2017-03-02T23:46:56.287079: step 7920, loss 0.000347357, acc 1
2017-03-02T23:46:56.389210: step 7921, loss 0.00294149, acc 1
2017-03-02T23:46:56.476205: step 7922, loss 7.80018e-05, acc 1
2017-03-02T23:46:56.588342: step 7923, loss 0.000114028, acc 1
2017-03-02T23:46:56.684188: step 7924, loss 0.000162031, acc 1
2017-03-02T23:46:56.789671: step 7925, loss 0.0008686, acc 1
2017-03-02T23:46:56.893876: step 7926, loss 0.0208046, acc 0.984375
2017-03-02T23:46:56.986517: step 7927, loss 0.000134936, acc 1
2017-03-02T23:46:57.113449: step 7928, loss 0.000486833, acc 1
2017-03-02T23:46:57.217804: step 7929, loss 7.46798e-05, acc 1
2017-03-02T23:46:57.320684: step 7930, loss 0.00087905, acc 1
2017-03-02T23:46:57.412317: step 7931, loss 0.000692126, acc 1
2017-03-02T23:46:57.514517: step 7932, loss 0.00108856, acc 1
2017-03-02T23:46:57.626263: step 7933, loss 0.000782547, acc 1
2017-03-02T23:46:57.730503: step 7934, loss 0.00244902, acc 1
2017-03-02T23:46:57.835726: step 7935, loss 0.000160056, acc 1
2017-03-02T23:46:57.927722: step 7936, loss 0.000987459, acc 1
2017-03-02T23:46:58.036139: step 7937, loss 0.000329428, acc 1
2017-03-02T23:46:58.127996: step 7938, loss 0.000228643, acc 1
2017-03-02T23:46:58.235502: step 7939, loss 0.000464528, acc 1
2017-03-02T23:46:58.348032: step 7940, loss 0.00113974, acc 1
2017-03-02T23:46:58.450445: step 7941, loss 0.000138583, acc 1
2017-03-02T23:46:58.553480: step 7942, loss 0.000440207, acc 1
2017-03-02T23:46:58.649819: step 7943, loss 0.000582145, acc 1
2017-03-02T23:46:58.748737: step 7944, loss 0.00014537, acc 1
2017-03-02T23:46:58.840882: step 7945, loss 0.00043894, acc 1
2017-03-02T23:46:58.955424: step 7946, loss 0.00241224, acc 1
2017-03-02T23:46:59.060341: step 7947, loss 0.044593, acc 0.984375
2017-03-02T23:46:59.162696: step 7948, loss 0.000880746, acc 1
2017-03-02T23:46:59.265759: step 7949, loss 0.000138981, acc 1
2017-03-02T23:46:59.359397: step 7950, loss 0.00575554, acc 1
2017-03-02T23:46:59.467572: step 7951, loss 0.00134193, acc 1
2017-03-02T23:46:59.564686: step 7952, loss 0.0042404, acc 1
2017-03-02T23:46:59.668543: step 7953, loss 0.00523745, acc 1
2017-03-02T23:46:59.770107: step 7954, loss 0.000232494, acc 1
2017-03-02T23:46:59.861169: step 7955, loss 0.00107758, acc 1
2017-03-02T23:46:59.964674: step 7956, loss 0.000483289, acc 1
2017-03-02T23:47:00.052387: step 7957, loss 0.00347719, acc 1
2017-03-02T23:47:00.153362: step 7958, loss 0.00157114, acc 1
2017-03-02T23:47:00.256639: step 7959, loss 0.00485629, acc 1
2017-03-02T23:47:00.358619: step 7960, loss 8.57631e-05, acc 1
2017-03-02T23:47:00.471069: step 7961, loss 0.000457478, acc 1
2017-03-02T23:47:00.561824: step 7962, loss 0.00181445, acc 1
2017-03-02T23:47:00.662745: step 7963, loss 0.000335959, acc 1
2017-03-02T23:47:00.753225: step 7964, loss 0.00251818, acc 1
2017-03-02T23:47:00.858173: step 7965, loss 0.000929099, acc 1
2017-03-02T23:47:00.957518: step 7966, loss 0.000607031, acc 1
2017-03-02T23:47:01.048899: step 7967, loss 0.000168983, acc 1
2017-03-02T23:47:01.149874: step 7968, loss 0.000151374, acc 1
2017-03-02T23:47:01.248354: step 7969, loss 0.000623338, acc 1
2017-03-02T23:47:01.358386: step 7970, loss 0.00660949, acc 1
2017-03-02T23:47:01.462108: step 7971, loss 0.000410912, acc 1
2017-03-02T23:47:01.567294: step 7972, loss 0.000727583, acc 1
2017-03-02T23:47:01.675767: step 7973, loss 0.000179917, acc 1
2017-03-02T23:47:01.772249: step 7974, loss 5.77582e-05, acc 1
2017-03-02T23:47:01.879712: step 7975, loss 0.000262769, acc 1
2017-03-02T23:47:01.971995: step 7976, loss 0.000137033, acc 1
2017-03-02T23:47:02.074987: step 7977, loss 0.00394068, acc 1
2017-03-02T23:47:02.178921: step 7978, loss 0.000237579, acc 1
2017-03-02T23:47:02.283812: step 7979, loss 0.000112933, acc 1
2017-03-02T23:47:02.383687: step 7980, loss 0.000804089, acc 1
2017-03-02T23:47:02.478346: step 7981, loss 0.000397882, acc 1
2017-03-02T23:47:02.577843: step 7982, loss 0.00140691, acc 1
2017-03-02T23:47:02.673884: step 7983, loss 0.00214768, acc 1
2017-03-02T23:47:02.781058: step 7984, loss 0.000383684, acc 1
2017-03-02T23:47:02.887847: step 7985, loss 0.000147832, acc 1
2017-03-02T23:47:02.980826: step 7986, loss 0.00330896, acc 1
2017-03-02T23:47:03.088674: step 7987, loss 8.98964e-05, acc 1
2017-03-02T23:47:03.180664: step 7988, loss 0.000285524, acc 1
2017-03-02T23:47:03.284957: step 7989, loss 0.000106052, acc 1
2017-03-02T23:47:03.393391: step 7990, loss 0.000116936, acc 1
2017-03-02T23:47:03.499132: step 7991, loss 0.000218057, acc 1
2017-03-02T23:47:03.592317: step 7992, loss 0.000552514, acc 1
2017-03-02T23:47:03.683727: step 7993, loss 0.000873654, acc 1
2017-03-02T23:47:03.788003: step 7994, loss 5.22953e-05, acc 1
2017-03-02T23:47:03.880499: step 7995, loss 3.87727e-05, acc 1
2017-03-02T23:47:03.981616: step 7996, loss 0.000304058, acc 1
2017-03-02T23:47:04.085471: step 7997, loss 0.000669227, acc 1
2017-03-02T23:47:04.193174: step 7998, loss 0.000121209, acc 1
2017-03-02T23:47:04.304374: step 7999, loss 0.000211412, acc 1
2017-03-02T23:47:04.392078: step 8000, loss 0.000231072, acc 1

Evaluation:
2017-03-02T23:47:04.451826: step 8000, loss 3.62615, acc 0.539792

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-8000

2017-03-02T23:47:04.941649: step 8001, loss 0.00409707, acc 1
2017-03-02T23:47:05.033997: step 8002, loss 0.000382798, acc 1
2017-03-02T23:47:05.127402: step 8003, loss 0.000175729, acc 1
2017-03-02T23:47:05.220611: step 8004, loss 0.000933911, acc 1
2017-03-02T23:47:05.333158: step 8005, loss 0.000561109, acc 1
2017-03-02T23:47:05.439474: step 8006, loss 0.000630315, acc 1
2017-03-02T23:47:05.549783: step 8007, loss 0.000320885, acc 1
2017-03-02T23:47:05.658125: step 8008, loss 0.000299745, acc 1
2017-03-02T23:47:05.753971: step 8009, loss 7.09747e-05, acc 1
2017-03-02T23:47:05.866737: step 8010, loss 0.000501481, acc 1
2017-03-02T23:47:05.961656: step 8011, loss 0.000111175, acc 1
2017-03-02T23:47:06.065665: step 8012, loss 0.0044797, acc 1
2017-03-02T23:47:06.162007: step 8013, loss 0.00107391, acc 1
2017-03-02T23:47:06.268703: step 8014, loss 0.00520065, acc 1
2017-03-02T23:47:06.376904: step 8015, loss 6.39631e-05, acc 1
2017-03-02T23:47:06.485439: step 8016, loss 0.00131105, acc 1
2017-03-02T23:47:06.590495: step 8017, loss 0.000696527, acc 1
2017-03-02T23:47:06.685045: step 8018, loss 0.00225149, acc 1
2017-03-02T23:47:06.789278: step 8019, loss 0.00015898, acc 1
2017-03-02T23:47:06.888529: step 8020, loss 0.000630866, acc 1
2017-03-02T23:47:06.992958: step 8021, loss 0.00408313, acc 1
2017-03-02T23:47:07.098171: step 8022, loss 0.000301018, acc 1
2017-03-02T23:47:07.194641: step 8023, loss 0.000316365, acc 1
2017-03-02T23:47:07.296208: step 8024, loss 0.00252035, acc 1
2017-03-02T23:47:07.389198: step 8025, loss 0.00055305, acc 1
2017-03-02T23:47:07.493553: step 8026, loss 0.00523319, acc 1
2017-03-02T23:47:07.593434: step 8027, loss 0.000231091, acc 1
2017-03-02T23:47:07.692866: step 8028, loss 0.000328313, acc 1
2017-03-02T23:47:07.803923: step 8029, loss 0.00213814, acc 1
2017-03-02T23:47:07.904883: step 8030, loss 0.000385721, acc 1
2017-03-02T23:47:08.011196: step 8031, loss 0.00536566, acc 1
2017-03-02T23:47:08.103909: step 8032, loss 7.00762e-05, acc 1
2017-03-02T23:47:08.204960: step 8033, loss 0.00186686, acc 1
2017-03-02T23:47:08.317253: step 8034, loss 0.0157111, acc 1
2017-03-02T23:47:08.433007: step 8035, loss 0.0219202, acc 0.984375
2017-03-02T23:47:08.540926: step 8036, loss 0.000307094, acc 1
2017-03-02T23:47:08.647455: step 8037, loss 0.00204539, acc 1
2017-03-02T23:47:08.743645: step 8038, loss 0.000904768, acc 1
2017-03-02T23:47:08.858553: step 8039, loss 0.00085973, acc 1
2017-03-02T23:47:08.965419: step 8040, loss 0.0246754, acc 0.984375
2017-03-02T23:47:09.069279: step 8041, loss 0.000197304, acc 1
2017-03-02T23:47:09.175595: step 8042, loss 0.000433301, acc 1
2017-03-02T23:47:09.269517: step 8043, loss 9.60773e-05, acc 1
2017-03-02T23:47:09.392522: step 8044, loss 0.000193809, acc 1
2017-03-02T23:47:09.486032: step 8045, loss 0.00104887, acc 1
2017-03-02T23:47:09.592937: step 8046, loss 0.000383636, acc 1
2017-03-02T23:47:09.692798: step 8047, loss 0.000811835, acc 1
2017-03-02T23:47:09.798285: step 8048, loss 0.00255649, acc 1
2017-03-02T23:47:09.907107: step 8049, loss 0.00296472, acc 1
2017-03-02T23:47:10.006910: step 8050, loss 0.000223654, acc 1
2017-03-02T23:47:10.113165: step 8051, loss 9.51335e-05, acc 1
2017-03-02T23:47:10.205444: step 8052, loss 0.00233842, acc 1
2017-03-02T23:47:10.308043: step 8053, loss 0.00314916, acc 1
2017-03-02T23:47:10.403509: step 8054, loss 0.0013687, acc 1
2017-03-02T23:47:10.518436: step 8055, loss 0.00103713, acc 1
2017-03-02T23:47:10.635506: step 8056, loss 0.000558734, acc 1
2017-03-02T23:47:10.742240: step 8057, loss 0.000934148, acc 1
2017-03-02T23:47:10.847608: step 8058, loss 0.00507613, acc 1
2017-03-02T23:47:10.942153: step 8059, loss 0.000473962, acc 1
2017-03-02T23:47:11.046820: step 8060, loss 0.00301803, acc 1
2017-03-02T23:47:11.140513: step 8061, loss 0.000170748, acc 1
2017-03-02T23:47:11.246155: step 8062, loss 0.000248105, acc 1
2017-03-02T23:47:11.345508: step 8063, loss 9.88933e-05, acc 1
2017-03-02T23:47:11.442118: step 8064, loss 0.000586791, acc 1
2017-03-02T23:47:11.549004: step 8065, loss 0.00145232, acc 1
2017-03-02T23:47:11.663040: step 8066, loss 0.000951738, acc 1
2017-03-02T23:47:11.762287: step 8067, loss 0.000511338, acc 1
2017-03-02T23:47:11.876064: step 8068, loss 0.011747, acc 0.984375
2017-03-02T23:47:11.981129: step 8069, loss 0.002432, acc 1
2017-03-02T23:47:12.074466: step 8070, loss 0.0157602, acc 0.984375
2017-03-02T23:47:12.183049: step 8071, loss 7.82964e-05, acc 1
2017-03-02T23:47:12.289666: step 8072, loss 0.00159002, acc 1
2017-03-02T23:47:12.393240: step 8073, loss 2.7929e-05, acc 1
2017-03-02T23:47:12.507913: step 8074, loss 0.00267174, acc 1
2017-03-02T23:47:12.603852: step 8075, loss 0.000159012, acc 1
2017-03-02T23:47:12.710737: step 8076, loss 0.000421624, acc 1
2017-03-02T23:47:12.794968: step 8077, loss 0.000462739, acc 1
2017-03-02T23:47:12.901652: step 8078, loss 0.0412112, acc 0.96875
2017-03-02T23:47:13.011899: step 8079, loss 0.0111183, acc 1
2017-03-02T23:47:13.115031: step 8080, loss 0.0011101, acc 1
2017-03-02T23:47:13.219333: step 8081, loss 0.000794438, acc 1
2017-03-02T23:47:13.318422: step 8082, loss 0.0029254, acc 1
2017-03-02T23:47:13.419477: step 8083, loss 0.000129056, acc 1
2017-03-02T23:47:13.512069: step 8084, loss 0.00237392, acc 1
2017-03-02T23:47:13.627522: step 8085, loss 0.000291813, acc 1
2017-03-02T23:47:13.732948: step 8086, loss 0.000931505, acc 1
2017-03-02T23:47:13.831909: step 8087, loss 0.000579794, acc 1
2017-03-02T23:47:13.928907: step 8088, loss 0.000109101, acc 1
2017-03-02T23:47:14.020621: step 8089, loss 0.00182732, acc 1
2017-03-02T23:47:14.129525: step 8090, loss 0.000364976, acc 1
2017-03-02T23:47:14.235605: step 8091, loss 0.0237477, acc 0.984375
2017-03-02T23:47:14.343288: step 8092, loss 0.00402465, acc 1
2017-03-02T23:47:14.459873: step 8093, loss 0.00261018, acc 1
2017-03-02T23:47:14.561329: step 8094, loss 0.000809434, acc 1
2017-03-02T23:47:14.661553: step 8095, loss 0.000154341, acc 1
2017-03-02T23:47:14.749622: step 8096, loss 0.000268062, acc 1
2017-03-02T23:47:14.858668: step 8097, loss 0.00431344, acc 1
2017-03-02T23:47:14.948568: step 8098, loss 0.0114407, acc 1
2017-03-02T23:47:15.057374: step 8099, loss 0.00126102, acc 1
2017-03-02T23:47:15.172519: step 8100, loss 0.000771699, acc 1

Evaluation:
2017-03-02T23:47:15.235688: step 8100, loss 3.6162, acc 0.529412

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-8100

2017-03-02T23:47:15.689602: step 8101, loss 0.00656947, acc 1
2017-03-02T23:47:15.796063: step 8102, loss 6.7426e-05, acc 1
2017-03-02T23:47:15.900779: step 8103, loss 0.000624231, acc 1
2017-03-02T23:47:15.998192: step 8104, loss 0.00014474, acc 1
2017-03-02T23:47:16.089387: step 8105, loss 0.000643748, acc 1
2017-03-02T23:47:16.193980: step 8106, loss 0.000147442, acc 1
2017-03-02T23:47:16.304936: step 8107, loss 0.00131625, acc 1
2017-03-02T23:47:16.415919: step 8108, loss 0.000234654, acc 1
2017-03-02T23:47:16.510374: step 8109, loss 0.0026105, acc 1
2017-03-02T23:47:16.602400: step 8110, loss 0.00431294, acc 1
2017-03-02T23:47:16.702844: step 8111, loss 0.000152275, acc 1
2017-03-02T23:47:16.798055: step 8112, loss 0.00327512, acc 1
2017-03-02T23:47:16.907972: step 8113, loss 0.000365554, acc 1
2017-03-02T23:47:17.023955: step 8114, loss 0.000770201, acc 1
2017-03-02T23:47:17.125360: step 8115, loss 0.000450499, acc 1
2017-03-02T23:47:17.233908: step 8116, loss 0.000640851, acc 1
2017-03-02T23:47:17.325932: step 8117, loss 0.000596884, acc 1
2017-03-02T23:47:17.420296: step 8118, loss 0.000293402, acc 1
2017-03-02T23:47:17.519156: step 8119, loss 0.000430761, acc 1
2017-03-02T23:47:17.623136: step 8120, loss 0.000498055, acc 1
2017-03-02T23:47:17.735750: step 8121, loss 0.000243116, acc 1
2017-03-02T23:47:17.834375: step 8122, loss 0.000264005, acc 1
2017-03-02T23:47:17.941266: step 8123, loss 3.07166e-05, acc 1
2017-03-02T23:47:18.042349: step 8124, loss 0.000716221, acc 1
2017-03-02T23:47:18.135685: step 8125, loss 0.00010378, acc 1
2017-03-02T23:47:18.241942: step 8126, loss 0.00291015, acc 1
2017-03-02T23:47:18.346291: step 8127, loss 4.47331e-05, acc 1
2017-03-02T23:47:18.454182: step 8128, loss 0.00971025, acc 1
2017-03-02T23:47:18.552124: step 8129, loss 9.27667e-05, acc 1
2017-03-02T23:47:18.663647: step 8130, loss 0.00274441, acc 1
2017-03-02T23:47:18.758438: step 8131, loss 0.00117659, acc 1
2017-03-02T23:47:18.859673: step 8132, loss 0.000103173, acc 1
2017-03-02T23:47:18.958125: step 8133, loss 0.000399299, acc 1
2017-03-02T23:47:19.068252: step 8134, loss 2.60304e-05, acc 1
2017-03-02T23:47:19.172371: step 8135, loss 0.000433642, acc 1
2017-03-02T23:47:19.285473: step 8136, loss 0.000946649, acc 1
2017-03-02T23:47:19.392669: step 8137, loss 0.000706623, acc 1
2017-03-02T23:47:19.482259: step 8138, loss 0.000743602, acc 1
2017-03-02T23:47:19.589547: step 8139, loss 0.0106341, acc 1
2017-03-02T23:47:19.686873: step 8140, loss 0.00691842, acc 1
2017-03-02T23:47:19.789535: step 8141, loss 0.00173035, acc 1
2017-03-02T23:47:19.896760: step 8142, loss 0.020843, acc 0.984375
2017-03-02T23:47:20.002113: step 8143, loss 0.000551158, acc 1
2017-03-02T23:47:20.107067: step 8144, loss 0.000323566, acc 1
2017-03-02T23:47:20.196330: step 8145, loss 0.00148337, acc 1
2017-03-02T23:47:20.303157: step 8146, loss 0.000532464, acc 1
2017-03-02T23:47:20.393890: step 8147, loss 0.0003008, acc 1
2017-03-02T23:47:20.496631: step 8148, loss 0.0005297, acc 1
2017-03-02T23:47:20.606558: step 8149, loss 0.00718998, acc 1
2017-03-02T23:47:20.708098: step 8150, loss 7.12828e-05, acc 1
2017-03-02T23:47:20.822319: step 8151, loss 0.000484734, acc 1
2017-03-02T23:47:20.919270: step 8152, loss 0.000141257, acc 1
2017-03-02T23:47:21.028489: step 8153, loss 0.000637978, acc 1
2017-03-02T23:47:21.116842: step 8154, loss 0.000872384, acc 1
2017-03-02T23:47:21.218425: step 8155, loss 0.000493231, acc 1
2017-03-02T23:47:21.322081: step 8156, loss 0.000322733, acc 1
2017-03-02T23:47:21.418406: step 8157, loss 0.000387644, acc 1
2017-03-02T23:47:21.521011: step 8158, loss 0.00408005, acc 1
2017-03-02T23:47:21.606367: step 8159, loss 0.00126747, acc 1
2017-03-02T23:47:21.712494: step 8160, loss 0.109858, acc 0.984375
2017-03-02T23:47:21.815778: step 8161, loss 0.000256404, acc 1
2017-03-02T23:47:21.927952: step 8162, loss 0.0045011, acc 1
2017-03-02T23:47:22.035154: step 8163, loss 0.000430672, acc 1
2017-03-02T23:47:22.137382: step 8164, loss 0.00022093, acc 1
2017-03-02T23:47:22.236957: step 8165, loss 0.000632098, acc 1
2017-03-02T23:47:22.335624: step 8166, loss 0.000186853, acc 1
2017-03-02T23:47:22.439631: step 8167, loss 0.00135604, acc 1
2017-03-02T23:47:22.542029: step 8168, loss 0.0249837, acc 0.984375
2017-03-02T23:47:22.641678: step 8169, loss 0.0305109, acc 0.984375
2017-03-02T23:47:22.738096: step 8170, loss 0.00112714, acc 1
2017-03-02T23:47:22.827791: step 8171, loss 0.000235575, acc 1
2017-03-02T23:47:22.931536: step 8172, loss 0.00197194, acc 1
2017-03-02T23:47:23.022017: step 8173, loss 0.0143403, acc 0.984375
2017-03-02T23:47:23.124571: step 8174, loss 0.000370023, acc 1
2017-03-02T23:47:23.228347: step 8175, loss 0.00179955, acc 1
2017-03-02T23:47:23.334816: step 8176, loss 0.000445134, acc 1
2017-03-02T23:47:23.447708: step 8177, loss 0.000132514, acc 1
2017-03-02T23:47:23.540661: step 8178, loss 0.00367673, acc 1
2017-03-02T23:47:23.640239: step 8179, loss 0.000258644, acc 1
2017-03-02T23:47:23.729925: step 8180, loss 0.000634667, acc 1
2017-03-02T23:47:23.838546: step 8181, loss 0.00141058, acc 1
2017-03-02T23:47:23.945599: step 8182, loss 0.000151856, acc 1
2017-03-02T23:47:24.053612: step 8183, loss 0.000972911, acc 1
2017-03-02T23:47:24.161617: step 8184, loss 0.000201461, acc 1
2017-03-02T23:47:24.252881: step 8185, loss 7.02569e-05, acc 1
2017-03-02T23:47:24.353956: step 8186, loss 0.035359, acc 0.984375
2017-03-02T23:47:24.442575: step 8187, loss 0.00180391, acc 1
2017-03-02T23:47:24.552006: step 8188, loss 0.00126023, acc 1
2017-03-02T23:47:24.660967: step 8189, loss 0.000533295, acc 1
2017-03-02T23:47:24.763380: step 8190, loss 7.7764e-05, acc 1
2017-03-02T23:47:24.863676: step 8191, loss 0.000185703, acc 1
2017-03-02T23:47:24.951676: step 8192, loss 0.000523875, acc 1
2017-03-02T23:47:25.053828: step 8193, loss 0.00148853, acc 1
2017-03-02T23:47:25.145509: step 8194, loss 0.00176223, acc 1
2017-03-02T23:47:25.254006: step 8195, loss 0.00111384, acc 1
2017-03-02T23:47:25.369081: step 8196, loss 0.000861819, acc 1
2017-03-02T23:47:25.463453: step 8197, loss 0.0090213, acc 1
2017-03-02T23:47:25.577533: step 8198, loss 0.0356174, acc 0.984375
2017-03-02T23:47:25.671413: step 8199, loss 0.000868971, acc 1
2017-03-02T23:47:25.766666: step 8200, loss 0.00180563, acc 1

Evaluation:
2017-03-02T23:47:25.820167: step 8200, loss 3.53653, acc 0.536332

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488526215/checkpoints/model-8200

