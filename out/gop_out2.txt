
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=1,2,3,4,5
L2_REG_LAMBDA=0.0
LABELS_DATA_FILE=./nlp_features/gop_onehotlabels.npy
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=200
NUM_FILTERS=128
SENTENCES_DATA_FILE=./nlp_features/gop_sentences_list_of_strings.npy
UNROLLED_LSTM=False

Loading data...
Vocabulary Size: 13924
Train/Dev split: 12484/1387
Writing to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630

2017-03-02T17:32:20.921138: step 1, loss 3.41974, acc 0.25
2017-03-02T17:32:21.001122: step 2, loss 1.87253, acc 0.375
2017-03-02T17:32:21.070092: step 3, loss 2.24352, acc 0.4375
2017-03-02T17:32:21.138808: step 4, loss 2.90838, acc 0.5
2017-03-02T17:32:21.205493: step 5, loss 2.34057, acc 0.578125
2017-03-02T17:32:21.272857: step 6, loss 2.17, acc 0.5625
2017-03-02T17:32:21.347673: step 7, loss 2.64404, acc 0.5
2017-03-02T17:32:21.422142: step 8, loss 1.75038, acc 0.5625
2017-03-02T17:32:21.496908: step 9, loss 1.93575, acc 0.5
2017-03-02T17:32:21.566708: step 10, loss 2.58858, acc 0.421875
2017-03-02T17:32:21.651072: step 11, loss 1.92131, acc 0.453125
2017-03-02T17:32:21.720843: step 12, loss 2.11611, acc 0.375
2017-03-02T17:32:21.786036: step 13, loss 2.0801, acc 0.46875
2017-03-02T17:32:21.869197: step 14, loss 1.65711, acc 0.515625
2017-03-02T17:32:21.942484: step 15, loss 2.44215, acc 0.3125
2017-03-02T17:32:22.016757: step 16, loss 2.20241, acc 0.375
2017-03-02T17:32:22.095934: step 17, loss 2.03004, acc 0.46875
2017-03-02T17:32:22.167336: step 18, loss 1.52121, acc 0.640625
2017-03-02T17:32:22.238415: step 19, loss 1.69791, acc 0.59375
2017-03-02T17:32:22.320188: step 20, loss 2.13645, acc 0.484375
2017-03-02T17:32:22.394120: step 21, loss 1.97194, acc 0.5625
2017-03-02T17:32:22.456559: step 22, loss 1.92875, acc 0.5625
2017-03-02T17:32:22.549432: step 23, loss 2.53641, acc 0.53125
2017-03-02T17:32:22.622656: step 24, loss 1.93189, acc 0.578125
2017-03-02T17:32:22.694944: step 25, loss 2.50022, acc 0.53125
2017-03-02T17:32:22.768308: step 26, loss 1.27203, acc 0.65625
2017-03-02T17:32:22.836224: step 27, loss 1.95562, acc 0.59375
2017-03-02T17:32:22.912096: step 28, loss 2.34654, acc 0.5625
2017-03-02T17:32:22.986781: step 29, loss 1.81604, acc 0.515625
2017-03-02T17:32:23.056710: step 30, loss 2.2751, acc 0.484375
2017-03-02T17:32:23.127605: step 31, loss 1.8904, acc 0.53125
2017-03-02T17:32:23.198379: step 32, loss 2.18769, acc 0.421875
2017-03-02T17:32:23.269834: step 33, loss 1.78841, acc 0.546875
2017-03-02T17:32:23.339954: step 34, loss 1.71423, acc 0.5625
2017-03-02T17:32:23.427837: step 35, loss 1.73837, acc 0.5625
2017-03-02T17:32:23.500767: step 36, loss 2.17414, acc 0.453125
2017-03-02T17:32:23.573421: step 37, loss 1.81338, acc 0.515625
2017-03-02T17:32:23.637391: step 38, loss 1.58854, acc 0.546875
2017-03-02T17:32:23.710342: step 39, loss 1.83584, acc 0.5
2017-03-02T17:32:23.775265: step 40, loss 1.79691, acc 0.53125
2017-03-02T17:32:23.843960: step 41, loss 1.67861, acc 0.53125
2017-03-02T17:32:23.924647: step 42, loss 1.71947, acc 0.53125
2017-03-02T17:32:23.996724: step 43, loss 1.66156, acc 0.5625
2017-03-02T17:32:24.084962: step 44, loss 2.00293, acc 0.421875
2017-03-02T17:32:24.161530: step 45, loss 1.75781, acc 0.546875
2017-03-02T17:32:24.234733: step 46, loss 1.47609, acc 0.5625
2017-03-02T17:32:24.310233: step 47, loss 1.90313, acc 0.53125
2017-03-02T17:32:24.371782: step 48, loss 1.86184, acc 0.578125
2017-03-02T17:32:24.445115: step 49, loss 1.76893, acc 0.59375
2017-03-02T17:32:24.509951: step 50, loss 1.57319, acc 0.59375
2017-03-02T17:32:24.584451: step 51, loss 2.03244, acc 0.5
2017-03-02T17:32:24.654677: step 52, loss 1.56807, acc 0.515625
2017-03-02T17:32:24.725999: step 53, loss 1.70867, acc 0.53125
2017-03-02T17:32:24.806260: step 54, loss 1.44416, acc 0.65625
2017-03-02T17:32:24.886036: step 55, loss 1.6609, acc 0.484375
2017-03-02T17:32:24.962115: step 56, loss 1.62625, acc 0.5625
2017-03-02T17:32:25.044325: step 57, loss 1.74426, acc 0.4375
2017-03-02T17:32:25.119763: step 58, loss 1.31834, acc 0.625
2017-03-02T17:32:25.186332: step 59, loss 1.36819, acc 0.5
2017-03-02T17:32:25.254536: step 60, loss 1.68829, acc 0.5
2017-03-02T17:32:25.324639: step 61, loss 2.14812, acc 0.484375
2017-03-02T17:32:25.394873: step 62, loss 1.57971, acc 0.5625
2017-03-02T17:32:25.466974: step 63, loss 1.81241, acc 0.453125
2017-03-02T17:32:25.538712: step 64, loss 1.45209, acc 0.578125
2017-03-02T17:32:25.611747: step 65, loss 1.59914, acc 0.46875
2017-03-02T17:32:25.687240: step 66, loss 1.85631, acc 0.546875
2017-03-02T17:32:25.766232: step 67, loss 1.68075, acc 0.421875
2017-03-02T17:32:25.847118: step 68, loss 1.35774, acc 0.515625
2017-03-02T17:32:25.913970: step 69, loss 1.60789, acc 0.5625
2017-03-02T17:32:25.981749: step 70, loss 1.35513, acc 0.59375
2017-03-02T17:32:26.052906: step 71, loss 1.13653, acc 0.640625
2017-03-02T17:32:26.125873: step 72, loss 1.63562, acc 0.515625
2017-03-02T17:32:26.200787: step 73, loss 1.45279, acc 0.671875
2017-03-02T17:32:26.279527: step 74, loss 1.9558, acc 0.546875
2017-03-02T17:32:26.349480: step 75, loss 1.74098, acc 0.578125
2017-03-02T17:32:26.417311: step 76, loss 1.39952, acc 0.578125
2017-03-02T17:32:26.498508: step 77, loss 1.79924, acc 0.5625
2017-03-02T17:32:26.579649: step 78, loss 1.46802, acc 0.578125
2017-03-02T17:32:26.650132: step 79, loss 1.56875, acc 0.515625
2017-03-02T17:32:26.719703: step 80, loss 1.60988, acc 0.5625
2017-03-02T17:32:26.798804: step 81, loss 1.39336, acc 0.53125
2017-03-02T17:32:26.870637: step 82, loss 2.17403, acc 0.5
2017-03-02T17:32:26.947450: step 83, loss 1.70624, acc 0.59375
2017-03-02T17:32:27.029973: step 84, loss 1.64432, acc 0.53125
2017-03-02T17:32:27.106858: step 85, loss 1.11103, acc 0.609375
2017-03-02T17:32:27.176844: step 86, loss 1.18049, acc 0.59375
2017-03-02T17:32:27.248645: step 87, loss 1.09025, acc 0.671875
2017-03-02T17:32:27.319349: step 88, loss 1.76041, acc 0.546875
2017-03-02T17:32:27.398886: step 89, loss 1.63511, acc 0.5
2017-03-02T17:32:27.474320: step 90, loss 1.62657, acc 0.59375
2017-03-02T17:32:27.551471: step 91, loss 1.54238, acc 0.609375
2017-03-02T17:32:27.622241: step 92, loss 1.84342, acc 0.515625
2017-03-02T17:32:27.705247: step 93, loss 2.09498, acc 0.453125
2017-03-02T17:32:27.779788: step 94, loss 1.26909, acc 0.59375
2017-03-02T17:32:27.861972: step 95, loss 1.7646, acc 0.53125
2017-03-02T17:32:27.928987: step 96, loss 1.54904, acc 0.5625
2017-03-02T17:32:27.998645: step 97, loss 1.54856, acc 0.546875
2017-03-02T17:32:28.080928: step 98, loss 1.40238, acc 0.546875
2017-03-02T17:32:28.154181: step 99, loss 1.67246, acc 0.46875
2017-03-02T17:32:28.228887: step 100, loss 1.81717, acc 0.484375

Evaluation:
2017-03-02T17:32:28.350130: step 100, loss 0.929548, acc 0.643836

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-100

2017-03-02T17:32:28.782906: step 101, loss 1.51549, acc 0.453125
2017-03-02T17:32:28.861535: step 102, loss 1.92516, acc 0.46875
2017-03-02T17:32:28.922657: step 103, loss 1.53862, acc 0.5
2017-03-02T17:32:28.998622: step 104, loss 1.36752, acc 0.453125
2017-03-02T17:32:29.070707: step 105, loss 1.84448, acc 0.46875
2017-03-02T17:32:29.138853: step 106, loss 1.61695, acc 0.609375
2017-03-02T17:32:29.211730: step 107, loss 1.33543, acc 0.5625
2017-03-02T17:32:29.295839: step 108, loss 1.48638, acc 0.625
2017-03-02T17:32:29.363048: step 109, loss 1.51086, acc 0.546875
2017-03-02T17:32:29.430262: step 110, loss 1.68742, acc 0.515625
2017-03-02T17:32:29.503069: step 111, loss 2.03628, acc 0.421875
2017-03-02T17:32:29.572937: step 112, loss 1.59839, acc 0.515625
2017-03-02T17:32:29.637466: step 113, loss 1.28128, acc 0.46875
2017-03-02T17:32:29.706280: step 114, loss 1.71904, acc 0.4375
2017-03-02T17:32:29.791997: step 115, loss 1.65827, acc 0.484375
2017-03-02T17:32:29.861951: step 116, loss 1.2506, acc 0.453125
2017-03-02T17:32:29.932240: step 117, loss 1.56544, acc 0.546875
2017-03-02T17:32:30.003138: step 118, loss 1.68131, acc 0.515625
2017-03-02T17:32:30.071796: step 119, loss 1.595, acc 0.625
2017-03-02T17:32:30.141516: step 120, loss 1.0197, acc 0.671875
2017-03-02T17:32:30.212454: step 121, loss 1.41383, acc 0.53125
2017-03-02T17:32:30.276369: step 122, loss 1.3063, acc 0.53125
2017-03-02T17:32:30.344395: step 123, loss 1.1812, acc 0.640625
2017-03-02T17:32:30.408794: step 124, loss 1.35044, acc 0.625
2017-03-02T17:32:30.482489: step 125, loss 1.48844, acc 0.53125
2017-03-02T17:32:30.560773: step 126, loss 1.50001, acc 0.546875
2017-03-02T17:32:30.634072: step 127, loss 1.10353, acc 0.671875
2017-03-02T17:32:30.704603: step 128, loss 1.52028, acc 0.5
2017-03-02T17:32:30.775850: step 129, loss 1.68512, acc 0.53125
2017-03-02T17:32:30.844845: step 130, loss 1.23759, acc 0.5625
2017-03-02T17:32:30.916839: step 131, loss 1.38275, acc 0.609375
2017-03-02T17:32:30.990148: step 132, loss 1.07073, acc 0.59375
2017-03-02T17:32:31.061739: step 133, loss 1.42755, acc 0.53125
2017-03-02T17:32:31.132604: step 134, loss 1.18885, acc 0.578125
2017-03-02T17:32:31.211955: step 135, loss 1.30676, acc 0.609375
2017-03-02T17:32:31.283967: step 136, loss 1.861, acc 0.484375
2017-03-02T17:32:31.363061: step 137, loss 1.64286, acc 0.53125
2017-03-02T17:32:31.430656: step 138, loss 1.36024, acc 0.59375
2017-03-02T17:32:31.504079: step 139, loss 1.13123, acc 0.625
2017-03-02T17:32:31.579409: step 140, loss 1.23991, acc 0.59375
2017-03-02T17:32:31.650287: step 141, loss 1.49952, acc 0.546875
2017-03-02T17:32:31.722756: step 142, loss 1.22165, acc 0.609375
2017-03-02T17:32:31.789230: step 143, loss 1.53506, acc 0.515625
2017-03-02T17:32:31.854368: step 144, loss 1.26573, acc 0.53125
2017-03-02T17:32:31.924739: step 145, loss 1.26712, acc 0.609375
2017-03-02T17:32:31.997294: step 146, loss 1.25666, acc 0.53125
2017-03-02T17:32:32.073424: step 147, loss 1.25786, acc 0.609375
2017-03-02T17:32:32.146562: step 148, loss 1.41794, acc 0.609375
2017-03-02T17:32:32.215651: step 149, loss 1.44268, acc 0.625
2017-03-02T17:32:32.287820: step 150, loss 1.51008, acc 0.546875
2017-03-02T17:32:32.365663: step 151, loss 1.6218, acc 0.5625
2017-03-02T17:32:32.441643: step 152, loss 1.19447, acc 0.671875
2017-03-02T17:32:32.526662: step 153, loss 1.55138, acc 0.671875
2017-03-02T17:32:32.596767: step 154, loss 1.49955, acc 0.578125
2017-03-02T17:32:32.667929: step 155, loss 1.21779, acc 0.53125
2017-03-02T17:32:32.752718: step 156, loss 1.48051, acc 0.5625
2017-03-02T17:32:32.822525: step 157, loss 1.08557, acc 0.59375
2017-03-02T17:32:32.895228: step 158, loss 1.33063, acc 0.5625
2017-03-02T17:32:32.972546: step 159, loss 1.31238, acc 0.578125
2017-03-02T17:32:33.043516: step 160, loss 1.75618, acc 0.46875
2017-03-02T17:32:33.114923: step 161, loss 1.53118, acc 0.53125
2017-03-02T17:32:33.188862: step 162, loss 1.33805, acc 0.59375
2017-03-02T17:32:33.260064: step 163, loss 1.23632, acc 0.53125
2017-03-02T17:32:33.332366: step 164, loss 1.22361, acc 0.625
2017-03-02T17:32:33.410529: step 165, loss 1.36875, acc 0.546875
2017-03-02T17:32:33.485782: step 166, loss 1.45608, acc 0.515625
2017-03-02T17:32:33.558098: step 167, loss 0.940775, acc 0.6875
2017-03-02T17:32:33.634553: step 168, loss 1.51827, acc 0.5
2017-03-02T17:32:33.715287: step 169, loss 0.96214, acc 0.65625
2017-03-02T17:32:33.787102: step 170, loss 1.65574, acc 0.453125
2017-03-02T17:32:33.863493: step 171, loss 1.33681, acc 0.640625
2017-03-02T17:32:33.938002: step 172, loss 1.41575, acc 0.53125
2017-03-02T17:32:34.011147: step 173, loss 1.29472, acc 0.671875
2017-03-02T17:32:34.085248: step 174, loss 0.996822, acc 0.59375
2017-03-02T17:32:34.163926: step 175, loss 1.26339, acc 0.546875
2017-03-02T17:32:34.236079: step 176, loss 1.38755, acc 0.578125
2017-03-02T17:32:34.303905: step 177, loss 1.49703, acc 0.515625
2017-03-02T17:32:34.362967: step 178, loss 1.63388, acc 0.515625
2017-03-02T17:32:34.442579: step 179, loss 1.18095, acc 0.578125
2017-03-02T17:32:34.521179: step 180, loss 1.56004, acc 0.484375
2017-03-02T17:32:34.590778: step 181, loss 1.51926, acc 0.625
2017-03-02T17:32:34.664561: step 182, loss 1.34237, acc 0.484375
2017-03-02T17:32:34.724782: step 183, loss 1.41492, acc 0.5
2017-03-02T17:32:34.794594: step 184, loss 1.59719, acc 0.515625
2017-03-02T17:32:34.872556: step 185, loss 1.67215, acc 0.5
2017-03-02T17:32:34.941868: step 186, loss 1.25688, acc 0.578125
2017-03-02T17:32:35.011189: step 187, loss 1.0554, acc 0.609375
2017-03-02T17:32:35.083379: step 188, loss 1.16343, acc 0.578125
2017-03-02T17:32:35.159571: step 189, loss 1.55429, acc 0.5
2017-03-02T17:32:35.234295: step 190, loss 0.96114, acc 0.640625
2017-03-02T17:32:35.308479: step 191, loss 1.26966, acc 0.53125
2017-03-02T17:32:35.379040: step 192, loss 1.47583, acc 0.59375
2017-03-02T17:32:35.452572: step 193, loss 1.91071, acc 0.34375
2017-03-02T17:32:35.520892: step 194, loss 1.39642, acc 0.46875
2017-03-02T17:32:35.591297: step 195, loss 1.2594, acc 0.546875
2017-03-02T17:32:36.096791: step 196, loss 1.4082, acc 0.5
2017-03-02T17:32:36.170817: step 197, loss 1.67313, acc 0.453125
2017-03-02T17:32:36.255600: step 198, loss 0.978346, acc 0.59375
2017-03-02T17:32:36.324061: step 199, loss 1.2994, acc 0.546875
2017-03-02T17:32:36.400805: step 200, loss 1.40056, acc 0.5

Evaluation:
2017-03-02T17:32:36.426380: step 200, loss 0.895134, acc 0.599135

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-200

2017-03-02T17:32:36.894863: step 201, loss 1.23059, acc 0.5625
2017-03-02T17:32:36.963763: step 202, loss 1.23377, acc 0.578125
2017-03-02T17:32:37.039754: step 203, loss 0.997645, acc 0.59375
2017-03-02T17:32:37.108547: step 204, loss 1.15025, acc 0.578125
2017-03-02T17:32:37.203932: step 205, loss 1.13396, acc 0.625
2017-03-02T17:32:37.274009: step 206, loss 1.10582, acc 0.65625
2017-03-02T17:32:37.354313: step 207, loss 0.89613, acc 0.671875
2017-03-02T17:32:37.430921: step 208, loss 1.15167, acc 0.671875
2017-03-02T17:32:37.505130: step 209, loss 1.11847, acc 0.671875
2017-03-02T17:32:37.576314: step 210, loss 0.988222, acc 0.609375
2017-03-02T17:32:37.646914: step 211, loss 1.14119, acc 0.5625
2017-03-02T17:32:37.740408: step 212, loss 0.97742, acc 0.625
2017-03-02T17:32:37.818515: step 213, loss 0.895929, acc 0.640625
2017-03-02T17:32:37.882839: step 214, loss 1.19417, acc 0.53125
2017-03-02T17:32:37.951522: step 215, loss 0.914967, acc 0.671875
2017-03-02T17:32:38.024227: step 216, loss 1.22737, acc 0.5625
2017-03-02T17:32:38.126422: step 217, loss 0.99721, acc 0.703125
2017-03-02T17:32:38.203042: step 218, loss 1.22251, acc 0.625
2017-03-02T17:32:38.272999: step 219, loss 1.0046, acc 0.671875
2017-03-02T17:32:38.348739: step 220, loss 1.33434, acc 0.515625
2017-03-02T17:32:38.425840: step 221, loss 1.56255, acc 0.484375
2017-03-02T17:32:38.490221: step 222, loss 0.976152, acc 0.6875
2017-03-02T17:32:38.559689: step 223, loss 1.40265, acc 0.5
2017-03-02T17:32:38.654848: step 224, loss 1.03574, acc 0.59375
2017-03-02T17:32:38.726704: step 225, loss 0.910273, acc 0.703125
2017-03-02T17:32:38.801358: step 226, loss 1.55009, acc 0.5625
2017-03-02T17:32:38.872658: step 227, loss 1.10888, acc 0.5625
2017-03-02T17:32:38.948252: step 228, loss 1.2157, acc 0.5625
2017-03-02T17:32:39.018751: step 229, loss 1.06154, acc 0.578125
2017-03-02T17:32:39.095044: step 230, loss 1.10084, acc 0.625
2017-03-02T17:32:39.165461: step 231, loss 1.13034, acc 0.546875
2017-03-02T17:32:39.231801: step 232, loss 1.14404, acc 0.609375
2017-03-02T17:32:39.302633: step 233, loss 1.12328, acc 0.640625
2017-03-02T17:32:39.389444: step 234, loss 1.14282, acc 0.59375
2017-03-02T17:32:39.463015: step 235, loss 1.30992, acc 0.546875
2017-03-02T17:32:39.555479: step 236, loss 1.3445, acc 0.609375
2017-03-02T17:32:39.624648: step 237, loss 0.793987, acc 0.71875
2017-03-02T17:32:39.698032: step 238, loss 1.01142, acc 0.5625
2017-03-02T17:32:39.769789: step 239, loss 0.689892, acc 0.75
2017-03-02T17:32:39.828667: step 240, loss 0.919846, acc 0.671875
2017-03-02T17:32:39.895883: step 241, loss 1.21768, acc 0.625
2017-03-02T17:32:39.963884: step 242, loss 0.745848, acc 0.703125
2017-03-02T17:32:40.047663: step 243, loss 1.05381, acc 0.65625
2017-03-02T17:32:40.129673: step 244, loss 1.03373, acc 0.59375
2017-03-02T17:32:40.201631: step 245, loss 0.896465, acc 0.65625
2017-03-02T17:32:40.278678: step 246, loss 1.06146, acc 0.640625
2017-03-02T17:32:40.349917: step 247, loss 1.34324, acc 0.53125
2017-03-02T17:32:40.423607: step 248, loss 1.40591, acc 0.546875
2017-03-02T17:32:40.497988: step 249, loss 1.21039, acc 0.609375
2017-03-02T17:32:40.563986: step 250, loss 0.898446, acc 0.65625
2017-03-02T17:32:40.635600: step 251, loss 0.960173, acc 0.625
2017-03-02T17:32:40.710486: step 252, loss 1.15892, acc 0.59375
2017-03-02T17:32:40.781402: step 253, loss 1.21868, acc 0.453125
2017-03-02T17:32:40.855133: step 254, loss 1.12651, acc 0.59375
2017-03-02T17:32:40.926725: step 255, loss 1.27927, acc 0.484375
2017-03-02T17:32:40.993660: step 256, loss 1.44276, acc 0.453125
2017-03-02T17:32:41.065817: step 257, loss 1.01053, acc 0.578125
2017-03-02T17:32:41.139709: step 258, loss 1.13969, acc 0.5625
2017-03-02T17:32:41.215946: step 259, loss 1.10723, acc 0.59375
2017-03-02T17:32:41.285648: step 260, loss 1.35783, acc 0.515625
2017-03-02T17:32:41.357112: step 261, loss 0.948453, acc 0.65625
2017-03-02T17:32:41.430376: step 262, loss 0.77316, acc 0.65625
2017-03-02T17:32:41.509204: step 263, loss 1.00068, acc 0.609375
2017-03-02T17:32:41.583559: step 264, loss 0.897924, acc 0.71875
2017-03-02T17:32:41.660232: step 265, loss 1.30826, acc 0.578125
2017-03-02T17:32:41.744492: step 266, loss 1.17199, acc 0.5625
2017-03-02T17:32:41.810638: step 267, loss 1.16664, acc 0.5625
2017-03-02T17:32:41.884942: step 268, loss 1.14978, acc 0.609375
2017-03-02T17:32:41.959213: step 269, loss 1.16513, acc 0.515625
2017-03-02T17:32:42.028470: step 270, loss 0.836704, acc 0.703125
2017-03-02T17:32:42.099222: step 271, loss 0.786203, acc 0.734375
2017-03-02T17:32:42.182143: step 272, loss 1.00251, acc 0.640625
2017-03-02T17:32:42.257264: step 273, loss 1.18691, acc 0.578125
2017-03-02T17:32:42.334853: step 274, loss 0.955623, acc 0.6875
2017-03-02T17:32:42.404458: step 275, loss 1.1856, acc 0.546875
2017-03-02T17:32:42.473098: step 276, loss 0.929375, acc 0.59375
2017-03-02T17:32:42.552427: step 277, loss 0.825348, acc 0.65625
2017-03-02T17:32:42.617458: step 278, loss 1.14474, acc 0.5625
2017-03-02T17:32:42.686518: step 279, loss 1.08948, acc 0.546875
2017-03-02T17:32:42.758666: step 280, loss 1.0526, acc 0.578125
2017-03-02T17:32:42.833077: step 281, loss 0.744885, acc 0.609375
2017-03-02T17:32:42.905828: step 282, loss 0.839068, acc 0.671875
2017-03-02T17:32:42.978667: step 283, loss 1.33715, acc 0.578125
2017-03-02T17:32:43.050166: step 284, loss 0.858306, acc 0.6875
2017-03-02T17:32:43.125372: step 285, loss 1.10538, acc 0.53125
2017-03-02T17:32:43.194046: step 286, loss 0.98994, acc 0.671875
2017-03-02T17:32:43.267661: step 287, loss 1.05044, acc 0.609375
2017-03-02T17:32:43.332558: step 288, loss 0.810544, acc 0.765625
2017-03-02T17:32:43.397291: step 289, loss 0.942231, acc 0.640625
2017-03-02T17:32:43.469298: step 290, loss 1.3346, acc 0.59375
2017-03-02T17:32:43.543570: step 291, loss 1.16014, acc 0.546875
2017-03-02T17:32:43.614717: step 292, loss 0.889168, acc 0.640625
2017-03-02T17:32:43.688565: step 293, loss 1.30869, acc 0.453125
2017-03-02T17:32:43.763646: step 294, loss 1.41695, acc 0.390625
2017-03-02T17:32:43.831995: step 295, loss 0.775681, acc 0.65625
2017-03-02T17:32:43.908206: step 296, loss 0.945556, acc 0.703125
2017-03-02T17:32:43.989907: step 297, loss 1.16828, acc 0.640625
2017-03-02T17:32:44.069212: step 298, loss 1.17843, acc 0.609375
2017-03-02T17:32:44.142947: step 299, loss 0.735173, acc 0.6875
2017-03-02T17:32:44.223467: step 300, loss 0.937432, acc 0.59375

Evaluation:
2017-03-02T17:32:44.257817: step 300, loss 0.928301, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-300

2017-03-02T17:32:44.685272: step 301, loss 1.07847, acc 0.640625
2017-03-02T17:32:44.753872: step 302, loss 0.784615, acc 0.640625
2017-03-02T17:32:44.827502: step 303, loss 0.961786, acc 0.671875
2017-03-02T17:32:44.904560: step 304, loss 0.98357, acc 0.578125
2017-03-02T17:32:44.974937: step 305, loss 0.887805, acc 0.640625
2017-03-02T17:32:45.051543: step 306, loss 1.05495, acc 0.640625
2017-03-02T17:32:45.124856: step 307, loss 1.13629, acc 0.625
2017-03-02T17:32:45.193815: step 308, loss 0.866791, acc 0.671875
2017-03-02T17:32:45.268339: step 309, loss 0.991637, acc 0.609375
2017-03-02T17:32:45.337692: step 310, loss 0.900076, acc 0.65625
2017-03-02T17:32:45.406500: step 311, loss 0.954429, acc 0.5625
2017-03-02T17:32:45.472167: step 312, loss 1.04902, acc 0.640625
2017-03-02T17:32:45.557231: step 313, loss 1.00246, acc 0.65625
2017-03-02T17:32:45.627387: step 314, loss 1.00478, acc 0.59375
2017-03-02T17:32:45.706240: step 315, loss 1.06269, acc 0.53125
2017-03-02T17:32:45.775647: step 316, loss 0.898479, acc 0.703125
2017-03-02T17:32:45.846603: step 317, loss 0.838679, acc 0.640625
2017-03-02T17:32:45.932083: step 318, loss 1.17732, acc 0.546875
2017-03-02T17:32:46.001897: step 319, loss 1.17513, acc 0.59375
2017-03-02T17:32:46.077415: step 320, loss 1.09355, acc 0.59375
2017-03-02T17:32:46.146972: step 321, loss 1.04541, acc 0.609375
2017-03-02T17:32:46.226431: step 322, loss 1.20188, acc 0.578125
2017-03-02T17:32:46.299619: step 323, loss 1.14309, acc 0.578125
2017-03-02T17:32:46.376882: step 324, loss 1.01813, acc 0.578125
2017-03-02T17:32:46.447645: step 325, loss 0.999661, acc 0.578125
2017-03-02T17:32:46.521083: step 326, loss 1.10369, acc 0.578125
2017-03-02T17:32:46.590483: step 327, loss 0.78644, acc 0.671875
2017-03-02T17:32:46.663220: step 328, loss 1.09277, acc 0.640625
2017-03-02T17:32:46.752865: step 329, loss 1.03511, acc 0.65625
2017-03-02T17:32:46.824960: step 330, loss 0.824764, acc 0.734375
2017-03-02T17:32:46.901489: step 331, loss 1.30602, acc 0.625
2017-03-02T17:32:46.971143: step 332, loss 0.813401, acc 0.6875
2017-03-02T17:32:47.045063: step 333, loss 1.00415, acc 0.71875
2017-03-02T17:32:47.118530: step 334, loss 1.00027, acc 0.640625
2017-03-02T17:32:47.193354: step 335, loss 0.915117, acc 0.625
2017-03-02T17:32:47.273470: step 336, loss 1.155, acc 0.515625
2017-03-02T17:32:47.353274: step 337, loss 1.35356, acc 0.390625
2017-03-02T17:32:47.419460: step 338, loss 0.958454, acc 0.5625
2017-03-02T17:32:47.513141: step 339, loss 0.985428, acc 0.578125
2017-03-02T17:32:47.583877: step 340, loss 1.04903, acc 0.5
2017-03-02T17:32:47.650291: step 341, loss 1.04543, acc 0.640625
2017-03-02T17:32:47.726939: step 342, loss 0.785292, acc 0.640625
2017-03-02T17:32:47.802874: step 343, loss 0.842198, acc 0.625
2017-03-02T17:32:47.874827: step 344, loss 1.05828, acc 0.59375
2017-03-02T17:32:47.940937: step 345, loss 1.26046, acc 0.546875
2017-03-02T17:32:48.015922: step 346, loss 0.840844, acc 0.671875
2017-03-02T17:32:48.099748: step 347, loss 0.856933, acc 0.703125
2017-03-02T17:32:48.176416: step 348, loss 1.01122, acc 0.609375
2017-03-02T17:32:48.249454: step 349, loss 0.918666, acc 0.625
2017-03-02T17:32:48.326806: step 350, loss 0.946194, acc 0.609375
2017-03-02T17:32:48.400667: step 351, loss 0.671862, acc 0.71875
2017-03-02T17:32:48.473295: step 352, loss 1.03039, acc 0.640625
2017-03-02T17:32:48.561972: step 353, loss 0.832949, acc 0.671875
2017-03-02T17:32:48.631267: step 354, loss 0.82089, acc 0.65625
2017-03-02T17:32:48.702872: step 355, loss 1.09569, acc 0.515625
2017-03-02T17:32:48.776086: step 356, loss 0.966104, acc 0.625
2017-03-02T17:32:48.846730: step 357, loss 0.892282, acc 0.625
2017-03-02T17:32:48.918884: step 358, loss 1.10133, acc 0.640625
2017-03-02T17:32:48.991094: step 359, loss 0.872542, acc 0.671875
2017-03-02T17:32:49.063164: step 360, loss 0.814869, acc 0.71875
2017-03-02T17:32:49.139930: step 361, loss 0.696419, acc 0.796875
2017-03-02T17:32:49.207770: step 362, loss 0.987019, acc 0.59375
2017-03-02T17:32:49.279458: step 363, loss 1.00994, acc 0.609375
2017-03-02T17:32:49.349711: step 364, loss 0.897472, acc 0.640625
2017-03-02T17:32:49.431096: step 365, loss 1.04407, acc 0.625
2017-03-02T17:32:49.493942: step 366, loss 1.16045, acc 0.5625
2017-03-02T17:32:49.564217: step 367, loss 0.817513, acc 0.640625
2017-03-02T17:32:49.639801: step 368, loss 1.01699, acc 0.578125
2017-03-02T17:32:49.716103: step 369, loss 0.795372, acc 0.703125
2017-03-02T17:32:49.789391: step 370, loss 0.895799, acc 0.59375
2017-03-02T17:32:49.866450: step 371, loss 1.03457, acc 0.5625
2017-03-02T17:32:49.940172: step 372, loss 0.908252, acc 0.640625
2017-03-02T17:32:50.015871: step 373, loss 0.921729, acc 0.625
2017-03-02T17:32:50.101640: step 374, loss 1.02914, acc 0.640625
2017-03-02T17:32:50.181998: step 375, loss 1.20984, acc 0.5625
2017-03-02T17:32:50.253574: step 376, loss 0.909607, acc 0.671875
2017-03-02T17:32:50.324680: step 377, loss 1.12297, acc 0.609375
2017-03-02T17:32:50.395541: step 378, loss 0.784338, acc 0.671875
2017-03-02T17:32:50.489138: step 379, loss 1.15631, acc 0.515625
2017-03-02T17:32:50.561827: step 380, loss 1.02673, acc 0.5625
2017-03-02T17:32:50.634656: step 381, loss 0.742874, acc 0.75
2017-03-02T17:32:50.727579: step 382, loss 0.778991, acc 0.671875
2017-03-02T17:32:50.810473: step 383, loss 1.17447, acc 0.484375
2017-03-02T17:32:50.879331: step 384, loss 0.663533, acc 0.6875
2017-03-02T17:32:50.947607: step 385, loss 0.861546, acc 0.640625
2017-03-02T17:32:51.022126: step 386, loss 0.628297, acc 0.8125
2017-03-02T17:32:51.089851: step 387, loss 1.14689, acc 0.53125
2017-03-02T17:32:51.157326: step 388, loss 0.936437, acc 0.640625
2017-03-02T17:32:51.235407: step 389, loss 0.98641, acc 0.640625
2017-03-02T17:32:51.311367: step 390, loss 1.06384, acc 0.5625
2017-03-02T17:32:51.386968: step 391, loss 0.765076, acc 0.703125
2017-03-02T17:32:51.463049: step 392, loss 1.84364, acc 0.5
2017-03-02T17:32:51.537241: step 393, loss 0.881397, acc 0.5625
2017-03-02T17:32:51.601327: step 394, loss 0.840969, acc 0.65625
2017-03-02T17:32:51.683015: step 395, loss 1.11925, acc 0.53125
2017-03-02T17:32:51.755510: step 396, loss 1.09904, acc 0.546875
2017-03-02T17:32:51.830384: step 397, loss 1.05184, acc 0.578125
2017-03-02T17:32:51.905531: step 398, loss 0.857204, acc 0.609375
2017-03-02T17:32:51.978611: step 399, loss 1.10113, acc 0.625
2017-03-02T17:32:52.049885: step 400, loss 0.698549, acc 0.703125

Evaluation:
2017-03-02T17:32:52.080684: step 400, loss 0.827883, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-400

2017-03-02T17:32:52.508067: step 401, loss 0.916224, acc 0.609375
2017-03-02T17:32:52.604408: step 402, loss 0.753154, acc 0.6875
2017-03-02T17:32:52.671345: step 403, loss 0.950185, acc 0.625
2017-03-02T17:32:52.743058: step 404, loss 0.92556, acc 0.671875
2017-03-02T17:32:52.823241: step 405, loss 0.900428, acc 0.671875
2017-03-02T17:32:52.884943: step 406, loss 0.93182, acc 0.5625
2017-03-02T17:32:52.960746: step 407, loss 0.944727, acc 0.65625
2017-03-02T17:32:53.029418: step 408, loss 0.820184, acc 0.625
2017-03-02T17:32:53.103063: step 409, loss 0.871534, acc 0.65625
2017-03-02T17:32:53.175866: step 410, loss 0.872229, acc 0.53125
2017-03-02T17:32:53.249629: step 411, loss 0.753242, acc 0.65625
2017-03-02T17:32:53.319896: step 412, loss 0.639982, acc 0.6875
2017-03-02T17:32:53.390319: step 413, loss 0.881211, acc 0.671875
2017-03-02T17:32:53.465191: step 414, loss 0.842745, acc 0.65625
2017-03-02T17:32:53.537942: step 415, loss 0.960459, acc 0.5625
2017-03-02T17:32:53.608068: step 416, loss 1.32727, acc 0.46875
2017-03-02T17:32:53.676400: step 417, loss 1.05449, acc 0.578125
2017-03-02T17:32:53.753926: step 418, loss 0.757405, acc 0.625
2017-03-02T17:32:53.825804: step 419, loss 0.745617, acc 0.609375
2017-03-02T17:32:53.900499: step 420, loss 0.859465, acc 0.625
2017-03-02T17:32:53.981348: step 421, loss 0.949587, acc 0.578125
2017-03-02T17:32:54.064115: step 422, loss 0.985443, acc 0.59375
2017-03-02T17:32:54.133819: step 423, loss 0.834317, acc 0.6875
2017-03-02T17:32:54.207255: step 424, loss 0.835394, acc 0.6875
2017-03-02T17:32:54.273528: step 425, loss 0.961264, acc 0.6875
2017-03-02T17:32:54.343058: step 426, loss 0.702251, acc 0.671875
2017-03-02T17:32:54.409393: step 427, loss 0.612217, acc 0.703125
2017-03-02T17:32:54.482052: step 428, loss 0.867445, acc 0.6875
2017-03-02T17:32:54.559548: step 429, loss 0.775836, acc 0.71875
2017-03-02T17:32:54.640144: step 430, loss 0.793712, acc 0.703125
2017-03-02T17:32:54.716574: step 431, loss 0.882865, acc 0.6875
2017-03-02T17:32:54.787992: step 432, loss 0.981756, acc 0.546875
2017-03-02T17:32:54.861359: step 433, loss 0.89279, acc 0.5625
2017-03-02T17:32:54.926671: step 434, loss 0.781951, acc 0.640625
2017-03-02T17:32:54.996670: step 435, loss 0.826266, acc 0.609375
2017-03-02T17:32:55.062844: step 436, loss 0.761669, acc 0.65625
2017-03-02T17:32:55.133136: step 437, loss 0.994502, acc 0.65625
2017-03-02T17:32:55.204678: step 438, loss 0.961719, acc 0.625
2017-03-02T17:32:55.272638: step 439, loss 0.645189, acc 0.703125
2017-03-02T17:32:55.339439: step 440, loss 0.676531, acc 0.703125
2017-03-02T17:32:55.421332: step 441, loss 0.69513, acc 0.703125
2017-03-02T17:32:55.492996: step 442, loss 0.967248, acc 0.65625
2017-03-02T17:32:55.595614: step 443, loss 0.764724, acc 0.703125
2017-03-02T17:32:55.658761: step 444, loss 0.67524, acc 0.765625
2017-03-02T17:32:55.755344: step 445, loss 0.70433, acc 0.6875
2017-03-02T17:32:55.828759: step 446, loss 0.802157, acc 0.703125
2017-03-02T17:32:55.901707: step 447, loss 0.810531, acc 0.640625
2017-03-02T17:32:55.976065: step 448, loss 0.713557, acc 0.71875
2017-03-02T17:32:56.060542: step 449, loss 1.11097, acc 0.515625
2017-03-02T17:32:56.133812: step 450, loss 0.81128, acc 0.65625
2017-03-02T17:32:56.202111: step 451, loss 0.804301, acc 0.671875
2017-03-02T17:32:56.274247: step 452, loss 0.973621, acc 0.59375
2017-03-02T17:32:56.344758: step 453, loss 0.807295, acc 0.625
2017-03-02T17:32:56.413662: step 454, loss 0.656841, acc 0.734375
2017-03-02T17:32:56.480088: step 455, loss 0.777524, acc 0.671875
2017-03-02T17:32:56.550779: step 456, loss 0.622736, acc 0.703125
2017-03-02T17:32:56.615807: step 457, loss 0.773055, acc 0.6875
2017-03-02T17:32:56.696081: step 458, loss 0.794253, acc 0.71875
2017-03-02T17:32:56.764859: step 459, loss 0.896145, acc 0.65625
2017-03-02T17:32:56.837206: step 460, loss 0.820061, acc 0.65625
2017-03-02T17:32:56.917774: step 461, loss 0.791747, acc 0.65625
2017-03-02T17:32:56.994548: step 462, loss 0.804145, acc 0.65625
2017-03-02T17:32:57.062154: step 463, loss 0.70297, acc 0.765625
2017-03-02T17:32:57.128664: step 464, loss 0.616019, acc 0.65625
2017-03-02T17:32:57.201967: step 465, loss 0.867202, acc 0.703125
2017-03-02T17:32:57.298802: step 466, loss 1.13292, acc 0.609375
2017-03-02T17:32:57.373093: step 467, loss 0.667262, acc 0.671875
2017-03-02T17:32:57.446056: step 468, loss 0.834069, acc 0.640625
2017-03-02T17:32:57.518362: step 469, loss 1.00409, acc 0.59375
2017-03-02T17:32:57.584653: step 470, loss 0.791824, acc 0.671875
2017-03-02T17:32:57.653941: step 471, loss 0.776131, acc 0.71875
2017-03-02T17:32:57.725256: step 472, loss 0.704016, acc 0.734375
2017-03-02T17:32:57.797845: step 473, loss 0.87899, acc 0.5625
2017-03-02T17:32:57.867025: step 474, loss 0.651408, acc 0.765625
2017-03-02T17:32:57.938711: step 475, loss 0.619238, acc 0.796875
2017-03-02T17:32:58.017140: step 476, loss 0.991647, acc 0.609375
2017-03-02T17:32:58.090753: step 477, loss 0.958083, acc 0.59375
2017-03-02T17:32:58.158733: step 478, loss 0.845306, acc 0.671875
2017-03-02T17:32:58.239763: step 479, loss 0.831199, acc 0.609375
2017-03-02T17:32:58.311158: step 480, loss 0.892294, acc 0.625
2017-03-02T17:32:58.387038: step 481, loss 0.885554, acc 0.609375
2017-03-02T17:32:58.455670: step 482, loss 0.683246, acc 0.71875
2017-03-02T17:32:58.524635: step 483, loss 0.94281, acc 0.578125
2017-03-02T17:32:58.593729: step 484, loss 0.916338, acc 0.59375
2017-03-02T17:32:58.663647: step 485, loss 0.768408, acc 0.625
2017-03-02T17:32:58.734110: step 486, loss 1.03947, acc 0.546875
2017-03-02T17:32:58.806984: step 487, loss 0.83339, acc 0.625
2017-03-02T17:32:58.891490: step 488, loss 0.707524, acc 0.703125
2017-03-02T17:32:58.961918: step 489, loss 1.06231, acc 0.640625
2017-03-02T17:32:59.034037: step 490, loss 0.829882, acc 0.6875
2017-03-02T17:32:59.113389: step 491, loss 0.919248, acc 0.59375
2017-03-02T17:32:59.179882: step 492, loss 0.807228, acc 0.65625
2017-03-02T17:32:59.243901: step 493, loss 0.952536, acc 0.546875
2017-03-02T17:32:59.319532: step 494, loss 0.693074, acc 0.65625
2017-03-02T17:32:59.392834: step 495, loss 0.925912, acc 0.640625
2017-03-02T17:32:59.466559: step 496, loss 0.646001, acc 0.734375
2017-03-02T17:32:59.538622: step 497, loss 0.875887, acc 0.65625
2017-03-02T17:32:59.614704: step 498, loss 0.943075, acc 0.625
2017-03-02T17:32:59.684863: step 499, loss 0.71745, acc 0.640625
2017-03-02T17:32:59.757329: step 500, loss 0.967786, acc 0.703125

Evaluation:
2017-03-02T17:32:59.781261: step 500, loss 0.836777, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-500

2017-03-02T17:33:00.233860: step 501, loss 0.81377, acc 0.640625
2017-03-02T17:33:00.305568: step 502, loss 0.766833, acc 0.6875
2017-03-02T17:33:00.377878: step 503, loss 0.726139, acc 0.6875
2017-03-02T17:33:00.448172: step 504, loss 0.77389, acc 0.640625
2017-03-02T17:33:00.516588: step 505, loss 0.613612, acc 0.78125
2017-03-02T17:33:00.588051: step 506, loss 0.780505, acc 0.59375
2017-03-02T17:33:00.661380: step 507, loss 0.829701, acc 0.65625
2017-03-02T17:33:00.738738: step 508, loss 0.846773, acc 0.6875
2017-03-02T17:33:00.810716: step 509, loss 0.686099, acc 0.6875
2017-03-02T17:33:00.886963: step 510, loss 0.818658, acc 0.65625
2017-03-02T17:33:00.962436: step 511, loss 0.70546, acc 0.71875
2017-03-02T17:33:01.033664: step 512, loss 0.734259, acc 0.6875
2017-03-02T17:33:01.106883: step 513, loss 0.922298, acc 0.625
2017-03-02T17:33:01.177969: step 514, loss 0.84793, acc 0.609375
2017-03-02T17:33:01.252744: step 515, loss 0.811872, acc 0.640625
2017-03-02T17:33:01.313515: step 516, loss 0.714597, acc 0.640625
2017-03-02T17:33:01.381192: step 517, loss 0.684501, acc 0.65625
2017-03-02T17:33:01.453577: step 518, loss 0.741751, acc 0.734375
2017-03-02T17:33:01.529876: step 519, loss 0.753956, acc 0.640625
2017-03-02T17:33:01.604436: step 520, loss 0.886353, acc 0.625
2017-03-02T17:33:01.677741: step 521, loss 0.787505, acc 0.671875
2017-03-02T17:33:01.748552: step 522, loss 0.693083, acc 0.734375
2017-03-02T17:33:01.817164: step 523, loss 0.443102, acc 0.828125
2017-03-02T17:33:01.889900: step 524, loss 0.98137, acc 0.65625
2017-03-02T17:33:01.961441: step 525, loss 0.656203, acc 0.75
2017-03-02T17:33:02.033085: step 526, loss 0.854882, acc 0.65625
2017-03-02T17:33:02.115775: step 527, loss 0.832607, acc 0.6875
2017-03-02T17:33:02.190347: step 528, loss 0.909331, acc 0.609375
2017-03-02T17:33:02.268044: step 529, loss 1.16192, acc 0.546875
2017-03-02T17:33:02.342482: step 530, loss 0.846682, acc 0.65625
2017-03-02T17:33:02.418994: step 531, loss 0.74187, acc 0.640625
2017-03-02T17:33:02.492603: step 532, loss 0.759291, acc 0.703125
2017-03-02T17:33:02.561940: step 533, loss 0.834373, acc 0.609375
2017-03-02T17:33:02.655713: step 534, loss 0.821916, acc 0.671875
2017-03-02T17:33:02.718295: step 535, loss 0.777094, acc 0.671875
2017-03-02T17:33:02.797517: step 536, loss 0.735823, acc 0.75
2017-03-02T17:33:02.872830: step 537, loss 0.615882, acc 0.71875
2017-03-02T17:33:02.942955: step 538, loss 0.899604, acc 0.625
2017-03-02T17:33:03.019337: step 539, loss 0.736363, acc 0.625
2017-03-02T17:33:03.092180: step 540, loss 0.957533, acc 0.640625
2017-03-02T17:33:03.169027: step 541, loss 0.746793, acc 0.65625
2017-03-02T17:33:03.239374: step 542, loss 0.818801, acc 0.671875
2017-03-02T17:33:03.314425: step 543, loss 0.813231, acc 0.671875
2017-03-02T17:33:03.392565: step 544, loss 0.748931, acc 0.6875
2017-03-02T17:33:03.465619: step 545, loss 0.702745, acc 0.65625
2017-03-02T17:33:03.534523: step 546, loss 1.02967, acc 0.609375
2017-03-02T17:33:03.605841: step 547, loss 0.713012, acc 0.734375
2017-03-02T17:33:03.680491: step 548, loss 0.894657, acc 0.609375
2017-03-02T17:33:03.749005: step 549, loss 0.996648, acc 0.578125
2017-03-02T17:33:03.818692: step 550, loss 0.495744, acc 0.8125
2017-03-02T17:33:03.888106: step 551, loss 0.849824, acc 0.671875
2017-03-02T17:33:03.955013: step 552, loss 0.867521, acc 0.640625
2017-03-02T17:33:04.024904: step 553, loss 0.870397, acc 0.625
2017-03-02T17:33:04.096742: step 554, loss 0.863521, acc 0.71875
2017-03-02T17:33:04.177469: step 555, loss 0.885784, acc 0.6875
2017-03-02T17:33:04.267746: step 556, loss 0.82836, acc 0.65625
2017-03-02T17:33:04.339595: step 557, loss 0.71284, acc 0.59375
2017-03-02T17:33:04.419595: step 558, loss 0.657595, acc 0.71875
2017-03-02T17:33:04.490589: step 559, loss 0.686025, acc 0.671875
2017-03-02T17:33:04.565156: step 560, loss 0.648111, acc 0.71875
2017-03-02T17:33:04.635225: step 561, loss 0.832318, acc 0.640625
2017-03-02T17:33:04.706907: step 562, loss 0.821266, acc 0.640625
2017-03-02T17:33:04.776896: step 563, loss 0.738345, acc 0.640625
2017-03-02T17:33:04.850951: step 564, loss 0.857616, acc 0.578125
2017-03-02T17:33:04.921451: step 565, loss 0.945245, acc 0.609375
2017-03-02T17:33:04.996220: step 566, loss 0.87697, acc 0.671875
2017-03-02T17:33:05.085059: step 567, loss 0.835258, acc 0.59375
2017-03-02T17:33:05.155017: step 568, loss 0.969882, acc 0.515625
2017-03-02T17:33:05.227843: step 569, loss 0.887295, acc 0.640625
2017-03-02T17:33:05.307566: step 570, loss 0.856179, acc 0.65625
2017-03-02T17:33:05.378313: step 571, loss 0.694893, acc 0.71875
2017-03-02T17:33:05.449450: step 572, loss 0.662116, acc 0.734375
2017-03-02T17:33:05.523540: step 573, loss 0.799995, acc 0.71875
2017-03-02T17:33:05.603986: step 574, loss 0.743667, acc 0.734375
2017-03-02T17:33:05.689657: step 575, loss 0.970222, acc 0.671875
2017-03-02T17:33:05.763730: step 576, loss 0.993964, acc 0.546875
2017-03-02T17:33:05.835630: step 577, loss 0.7696, acc 0.734375
2017-03-02T17:33:05.905991: step 578, loss 0.858927, acc 0.671875
2017-03-02T17:33:05.983990: step 579, loss 0.850156, acc 0.65625
2017-03-02T17:33:06.064779: step 580, loss 0.739055, acc 0.71875
2017-03-02T17:33:06.146376: step 581, loss 0.636226, acc 0.703125
2017-03-02T17:33:06.219782: step 582, loss 0.670006, acc 0.71875
2017-03-02T17:33:06.292753: step 583, loss 0.826326, acc 0.703125
2017-03-02T17:33:06.362844: step 584, loss 0.595527, acc 0.765625
2017-03-02T17:33:06.434028: step 585, loss 0.757033, acc 0.75
2017-03-02T17:33:06.504331: step 586, loss 0.845742, acc 0.59375
2017-03-02T17:33:06.582475: step 587, loss 0.758813, acc 0.703125
2017-03-02T17:33:06.650318: step 588, loss 0.464396, acc 0.75
2017-03-02T17:33:06.722989: step 589, loss 0.738687, acc 0.734375
2017-03-02T17:33:06.791831: step 590, loss 0.7726, acc 0.671875
2017-03-02T17:33:06.860885: step 591, loss 0.913912, acc 0.609375
2017-03-02T17:33:06.931787: step 592, loss 0.884836, acc 0.625
2017-03-02T17:33:07.008945: step 593, loss 0.710344, acc 0.71875
2017-03-02T17:33:07.085393: step 594, loss 0.622014, acc 0.734375
2017-03-02T17:33:07.157262: step 595, loss 0.732256, acc 0.734375
2017-03-02T17:33:07.230617: step 596, loss 0.628343, acc 0.6875
2017-03-02T17:33:07.304143: step 597, loss 0.495477, acc 0.8125
2017-03-02T17:33:07.385117: step 598, loss 0.79769, acc 0.65625
2017-03-02T17:33:07.455283: step 599, loss 0.728244, acc 0.734375
2017-03-02T17:33:07.522773: step 600, loss 0.710523, acc 0.703125

Evaluation:
2017-03-02T17:33:07.556743: step 600, loss 0.835352, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-600

2017-03-02T17:33:08.025275: step 601, loss 0.555258, acc 0.78125
2017-03-02T17:33:08.094277: step 602, loss 0.794468, acc 0.65625
2017-03-02T17:33:08.163368: step 603, loss 0.634688, acc 0.796875
2017-03-02T17:33:08.235187: step 604, loss 0.883722, acc 0.59375
2017-03-02T17:33:08.311589: step 605, loss 0.663867, acc 0.734375
2017-03-02T17:33:08.382705: step 606, loss 0.919023, acc 0.640625
2017-03-02T17:33:08.473026: step 607, loss 0.808484, acc 0.671875
2017-03-02T17:33:08.546772: step 608, loss 0.650581, acc 0.75
2017-03-02T17:33:08.615092: step 609, loss 0.916713, acc 0.609375
2017-03-02T17:33:08.683939: step 610, loss 0.815699, acc 0.6875
2017-03-02T17:33:08.759290: step 611, loss 0.827236, acc 0.59375
2017-03-02T17:33:08.827044: step 612, loss 0.721963, acc 0.671875
2017-03-02T17:33:08.893968: step 613, loss 0.731523, acc 0.6875
2017-03-02T17:33:08.971840: step 614, loss 0.809327, acc 0.640625
2017-03-02T17:33:09.042129: step 615, loss 0.707651, acc 0.6875
2017-03-02T17:33:09.111335: step 616, loss 0.735166, acc 0.609375
2017-03-02T17:33:09.184805: step 617, loss 0.739561, acc 0.65625
2017-03-02T17:33:09.257300: step 618, loss 0.934075, acc 0.671875
2017-03-02T17:33:09.330353: step 619, loss 0.854622, acc 0.703125
2017-03-02T17:33:09.402129: step 620, loss 0.787388, acc 0.6875
2017-03-02T17:33:09.468477: step 621, loss 0.743867, acc 0.671875
2017-03-02T17:33:09.543017: step 622, loss 0.763743, acc 0.640625
2017-03-02T17:33:09.646649: step 623, loss 0.744271, acc 0.6875
2017-03-02T17:33:09.718364: step 624, loss 0.828715, acc 0.671875
2017-03-02T17:33:09.799058: step 625, loss 0.743323, acc 0.6875
2017-03-02T17:33:09.870266: step 626, loss 0.849738, acc 0.671875
2017-03-02T17:33:09.945846: step 627, loss 0.612383, acc 0.75
2017-03-02T17:33:10.021462: step 628, loss 0.70696, acc 0.734375
2017-03-02T17:33:10.097618: step 629, loss 0.624933, acc 0.765625
2017-03-02T17:33:10.165456: step 630, loss 0.742238, acc 0.6875
2017-03-02T17:33:10.236372: step 631, loss 0.593759, acc 0.765625
2017-03-02T17:33:10.319690: step 632, loss 0.524587, acc 0.78125
2017-03-02T17:33:10.388571: step 633, loss 0.737869, acc 0.71875
2017-03-02T17:33:10.469926: step 634, loss 0.731065, acc 0.6875
2017-03-02T17:33:10.537859: step 635, loss 0.705385, acc 0.6875
2017-03-02T17:33:10.615233: step 636, loss 0.736708, acc 0.703125
2017-03-02T17:33:10.685633: step 637, loss 0.782725, acc 0.6875
2017-03-02T17:33:10.758362: step 638, loss 0.74176, acc 0.671875
2017-03-02T17:33:10.832239: step 639, loss 0.925866, acc 0.671875
2017-03-02T17:33:10.898697: step 640, loss 0.726663, acc 0.78125
2017-03-02T17:33:10.970013: step 641, loss 0.659341, acc 0.71875
2017-03-02T17:33:11.043003: step 642, loss 0.849342, acc 0.71875
2017-03-02T17:33:11.113969: step 643, loss 0.710215, acc 0.71875
2017-03-02T17:33:11.188232: step 644, loss 0.508256, acc 0.796875
2017-03-02T17:33:11.261625: step 645, loss 0.811045, acc 0.703125
2017-03-02T17:33:11.337045: step 646, loss 0.816658, acc 0.703125
2017-03-02T17:33:11.407277: step 647, loss 0.651239, acc 0.71875
2017-03-02T17:33:11.483633: step 648, loss 0.703922, acc 0.6875
2017-03-02T17:33:11.547875: step 649, loss 0.695093, acc 0.734375
2017-03-02T17:33:11.620164: step 650, loss 0.759183, acc 0.671875
2017-03-02T17:33:11.692399: step 651, loss 0.710267, acc 0.671875
2017-03-02T17:33:11.782540: step 652, loss 0.68574, acc 0.765625
2017-03-02T17:33:11.857591: step 653, loss 0.61924, acc 0.765625
2017-03-02T17:33:11.933494: step 654, loss 0.873388, acc 0.625
2017-03-02T17:33:12.012715: step 655, loss 0.796373, acc 0.703125
2017-03-02T17:33:12.091269: step 656, loss 0.717201, acc 0.703125
2017-03-02T17:33:12.161254: step 657, loss 0.871218, acc 0.609375
2017-03-02T17:33:12.229013: step 658, loss 0.659386, acc 0.671875
2017-03-02T17:33:12.298454: step 659, loss 0.604221, acc 0.734375
2017-03-02T17:33:12.367186: step 660, loss 0.87042, acc 0.640625
2017-03-02T17:33:12.441935: step 661, loss 0.573197, acc 0.71875
2017-03-02T17:33:12.509141: step 662, loss 0.893867, acc 0.625
2017-03-02T17:33:12.579931: step 663, loss 0.820061, acc 0.625
2017-03-02T17:33:12.662596: step 664, loss 0.763844, acc 0.734375
2017-03-02T17:33:12.741078: step 665, loss 0.646829, acc 0.8125
2017-03-02T17:33:12.811638: step 666, loss 0.809187, acc 0.65625
2017-03-02T17:33:12.886100: step 667, loss 0.736299, acc 0.65625
2017-03-02T17:33:12.952546: step 668, loss 0.720422, acc 0.71875
2017-03-02T17:33:13.023961: step 669, loss 0.632879, acc 0.671875
2017-03-02T17:33:13.096475: step 670, loss 0.614849, acc 0.734375
2017-03-02T17:33:13.170883: step 671, loss 0.734474, acc 0.71875
2017-03-02T17:33:13.239859: step 672, loss 0.981837, acc 0.59375
2017-03-02T17:33:13.314778: step 673, loss 0.825658, acc 0.6875
2017-03-02T17:33:13.386159: step 674, loss 0.847939, acc 0.671875
2017-03-02T17:33:13.456829: step 675, loss 0.622672, acc 0.78125
2017-03-02T17:33:13.534717: step 676, loss 0.703755, acc 0.734375
2017-03-02T17:33:13.604390: step 677, loss 0.835935, acc 0.59375
2017-03-02T17:33:13.674638: step 678, loss 0.65454, acc 0.71875
2017-03-02T17:33:13.744280: step 679, loss 0.863513, acc 0.640625
2017-03-02T17:33:13.819664: step 680, loss 0.620568, acc 0.734375
2017-03-02T17:33:13.891087: step 681, loss 0.650086, acc 0.703125
2017-03-02T17:33:13.963087: step 682, loss 0.81525, acc 0.640625
2017-03-02T17:33:14.035372: step 683, loss 0.917464, acc 0.609375
2017-03-02T17:33:14.106415: step 684, loss 0.652207, acc 0.75
2017-03-02T17:33:14.180444: step 685, loss 0.579495, acc 0.703125
2017-03-02T17:33:14.260314: step 686, loss 0.67949, acc 0.71875
2017-03-02T17:33:14.331614: step 687, loss 0.816521, acc 0.640625
2017-03-02T17:33:14.399499: step 688, loss 0.732493, acc 0.703125
2017-03-02T17:33:14.469633: step 689, loss 0.528263, acc 0.734375
2017-03-02T17:33:14.540726: step 690, loss 0.700402, acc 0.71875
2017-03-02T17:33:14.615416: step 691, loss 0.664838, acc 0.75
2017-03-02T17:33:14.680209: step 692, loss 0.851933, acc 0.59375
2017-03-02T17:33:14.758403: step 693, loss 0.673175, acc 0.75
2017-03-02T17:33:14.839383: step 694, loss 0.793586, acc 0.6875
2017-03-02T17:33:14.904106: step 695, loss 0.725975, acc 0.703125
2017-03-02T17:33:14.985122: step 696, loss 0.768532, acc 0.703125
2017-03-02T17:33:15.052272: step 697, loss 0.644027, acc 0.71875
2017-03-02T17:33:15.120523: step 698, loss 0.741604, acc 0.734375
2017-03-02T17:33:15.180534: step 699, loss 0.820077, acc 0.6875
2017-03-02T17:33:15.249371: step 700, loss 0.769909, acc 0.671875

Evaluation:
2017-03-02T17:33:15.282938: step 700, loss 0.769763, acc 0.672675

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-700

2017-03-02T17:33:15.736481: step 701, loss 0.704636, acc 0.6875
2017-03-02T17:33:15.803535: step 702, loss 0.687568, acc 0.703125
2017-03-02T17:33:15.872903: step 703, loss 0.616095, acc 0.8125
2017-03-02T17:33:15.941457: step 704, loss 0.966198, acc 0.578125
2017-03-02T17:33:16.030094: step 705, loss 0.728748, acc 0.71875
2017-03-02T17:33:16.115114: step 706, loss 0.706355, acc 0.6875
2017-03-02T17:33:16.199866: step 707, loss 0.787621, acc 0.640625
2017-03-02T17:33:16.274861: step 708, loss 0.614352, acc 0.703125
2017-03-02T17:33:16.355102: step 709, loss 0.846307, acc 0.625
2017-03-02T17:33:16.424219: step 710, loss 0.727832, acc 0.609375
2017-03-02T17:33:16.491326: step 711, loss 0.689964, acc 0.75
2017-03-02T17:33:16.562467: step 712, loss 0.674709, acc 0.703125
2017-03-02T17:33:16.633216: step 713, loss 0.625235, acc 0.765625
2017-03-02T17:33:16.720718: step 714, loss 0.675929, acc 0.703125
2017-03-02T17:33:16.799531: step 715, loss 0.744077, acc 0.59375
2017-03-02T17:33:16.872064: step 716, loss 0.722384, acc 0.703125
2017-03-02T17:33:16.944720: step 717, loss 0.634097, acc 0.75
2017-03-02T17:33:17.020155: step 718, loss 0.866698, acc 0.65625
2017-03-02T17:33:17.085970: step 719, loss 0.868732, acc 0.5625
2017-03-02T17:33:17.152044: step 720, loss 0.754639, acc 0.640625
2017-03-02T17:33:17.222227: step 721, loss 0.688593, acc 0.796875
2017-03-02T17:33:17.297972: step 722, loss 0.561283, acc 0.765625
2017-03-02T17:33:17.371779: step 723, loss 0.852977, acc 0.59375
2017-03-02T17:33:17.435797: step 724, loss 0.638646, acc 0.734375
2017-03-02T17:33:17.509882: step 725, loss 0.739562, acc 0.703125
2017-03-02T17:33:17.584597: step 726, loss 0.9249, acc 0.625
2017-03-02T17:33:17.658556: step 727, loss 0.710361, acc 0.65625
2017-03-02T17:33:17.744782: step 728, loss 0.64069, acc 0.6875
2017-03-02T17:33:17.823705: step 729, loss 0.679903, acc 0.703125
2017-03-02T17:33:17.903911: step 730, loss 0.849791, acc 0.6875
2017-03-02T17:33:17.974766: step 731, loss 0.936581, acc 0.515625
2017-03-02T17:33:18.046481: step 732, loss 0.70025, acc 0.75
2017-03-02T17:33:18.117243: step 733, loss 0.752297, acc 0.640625
2017-03-02T17:33:18.189310: step 734, loss 0.7411, acc 0.671875
2017-03-02T17:33:18.267759: step 735, loss 0.658182, acc 0.796875
2017-03-02T17:33:18.340924: step 736, loss 0.943031, acc 0.671875
2017-03-02T17:33:18.415245: step 737, loss 0.906326, acc 0.6875
2017-03-02T17:33:18.483162: step 738, loss 0.741509, acc 0.65625
2017-03-02T17:33:18.548593: step 739, loss 0.631499, acc 0.734375
2017-03-02T17:33:18.618495: step 740, loss 0.725329, acc 0.71875
2017-03-02T17:33:18.691601: step 741, loss 0.570116, acc 0.8125
2017-03-02T17:33:18.782236: step 742, loss 0.929991, acc 0.65625
2017-03-02T17:33:18.850258: step 743, loss 0.786183, acc 0.71875
2017-03-02T17:33:18.921381: step 744, loss 0.634931, acc 0.75
2017-03-02T17:33:18.989539: step 745, loss 0.865168, acc 0.640625
2017-03-02T17:33:19.060763: step 746, loss 0.805859, acc 0.65625
2017-03-02T17:33:19.136782: step 747, loss 0.85437, acc 0.5625
2017-03-02T17:33:19.205626: step 748, loss 0.78427, acc 0.703125
2017-03-02T17:33:19.278569: step 749, loss 0.798045, acc 0.640625
2017-03-02T17:33:19.351111: step 750, loss 0.725959, acc 0.71875
2017-03-02T17:33:19.422689: step 751, loss 0.599546, acc 0.765625
2017-03-02T17:33:19.504102: step 752, loss 0.699079, acc 0.703125
2017-03-02T17:33:19.576866: step 753, loss 0.655399, acc 0.75
2017-03-02T17:33:19.643828: step 754, loss 0.625739, acc 0.8125
2017-03-02T17:33:19.720159: step 755, loss 0.813136, acc 0.6875
2017-03-02T17:33:19.791816: step 756, loss 0.766269, acc 0.71875
2017-03-02T17:33:19.869168: step 757, loss 0.807519, acc 0.6875
2017-03-02T17:33:19.936518: step 758, loss 0.841468, acc 0.640625
2017-03-02T17:33:20.009095: step 759, loss 0.738274, acc 0.6875
2017-03-02T17:33:20.085350: step 760, loss 0.719436, acc 0.6875
2017-03-02T17:33:20.150219: step 761, loss 0.733673, acc 0.671875
2017-03-02T17:33:20.235179: step 762, loss 0.783314, acc 0.65625
2017-03-02T17:33:20.309024: step 763, loss 0.938076, acc 0.59375
2017-03-02T17:33:20.377568: step 764, loss 0.666674, acc 0.6875
2017-03-02T17:33:20.460782: step 765, loss 0.776303, acc 0.671875
2017-03-02T17:33:20.538556: step 766, loss 0.584086, acc 0.71875
2017-03-02T17:33:20.612158: step 767, loss 0.607758, acc 0.765625
2017-03-02T17:33:20.687381: step 768, loss 0.700982, acc 0.71875
2017-03-02T17:33:20.757139: step 769, loss 0.609544, acc 0.75
2017-03-02T17:33:20.827566: step 770, loss 0.617687, acc 0.734375
2017-03-02T17:33:20.899113: step 771, loss 1.08742, acc 0.59375
2017-03-02T17:33:20.981048: step 772, loss 0.659487, acc 0.71875
2017-03-02T17:33:21.051651: step 773, loss 0.761688, acc 0.71875
2017-03-02T17:33:21.125633: step 774, loss 0.933723, acc 0.609375
2017-03-02T17:33:21.194709: step 775, loss 0.74419, acc 0.6875
2017-03-02T17:33:21.263397: step 776, loss 0.921925, acc 0.515625
2017-03-02T17:33:21.340173: step 777, loss 0.625995, acc 0.703125
2017-03-02T17:33:21.414024: step 778, loss 1.16159, acc 0.5
2017-03-02T17:33:21.485345: step 779, loss 0.679588, acc 0.765625
2017-03-02T17:33:21.551273: step 780, loss 0.786154, acc 0.671875
2017-03-02T17:33:21.616650: step 781, loss 0.709646, acc 0.75
2017-03-02T17:33:21.690949: step 782, loss 0.518674, acc 0.796875
2017-03-02T17:33:21.757875: step 783, loss 0.731133, acc 0.671875
2017-03-02T17:33:21.828054: step 784, loss 0.477977, acc 0.75
2017-03-02T17:33:21.901291: step 785, loss 0.731496, acc 0.703125
2017-03-02T17:33:21.971389: step 786, loss 0.495836, acc 0.84375
2017-03-02T17:33:22.048350: step 787, loss 0.857754, acc 0.65625
2017-03-02T17:33:22.120416: step 788, loss 0.747637, acc 0.71875
2017-03-02T17:33:22.191663: step 789, loss 0.53938, acc 0.8125
2017-03-02T17:33:22.258620: step 790, loss 0.745334, acc 0.65625
2017-03-02T17:33:22.336901: step 791, loss 0.71559, acc 0.734375
2017-03-02T17:33:22.409825: step 792, loss 0.660694, acc 0.765625
2017-03-02T17:33:22.483061: step 793, loss 0.624661, acc 0.75
2017-03-02T17:33:22.564163: step 794, loss 0.549615, acc 0.78125
2017-03-02T17:33:22.637243: step 795, loss 0.497591, acc 0.78125
2017-03-02T17:33:22.714442: step 796, loss 0.635338, acc 0.65625
2017-03-02T17:33:22.788549: step 797, loss 0.561507, acc 0.75
2017-03-02T17:33:22.866740: step 798, loss 0.621532, acc 0.765625
2017-03-02T17:33:22.944893: step 799, loss 0.49504, acc 0.78125
2017-03-02T17:33:23.015509: step 800, loss 0.674619, acc 0.71875

Evaluation:
2017-03-02T17:33:23.049791: step 800, loss 0.815825, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-800

2017-03-02T17:33:23.530323: step 801, loss 0.618316, acc 0.734375
2017-03-02T17:33:23.620828: step 802, loss 0.724726, acc 0.703125
2017-03-02T17:33:23.694070: step 803, loss 0.672915, acc 0.734375
2017-03-02T17:33:23.764925: step 804, loss 0.722198, acc 0.734375
2017-03-02T17:33:23.847450: step 805, loss 0.579157, acc 0.765625
2017-03-02T17:33:23.918582: step 806, loss 0.582233, acc 0.796875
2017-03-02T17:33:23.984339: step 807, loss 0.708804, acc 0.671875
2017-03-02T17:33:24.052179: step 808, loss 0.663433, acc 0.625
2017-03-02T17:33:24.126713: step 809, loss 0.661687, acc 0.6875
2017-03-02T17:33:24.216802: step 810, loss 0.643276, acc 0.75
2017-03-02T17:33:24.291348: step 811, loss 0.505443, acc 0.765625
2017-03-02T17:33:24.366822: step 812, loss 0.434434, acc 0.828125
2017-03-02T17:33:24.440555: step 813, loss 0.545671, acc 0.8125
2017-03-02T17:33:24.519476: step 814, loss 0.636503, acc 0.75
2017-03-02T17:33:24.600487: step 815, loss 0.489905, acc 0.8125
2017-03-02T17:33:24.667293: step 816, loss 0.662816, acc 0.734375
2017-03-02T17:33:24.732110: step 817, loss 0.625187, acc 0.734375
2017-03-02T17:33:24.811588: step 818, loss 0.71134, acc 0.71875
2017-03-02T17:33:24.894379: step 819, loss 0.638378, acc 0.6875
2017-03-02T17:33:24.979927: step 820, loss 0.470512, acc 0.859375
2017-03-02T17:33:25.055056: step 821, loss 0.729514, acc 0.671875
2017-03-02T17:33:25.127995: step 822, loss 0.482961, acc 0.84375
2017-03-02T17:33:25.203977: step 823, loss 0.67469, acc 0.65625
2017-03-02T17:33:25.277370: step 824, loss 0.770226, acc 0.6875
2017-03-02T17:33:25.344502: step 825, loss 0.765757, acc 0.65625
2017-03-02T17:33:25.411848: step 826, loss 0.811152, acc 0.671875
2017-03-02T17:33:25.490545: step 827, loss 0.724752, acc 0.6875
2017-03-02T17:33:25.562882: step 828, loss 0.589401, acc 0.78125
2017-03-02T17:33:25.639812: step 829, loss 0.679661, acc 0.71875
2017-03-02T17:33:25.712143: step 830, loss 0.621245, acc 0.78125
2017-03-02T17:33:25.792532: step 831, loss 0.590078, acc 0.796875
2017-03-02T17:33:25.867518: step 832, loss 0.651282, acc 0.734375
2017-03-02T17:33:25.938394: step 833, loss 0.581584, acc 0.796875
2017-03-02T17:33:26.006653: step 834, loss 0.660172, acc 0.71875
2017-03-02T17:33:26.078687: step 835, loss 0.726041, acc 0.640625
2017-03-02T17:33:26.149495: step 836, loss 0.604675, acc 0.734375
2017-03-02T17:33:26.223935: step 837, loss 0.590009, acc 0.6875
2017-03-02T17:33:26.303063: step 838, loss 0.776977, acc 0.703125
2017-03-02T17:33:26.373454: step 839, loss 0.675947, acc 0.703125
2017-03-02T17:33:26.449317: step 840, loss 0.625513, acc 0.71875
2017-03-02T17:33:26.528099: step 841, loss 0.576093, acc 0.75
2017-03-02T17:33:26.602696: step 842, loss 0.752066, acc 0.703125
2017-03-02T17:33:26.679075: step 843, loss 0.54617, acc 0.78125
2017-03-02T17:33:26.743937: step 844, loss 0.66255, acc 0.75
2017-03-02T17:33:26.812404: step 845, loss 0.519127, acc 0.796875
2017-03-02T17:33:26.883148: step 846, loss 0.842944, acc 0.65625
2017-03-02T17:33:26.961034: step 847, loss 0.818929, acc 0.703125
2017-03-02T17:33:27.038141: step 848, loss 0.667825, acc 0.765625
2017-03-02T17:33:27.113344: step 849, loss 0.65471, acc 0.703125
2017-03-02T17:33:27.185261: step 850, loss 0.705686, acc 0.671875
2017-03-02T17:33:27.261101: step 851, loss 0.769862, acc 0.65625
2017-03-02T17:33:27.346993: step 852, loss 0.711963, acc 0.6875
2017-03-02T17:33:27.414270: step 853, loss 0.869368, acc 0.625
2017-03-02T17:33:27.479928: step 854, loss 0.785051, acc 0.609375
2017-03-02T17:33:27.553575: step 855, loss 0.824618, acc 0.609375
2017-03-02T17:33:27.631112: step 856, loss 0.507346, acc 0.8125
2017-03-02T17:33:27.702145: step 857, loss 0.723004, acc 0.71875
2017-03-02T17:33:27.771263: step 858, loss 0.704476, acc 0.703125
2017-03-02T17:33:27.839950: step 859, loss 0.781392, acc 0.65625
2017-03-02T17:33:27.906798: step 860, loss 0.602302, acc 0.765625
2017-03-02T17:33:27.980640: step 861, loss 0.633115, acc 0.75
2017-03-02T17:33:28.062752: step 862, loss 0.759302, acc 0.78125
2017-03-02T17:33:28.140553: step 863, loss 0.614957, acc 0.71875
2017-03-02T17:33:28.208095: step 864, loss 0.763472, acc 0.75
2017-03-02T17:33:28.281185: step 865, loss 0.673135, acc 0.671875
2017-03-02T17:33:28.360075: step 866, loss 0.758144, acc 0.65625
2017-03-02T17:33:28.433577: step 867, loss 0.691605, acc 0.65625
2017-03-02T17:33:28.507905: step 868, loss 0.750878, acc 0.640625
2017-03-02T17:33:28.574118: step 869, loss 0.529537, acc 0.84375
2017-03-02T17:33:28.648190: step 870, loss 0.889028, acc 0.65625
2017-03-02T17:33:28.726666: step 871, loss 0.902768, acc 0.65625
2017-03-02T17:33:28.798698: step 872, loss 0.752172, acc 0.703125
2017-03-02T17:33:28.865357: step 873, loss 0.521585, acc 0.765625
2017-03-02T17:33:28.936536: step 874, loss 0.655593, acc 0.765625
2017-03-02T17:33:29.012946: step 875, loss 0.732633, acc 0.671875
2017-03-02T17:33:29.084596: step 876, loss 0.695477, acc 0.6875
2017-03-02T17:33:29.162859: step 877, loss 0.616984, acc 0.734375
2017-03-02T17:33:29.234558: step 878, loss 0.724752, acc 0.6875
2017-03-02T17:33:29.300837: step 879, loss 0.637392, acc 0.734375
2017-03-02T17:33:29.374353: step 880, loss 0.631508, acc 0.796875
2017-03-02T17:33:29.439367: step 881, loss 0.850611, acc 0.640625
2017-03-02T17:33:29.514334: step 882, loss 0.840794, acc 0.671875
2017-03-02T17:33:29.584684: step 883, loss 0.591744, acc 0.71875
2017-03-02T17:33:29.655977: step 884, loss 0.6943, acc 0.734375
2017-03-02T17:33:29.728682: step 885, loss 0.929153, acc 0.5625
2017-03-02T17:33:29.816683: step 886, loss 0.653058, acc 0.765625
2017-03-02T17:33:29.895240: step 887, loss 0.870133, acc 0.609375
2017-03-02T17:33:29.966248: step 888, loss 0.745208, acc 0.640625
2017-03-02T17:33:30.040308: step 889, loss 0.575789, acc 0.75
2017-03-02T17:33:30.113842: step 890, loss 0.58375, acc 0.734375
2017-03-02T17:33:30.185412: step 891, loss 0.509196, acc 0.765625
2017-03-02T17:33:30.265995: step 892, loss 0.591743, acc 0.796875
2017-03-02T17:33:30.337841: step 893, loss 0.697743, acc 0.6875
2017-03-02T17:33:30.409297: step 894, loss 0.749103, acc 0.703125
2017-03-02T17:33:30.481391: step 895, loss 0.666129, acc 0.734375
2017-03-02T17:33:30.551563: step 896, loss 0.779733, acc 0.671875
2017-03-02T17:33:30.610170: step 897, loss 0.664598, acc 0.703125
2017-03-02T17:33:30.679849: step 898, loss 0.569866, acc 0.8125
2017-03-02T17:33:30.752969: step 899, loss 0.597144, acc 0.765625
2017-03-02T17:33:30.830726: step 900, loss 0.417666, acc 0.875

Evaluation:
2017-03-02T17:33:30.856497: step 900, loss 0.748551, acc 0.680606

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-900

2017-03-02T17:33:31.363574: step 901, loss 0.57156, acc 0.78125
2017-03-02T17:33:31.435144: step 902, loss 0.69104, acc 0.765625
2017-03-02T17:33:31.503890: step 903, loss 0.775959, acc 0.671875
2017-03-02T17:33:31.574208: step 904, loss 0.689126, acc 0.75
2017-03-02T17:33:31.640987: step 905, loss 0.621494, acc 0.734375
2017-03-02T17:33:31.726034: step 906, loss 0.668282, acc 0.6875
2017-03-02T17:33:31.801173: step 907, loss 0.626323, acc 0.6875
2017-03-02T17:33:31.876641: step 908, loss 0.915983, acc 0.609375
2017-03-02T17:33:31.952904: step 909, loss 0.657383, acc 0.671875
2017-03-02T17:33:32.023636: step 910, loss 0.685043, acc 0.703125
2017-03-02T17:33:32.106356: step 911, loss 0.547077, acc 0.765625
2017-03-02T17:33:32.178276: step 912, loss 0.833431, acc 0.65625
2017-03-02T17:33:32.242711: step 913, loss 0.532692, acc 0.75
2017-03-02T17:33:32.303878: step 914, loss 0.604173, acc 0.734375
2017-03-02T17:33:32.382232: step 915, loss 0.673436, acc 0.703125
2017-03-02T17:33:32.453032: step 916, loss 0.629051, acc 0.671875
2017-03-02T17:33:32.524802: step 917, loss 0.65391, acc 0.703125
2017-03-02T17:33:32.596345: step 918, loss 0.588475, acc 0.734375
2017-03-02T17:33:32.668121: step 919, loss 0.620167, acc 0.71875
2017-03-02T17:33:32.754454: step 920, loss 0.865492, acc 0.59375
2017-03-02T17:33:32.831934: step 921, loss 0.762336, acc 0.625
2017-03-02T17:33:32.897390: step 922, loss 0.785933, acc 0.6875
2017-03-02T17:33:32.964603: step 923, loss 0.830743, acc 0.640625
2017-03-02T17:33:33.030045: step 924, loss 0.610748, acc 0.71875
2017-03-02T17:33:33.102955: step 925, loss 0.403701, acc 0.8125
2017-03-02T17:33:33.179320: step 926, loss 0.566246, acc 0.75
2017-03-02T17:33:33.248860: step 927, loss 0.835394, acc 0.71875
2017-03-02T17:33:33.320207: step 928, loss 0.632491, acc 0.796875
2017-03-02T17:33:33.398201: step 929, loss 0.740728, acc 0.6875
2017-03-02T17:33:33.471601: step 930, loss 0.614402, acc 0.796875
2017-03-02T17:33:33.546380: step 931, loss 0.628504, acc 0.71875
2017-03-02T17:33:33.614583: step 932, loss 0.797137, acc 0.6875
2017-03-02T17:33:33.686161: step 933, loss 0.601894, acc 0.765625
2017-03-02T17:33:33.772523: step 934, loss 0.650613, acc 0.765625
2017-03-02T17:33:33.846508: step 935, loss 0.613108, acc 0.6875
2017-03-02T17:33:33.914887: step 936, loss 0.738763, acc 0.671875
2017-03-02T17:33:33.982645: step 937, loss 0.801702, acc 0.609375
2017-03-02T17:33:34.058298: step 938, loss 0.555831, acc 0.78125
2017-03-02T17:33:34.138024: step 939, loss 0.814077, acc 0.609375
2017-03-02T17:33:34.220733: step 940, loss 0.61691, acc 0.8125
2017-03-02T17:33:34.288674: step 941, loss 0.773929, acc 0.703125
2017-03-02T17:33:34.359595: step 942, loss 0.854655, acc 0.640625
2017-03-02T17:33:34.431870: step 943, loss 0.574369, acc 0.8125
2017-03-02T17:33:34.504300: step 944, loss 0.686317, acc 0.6875
2017-03-02T17:33:34.576567: step 945, loss 0.767751, acc 0.6875
2017-03-02T17:33:34.652328: step 946, loss 0.705476, acc 0.6875
2017-03-02T17:33:34.723007: step 947, loss 0.650147, acc 0.734375
2017-03-02T17:33:34.786578: step 948, loss 0.566017, acc 0.796875
2017-03-02T17:33:34.874056: step 949, loss 0.527595, acc 0.8125
2017-03-02T17:33:34.947129: step 950, loss 0.618824, acc 0.75
2017-03-02T17:33:35.018260: step 951, loss 0.686871, acc 0.71875
2017-03-02T17:33:35.078011: step 952, loss 0.564227, acc 0.796875
2017-03-02T17:33:35.152589: step 953, loss 0.602185, acc 0.734375
2017-03-02T17:33:35.235535: step 954, loss 0.901145, acc 0.609375
2017-03-02T17:33:35.305561: step 955, loss 0.707026, acc 0.765625
2017-03-02T17:33:35.379659: step 956, loss 0.636508, acc 0.8125
2017-03-02T17:33:35.456606: step 957, loss 0.821432, acc 0.640625
2017-03-02T17:33:35.527467: step 958, loss 0.792787, acc 0.65625
2017-03-02T17:33:35.605882: step 959, loss 0.757722, acc 0.671875
2017-03-02T17:33:35.675535: step 960, loss 0.669757, acc 0.703125
2017-03-02T17:33:35.740640: step 961, loss 0.706213, acc 0.71875
2017-03-02T17:33:35.815066: step 962, loss 0.641818, acc 0.765625
2017-03-02T17:33:35.889448: step 963, loss 0.636626, acc 0.734375
2017-03-02T17:33:35.961485: step 964, loss 0.602521, acc 0.75
2017-03-02T17:33:36.048912: step 965, loss 0.618148, acc 0.734375
2017-03-02T17:33:36.123342: step 966, loss 0.744517, acc 0.703125
2017-03-02T17:33:36.205827: step 967, loss 0.606516, acc 0.765625
2017-03-02T17:33:36.279676: step 968, loss 0.516845, acc 0.734375
2017-03-02T17:33:36.348844: step 969, loss 0.844853, acc 0.625
2017-03-02T17:33:36.421186: step 970, loss 0.769128, acc 0.6875
2017-03-02T17:33:36.491331: step 971, loss 0.566205, acc 0.703125
2017-03-02T17:33:36.567102: step 972, loss 0.735799, acc 0.65625
2017-03-02T17:33:36.644932: step 973, loss 0.595659, acc 0.765625
2017-03-02T17:33:36.718067: step 974, loss 0.715206, acc 0.71875
2017-03-02T17:33:36.796983: step 975, loss 0.773979, acc 0.6875
2017-03-02T17:33:36.873662: step 976, loss 0.74341, acc 0.65625
2017-03-02T17:33:36.943304: step 977, loss 0.696642, acc 0.703125
2017-03-02T17:33:37.021547: step 978, loss 0.665616, acc 0.75
2017-03-02T17:33:37.090337: step 979, loss 0.798177, acc 0.671875
2017-03-02T17:33:37.154506: step 980, loss 0.259533, acc 1
2017-03-02T17:33:37.225256: step 981, loss 0.607749, acc 0.765625
2017-03-02T17:33:37.303131: step 982, loss 0.529443, acc 0.796875
2017-03-02T17:33:37.375717: step 983, loss 0.639993, acc 0.734375
2017-03-02T17:33:37.448868: step 984, loss 0.509567, acc 0.78125
2017-03-02T17:33:37.521544: step 985, loss 0.577545, acc 0.765625
2017-03-02T17:33:37.594005: step 986, loss 0.663421, acc 0.734375
2017-03-02T17:33:37.672558: step 987, loss 0.660972, acc 0.796875
2017-03-02T17:33:37.738464: step 988, loss 0.602045, acc 0.78125
2017-03-02T17:33:37.807010: step 989, loss 0.646193, acc 0.6875
2017-03-02T17:33:37.880929: step 990, loss 0.50154, acc 0.796875
2017-03-02T17:33:37.954395: step 991, loss 0.746041, acc 0.703125
2017-03-02T17:33:38.025367: step 992, loss 0.585013, acc 0.734375
2017-03-02T17:33:38.090238: step 993, loss 0.55309, acc 0.796875
2017-03-02T17:33:38.167341: step 994, loss 0.541381, acc 0.765625
2017-03-02T17:33:38.240229: step 995, loss 0.651706, acc 0.671875
2017-03-02T17:33:38.315392: step 996, loss 0.759875, acc 0.71875
2017-03-02T17:33:38.386115: step 997, loss 0.678934, acc 0.6875
2017-03-02T17:33:38.453933: step 998, loss 0.471223, acc 0.796875
2017-03-02T17:33:38.523154: step 999, loss 0.711246, acc 0.6875
2017-03-02T17:33:38.606408: step 1000, loss 0.432026, acc 0.84375

Evaluation:
2017-03-02T17:33:38.641585: step 1000, loss 0.756154, acc 0.678443

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1000

2017-03-02T17:33:39.131039: step 1001, loss 0.709724, acc 0.65625
2017-03-02T17:33:39.201299: step 1002, loss 0.747405, acc 0.65625
2017-03-02T17:33:39.273604: step 1003, loss 0.717531, acc 0.703125
2017-03-02T17:33:39.343681: step 1004, loss 0.652034, acc 0.671875
2017-03-02T17:33:39.412668: step 1005, loss 0.543816, acc 0.734375
2017-03-02T17:33:39.485351: step 1006, loss 0.671592, acc 0.734375
2017-03-02T17:33:39.553309: step 1007, loss 0.654393, acc 0.6875
2017-03-02T17:33:39.629605: step 1008, loss 0.503393, acc 0.8125
2017-03-02T17:33:39.709148: step 1009, loss 0.56654, acc 0.796875
2017-03-02T17:33:39.780617: step 1010, loss 0.690593, acc 0.75
2017-03-02T17:33:39.851268: step 1011, loss 0.711836, acc 0.71875
2017-03-02T17:33:39.927785: step 1012, loss 0.586915, acc 0.734375
2017-03-02T17:33:40.002051: step 1013, loss 0.579875, acc 0.78125
2017-03-02T17:33:40.077182: step 1014, loss 0.648861, acc 0.734375
2017-03-02T17:33:40.162281: step 1015, loss 0.609762, acc 0.765625
2017-03-02T17:33:40.238613: step 1016, loss 0.477216, acc 0.75
2017-03-02T17:33:40.318265: step 1017, loss 0.70984, acc 0.671875
2017-03-02T17:33:40.388471: step 1018, loss 0.658751, acc 0.734375
2017-03-02T17:33:40.457871: step 1019, loss 0.526359, acc 0.765625
2017-03-02T17:33:40.524841: step 1020, loss 0.623744, acc 0.6875
2017-03-02T17:33:40.591432: step 1021, loss 0.822997, acc 0.65625
2017-03-02T17:33:40.668523: step 1022, loss 0.514319, acc 0.796875
2017-03-02T17:33:40.742901: step 1023, loss 0.50079, acc 0.828125
2017-03-02T17:33:40.815013: step 1024, loss 0.552177, acc 0.75
2017-03-02T17:33:40.915388: step 1025, loss 0.664971, acc 0.71875
2017-03-02T17:33:40.982936: step 1026, loss 0.556099, acc 0.765625
2017-03-02T17:33:41.054310: step 1027, loss 0.622789, acc 0.78125
2017-03-02T17:33:41.142438: step 1028, loss 0.693875, acc 0.734375
2017-03-02T17:33:41.208554: step 1029, loss 0.505033, acc 0.828125
2017-03-02T17:33:41.287382: step 1030, loss 0.568418, acc 0.765625
2017-03-02T17:33:41.353393: step 1031, loss 0.658386, acc 0.671875
2017-03-02T17:33:41.422975: step 1032, loss 0.622721, acc 0.75
2017-03-02T17:33:41.500495: step 1033, loss 0.870569, acc 0.59375
2017-03-02T17:33:41.578146: step 1034, loss 0.55079, acc 0.734375
2017-03-02T17:33:41.647848: step 1035, loss 0.59219, acc 0.78125
2017-03-02T17:33:41.722792: step 1036, loss 0.561679, acc 0.734375
2017-03-02T17:33:41.793660: step 1037, loss 0.69677, acc 0.703125
2017-03-02T17:33:41.867003: step 1038, loss 0.488616, acc 0.796875
2017-03-02T17:33:41.932588: step 1039, loss 0.468123, acc 0.828125
2017-03-02T17:33:42.012157: step 1040, loss 0.778865, acc 0.671875
2017-03-02T17:33:42.090828: step 1041, loss 0.658328, acc 0.6875
2017-03-02T17:33:42.164058: step 1042, loss 0.662246, acc 0.734375
2017-03-02T17:33:42.233744: step 1043, loss 0.554478, acc 0.703125
2017-03-02T17:33:42.305170: step 1044, loss 0.502063, acc 0.78125
2017-03-02T17:33:42.379433: step 1045, loss 0.434962, acc 0.84375
2017-03-02T17:33:42.448721: step 1046, loss 0.523348, acc 0.78125
2017-03-02T17:33:42.532132: step 1047, loss 0.670335, acc 0.703125
2017-03-02T17:33:42.595181: step 1048, loss 0.513804, acc 0.78125
2017-03-02T17:33:42.669587: step 1049, loss 0.499486, acc 0.828125
2017-03-02T17:33:42.739825: step 1050, loss 0.582494, acc 0.765625
2017-03-02T17:33:42.811368: step 1051, loss 0.500907, acc 0.734375
2017-03-02T17:33:42.888221: step 1052, loss 0.536683, acc 0.75
2017-03-02T17:33:42.962848: step 1053, loss 0.537922, acc 0.75
2017-03-02T17:33:43.036556: step 1054, loss 0.566972, acc 0.765625
2017-03-02T17:33:43.110335: step 1055, loss 0.607088, acc 0.75
2017-03-02T17:33:43.188948: step 1056, loss 0.44146, acc 0.8125
2017-03-02T17:33:43.256480: step 1057, loss 0.634207, acc 0.734375
2017-03-02T17:33:43.332555: step 1058, loss 0.681063, acc 0.71875
2017-03-02T17:33:43.407084: step 1059, loss 0.423304, acc 0.84375
2017-03-02T17:33:43.483779: step 1060, loss 0.518873, acc 0.765625
2017-03-02T17:33:43.568662: step 1061, loss 0.615769, acc 0.765625
2017-03-02T17:33:43.638321: step 1062, loss 0.636079, acc 0.71875
2017-03-02T17:33:43.710053: step 1063, loss 0.592893, acc 0.765625
2017-03-02T17:33:43.787769: step 1064, loss 0.734568, acc 0.703125
2017-03-02T17:33:43.855865: step 1065, loss 0.567853, acc 0.75
2017-03-02T17:33:43.922995: step 1066, loss 0.53726, acc 0.75
2017-03-02T17:33:43.997520: step 1067, loss 0.582935, acc 0.796875
2017-03-02T17:33:44.078447: step 1068, loss 0.643046, acc 0.75
2017-03-02T17:33:44.158621: step 1069, loss 0.624233, acc 0.75
2017-03-02T17:33:44.229618: step 1070, loss 0.630456, acc 0.734375
2017-03-02T17:33:44.310185: step 1071, loss 0.657766, acc 0.671875
2017-03-02T17:33:44.382477: step 1072, loss 0.673318, acc 0.734375
2017-03-02T17:33:44.453346: step 1073, loss 0.667421, acc 0.734375
2017-03-02T17:33:44.525022: step 1074, loss 0.594543, acc 0.8125
2017-03-02T17:33:44.597305: step 1075, loss 0.601062, acc 0.75
2017-03-02T17:33:44.676560: step 1076, loss 0.592021, acc 0.765625
2017-03-02T17:33:44.755569: step 1077, loss 0.503854, acc 0.78125
2017-03-02T17:33:44.837359: step 1078, loss 0.490687, acc 0.78125
2017-03-02T17:33:44.908692: step 1079, loss 0.4232, acc 0.796875
2017-03-02T17:33:44.982207: step 1080, loss 0.635767, acc 0.734375
2017-03-02T17:33:45.053282: step 1081, loss 0.660926, acc 0.765625
2017-03-02T17:33:45.128651: step 1082, loss 0.688545, acc 0.75
2017-03-02T17:33:45.199316: step 1083, loss 0.632477, acc 0.6875
2017-03-02T17:33:45.275323: step 1084, loss 0.757009, acc 0.75
2017-03-02T17:33:45.352447: step 1085, loss 0.784085, acc 0.6875
2017-03-02T17:33:45.417456: step 1086, loss 0.666067, acc 0.671875
2017-03-02T17:33:45.492290: step 1087, loss 0.692079, acc 0.75
2017-03-02T17:33:45.565878: step 1088, loss 0.58911, acc 0.796875
2017-03-02T17:33:45.639151: step 1089, loss 0.579949, acc 0.75
2017-03-02T17:33:45.713773: step 1090, loss 0.653091, acc 0.796875
2017-03-02T17:33:45.788358: step 1091, loss 0.655045, acc 0.671875
2017-03-02T17:33:45.872101: step 1092, loss 0.633673, acc 0.75
2017-03-02T17:33:45.947575: step 1093, loss 0.604021, acc 0.671875
2017-03-02T17:33:46.014189: step 1094, loss 0.758248, acc 0.640625
2017-03-02T17:33:46.092349: step 1095, loss 0.553268, acc 0.75
2017-03-02T17:33:46.163335: step 1096, loss 0.557318, acc 0.828125
2017-03-02T17:33:46.246114: step 1097, loss 0.724072, acc 0.65625
2017-03-02T17:33:46.325752: step 1098, loss 0.385709, acc 0.84375
2017-03-02T17:33:46.397735: step 1099, loss 0.460548, acc 0.8125
2017-03-02T17:33:46.468552: step 1100, loss 0.51381, acc 0.796875

Evaluation:
2017-03-02T17:33:46.505351: step 1100, loss 0.741438, acc 0.687094

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1100

2017-03-02T17:33:46.971829: step 1101, loss 0.642641, acc 0.75
2017-03-02T17:33:47.057107: step 1102, loss 0.640584, acc 0.71875
2017-03-02T17:33:47.132748: step 1103, loss 0.61334, acc 0.734375
2017-03-02T17:33:47.203195: step 1104, loss 0.735378, acc 0.671875
2017-03-02T17:33:47.301219: step 1105, loss 0.555497, acc 0.765625
2017-03-02T17:33:47.370267: step 1106, loss 0.763554, acc 0.703125
2017-03-02T17:33:47.437974: step 1107, loss 0.471341, acc 0.796875
2017-03-02T17:33:47.500896: step 1108, loss 0.371183, acc 0.875
2017-03-02T17:33:47.575159: step 1109, loss 0.581076, acc 0.765625
2017-03-02T17:33:47.646813: step 1110, loss 0.523895, acc 0.8125
2017-03-02T17:33:47.715937: step 1111, loss 0.54888, acc 0.8125
2017-03-02T17:33:47.788877: step 1112, loss 0.492634, acc 0.859375
2017-03-02T17:33:47.861019: step 1113, loss 0.631811, acc 0.71875
2017-03-02T17:33:47.935713: step 1114, loss 0.787554, acc 0.59375
2017-03-02T17:33:48.014397: step 1115, loss 0.544994, acc 0.734375
2017-03-02T17:33:48.087669: step 1116, loss 0.583834, acc 0.75
2017-03-02T17:33:48.158191: step 1117, loss 0.663437, acc 0.75
2017-03-02T17:33:48.229996: step 1118, loss 0.737494, acc 0.703125
2017-03-02T17:33:48.308220: step 1119, loss 0.66484, acc 0.703125
2017-03-02T17:33:48.382898: step 1120, loss 0.689688, acc 0.640625
2017-03-02T17:33:48.466364: step 1121, loss 0.440519, acc 0.8125
2017-03-02T17:33:48.541885: step 1122, loss 0.763572, acc 0.671875
2017-03-02T17:33:48.617523: step 1123, loss 0.691573, acc 0.75
2017-03-02T17:33:48.690294: step 1124, loss 0.414216, acc 0.875
2017-03-02T17:33:48.756901: step 1125, loss 0.509389, acc 0.765625
2017-03-02T17:33:48.831184: step 1126, loss 0.611764, acc 0.78125
2017-03-02T17:33:48.896917: step 1127, loss 0.727378, acc 0.71875
2017-03-02T17:33:48.961451: step 1128, loss 0.797286, acc 0.6875
2017-03-02T17:33:49.032135: step 1129, loss 0.833971, acc 0.671875
2017-03-02T17:33:49.104980: step 1130, loss 0.593984, acc 0.6875
2017-03-02T17:33:49.179374: step 1131, loss 0.543477, acc 0.828125
2017-03-02T17:33:49.251836: step 1132, loss 0.578974, acc 0.75
2017-03-02T17:33:49.324235: step 1133, loss 0.749081, acc 0.703125
2017-03-02T17:33:49.410927: step 1134, loss 0.691672, acc 0.6875
2017-03-02T17:33:49.490067: step 1135, loss 0.605692, acc 0.78125
2017-03-02T17:33:49.559364: step 1136, loss 0.762441, acc 0.703125
2017-03-02T17:33:49.633062: step 1137, loss 0.489406, acc 0.796875
2017-03-02T17:33:49.725039: step 1138, loss 0.702917, acc 0.71875
2017-03-02T17:33:49.797020: step 1139, loss 0.991896, acc 0.609375
2017-03-02T17:33:49.875503: step 1140, loss 0.559179, acc 0.75
2017-03-02T17:33:49.955875: step 1141, loss 0.625816, acc 0.703125
2017-03-02T17:33:50.037148: step 1142, loss 0.680326, acc 0.640625
2017-03-02T17:33:50.108539: step 1143, loss 0.709442, acc 0.796875
2017-03-02T17:33:50.175004: step 1144, loss 0.50125, acc 0.796875
2017-03-02T17:33:50.250117: step 1145, loss 0.653275, acc 0.765625
2017-03-02T17:33:50.332546: step 1146, loss 0.558552, acc 0.796875
2017-03-02T17:33:50.405887: step 1147, loss 0.479562, acc 0.765625
2017-03-02T17:33:50.478484: step 1148, loss 0.612182, acc 0.734375
2017-03-02T17:33:50.551863: step 1149, loss 0.61428, acc 0.6875
2017-03-02T17:33:50.625322: step 1150, loss 0.642841, acc 0.71875
2017-03-02T17:33:50.693481: step 1151, loss 0.615805, acc 0.734375
2017-03-02T17:33:50.770455: step 1152, loss 0.577064, acc 0.78125
2017-03-02T17:33:50.849639: step 1153, loss 0.527457, acc 0.78125
2017-03-02T17:33:50.924295: step 1154, loss 0.840734, acc 0.671875
2017-03-02T17:33:50.991446: step 1155, loss 0.619631, acc 0.703125
2017-03-02T17:33:51.067727: step 1156, loss 0.754009, acc 0.703125
2017-03-02T17:33:51.140584: step 1157, loss 0.63911, acc 0.703125
2017-03-02T17:33:51.217739: step 1158, loss 0.5774, acc 0.75
2017-03-02T17:33:51.292755: step 1159, loss 0.519673, acc 0.8125
2017-03-02T17:33:51.362603: step 1160, loss 0.921817, acc 0.5625
2017-03-02T17:33:51.436024: step 1161, loss 0.733302, acc 0.6875
2017-03-02T17:33:51.509284: step 1162, loss 0.596191, acc 0.75
2017-03-02T17:33:51.573798: step 1163, loss 0.432069, acc 0.828125
2017-03-02T17:33:51.639356: step 1164, loss 0.587427, acc 0.765625
2017-03-02T17:33:51.715927: step 1165, loss 0.469016, acc 0.84375
2017-03-02T17:33:51.787105: step 1166, loss 0.58038, acc 0.75
2017-03-02T17:33:51.857720: step 1167, loss 0.615461, acc 0.78125
2017-03-02T17:33:51.932717: step 1168, loss 0.606895, acc 0.75
2017-03-02T17:33:52.005202: step 1169, loss 0.564541, acc 0.78125
2017-03-02T17:33:52.072047: step 1170, loss 0.677857, acc 0.765625
2017-03-02T17:33:52.145399: step 1171, loss 0.661027, acc 0.71875
2017-03-02T17:33:52.218051: step 1172, loss 0.606276, acc 0.734375
2017-03-02T17:33:52.295350: step 1173, loss 0.767677, acc 0.703125
2017-03-02T17:33:52.362269: step 1174, loss 0.570668, acc 0.75
2017-03-02T17:33:52.431780: step 1175, loss 0.518463, acc 0.828125
2017-03-02T17:33:52.504734: step 1176, loss 0.499635, acc 0.75
2017-03-02T17:33:52.583411: step 1177, loss 0.549857, acc 0.8125
2017-03-02T17:33:52.659429: step 1178, loss 0.485148, acc 0.859375
2017-03-02T17:33:52.746011: step 1179, loss 0.466625, acc 0.828125
2017-03-02T17:33:52.816481: step 1180, loss 0.505756, acc 0.84375
2017-03-02T17:33:52.904787: step 1181, loss 0.639731, acc 0.796875
2017-03-02T17:33:52.973101: step 1182, loss 0.623107, acc 0.765625
2017-03-02T17:33:53.045341: step 1183, loss 0.34897, acc 0.84375
2017-03-02T17:33:53.123098: step 1184, loss 0.712627, acc 0.734375
2017-03-02T17:33:53.205184: step 1185, loss 0.628866, acc 0.734375
2017-03-02T17:33:53.288242: step 1186, loss 0.459648, acc 0.796875
2017-03-02T17:33:53.360627: step 1187, loss 0.509088, acc 0.828125
2017-03-02T17:33:53.433349: step 1188, loss 0.528122, acc 0.796875
2017-03-02T17:33:53.507817: step 1189, loss 0.513279, acc 0.8125
2017-03-02T17:33:53.586420: step 1190, loss 0.584526, acc 0.75
2017-03-02T17:33:53.650148: step 1191, loss 0.621323, acc 0.734375
2017-03-02T17:33:53.713805: step 1192, loss 0.501269, acc 0.8125
2017-03-02T17:33:53.789993: step 1193, loss 0.473332, acc 0.828125
2017-03-02T17:33:53.861487: step 1194, loss 0.542956, acc 0.765625
2017-03-02T17:33:53.933855: step 1195, loss 0.581731, acc 0.765625
2017-03-02T17:33:54.029355: step 1196, loss 0.485727, acc 0.8125
2017-03-02T17:33:54.102273: step 1197, loss 0.543492, acc 0.75
2017-03-02T17:33:54.172172: step 1198, loss 0.757429, acc 0.671875
2017-03-02T17:33:54.243864: step 1199, loss 0.559106, acc 0.734375
2017-03-02T17:33:54.309919: step 1200, loss 0.704988, acc 0.703125

Evaluation:
2017-03-02T17:33:54.339256: step 1200, loss 0.768271, acc 0.685652

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1200

2017-03-02T17:33:54.784005: step 1201, loss 0.630655, acc 0.703125
2017-03-02T17:33:54.856050: step 1202, loss 0.455504, acc 0.84375
2017-03-02T17:33:54.928850: step 1203, loss 0.429557, acc 0.828125
2017-03-02T17:33:54.996709: step 1204, loss 0.641771, acc 0.65625
2017-03-02T17:33:55.061932: step 1205, loss 0.558057, acc 0.75
2017-03-02T17:33:55.136015: step 1206, loss 0.52956, acc 0.796875
2017-03-02T17:33:55.211043: step 1207, loss 0.55482, acc 0.71875
2017-03-02T17:33:55.305332: step 1208, loss 0.49676, acc 0.8125
2017-03-02T17:33:55.376463: step 1209, loss 0.553353, acc 0.8125
2017-03-02T17:33:55.446952: step 1210, loss 0.580639, acc 0.734375
2017-03-02T17:33:55.517921: step 1211, loss 0.522401, acc 0.796875
2017-03-02T17:33:55.603722: step 1212, loss 0.407504, acc 0.84375
2017-03-02T17:33:55.675057: step 1213, loss 0.63294, acc 0.796875
2017-03-02T17:33:55.744183: step 1214, loss 0.504473, acc 0.828125
2017-03-02T17:33:55.819474: step 1215, loss 0.74771, acc 0.640625
2017-03-02T17:33:55.892815: step 1216, loss 0.615733, acc 0.75
2017-03-02T17:33:55.962768: step 1217, loss 0.59148, acc 0.78125
2017-03-02T17:33:56.035222: step 1218, loss 0.708999, acc 0.703125
2017-03-02T17:33:56.106361: step 1219, loss 0.582729, acc 0.75
2017-03-02T17:33:56.176112: step 1220, loss 0.60017, acc 0.734375
2017-03-02T17:33:56.253541: step 1221, loss 0.403405, acc 0.84375
2017-03-02T17:33:56.320580: step 1222, loss 0.447725, acc 0.84375
2017-03-02T17:33:56.403522: step 1223, loss 0.61138, acc 0.765625
2017-03-02T17:33:56.472166: step 1224, loss 0.500443, acc 0.78125
2017-03-02T17:33:56.547840: step 1225, loss 0.648858, acc 0.734375
2017-03-02T17:33:56.628330: step 1226, loss 0.561187, acc 0.796875
2017-03-02T17:33:56.702620: step 1227, loss 0.758591, acc 0.703125
2017-03-02T17:33:56.775225: step 1228, loss 0.572064, acc 0.78125
2017-03-02T17:33:56.846091: step 1229, loss 0.797906, acc 0.625
2017-03-02T17:33:56.923625: step 1230, loss 0.549843, acc 0.75
2017-03-02T17:33:56.996582: step 1231, loss 0.540462, acc 0.828125
2017-03-02T17:33:57.064897: step 1232, loss 0.571271, acc 0.6875
2017-03-02T17:33:57.135204: step 1233, loss 0.643579, acc 0.6875
2017-03-02T17:33:57.208483: step 1234, loss 0.682542, acc 0.6875
2017-03-02T17:33:57.280426: step 1235, loss 0.503834, acc 0.796875
2017-03-02T17:33:57.368878: step 1236, loss 0.712461, acc 0.703125
2017-03-02T17:33:57.439818: step 1237, loss 0.508309, acc 0.78125
2017-03-02T17:33:57.509276: step 1238, loss 0.587483, acc 0.671875
2017-03-02T17:33:57.580704: step 1239, loss 0.661794, acc 0.71875
2017-03-02T17:33:57.652864: step 1240, loss 0.556364, acc 0.796875
2017-03-02T17:33:57.725940: step 1241, loss 0.505029, acc 0.78125
2017-03-02T17:33:57.799427: step 1242, loss 0.595086, acc 0.796875
2017-03-02T17:33:57.868461: step 1243, loss 0.618782, acc 0.8125
2017-03-02T17:33:57.951640: step 1244, loss 0.61453, acc 0.75
2017-03-02T17:33:58.029738: step 1245, loss 0.782162, acc 0.6875
2017-03-02T17:33:58.101096: step 1246, loss 0.572089, acc 0.78125
2017-03-02T17:33:58.171443: step 1247, loss 0.755425, acc 0.6875
2017-03-02T17:33:58.238913: step 1248, loss 0.488646, acc 0.828125
2017-03-02T17:33:58.307873: step 1249, loss 0.613896, acc 0.75
2017-03-02T17:33:58.383245: step 1250, loss 0.519082, acc 0.8125
2017-03-02T17:33:58.458691: step 1251, loss 0.601249, acc 0.71875
2017-03-02T17:33:58.527284: step 1252, loss 0.562467, acc 0.734375
2017-03-02T17:33:58.603345: step 1253, loss 0.522014, acc 0.78125
2017-03-02T17:33:58.675558: step 1254, loss 0.579726, acc 0.796875
2017-03-02T17:33:58.746347: step 1255, loss 0.469539, acc 0.796875
2017-03-02T17:33:58.817361: step 1256, loss 0.634408, acc 0.71875
2017-03-02T17:33:58.891209: step 1257, loss 0.620765, acc 0.765625
2017-03-02T17:33:58.954551: step 1258, loss 0.599575, acc 0.71875
2017-03-02T17:33:59.030468: step 1259, loss 0.603989, acc 0.75
2017-03-02T17:33:59.100450: step 1260, loss 0.673243, acc 0.734375
2017-03-02T17:33:59.180499: step 1261, loss 0.647848, acc 0.703125
2017-03-02T17:33:59.249254: step 1262, loss 0.394625, acc 0.8125
2017-03-02T17:33:59.324207: step 1263, loss 0.747649, acc 0.71875
2017-03-02T17:33:59.424735: step 1264, loss 0.657232, acc 0.703125
2017-03-02T17:33:59.500626: step 1265, loss 0.529139, acc 0.8125
2017-03-02T17:33:59.585072: step 1266, loss 0.539346, acc 0.78125
2017-03-02T17:33:59.659393: step 1267, loss 0.622383, acc 0.75
2017-03-02T17:33:59.729413: step 1268, loss 0.590979, acc 0.765625
2017-03-02T17:33:59.802356: step 1269, loss 0.637718, acc 0.796875
2017-03-02T17:33:59.884332: step 1270, loss 0.556081, acc 0.8125
2017-03-02T17:33:59.949070: step 1271, loss 0.580515, acc 0.78125
2017-03-02T17:34:00.020826: step 1272, loss 0.654937, acc 0.71875
2017-03-02T17:34:00.095120: step 1273, loss 0.538536, acc 0.765625
2017-03-02T17:34:00.161223: step 1274, loss 0.49915, acc 0.78125
2017-03-02T17:34:00.231085: step 1275, loss 0.551856, acc 0.71875
2017-03-02T17:34:00.303231: step 1276, loss 0.572371, acc 0.765625
2017-03-02T17:34:00.376185: step 1277, loss 0.436155, acc 0.796875
2017-03-02T17:34:00.449426: step 1278, loss 0.7857, acc 0.65625
2017-03-02T17:34:00.521923: step 1279, loss 0.397556, acc 0.8125
2017-03-02T17:34:00.591827: step 1280, loss 0.522441, acc 0.765625
2017-03-02T17:34:00.669780: step 1281, loss 0.602156, acc 0.796875
2017-03-02T17:34:00.758391: step 1282, loss 0.678903, acc 0.703125
2017-03-02T17:34:00.836148: step 1283, loss 0.374699, acc 0.890625
2017-03-02T17:34:00.908538: step 1284, loss 0.529271, acc 0.78125
2017-03-02T17:34:00.979888: step 1285, loss 0.635957, acc 0.703125
2017-03-02T17:34:01.051984: step 1286, loss 0.766209, acc 0.640625
2017-03-02T17:34:01.123182: step 1287, loss 0.792076, acc 0.6875
2017-03-02T17:34:01.185364: step 1288, loss 0.669199, acc 0.734375
2017-03-02T17:34:01.248218: step 1289, loss 0.418715, acc 0.828125
2017-03-02T17:34:01.323437: step 1290, loss 0.655093, acc 0.765625
2017-03-02T17:34:01.403015: step 1291, loss 0.467933, acc 0.8125
2017-03-02T17:34:01.479039: step 1292, loss 0.636895, acc 0.6875
2017-03-02T17:34:01.553474: step 1293, loss 0.76099, acc 0.6875
2017-03-02T17:34:01.627204: step 1294, loss 0.525447, acc 0.8125
2017-03-02T17:34:01.694978: step 1295, loss 0.62584, acc 0.765625
2017-03-02T17:34:01.765925: step 1296, loss 0.47039, acc 0.828125
2017-03-02T17:34:01.842708: step 1297, loss 0.604582, acc 0.75
2017-03-02T17:34:01.913295: step 1298, loss 0.462429, acc 0.84375
2017-03-02T17:34:01.974685: step 1299, loss 0.553075, acc 0.796875
2017-03-02T17:34:02.042981: step 1300, loss 0.609258, acc 0.75

Evaluation:
2017-03-02T17:34:02.075818: step 1300, loss 0.758739, acc 0.69142

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1300

2017-03-02T17:34:02.525142: step 1301, loss 0.806029, acc 0.671875
2017-03-02T17:34:02.589499: step 1302, loss 0.500234, acc 0.796875
2017-03-02T17:34:02.659193: step 1303, loss 0.601469, acc 0.734375
2017-03-02T17:34:02.732084: step 1304, loss 0.611275, acc 0.765625
2017-03-02T17:34:02.806040: step 1305, loss 0.468872, acc 0.78125
2017-03-02T17:34:02.880191: step 1306, loss 0.528741, acc 0.765625
2017-03-02T17:34:02.955374: step 1307, loss 0.534201, acc 0.828125
2017-03-02T17:34:03.031139: step 1308, loss 0.457242, acc 0.8125
2017-03-02T17:34:03.121742: step 1309, loss 0.605455, acc 0.734375
2017-03-02T17:34:03.194941: step 1310, loss 0.532432, acc 0.765625
2017-03-02T17:34:03.261925: step 1311, loss 0.556537, acc 0.78125
2017-03-02T17:34:03.328361: step 1312, loss 0.573997, acc 0.765625
2017-03-02T17:34:03.408444: step 1313, loss 0.514874, acc 0.75
2017-03-02T17:34:03.491373: step 1314, loss 0.505118, acc 0.765625
2017-03-02T17:34:03.558620: step 1315, loss 0.660917, acc 0.703125
2017-03-02T17:34:03.629997: step 1316, loss 0.678053, acc 0.703125
2017-03-02T17:34:03.706478: step 1317, loss 0.411606, acc 0.84375
2017-03-02T17:34:03.777051: step 1318, loss 0.681208, acc 0.71875
2017-03-02T17:34:03.850790: step 1319, loss 0.477206, acc 0.84375
2017-03-02T17:34:03.936725: step 1320, loss 0.890303, acc 0.65625
2017-03-02T17:34:04.005427: step 1321, loss 0.579732, acc 0.75
2017-03-02T17:34:04.078283: step 1322, loss 0.506888, acc 0.796875
2017-03-02T17:34:04.157682: step 1323, loss 0.540313, acc 0.78125
2017-03-02T17:34:04.230339: step 1324, loss 0.655844, acc 0.75
2017-03-02T17:34:04.305970: step 1325, loss 0.804706, acc 0.6875
2017-03-02T17:34:04.378235: step 1326, loss 0.520726, acc 0.796875
2017-03-02T17:34:04.447415: step 1327, loss 0.678672, acc 0.71875
2017-03-02T17:34:04.519791: step 1328, loss 0.590334, acc 0.75
2017-03-02T17:34:04.593996: step 1329, loss 0.540415, acc 0.765625
2017-03-02T17:34:04.660386: step 1330, loss 0.555686, acc 0.796875
2017-03-02T17:34:04.725186: step 1331, loss 0.68806, acc 0.75
2017-03-02T17:34:04.799234: step 1332, loss 0.622444, acc 0.71875
2017-03-02T17:34:04.873971: step 1333, loss 0.587434, acc 0.78125
2017-03-02T17:34:04.935640: step 1334, loss 0.492375, acc 0.8125
2017-03-02T17:34:05.006758: step 1335, loss 0.555073, acc 0.75
2017-03-02T17:34:05.083263: step 1336, loss 0.653436, acc 0.734375
2017-03-02T17:34:05.153274: step 1337, loss 0.591669, acc 0.703125
2017-03-02T17:34:05.227363: step 1338, loss 0.532794, acc 0.84375
2017-03-02T17:34:05.294312: step 1339, loss 0.365744, acc 0.828125
2017-03-02T17:34:05.362820: step 1340, loss 0.534894, acc 0.765625
2017-03-02T17:34:05.429329: step 1341, loss 0.691748, acc 0.625
2017-03-02T17:34:05.501568: step 1342, loss 0.769163, acc 0.6875
2017-03-02T17:34:05.574039: step 1343, loss 0.635282, acc 0.65625
2017-03-02T17:34:05.646009: step 1344, loss 0.507322, acc 0.78125
2017-03-02T17:34:05.725035: step 1345, loss 0.577126, acc 0.765625
2017-03-02T17:34:05.797416: step 1346, loss 0.425773, acc 0.859375
2017-03-02T17:34:05.872785: step 1347, loss 0.507414, acc 0.765625
2017-03-02T17:34:05.953598: step 1348, loss 0.71799, acc 0.71875
2017-03-02T17:34:06.029749: step 1349, loss 0.661813, acc 0.703125
2017-03-02T17:34:06.101086: step 1350, loss 0.593034, acc 0.765625
2017-03-02T17:34:06.172393: step 1351, loss 0.421328, acc 0.859375
2017-03-02T17:34:06.252300: step 1352, loss 0.530413, acc 0.78125
2017-03-02T17:34:06.328055: step 1353, loss 0.577542, acc 0.703125
2017-03-02T17:34:06.406033: step 1354, loss 0.516174, acc 0.734375
2017-03-02T17:34:06.475748: step 1355, loss 0.572755, acc 0.75
2017-03-02T17:34:06.551816: step 1356, loss 0.635486, acc 0.6875
2017-03-02T17:34:06.618576: step 1357, loss 0.479404, acc 0.796875
2017-03-02T17:34:06.694000: step 1358, loss 0.62136, acc 0.75
2017-03-02T17:34:06.768842: step 1359, loss 0.883925, acc 0.6875
2017-03-02T17:34:06.838408: step 1360, loss 0.561337, acc 0.796875
2017-03-02T17:34:06.906697: step 1361, loss 0.664976, acc 0.734375
2017-03-02T17:34:06.997017: step 1362, loss 0.708056, acc 0.71875
2017-03-02T17:34:07.083058: step 1363, loss 0.571029, acc 0.78125
2017-03-02T17:34:07.162857: step 1364, loss 0.606998, acc 0.6875
2017-03-02T17:34:07.234195: step 1365, loss 0.498403, acc 0.78125
2017-03-02T17:34:07.315958: step 1366, loss 0.486431, acc 0.765625
2017-03-02T17:34:07.385573: step 1367, loss 0.490948, acc 0.796875
2017-03-02T17:34:07.455874: step 1368, loss 0.50209, acc 0.875
2017-03-02T17:34:07.544860: step 1369, loss 0.593679, acc 0.75
2017-03-02T17:34:07.606770: step 1370, loss 0.595929, acc 0.75
2017-03-02T17:34:07.683259: step 1371, loss 0.547619, acc 0.765625
2017-03-02T17:34:07.761908: step 1372, loss 0.681611, acc 0.75
2017-03-02T17:34:07.837839: step 1373, loss 0.486641, acc 0.796875
2017-03-02T17:34:07.902719: step 1374, loss 0.436648, acc 0.84375
2017-03-02T17:34:07.970883: step 1375, loss 0.610222, acc 0.765625
2017-03-02T17:34:08.038390: step 1376, loss 0.550119, acc 0.75
2017-03-02T17:34:08.109267: step 1377, loss 0.51763, acc 0.859375
2017-03-02T17:34:08.182396: step 1378, loss 0.607195, acc 0.765625
2017-03-02T17:34:08.257752: step 1379, loss 0.352755, acc 0.84375
2017-03-02T17:34:08.325289: step 1380, loss 0.567869, acc 0.765625
2017-03-02T17:34:08.403850: step 1381, loss 0.440556, acc 0.84375
2017-03-02T17:34:08.481338: step 1382, loss 0.424875, acc 0.828125
2017-03-02T17:34:08.558824: step 1383, loss 0.516363, acc 0.78125
2017-03-02T17:34:08.636632: step 1384, loss 0.437708, acc 0.828125
2017-03-02T17:34:08.713411: step 1385, loss 0.426094, acc 0.796875
2017-03-02T17:34:08.785279: step 1386, loss 0.494315, acc 0.8125
2017-03-02T17:34:08.851537: step 1387, loss 0.580184, acc 0.8125
2017-03-02T17:34:08.921766: step 1388, loss 0.493811, acc 0.828125
2017-03-02T17:34:08.995566: step 1389, loss 0.685276, acc 0.8125
2017-03-02T17:34:09.067131: step 1390, loss 0.702233, acc 0.734375
2017-03-02T17:34:09.165185: step 1391, loss 0.578904, acc 0.75
2017-03-02T17:34:09.242775: step 1392, loss 0.522212, acc 0.78125
2017-03-02T17:34:09.311871: step 1393, loss 0.471761, acc 0.828125
2017-03-02T17:34:09.387072: step 1394, loss 0.473156, acc 0.8125
2017-03-02T17:34:09.457869: step 1395, loss 0.462579, acc 0.828125
2017-03-02T17:34:09.529600: step 1396, loss 0.58103, acc 0.75
2017-03-02T17:34:09.605049: step 1397, loss 0.459357, acc 0.859375
2017-03-02T17:34:09.677641: step 1398, loss 0.500358, acc 0.828125
2017-03-02T17:34:09.749198: step 1399, loss 0.447151, acc 0.890625
2017-03-02T17:34:09.823981: step 1400, loss 0.602677, acc 0.765625

Evaluation:
2017-03-02T17:34:09.860732: step 1400, loss 0.752289, acc 0.694304

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1400

2017-03-02T17:34:10.323987: step 1401, loss 0.347192, acc 0.890625
2017-03-02T17:34:10.399595: step 1402, loss 0.487728, acc 0.796875
2017-03-02T17:34:10.470208: step 1403, loss 0.591003, acc 0.75
2017-03-02T17:34:10.542425: step 1404, loss 0.493734, acc 0.78125
2017-03-02T17:34:10.613485: step 1405, loss 0.552529, acc 0.765625
2017-03-02T17:34:10.684973: step 1406, loss 0.482074, acc 0.828125
2017-03-02T17:34:10.756704: step 1407, loss 0.39557, acc 0.859375
2017-03-02T17:34:10.829747: step 1408, loss 0.526496, acc 0.84375
2017-03-02T17:34:10.895021: step 1409, loss 0.527393, acc 0.765625
2017-03-02T17:34:10.963451: step 1410, loss 0.593974, acc 0.765625
2017-03-02T17:34:11.035297: step 1411, loss 0.347096, acc 0.90625
2017-03-02T17:34:11.109424: step 1412, loss 0.630087, acc 0.859375
2017-03-02T17:34:11.181424: step 1413, loss 0.594134, acc 0.75
2017-03-02T17:34:11.248852: step 1414, loss 0.4337, acc 0.828125
2017-03-02T17:34:11.322027: step 1415, loss 0.534555, acc 0.78125
2017-03-02T17:34:11.391725: step 1416, loss 0.384951, acc 0.875
2017-03-02T17:34:11.463768: step 1417, loss 0.56691, acc 0.8125
2017-03-02T17:34:11.536854: step 1418, loss 0.524469, acc 0.8125
2017-03-02T17:34:11.616954: step 1419, loss 0.476314, acc 0.84375
2017-03-02T17:34:11.685140: step 1420, loss 0.552757, acc 0.734375
2017-03-02T17:34:11.754453: step 1421, loss 0.420599, acc 0.828125
2017-03-02T17:34:11.828122: step 1422, loss 0.649876, acc 0.734375
2017-03-02T17:34:11.900674: step 1423, loss 0.453302, acc 0.796875
2017-03-02T17:34:11.977854: step 1424, loss 0.527636, acc 0.75
2017-03-02T17:34:12.054056: step 1425, loss 0.643598, acc 0.71875
2017-03-02T17:34:12.133127: step 1426, loss 0.453822, acc 0.90625
2017-03-02T17:34:12.211644: step 1427, loss 0.777936, acc 0.65625
2017-03-02T17:34:12.281099: step 1428, loss 0.350323, acc 0.84375
2017-03-02T17:34:12.350654: step 1429, loss 0.647216, acc 0.734375
2017-03-02T17:34:12.420499: step 1430, loss 0.57618, acc 0.765625
2017-03-02T17:34:12.498549: step 1431, loss 0.474633, acc 0.84375
2017-03-02T17:34:12.569198: step 1432, loss 0.40511, acc 0.84375
2017-03-02T17:34:12.632068: step 1433, loss 0.684472, acc 0.734375
2017-03-02T17:34:12.702634: step 1434, loss 0.430595, acc 0.828125
2017-03-02T17:34:12.775203: step 1435, loss 0.567325, acc 0.765625
2017-03-02T17:34:12.838732: step 1436, loss 0.656295, acc 0.734375
2017-03-02T17:34:12.916488: step 1437, loss 0.557458, acc 0.796875
2017-03-02T17:34:12.996411: step 1438, loss 0.514903, acc 0.828125
2017-03-02T17:34:13.063205: step 1439, loss 0.616986, acc 0.703125
2017-03-02T17:34:13.132833: step 1440, loss 0.435806, acc 0.78125
2017-03-02T17:34:13.205897: step 1441, loss 0.600136, acc 0.65625
2017-03-02T17:34:13.279294: step 1442, loss 0.684996, acc 0.75
2017-03-02T17:34:13.356279: step 1443, loss 0.515526, acc 0.75
2017-03-02T17:34:13.428547: step 1444, loss 0.464409, acc 0.78125
2017-03-02T17:34:13.498152: step 1445, loss 0.562417, acc 0.734375
2017-03-02T17:34:13.577645: step 1446, loss 0.490442, acc 0.796875
2017-03-02T17:34:13.648783: step 1447, loss 0.664019, acc 0.703125
2017-03-02T17:34:13.714332: step 1448, loss 0.660301, acc 0.765625
2017-03-02T17:34:13.786023: step 1449, loss 0.411791, acc 0.828125
2017-03-02T17:34:13.856307: step 1450, loss 0.517484, acc 0.8125
2017-03-02T17:34:13.920381: step 1451, loss 0.432612, acc 0.875
2017-03-02T17:34:14.021162: step 1452, loss 0.605581, acc 0.796875
2017-03-02T17:34:14.093921: step 1453, loss 0.448049, acc 0.84375
2017-03-02T17:34:14.166778: step 1454, loss 0.579736, acc 0.828125
2017-03-02T17:34:14.228950: step 1455, loss 0.639987, acc 0.703125
2017-03-02T17:34:14.299329: step 1456, loss 0.485805, acc 0.84375
2017-03-02T17:34:14.365716: step 1457, loss 0.597581, acc 0.8125
2017-03-02T17:34:14.426457: step 1458, loss 0.421593, acc 0.84375
2017-03-02T17:34:14.498892: step 1459, loss 0.626005, acc 0.796875
2017-03-02T17:34:14.566273: step 1460, loss 0.560335, acc 0.78125
2017-03-02T17:34:14.640051: step 1461, loss 0.519293, acc 0.78125
2017-03-02T17:34:14.706521: step 1462, loss 0.427597, acc 0.828125
2017-03-02T17:34:14.778820: step 1463, loss 0.458204, acc 0.84375
2017-03-02T17:34:14.848468: step 1464, loss 0.378725, acc 0.84375
2017-03-02T17:34:14.919232: step 1465, loss 0.73668, acc 0.6875
2017-03-02T17:34:14.991963: step 1466, loss 0.444628, acc 0.796875
2017-03-02T17:34:15.081229: step 1467, loss 0.584762, acc 0.75
2017-03-02T17:34:15.149483: step 1468, loss 0.488499, acc 0.84375
2017-03-02T17:34:15.224974: step 1469, loss 0.571339, acc 0.765625
2017-03-02T17:34:15.305840: step 1470, loss 0.492116, acc 0.796875
2017-03-02T17:34:15.380912: step 1471, loss 0.510501, acc 0.796875
2017-03-02T17:34:15.453843: step 1472, loss 0.510673, acc 0.796875
2017-03-02T17:34:15.522912: step 1473, loss 0.521613, acc 0.8125
2017-03-02T17:34:15.603249: step 1474, loss 0.479913, acc 0.796875
2017-03-02T17:34:15.679532: step 1475, loss 0.477996, acc 0.796875
2017-03-02T17:34:15.746753: step 1476, loss 0.604466, acc 0.78125
2017-03-02T17:34:15.820570: step 1477, loss 0.510018, acc 0.734375
2017-03-02T17:34:15.908738: step 1478, loss 0.647048, acc 0.703125
2017-03-02T17:34:15.978623: step 1479, loss 0.627062, acc 0.78125
2017-03-02T17:34:16.049122: step 1480, loss 0.577106, acc 0.765625
2017-03-02T17:34:16.119039: step 1481, loss 0.51875, acc 0.78125
2017-03-02T17:34:16.190522: step 1482, loss 0.458587, acc 0.84375
2017-03-02T17:34:16.256158: step 1483, loss 0.683285, acc 0.71875
2017-03-02T17:34:16.329700: step 1484, loss 0.503971, acc 0.765625
2017-03-02T17:34:16.394789: step 1485, loss 0.57767, acc 0.8125
2017-03-02T17:34:16.466193: step 1486, loss 0.616471, acc 0.75
2017-03-02T17:34:16.539089: step 1487, loss 0.448647, acc 0.8125
2017-03-02T17:34:16.626726: step 1488, loss 0.463241, acc 0.90625
2017-03-02T17:34:16.696979: step 1489, loss 0.623506, acc 0.75
2017-03-02T17:34:16.775238: step 1490, loss 0.522905, acc 0.734375
2017-03-02T17:34:16.846234: step 1491, loss 0.473215, acc 0.828125
2017-03-02T17:34:16.924890: step 1492, loss 0.483903, acc 0.84375
2017-03-02T17:34:16.997317: step 1493, loss 0.487607, acc 0.828125
2017-03-02T17:34:17.071959: step 1494, loss 0.49, acc 0.78125
2017-03-02T17:34:17.152427: step 1495, loss 0.469121, acc 0.8125
2017-03-02T17:34:17.227507: step 1496, loss 0.533333, acc 0.765625
2017-03-02T17:34:17.299638: step 1497, loss 0.516568, acc 0.71875
2017-03-02T17:34:17.375381: step 1498, loss 0.379529, acc 0.875
2017-03-02T17:34:17.461169: step 1499, loss 0.565457, acc 0.796875
2017-03-02T17:34:17.538281: step 1500, loss 0.447385, acc 0.8125

Evaluation:
2017-03-02T17:34:17.571177: step 1500, loss 0.765974, acc 0.694304

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1500

2017-03-02T17:34:18.037785: step 1501, loss 0.441782, acc 0.796875
2017-03-02T17:34:18.113618: step 1502, loss 0.487154, acc 0.828125
2017-03-02T17:34:18.183728: step 1503, loss 0.373384, acc 0.84375
2017-03-02T17:34:18.260663: step 1504, loss 0.504923, acc 0.8125
2017-03-02T17:34:18.337852: step 1505, loss 0.393998, acc 0.859375
2017-03-02T17:34:18.408633: step 1506, loss 0.462308, acc 0.8125
2017-03-02T17:34:18.477156: step 1507, loss 0.607442, acc 0.734375
2017-03-02T17:34:18.541461: step 1508, loss 0.504493, acc 0.78125
2017-03-02T17:34:18.607200: step 1509, loss 0.423224, acc 0.859375
2017-03-02T17:34:18.683013: step 1510, loss 0.474477, acc 0.859375
2017-03-02T17:34:18.765741: step 1511, loss 0.510594, acc 0.796875
2017-03-02T17:34:18.836803: step 1512, loss 0.476019, acc 0.8125
2017-03-02T17:34:18.913634: step 1513, loss 0.488871, acc 0.796875
2017-03-02T17:34:18.986686: step 1514, loss 0.522968, acc 0.765625
2017-03-02T17:34:19.060042: step 1515, loss 0.552544, acc 0.796875
2017-03-02T17:34:19.137490: step 1516, loss 0.485435, acc 0.828125
2017-03-02T17:34:19.205232: step 1517, loss 0.47955, acc 0.8125
2017-03-02T17:34:19.270823: step 1518, loss 0.651929, acc 0.734375
2017-03-02T17:34:19.344363: step 1519, loss 0.571008, acc 0.8125
2017-03-02T17:34:19.417995: step 1520, loss 0.557262, acc 0.703125
2017-03-02T17:34:19.492408: step 1521, loss 0.442093, acc 0.84375
2017-03-02T17:34:19.570623: step 1522, loss 0.594016, acc 0.75
2017-03-02T17:34:19.643292: step 1523, loss 0.501511, acc 0.8125
2017-03-02T17:34:19.721926: step 1524, loss 0.583689, acc 0.796875
2017-03-02T17:34:19.795465: step 1525, loss 0.365355, acc 0.875
2017-03-02T17:34:19.854831: step 1526, loss 0.568816, acc 0.6875
2017-03-02T17:34:19.927086: step 1527, loss 0.724481, acc 0.71875
2017-03-02T17:34:20.002900: step 1528, loss 0.538011, acc 0.78125
2017-03-02T17:34:20.079363: step 1529, loss 0.57189, acc 0.734375
2017-03-02T17:34:20.152848: step 1530, loss 0.513485, acc 0.796875
2017-03-02T17:34:20.232723: step 1531, loss 0.690648, acc 0.734375
2017-03-02T17:34:20.308664: step 1532, loss 0.519751, acc 0.75
2017-03-02T17:34:20.380881: step 1533, loss 0.740853, acc 0.734375
2017-03-02T17:34:20.455467: step 1534, loss 0.520092, acc 0.734375
2017-03-02T17:34:20.524683: step 1535, loss 0.444722, acc 0.84375
2017-03-02T17:34:20.589187: step 1536, loss 0.5195, acc 0.84375
2017-03-02T17:34:20.656866: step 1537, loss 0.570261, acc 0.859375
2017-03-02T17:34:20.738166: step 1538, loss 0.672371, acc 0.78125
2017-03-02T17:34:20.806833: step 1539, loss 0.554547, acc 0.75
2017-03-02T17:34:20.893923: step 1540, loss 0.699429, acc 0.75
2017-03-02T17:34:20.970327: step 1541, loss 0.370734, acc 0.875
2017-03-02T17:34:21.050424: step 1542, loss 0.564201, acc 0.78125
2017-03-02T17:34:21.128988: step 1543, loss 0.469475, acc 0.859375
2017-03-02T17:34:21.192969: step 1544, loss 0.626769, acc 0.703125
2017-03-02T17:34:21.267612: step 1545, loss 0.522415, acc 0.734375
2017-03-02T17:34:21.333839: step 1546, loss 0.622908, acc 0.6875
2017-03-02T17:34:21.411138: step 1547, loss 0.427989, acc 0.828125
2017-03-02T17:34:21.485908: step 1548, loss 0.504253, acc 0.796875
2017-03-02T17:34:21.557284: step 1549, loss 0.405477, acc 0.84375
2017-03-02T17:34:21.630596: step 1550, loss 0.389088, acc 0.859375
2017-03-02T17:34:21.701115: step 1551, loss 0.483017, acc 0.796875
2017-03-02T17:34:21.774758: step 1552, loss 0.462716, acc 0.859375
2017-03-02T17:34:21.851277: step 1553, loss 0.599822, acc 0.703125
2017-03-02T17:34:21.934307: step 1554, loss 0.530288, acc 0.796875
2017-03-02T17:34:22.002406: step 1555, loss 0.388435, acc 0.875
2017-03-02T17:34:22.085589: step 1556, loss 0.385636, acc 0.84375
2017-03-02T17:34:22.161760: step 1557, loss 0.727874, acc 0.75
2017-03-02T17:34:22.236692: step 1558, loss 0.62927, acc 0.765625
2017-03-02T17:34:22.318889: step 1559, loss 0.616704, acc 0.734375
2017-03-02T17:34:22.386328: step 1560, loss 0.568774, acc 0.765625
2017-03-02T17:34:22.457819: step 1561, loss 0.539047, acc 0.765625
2017-03-02T17:34:22.532410: step 1562, loss 0.447128, acc 0.828125
2017-03-02T17:34:22.601995: step 1563, loss 0.52204, acc 0.8125
2017-03-02T17:34:22.672333: step 1564, loss 0.654646, acc 0.75
2017-03-02T17:34:22.743892: step 1565, loss 0.408466, acc 0.796875
2017-03-02T17:34:22.833989: step 1566, loss 0.767388, acc 0.640625
2017-03-02T17:34:22.915850: step 1567, loss 0.700034, acc 0.6875
2017-03-02T17:34:22.983705: step 1568, loss 1.12887, acc 0.5
2017-03-02T17:34:23.063859: step 1569, loss 0.502552, acc 0.8125
2017-03-02T17:34:23.132076: step 1570, loss 0.495889, acc 0.78125
2017-03-02T17:34:23.203820: step 1571, loss 0.374757, acc 0.859375
2017-03-02T17:34:23.268728: step 1572, loss 0.518489, acc 0.828125
2017-03-02T17:34:23.342414: step 1573, loss 0.510088, acc 0.859375
2017-03-02T17:34:23.412962: step 1574, loss 0.341146, acc 0.84375
2017-03-02T17:34:23.489668: step 1575, loss 0.609687, acc 0.734375
2017-03-02T17:34:23.567954: step 1576, loss 0.335797, acc 0.875
2017-03-02T17:34:23.645319: step 1577, loss 0.491882, acc 0.84375
2017-03-02T17:34:23.719405: step 1578, loss 0.28074, acc 0.90625
2017-03-02T17:34:23.795625: step 1579, loss 0.553889, acc 0.78125
2017-03-02T17:34:23.869026: step 1580, loss 0.482985, acc 0.796875
2017-03-02T17:34:23.948023: step 1581, loss 0.379317, acc 0.859375
2017-03-02T17:34:24.015414: step 1582, loss 0.348239, acc 0.84375
2017-03-02T17:34:24.078775: step 1583, loss 0.373982, acc 0.875
2017-03-02T17:34:24.152825: step 1584, loss 0.605458, acc 0.8125
2017-03-02T17:34:24.222821: step 1585, loss 0.464188, acc 0.828125
2017-03-02T17:34:24.295951: step 1586, loss 0.605429, acc 0.6875
2017-03-02T17:34:24.374601: step 1587, loss 0.639224, acc 0.78125
2017-03-02T17:34:24.461389: step 1588, loss 0.390259, acc 0.8125
2017-03-02T17:34:24.533171: step 1589, loss 0.40146, acc 0.828125
2017-03-02T17:34:24.604539: step 1590, loss 0.407137, acc 0.859375
2017-03-02T17:34:24.673907: step 1591, loss 0.568646, acc 0.71875
2017-03-02T17:34:24.740667: step 1592, loss 0.501568, acc 0.828125
2017-03-02T17:34:24.819811: step 1593, loss 0.48387, acc 0.796875
2017-03-02T17:34:24.898096: step 1594, loss 0.466915, acc 0.796875
2017-03-02T17:34:24.965257: step 1595, loss 0.513047, acc 0.8125
2017-03-02T17:34:25.041614: step 1596, loss 0.440086, acc 0.875
2017-03-02T17:34:25.117705: step 1597, loss 0.527425, acc 0.78125
2017-03-02T17:34:25.192234: step 1598, loss 0.580601, acc 0.8125
2017-03-02T17:34:25.267300: step 1599, loss 0.395554, acc 0.859375
2017-03-02T17:34:25.336408: step 1600, loss 0.363851, acc 0.921875

Evaluation:
2017-03-02T17:34:25.361927: step 1600, loss 0.768803, acc 0.69142

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1600

2017-03-02T17:34:25.814366: step 1601, loss 0.44428, acc 0.8125
2017-03-02T17:34:25.890455: step 1602, loss 0.372967, acc 0.890625
2017-03-02T17:34:25.964097: step 1603, loss 0.470172, acc 0.8125
2017-03-02T17:34:26.029215: step 1604, loss 0.366771, acc 0.890625
2017-03-02T17:34:26.098478: step 1605, loss 0.691726, acc 0.75
2017-03-02T17:34:26.167583: step 1606, loss 0.455458, acc 0.828125
2017-03-02T17:34:26.240055: step 1607, loss 0.507426, acc 0.765625
2017-03-02T17:34:26.313535: step 1608, loss 0.476571, acc 0.84375
2017-03-02T17:34:26.390328: step 1609, loss 0.547336, acc 0.765625
2017-03-02T17:34:26.465363: step 1610, loss 0.432269, acc 0.875
2017-03-02T17:34:26.536238: step 1611, loss 0.437724, acc 0.828125
2017-03-02T17:34:26.608003: step 1612, loss 0.615694, acc 0.71875
2017-03-02T17:34:26.685178: step 1613, loss 0.554308, acc 0.765625
2017-03-02T17:34:26.758416: step 1614, loss 0.503802, acc 0.78125
2017-03-02T17:34:26.829830: step 1615, loss 0.41045, acc 0.84375
2017-03-02T17:34:26.916541: step 1616, loss 0.58981, acc 0.796875
2017-03-02T17:34:26.990881: step 1617, loss 0.615475, acc 0.78125
2017-03-02T17:34:27.064213: step 1618, loss 0.536522, acc 0.796875
2017-03-02T17:34:27.138855: step 1619, loss 0.503403, acc 0.84375
2017-03-02T17:34:27.207302: step 1620, loss 0.482719, acc 0.859375
2017-03-02T17:34:27.280293: step 1621, loss 0.694651, acc 0.734375
2017-03-02T17:34:27.351422: step 1622, loss 0.333332, acc 0.890625
2017-03-02T17:34:27.420090: step 1623, loss 0.348593, acc 0.828125
2017-03-02T17:34:27.485980: step 1624, loss 0.453877, acc 0.828125
2017-03-02T17:34:27.558602: step 1625, loss 0.365004, acc 0.84375
2017-03-02T17:34:27.647302: step 1626, loss 0.424879, acc 0.8125
2017-03-02T17:34:27.730917: step 1627, loss 0.482817, acc 0.84375
2017-03-02T17:34:27.798833: step 1628, loss 0.385584, acc 0.84375
2017-03-02T17:34:27.877755: step 1629, loss 0.598972, acc 0.828125
2017-03-02T17:34:27.944713: step 1630, loss 0.499628, acc 0.78125
2017-03-02T17:34:28.028174: step 1631, loss 0.38839, acc 0.84375
2017-03-02T17:34:28.106006: step 1632, loss 0.362273, acc 0.84375
2017-03-02T17:34:28.178471: step 1633, loss 0.522234, acc 0.875
2017-03-02T17:34:28.252862: step 1634, loss 0.579306, acc 0.734375
2017-03-02T17:34:28.325025: step 1635, loss 0.39715, acc 0.890625
2017-03-02T17:34:28.398536: step 1636, loss 0.432258, acc 0.859375
2017-03-02T17:34:28.481912: step 1637, loss 0.510245, acc 0.8125
2017-03-02T17:34:28.557935: step 1638, loss 0.553218, acc 0.8125
2017-03-02T17:34:28.630892: step 1639, loss 0.425108, acc 0.84375
2017-03-02T17:34:28.714154: step 1640, loss 0.381083, acc 0.828125
2017-03-02T17:34:28.781400: step 1641, loss 0.539198, acc 0.765625
2017-03-02T17:34:28.849642: step 1642, loss 0.539762, acc 0.78125
2017-03-02T17:34:28.918511: step 1643, loss 0.510733, acc 0.75
2017-03-02T17:34:28.988529: step 1644, loss 0.309575, acc 0.921875
2017-03-02T17:34:29.060144: step 1645, loss 0.416404, acc 0.828125
2017-03-02T17:34:29.132125: step 1646, loss 0.477769, acc 0.859375
2017-03-02T17:34:29.204610: step 1647, loss 0.437199, acc 0.828125
2017-03-02T17:34:29.279198: step 1648, loss 0.610892, acc 0.796875
2017-03-02T17:34:29.347633: step 1649, loss 0.38051, acc 0.84375
2017-03-02T17:34:29.422578: step 1650, loss 0.45763, acc 0.8125
2017-03-02T17:34:29.483945: step 1651, loss 0.448225, acc 0.859375
2017-03-02T17:34:29.548983: step 1652, loss 0.654047, acc 0.71875
2017-03-02T17:34:29.620556: step 1653, loss 0.545157, acc 0.75
2017-03-02T17:34:29.693584: step 1654, loss 0.570334, acc 0.796875
2017-03-02T17:34:29.765140: step 1655, loss 0.589517, acc 0.84375
2017-03-02T17:34:29.836980: step 1656, loss 0.600419, acc 0.703125
2017-03-02T17:34:29.905702: step 1657, loss 0.449884, acc 0.84375
2017-03-02T17:34:29.977717: step 1658, loss 0.500036, acc 0.796875
2017-03-02T17:34:30.050664: step 1659, loss 0.422911, acc 0.796875
2017-03-02T17:34:30.127028: step 1660, loss 0.493863, acc 0.828125
2017-03-02T17:34:30.197770: step 1661, loss 0.588872, acc 0.8125
2017-03-02T17:34:30.269717: step 1662, loss 0.622658, acc 0.765625
2017-03-02T17:34:30.345157: step 1663, loss 0.629203, acc 0.75
2017-03-02T17:34:30.414274: step 1664, loss 0.590607, acc 0.796875
2017-03-02T17:34:30.488707: step 1665, loss 0.416199, acc 0.890625
2017-03-02T17:34:30.568291: step 1666, loss 0.680094, acc 0.78125
2017-03-02T17:34:30.640867: step 1667, loss 0.377602, acc 0.828125
2017-03-02T17:34:30.711221: step 1668, loss 0.492465, acc 0.8125
2017-03-02T17:34:30.790789: step 1669, loss 0.468299, acc 0.8125
2017-03-02T17:34:30.861721: step 1670, loss 0.424511, acc 0.84375
2017-03-02T17:34:30.945776: step 1671, loss 0.320404, acc 0.921875
2017-03-02T17:34:31.030377: step 1672, loss 0.532775, acc 0.8125
2017-03-02T17:34:31.101928: step 1673, loss 0.402135, acc 0.828125
2017-03-02T17:34:31.168489: step 1674, loss 0.496559, acc 0.8125
2017-03-02T17:34:31.243214: step 1675, loss 0.427328, acc 0.796875
2017-03-02T17:34:31.314746: step 1676, loss 0.386809, acc 0.875
2017-03-02T17:34:31.389818: step 1677, loss 0.383376, acc 0.8125
2017-03-02T17:34:31.454109: step 1678, loss 0.707057, acc 0.78125
2017-03-02T17:34:31.546194: step 1679, loss 0.448323, acc 0.828125
2017-03-02T17:34:31.613484: step 1680, loss 0.548764, acc 0.796875
2017-03-02T17:34:31.675433: step 1681, loss 0.617225, acc 0.796875
2017-03-02T17:34:31.749235: step 1682, loss 0.635204, acc 0.734375
2017-03-02T17:34:31.831872: step 1683, loss 0.45903, acc 0.796875
2017-03-02T17:34:31.921375: step 1684, loss 0.433, acc 0.78125
2017-03-02T17:34:31.997336: step 1685, loss 0.55737, acc 0.78125
2017-03-02T17:34:32.060702: step 1686, loss 0.377718, acc 0.84375
2017-03-02T17:34:32.131853: step 1687, loss 0.419379, acc 0.796875
2017-03-02T17:34:32.212173: step 1688, loss 0.436159, acc 0.8125
2017-03-02T17:34:32.283829: step 1689, loss 0.504473, acc 0.765625
2017-03-02T17:34:32.354395: step 1690, loss 0.537391, acc 0.8125
2017-03-02T17:34:32.427322: step 1691, loss 0.624937, acc 0.796875
2017-03-02T17:34:32.501073: step 1692, loss 0.423852, acc 0.78125
2017-03-02T17:34:32.559192: step 1693, loss 0.717448, acc 0.703125
2017-03-02T17:34:32.632214: step 1694, loss 0.485582, acc 0.765625
2017-03-02T17:34:32.697110: step 1695, loss 0.510173, acc 0.828125
2017-03-02T17:34:32.774218: step 1696, loss 0.664207, acc 0.796875
2017-03-02T17:34:32.845282: step 1697, loss 0.559077, acc 0.75
2017-03-02T17:34:32.922526: step 1698, loss 0.406046, acc 0.84375
2017-03-02T17:34:32.989068: step 1699, loss 0.511227, acc 0.734375
2017-03-02T17:34:33.055738: step 1700, loss 0.503366, acc 0.828125

Evaluation:
2017-03-02T17:34:33.084954: step 1700, loss 0.783375, acc 0.68421

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1700

2017-03-02T17:34:33.533633: step 1701, loss 0.413718, acc 0.828125
2017-03-02T17:34:33.606551: step 1702, loss 0.490544, acc 0.8125
2017-03-02T17:34:33.678958: step 1703, loss 0.48387, acc 0.828125
2017-03-02T17:34:33.747184: step 1704, loss 0.413561, acc 0.828125
2017-03-02T17:34:33.824847: step 1705, loss 0.378385, acc 0.859375
2017-03-02T17:34:33.901508: step 1706, loss 0.565621, acc 0.796875
2017-03-02T17:34:33.976014: step 1707, loss 0.616702, acc 0.78125
2017-03-02T17:34:34.048756: step 1708, loss 0.614985, acc 0.78125
2017-03-02T17:34:34.121723: step 1709, loss 0.600792, acc 0.8125
2017-03-02T17:34:34.196239: step 1710, loss 0.553609, acc 0.78125
2017-03-02T17:34:34.266411: step 1711, loss 0.480246, acc 0.765625
2017-03-02T17:34:34.337025: step 1712, loss 0.424866, acc 0.8125
2017-03-02T17:34:34.419208: step 1713, loss 0.579117, acc 0.8125
2017-03-02T17:34:34.492088: step 1714, loss 0.480215, acc 0.84375
2017-03-02T17:34:34.572117: step 1715, loss 0.540039, acc 0.796875
2017-03-02T17:34:34.645418: step 1716, loss 0.504367, acc 0.828125
2017-03-02T17:34:34.714549: step 1717, loss 0.478325, acc 0.8125
2017-03-02T17:34:34.775551: step 1718, loss 0.522252, acc 0.765625
2017-03-02T17:34:34.844818: step 1719, loss 0.597093, acc 0.75
2017-03-02T17:34:34.913967: step 1720, loss 0.503917, acc 0.765625
2017-03-02T17:34:34.986189: step 1721, loss 0.380019, acc 0.859375
2017-03-02T17:34:35.055039: step 1722, loss 0.609183, acc 0.765625
2017-03-02T17:34:35.116817: step 1723, loss 0.438617, acc 0.796875
2017-03-02T17:34:35.188361: step 1724, loss 0.558912, acc 0.734375
2017-03-02T17:34:35.258869: step 1725, loss 0.495954, acc 0.796875
2017-03-02T17:34:35.331038: step 1726, loss 0.449452, acc 0.828125
2017-03-02T17:34:35.402713: step 1727, loss 0.512173, acc 0.734375
2017-03-02T17:34:35.473832: step 1728, loss 0.485798, acc 0.84375
2017-03-02T17:34:35.545880: step 1729, loss 0.561381, acc 0.828125
2017-03-02T17:34:35.619034: step 1730, loss 0.386155, acc 0.859375
2017-03-02T17:34:35.689966: step 1731, loss 0.550063, acc 0.78125
2017-03-02T17:34:35.758438: step 1732, loss 0.579937, acc 0.75
2017-03-02T17:34:35.822589: step 1733, loss 0.51515, acc 0.78125
2017-03-02T17:34:35.894491: step 1734, loss 0.420576, acc 0.796875
2017-03-02T17:34:35.965771: step 1735, loss 0.346422, acc 0.875
2017-03-02T17:34:36.039882: step 1736, loss 0.537451, acc 0.796875
2017-03-02T17:34:36.114165: step 1737, loss 0.524869, acc 0.796875
2017-03-02T17:34:36.186548: step 1738, loss 0.306223, acc 0.90625
2017-03-02T17:34:36.260166: step 1739, loss 0.518164, acc 0.734375
2017-03-02T17:34:36.339092: step 1740, loss 0.550419, acc 0.734375
2017-03-02T17:34:36.404042: step 1741, loss 0.330732, acc 0.9375
2017-03-02T17:34:36.472159: step 1742, loss 0.558528, acc 0.78125
2017-03-02T17:34:36.545930: step 1743, loss 0.414162, acc 0.828125
2017-03-02T17:34:36.616618: step 1744, loss 0.396508, acc 0.890625
2017-03-02T17:34:36.691279: step 1745, loss 0.452954, acc 0.796875
2017-03-02T17:34:36.769311: step 1746, loss 0.470265, acc 0.796875
2017-03-02T17:34:36.836747: step 1747, loss 0.565265, acc 0.8125
2017-03-02T17:34:36.901324: step 1748, loss 0.507032, acc 0.796875
2017-03-02T17:34:36.975768: step 1749, loss 0.491451, acc 0.75
2017-03-02T17:34:37.047466: step 1750, loss 0.587848, acc 0.71875
2017-03-02T17:34:37.118207: step 1751, loss 0.478916, acc 0.8125
2017-03-02T17:34:37.190715: step 1752, loss 0.281866, acc 0.890625
2017-03-02T17:34:37.266442: step 1753, loss 0.518871, acc 0.796875
2017-03-02T17:34:37.336214: step 1754, loss 0.318408, acc 0.90625
2017-03-02T17:34:37.411866: step 1755, loss 0.469099, acc 0.828125
2017-03-02T17:34:37.486216: step 1756, loss 0.510919, acc 0.796875
2017-03-02T17:34:37.574654: step 1757, loss 0.629853, acc 0.78125
2017-03-02T17:34:37.657456: step 1758, loss 0.534398, acc 0.859375
2017-03-02T17:34:37.741939: step 1759, loss 0.301246, acc 0.921875
2017-03-02T17:34:37.809116: step 1760, loss 0.48511, acc 0.8125
2017-03-02T17:34:37.875225: step 1761, loss 0.515803, acc 0.796875
2017-03-02T17:34:37.952526: step 1762, loss 0.397992, acc 0.84375
2017-03-02T17:34:38.024369: step 1763, loss 0.448614, acc 0.78125
2017-03-02T17:34:38.105602: step 1764, loss 0.396384, acc 0.75
2017-03-02T17:34:38.180356: step 1765, loss 0.342403, acc 0.890625
2017-03-02T17:34:38.258052: step 1766, loss 0.384046, acc 0.859375
2017-03-02T17:34:38.333079: step 1767, loss 0.485258, acc 0.859375
2017-03-02T17:34:38.404906: step 1768, loss 0.484324, acc 0.78125
2017-03-02T17:34:38.479583: step 1769, loss 0.634953, acc 0.796875
2017-03-02T17:34:38.549857: step 1770, loss 0.333842, acc 0.890625
2017-03-02T17:34:38.621615: step 1771, loss 0.41421, acc 0.8125
2017-03-02T17:34:38.691677: step 1772, loss 0.358299, acc 0.859375
2017-03-02T17:34:38.772171: step 1773, loss 0.771758, acc 0.78125
2017-03-02T17:34:38.848561: step 1774, loss 0.359038, acc 0.84375
2017-03-02T17:34:38.920859: step 1775, loss 0.462248, acc 0.78125
2017-03-02T17:34:38.992699: step 1776, loss 0.363443, acc 0.890625
2017-03-02T17:34:39.072920: step 1777, loss 0.405439, acc 0.84375
2017-03-02T17:34:39.139125: step 1778, loss 0.30055, acc 0.859375
2017-03-02T17:34:39.213272: step 1779, loss 0.476475, acc 0.796875
2017-03-02T17:34:39.281177: step 1780, loss 0.42807, acc 0.859375
2017-03-02T17:34:39.353676: step 1781, loss 0.357044, acc 0.828125
2017-03-02T17:34:39.428297: step 1782, loss 0.408562, acc 0.84375
2017-03-02T17:34:39.498256: step 1783, loss 0.411475, acc 0.90625
2017-03-02T17:34:39.569651: step 1784, loss 0.462783, acc 0.78125
2017-03-02T17:34:39.643378: step 1785, loss 0.419923, acc 0.828125
2017-03-02T17:34:39.712466: step 1786, loss 0.556853, acc 0.78125
2017-03-02T17:34:39.785169: step 1787, loss 0.326196, acc 0.890625
2017-03-02T17:34:39.854246: step 1788, loss 0.400876, acc 0.828125
2017-03-02T17:34:39.921602: step 1789, loss 0.401912, acc 0.859375
2017-03-02T17:34:39.991487: step 1790, loss 0.460083, acc 0.78125
2017-03-02T17:34:40.061812: step 1791, loss 0.576759, acc 0.796875
2017-03-02T17:34:40.134788: step 1792, loss 0.322655, acc 0.859375
2017-03-02T17:34:40.207271: step 1793, loss 0.339383, acc 0.828125
2017-03-02T17:34:40.277354: step 1794, loss 0.337837, acc 0.859375
2017-03-02T17:34:40.350970: step 1795, loss 0.375107, acc 0.90625
2017-03-02T17:34:40.424049: step 1796, loss 0.358432, acc 0.859375
2017-03-02T17:34:40.508626: step 1797, loss 0.631742, acc 0.765625
2017-03-02T17:34:40.576658: step 1798, loss 0.455674, acc 0.78125
2017-03-02T17:34:40.647052: step 1799, loss 0.457744, acc 0.828125
2017-03-02T17:34:40.721513: step 1800, loss 0.313264, acc 0.890625

Evaluation:
2017-03-02T17:34:40.755433: step 1800, loss 0.788517, acc 0.687815

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1800

2017-03-02T17:34:41.203159: step 1801, loss 0.45333, acc 0.765625
2017-03-02T17:34:41.270815: step 1802, loss 0.467255, acc 0.859375
2017-03-02T17:34:41.340808: step 1803, loss 0.292808, acc 0.90625
2017-03-02T17:34:41.416617: step 1804, loss 0.483188, acc 0.796875
2017-03-02T17:34:41.489892: step 1805, loss 0.367224, acc 0.90625
2017-03-02T17:34:41.561768: step 1806, loss 0.451724, acc 0.796875
2017-03-02T17:34:41.636221: step 1807, loss 0.336673, acc 0.875
2017-03-02T17:34:41.705580: step 1808, loss 0.341751, acc 0.84375
2017-03-02T17:34:41.775896: step 1809, loss 0.331253, acc 0.875
2017-03-02T17:34:41.850690: step 1810, loss 0.48589, acc 0.8125
2017-03-02T17:34:41.919800: step 1811, loss 0.355102, acc 0.875
2017-03-02T17:34:41.995288: step 1812, loss 0.497582, acc 0.828125
2017-03-02T17:34:42.064615: step 1813, loss 0.386671, acc 0.8125
2017-03-02T17:34:42.138106: step 1814, loss 0.45769, acc 0.796875
2017-03-02T17:34:42.208292: step 1815, loss 0.497217, acc 0.765625
2017-03-02T17:34:42.281975: step 1816, loss 0.53574, acc 0.84375
2017-03-02T17:34:42.369306: step 1817, loss 0.616533, acc 0.75
2017-03-02T17:34:42.439467: step 1818, loss 0.398733, acc 0.8125
2017-03-02T17:34:42.509815: step 1819, loss 0.373891, acc 0.84375
2017-03-02T17:34:42.582926: step 1820, loss 0.704263, acc 0.796875
2017-03-02T17:34:42.654201: step 1821, loss 0.302581, acc 0.90625
2017-03-02T17:34:42.725321: step 1822, loss 0.478275, acc 0.828125
2017-03-02T17:34:42.798347: step 1823, loss 0.419258, acc 0.8125
2017-03-02T17:34:42.864719: step 1824, loss 0.417087, acc 0.796875
2017-03-02T17:34:42.937557: step 1825, loss 0.35903, acc 0.84375
2017-03-02T17:34:43.015331: step 1826, loss 0.453164, acc 0.8125
2017-03-02T17:34:43.084164: step 1827, loss 0.527328, acc 0.78125
2017-03-02T17:34:43.158929: step 1828, loss 0.38225, acc 0.890625
2017-03-02T17:34:43.225470: step 1829, loss 0.309957, acc 0.84375
2017-03-02T17:34:43.297306: step 1830, loss 0.386172, acc 0.828125
2017-03-02T17:34:43.365843: step 1831, loss 0.324201, acc 0.890625
2017-03-02T17:34:43.429127: step 1832, loss 0.588186, acc 0.765625
2017-03-02T17:34:43.499339: step 1833, loss 0.395167, acc 0.84375
2017-03-02T17:34:43.575292: step 1834, loss 0.493015, acc 0.84375
2017-03-02T17:34:43.649940: step 1835, loss 0.319609, acc 0.875
2017-03-02T17:34:43.720744: step 1836, loss 0.473645, acc 0.828125
2017-03-02T17:34:43.804437: step 1837, loss 0.494344, acc 0.828125
2017-03-02T17:34:43.877561: step 1838, loss 0.475554, acc 0.828125
2017-03-02T17:34:43.950152: step 1839, loss 0.299268, acc 0.90625
2017-03-02T17:34:44.018555: step 1840, loss 0.54825, acc 0.796875
2017-03-02T17:34:44.084781: step 1841, loss 0.517065, acc 0.765625
2017-03-02T17:34:44.173649: step 1842, loss 0.437672, acc 0.84375
2017-03-02T17:34:44.247841: step 1843, loss 0.313251, acc 0.890625
2017-03-02T17:34:44.326087: step 1844, loss 0.427153, acc 0.796875
2017-03-02T17:34:44.406843: step 1845, loss 0.280652, acc 0.890625
2017-03-02T17:34:44.477836: step 1846, loss 0.423301, acc 0.890625
2017-03-02T17:34:44.551266: step 1847, loss 0.442266, acc 0.828125
2017-03-02T17:34:44.621556: step 1848, loss 0.468885, acc 0.84375
2017-03-02T17:34:44.686073: step 1849, loss 0.512316, acc 0.765625
2017-03-02T17:34:44.747373: step 1850, loss 0.567828, acc 0.75
2017-03-02T17:34:44.812297: step 1851, loss 0.500923, acc 0.78125
2017-03-02T17:34:44.885887: step 1852, loss 0.486074, acc 0.796875
2017-03-02T17:34:44.957586: step 1853, loss 0.291187, acc 0.921875
2017-03-02T17:34:45.031344: step 1854, loss 0.555611, acc 0.828125
2017-03-02T17:34:45.110559: step 1855, loss 0.42616, acc 0.796875
2017-03-02T17:34:45.188602: step 1856, loss 0.491775, acc 0.765625
2017-03-02T17:34:45.261833: step 1857, loss 0.500598, acc 0.8125
2017-03-02T17:34:45.344799: step 1858, loss 0.332093, acc 0.84375
2017-03-02T17:34:45.413331: step 1859, loss 0.461277, acc 0.84375
2017-03-02T17:34:45.481644: step 1860, loss 0.472952, acc 0.8125
2017-03-02T17:34:45.563426: step 1861, loss 0.414344, acc 0.859375
2017-03-02T17:34:45.638149: step 1862, loss 0.537324, acc 0.828125
2017-03-02T17:34:45.709686: step 1863, loss 0.385136, acc 0.828125
2017-03-02T17:34:45.783957: step 1864, loss 0.621972, acc 0.765625
2017-03-02T17:34:45.856422: step 1865, loss 0.557263, acc 0.796875
2017-03-02T17:34:45.927763: step 1866, loss 0.369009, acc 0.875
2017-03-02T17:34:46.004747: step 1867, loss 0.540216, acc 0.8125
2017-03-02T17:34:46.076625: step 1868, loss 0.608136, acc 0.71875
2017-03-02T17:34:46.145983: step 1869, loss 0.337135, acc 0.875
2017-03-02T17:34:46.221523: step 1870, loss 0.413879, acc 0.84375
2017-03-02T17:34:46.298401: step 1871, loss 0.445317, acc 0.859375
2017-03-02T17:34:46.374451: step 1872, loss 0.451688, acc 0.75
2017-03-02T17:34:46.449789: step 1873, loss 0.32785, acc 0.890625
2017-03-02T17:34:46.518810: step 1874, loss 0.685199, acc 0.765625
2017-03-02T17:34:46.589859: step 1875, loss 0.426792, acc 0.859375
2017-03-02T17:34:46.667865: step 1876, loss 0.397444, acc 0.859375
2017-03-02T17:34:46.739080: step 1877, loss 0.373628, acc 0.859375
2017-03-02T17:34:46.812543: step 1878, loss 0.377964, acc 0.875
2017-03-02T17:34:46.880753: step 1879, loss 0.35223, acc 0.875
2017-03-02T17:34:46.951641: step 1880, loss 0.348761, acc 0.84375
2017-03-02T17:34:47.035841: step 1881, loss 0.730764, acc 0.734375
2017-03-02T17:34:47.112413: step 1882, loss 0.313953, acc 0.859375
2017-03-02T17:34:47.179069: step 1883, loss 0.215378, acc 0.9375
2017-03-02T17:34:47.251712: step 1884, loss 0.289321, acc 0.859375
2017-03-02T17:34:47.328068: step 1885, loss 0.370061, acc 0.828125
2017-03-02T17:34:47.400706: step 1886, loss 0.454777, acc 0.828125
2017-03-02T17:34:47.467524: step 1887, loss 0.400606, acc 0.84375
2017-03-02T17:34:47.537181: step 1888, loss 0.462337, acc 0.78125
2017-03-02T17:34:47.611052: step 1889, loss 0.415884, acc 0.8125
2017-03-02T17:34:47.685286: step 1890, loss 0.479388, acc 0.84375
2017-03-02T17:34:47.765536: step 1891, loss 0.413888, acc 0.84375
2017-03-02T17:34:47.831283: step 1892, loss 0.497412, acc 0.8125
2017-03-02T17:34:47.901336: step 1893, loss 0.404052, acc 0.84375
2017-03-02T17:34:47.972563: step 1894, loss 0.37008, acc 0.84375
2017-03-02T17:34:48.043846: step 1895, loss 0.416989, acc 0.828125
2017-03-02T17:34:48.118619: step 1896, loss 0.449044, acc 0.875
2017-03-02T17:34:48.189686: step 1897, loss 0.66217, acc 0.796875
2017-03-02T17:34:48.265186: step 1898, loss 0.493549, acc 0.765625
2017-03-02T17:34:48.338981: step 1899, loss 0.402408, acc 0.875
2017-03-02T17:34:48.411405: step 1900, loss 0.737867, acc 0.734375

Evaluation:
2017-03-02T17:34:48.445190: step 1900, loss 0.811968, acc 0.680606

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-1900

2017-03-02T17:34:48.881428: step 1901, loss 0.32525, acc 0.828125
2017-03-02T17:34:48.951704: step 1902, loss 0.521571, acc 0.78125
2017-03-02T17:34:49.025620: step 1903, loss 0.449036, acc 0.78125
2017-03-02T17:34:49.102392: step 1904, loss 0.464718, acc 0.828125
2017-03-02T17:34:49.176927: step 1905, loss 0.47528, acc 0.796875
2017-03-02T17:34:49.261547: step 1906, loss 0.363012, acc 0.84375
2017-03-02T17:34:49.334726: step 1907, loss 0.261341, acc 0.90625
2017-03-02T17:34:49.407409: step 1908, loss 0.345443, acc 0.859375
2017-03-02T17:34:49.480722: step 1909, loss 0.546806, acc 0.84375
2017-03-02T17:34:49.560677: step 1910, loss 0.616966, acc 0.796875
2017-03-02T17:34:49.633338: step 1911, loss 0.754751, acc 0.734375
2017-03-02T17:34:49.707403: step 1912, loss 0.58202, acc 0.765625
2017-03-02T17:34:49.787653: step 1913, loss 0.506662, acc 0.84375
2017-03-02T17:34:49.872696: step 1914, loss 0.483941, acc 0.84375
2017-03-02T17:34:49.955647: step 1915, loss 0.420741, acc 0.84375
2017-03-02T17:34:50.030610: step 1916, loss 0.351222, acc 0.859375
2017-03-02T17:34:50.107004: step 1917, loss 0.423519, acc 0.8125
2017-03-02T17:34:50.179437: step 1918, loss 0.302111, acc 0.9375
2017-03-02T17:34:50.247749: step 1919, loss 0.366126, acc 0.875
2017-03-02T17:34:50.320063: step 1920, loss 0.422791, acc 0.8125
2017-03-02T17:34:50.394245: step 1921, loss 0.520128, acc 0.796875
2017-03-02T17:34:50.465476: step 1922, loss 0.527699, acc 0.859375
2017-03-02T17:34:50.537520: step 1923, loss 0.448335, acc 0.859375
2017-03-02T17:34:50.610631: step 1924, loss 0.33951, acc 0.859375
2017-03-02T17:34:50.678350: step 1925, loss 0.419315, acc 0.8125
2017-03-02T17:34:50.756628: step 1926, loss 0.532357, acc 0.796875
2017-03-02T17:34:50.830535: step 1927, loss 0.311457, acc 0.921875
2017-03-02T17:34:50.902787: step 1928, loss 0.351476, acc 0.921875
2017-03-02T17:34:50.975290: step 1929, loss 0.349541, acc 0.859375
2017-03-02T17:34:51.045366: step 1930, loss 0.650657, acc 0.75
2017-03-02T17:34:51.123712: step 1931, loss 0.326669, acc 0.859375
2017-03-02T17:34:51.200838: step 1932, loss 0.503128, acc 0.796875
2017-03-02T17:34:51.276197: step 1933, loss 0.305369, acc 0.859375
2017-03-02T17:34:51.354341: step 1934, loss 0.29005, acc 0.875
2017-03-02T17:34:51.431950: step 1935, loss 0.608727, acc 0.765625
2017-03-02T17:34:51.509892: step 1936, loss 0.437199, acc 0.859375
2017-03-02T17:34:51.591485: step 1937, loss 0.577155, acc 0.765625
2017-03-02T17:34:51.663466: step 1938, loss 0.350271, acc 0.84375
2017-03-02T17:34:51.735615: step 1939, loss 0.417457, acc 0.875
2017-03-02T17:34:51.809593: step 1940, loss 0.59187, acc 0.78125
2017-03-02T17:34:51.883398: step 1941, loss 0.407491, acc 0.859375
2017-03-02T17:34:51.958578: step 1942, loss 0.54963, acc 0.75
2017-03-02T17:34:52.029842: step 1943, loss 0.440574, acc 0.84375
2017-03-02T17:34:52.100690: step 1944, loss 0.389758, acc 0.875
2017-03-02T17:34:52.175474: step 1945, loss 0.46018, acc 0.8125
2017-03-02T17:34:52.241380: step 1946, loss 0.623634, acc 0.765625
2017-03-02T17:34:52.325075: step 1947, loss 0.390399, acc 0.84375
2017-03-02T17:34:52.400620: step 1948, loss 0.556383, acc 0.828125
2017-03-02T17:34:52.477071: step 1949, loss 0.602534, acc 0.734375
2017-03-02T17:34:52.556607: step 1950, loss 0.377548, acc 0.828125
2017-03-02T17:34:52.625668: step 1951, loss 0.436851, acc 0.84375
2017-03-02T17:34:52.711490: step 1952, loss 0.352022, acc 0.859375
2017-03-02T17:34:52.782122: step 1953, loss 0.376448, acc 0.828125
2017-03-02T17:34:52.853748: step 1954, loss 0.50361, acc 0.796875
2017-03-02T17:34:52.920216: step 1955, loss 0.367028, acc 0.859375
2017-03-02T17:34:52.998311: step 1956, loss 0.460971, acc 0.859375
2017-03-02T17:34:53.070972: step 1957, loss 0.442595, acc 0.8125
2017-03-02T17:34:53.143789: step 1958, loss 0.50653, acc 0.8125
2017-03-02T17:34:53.215810: step 1959, loss 0.576554, acc 0.796875
2017-03-02T17:34:53.283708: step 1960, loss 0.111148, acc 1
2017-03-02T17:34:53.357781: step 1961, loss 0.353467, acc 0.90625
2017-03-02T17:34:53.447752: step 1962, loss 0.270828, acc 0.875
2017-03-02T17:34:53.529348: step 1963, loss 0.347417, acc 0.875
2017-03-02T17:34:53.606612: step 1964, loss 0.422645, acc 0.765625
2017-03-02T17:34:53.673314: step 1965, loss 0.416361, acc 0.828125
2017-03-02T17:34:53.743257: step 1966, loss 0.376063, acc 0.875
2017-03-02T17:34:53.812569: step 1967, loss 0.47795, acc 0.84375
2017-03-02T17:34:53.885333: step 1968, loss 0.314634, acc 0.921875
2017-03-02T17:34:53.954280: step 1969, loss 0.278899, acc 0.890625
2017-03-02T17:34:54.026927: step 1970, loss 0.36853, acc 0.890625
2017-03-02T17:34:54.114604: step 1971, loss 0.387654, acc 0.828125
2017-03-02T17:34:54.186432: step 1972, loss 0.417679, acc 0.890625
2017-03-02T17:34:54.271021: step 1973, loss 0.468889, acc 0.8125
2017-03-02T17:34:54.346764: step 1974, loss 0.536507, acc 0.78125
2017-03-02T17:34:54.413657: step 1975, loss 0.38567, acc 0.859375
2017-03-02T17:34:54.485679: step 1976, loss 0.325281, acc 0.890625
2017-03-02T17:34:54.558584: step 1977, loss 0.372268, acc 0.90625
2017-03-02T17:34:54.636319: step 1978, loss 0.348384, acc 0.828125
2017-03-02T17:34:54.712396: step 1979, loss 0.384074, acc 0.890625
2017-03-02T17:34:54.795904: step 1980, loss 0.281574, acc 0.859375
2017-03-02T17:34:54.868038: step 1981, loss 0.37114, acc 0.84375
2017-03-02T17:34:54.941278: step 1982, loss 0.410082, acc 0.8125
2017-03-02T17:34:55.010840: step 1983, loss 0.295677, acc 0.875
2017-03-02T17:34:55.082520: step 1984, loss 0.570682, acc 0.734375
2017-03-02T17:34:55.175581: step 1985, loss 0.50006, acc 0.8125
2017-03-02T17:34:55.254526: step 1986, loss 0.560506, acc 0.84375
2017-03-02T17:34:55.324796: step 1987, loss 0.270203, acc 0.90625
2017-03-02T17:34:55.410523: step 1988, loss 0.392243, acc 0.859375
2017-03-02T17:34:55.487077: step 1989, loss 0.340694, acc 0.84375
2017-03-02T17:34:55.558665: step 1990, loss 0.38105, acc 0.90625
2017-03-02T17:34:55.638368: step 1991, loss 0.322459, acc 0.859375
2017-03-02T17:34:55.705707: step 1992, loss 0.337024, acc 0.921875
2017-03-02T17:34:55.776583: step 1993, loss 0.479432, acc 0.859375
2017-03-02T17:34:55.862447: step 1994, loss 0.439693, acc 0.796875
2017-03-02T17:34:55.942190: step 1995, loss 0.340457, acc 0.875
2017-03-02T17:34:56.013034: step 1996, loss 0.360043, acc 0.84375
2017-03-02T17:34:56.095574: step 1997, loss 0.492231, acc 0.859375
2017-03-02T17:34:56.164177: step 1998, loss 0.429785, acc 0.859375
2017-03-02T17:34:56.250271: step 1999, loss 0.252297, acc 0.90625
2017-03-02T17:34:56.322668: step 2000, loss 0.535947, acc 0.859375

Evaluation:
2017-03-02T17:34:56.348388: step 2000, loss 0.804464, acc 0.68349

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2000

2017-03-02T17:34:56.789182: step 2001, loss 0.408164, acc 0.84375
2017-03-02T17:34:56.851673: step 2002, loss 0.469993, acc 0.8125
2017-03-02T17:34:56.922087: step 2003, loss 0.382046, acc 0.84375
2017-03-02T17:34:56.993791: step 2004, loss 0.338241, acc 0.921875
2017-03-02T17:34:57.071039: step 2005, loss 0.274175, acc 0.890625
2017-03-02T17:34:57.139428: step 2006, loss 0.548805, acc 0.828125
2017-03-02T17:34:57.201669: step 2007, loss 0.417208, acc 0.875
2017-03-02T17:34:57.283297: step 2008, loss 0.384999, acc 0.875
2017-03-02T17:34:57.355258: step 2009, loss 0.485677, acc 0.8125
2017-03-02T17:34:57.429479: step 2010, loss 0.594424, acc 0.75
2017-03-02T17:34:57.505547: step 2011, loss 0.234046, acc 0.953125
2017-03-02T17:34:57.580732: step 2012, loss 0.301899, acc 0.90625
2017-03-02T17:34:57.654679: step 2013, loss 0.251426, acc 0.921875
2017-03-02T17:34:57.733433: step 2014, loss 0.386695, acc 0.859375
2017-03-02T17:34:57.799579: step 2015, loss 0.521241, acc 0.8125
2017-03-02T17:34:57.868369: step 2016, loss 0.406292, acc 0.828125
2017-03-02T17:34:57.945654: step 2017, loss 0.385994, acc 0.859375
2017-03-02T17:34:58.036362: step 2018, loss 0.40255, acc 0.828125
2017-03-02T17:34:58.106088: step 2019, loss 0.268224, acc 0.890625
2017-03-02T17:34:58.167820: step 2020, loss 0.634343, acc 0.8125
2017-03-02T17:34:58.241521: step 2021, loss 0.395223, acc 0.875
2017-03-02T17:34:58.310307: step 2022, loss 0.558494, acc 0.78125
2017-03-02T17:34:58.383752: step 2023, loss 0.444494, acc 0.859375
2017-03-02T17:34:58.451256: step 2024, loss 0.513061, acc 0.8125
2017-03-02T17:34:58.516054: step 2025, loss 0.57142, acc 0.8125
2017-03-02T17:34:58.579838: step 2026, loss 0.340446, acc 0.875
2017-03-02T17:34:58.649218: step 2027, loss 0.396447, acc 0.890625
2017-03-02T17:34:58.719374: step 2028, loss 0.461812, acc 0.796875
2017-03-02T17:34:58.791266: step 2029, loss 0.487302, acc 0.796875
2017-03-02T17:34:58.864751: step 2030, loss 0.400835, acc 0.796875
2017-03-02T17:34:58.937598: step 2031, loss 0.444125, acc 0.84375
2017-03-02T17:34:59.012651: step 2032, loss 0.416174, acc 0.8125
2017-03-02T17:34:59.089767: step 2033, loss 0.231145, acc 0.9375
2017-03-02T17:34:59.156252: step 2034, loss 0.395534, acc 0.828125
2017-03-02T17:34:59.221852: step 2035, loss 0.22237, acc 0.921875
2017-03-02T17:34:59.291211: step 2036, loss 0.368471, acc 0.84375
2017-03-02T17:34:59.375546: step 2037, loss 0.373876, acc 0.828125
2017-03-02T17:34:59.459658: step 2038, loss 0.286496, acc 0.90625
2017-03-02T17:34:59.535244: step 2039, loss 0.267472, acc 0.90625
2017-03-02T17:34:59.610589: step 2040, loss 0.371282, acc 0.90625
2017-03-02T17:34:59.682000: step 2041, loss 0.374376, acc 0.890625
2017-03-02T17:34:59.751245: step 2042, loss 0.338208, acc 0.875
2017-03-02T17:34:59.820626: step 2043, loss 0.518884, acc 0.8125
2017-03-02T17:34:59.882816: step 2044, loss 0.419766, acc 0.84375
2017-03-02T17:34:59.946230: step 2045, loss 0.322293, acc 0.875
2017-03-02T17:35:00.017429: step 2046, loss 0.337471, acc 0.859375
2017-03-02T17:35:00.086765: step 2047, loss 0.472148, acc 0.828125
2017-03-02T17:35:00.148936: step 2048, loss 0.440517, acc 0.859375
2017-03-02T17:35:00.222439: step 2049, loss 0.437138, acc 0.875
2017-03-02T17:35:00.294851: step 2050, loss 0.35713, acc 0.84375
2017-03-02T17:35:00.363889: step 2051, loss 0.355573, acc 0.828125
2017-03-02T17:35:00.430330: step 2052, loss 0.401532, acc 0.859375
2017-03-02T17:35:00.501016: step 2053, loss 0.417701, acc 0.875
2017-03-02T17:35:00.566896: step 2054, loss 0.545471, acc 0.8125
2017-03-02T17:35:00.632978: step 2055, loss 0.294212, acc 0.890625
2017-03-02T17:35:00.704793: step 2056, loss 0.490985, acc 0.859375
2017-03-02T17:35:00.774429: step 2057, loss 0.516845, acc 0.796875
2017-03-02T17:35:00.843581: step 2058, loss 0.447792, acc 0.8125
2017-03-02T17:35:00.916228: step 2059, loss 0.250972, acc 0.921875
2017-03-02T17:35:00.988469: step 2060, loss 0.276913, acc 0.875
2017-03-02T17:35:01.055530: step 2061, loss 0.454547, acc 0.859375
2017-03-02T17:35:01.133707: step 2062, loss 0.345558, acc 0.890625
2017-03-02T17:35:01.205785: step 2063, loss 0.49602, acc 0.765625
2017-03-02T17:35:01.277069: step 2064, loss 0.368989, acc 0.875
2017-03-02T17:35:01.346160: step 2065, loss 0.501953, acc 0.84375
2017-03-02T17:35:01.423330: step 2066, loss 0.451625, acc 0.84375
2017-03-02T17:35:01.493465: step 2067, loss 0.423725, acc 0.828125
2017-03-02T17:35:01.565529: step 2068, loss 0.349096, acc 0.921875
2017-03-02T17:35:01.638860: step 2069, loss 0.254195, acc 0.953125
2017-03-02T17:35:01.722961: step 2070, loss 0.455873, acc 0.828125
2017-03-02T17:35:01.794483: step 2071, loss 0.360723, acc 0.859375
2017-03-02T17:35:01.869057: step 2072, loss 0.253285, acc 0.9375
2017-03-02T17:35:01.950134: step 2073, loss 0.372736, acc 0.84375
2017-03-02T17:35:02.016648: step 2074, loss 0.303344, acc 0.875
2017-03-02T17:35:02.101300: step 2075, loss 0.376682, acc 0.8125
2017-03-02T17:35:02.175133: step 2076, loss 0.268857, acc 0.921875
2017-03-02T17:35:02.246912: step 2077, loss 0.348811, acc 0.875
2017-03-02T17:35:02.319836: step 2078, loss 0.32051, acc 0.875
2017-03-02T17:35:02.393734: step 2079, loss 0.307513, acc 0.890625
2017-03-02T17:35:02.467730: step 2080, loss 0.438935, acc 0.796875
2017-03-02T17:35:02.537647: step 2081, loss 0.236809, acc 0.921875
2017-03-02T17:35:02.607286: step 2082, loss 0.536883, acc 0.75
2017-03-02T17:35:02.669922: step 2083, loss 0.257206, acc 0.921875
2017-03-02T17:35:02.739952: step 2084, loss 0.466906, acc 0.828125
2017-03-02T17:35:02.807027: step 2085, loss 0.38896, acc 0.875
2017-03-02T17:35:02.874829: step 2086, loss 0.443395, acc 0.765625
2017-03-02T17:35:02.946034: step 2087, loss 0.613324, acc 0.765625
2017-03-02T17:35:03.015825: step 2088, loss 0.295094, acc 0.859375
2017-03-02T17:35:03.087257: step 2089, loss 0.365682, acc 0.875
2017-03-02T17:35:03.148231: step 2090, loss 0.311204, acc 0.875
2017-03-02T17:35:03.221528: step 2091, loss 0.293409, acc 0.9375
2017-03-02T17:35:03.284831: step 2092, loss 0.332277, acc 0.90625
2017-03-02T17:35:03.354216: step 2093, loss 0.387338, acc 0.84375
2017-03-02T17:35:03.416452: step 2094, loss 0.349274, acc 0.8125
2017-03-02T17:35:03.490304: step 2095, loss 0.416632, acc 0.859375
2017-03-02T17:35:03.560593: step 2096, loss 0.482476, acc 0.828125
2017-03-02T17:35:03.631533: step 2097, loss 0.402765, acc 0.84375
2017-03-02T17:35:03.703818: step 2098, loss 0.488174, acc 0.84375
2017-03-02T17:35:03.776163: step 2099, loss 0.335556, acc 0.828125
2017-03-02T17:35:03.852883: step 2100, loss 0.503867, acc 0.75

Evaluation:
2017-03-02T17:35:03.885518: step 2100, loss 0.822773, acc 0.687094

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2100

2017-03-02T17:35:04.337578: step 2101, loss 0.398774, acc 0.859375
2017-03-02T17:35:04.415926: step 2102, loss 0.422597, acc 0.828125
2017-03-02T17:35:04.489333: step 2103, loss 0.410263, acc 0.859375
2017-03-02T17:35:04.563994: step 2104, loss 0.322137, acc 0.890625
2017-03-02T17:35:04.635499: step 2105, loss 0.428272, acc 0.859375
2017-03-02T17:35:04.710130: step 2106, loss 0.549819, acc 0.8125
2017-03-02T17:35:04.780762: step 2107, loss 0.395462, acc 0.875
2017-03-02T17:35:04.863215: step 2108, loss 0.450079, acc 0.859375
2017-03-02T17:35:04.957224: step 2109, loss 0.32655, acc 0.859375
2017-03-02T17:35:05.035666: step 2110, loss 0.278645, acc 0.890625
2017-03-02T17:35:05.110157: step 2111, loss 0.594282, acc 0.8125
2017-03-02T17:35:05.177834: step 2112, loss 0.437513, acc 0.828125
2017-03-02T17:35:05.250702: step 2113, loss 0.549319, acc 0.796875
2017-03-02T17:35:05.326847: step 2114, loss 0.308678, acc 0.890625
2017-03-02T17:35:05.393512: step 2115, loss 0.465686, acc 0.8125
2017-03-02T17:35:05.463324: step 2116, loss 0.352429, acc 0.828125
2017-03-02T17:35:05.529461: step 2117, loss 0.57099, acc 0.734375
2017-03-02T17:35:05.608661: step 2118, loss 0.270032, acc 0.875
2017-03-02T17:35:05.689470: step 2119, loss 0.607621, acc 0.75
2017-03-02T17:35:05.778253: step 2120, loss 0.37548, acc 0.875
2017-03-02T17:35:05.849809: step 2121, loss 0.404525, acc 0.828125
2017-03-02T17:35:05.922252: step 2122, loss 0.363591, acc 0.84375
2017-03-02T17:35:05.993480: step 2123, loss 0.56622, acc 0.78125
2017-03-02T17:35:06.063616: step 2124, loss 0.312412, acc 0.875
2017-03-02T17:35:06.138838: step 2125, loss 0.335288, acc 0.859375
2017-03-02T17:35:06.205397: step 2126, loss 0.272804, acc 0.890625
2017-03-02T17:35:06.286767: step 2127, loss 0.452398, acc 0.8125
2017-03-02T17:35:06.355683: step 2128, loss 0.289372, acc 0.859375
2017-03-02T17:35:06.426527: step 2129, loss 0.473153, acc 0.859375
2017-03-02T17:35:06.498310: step 2130, loss 0.49277, acc 0.796875
2017-03-02T17:35:06.573802: step 2131, loss 0.333146, acc 0.875
2017-03-02T17:35:06.648474: step 2132, loss 0.340065, acc 0.828125
2017-03-02T17:35:06.733802: step 2133, loss 0.456936, acc 0.84375
2017-03-02T17:35:06.804322: step 2134, loss 0.539771, acc 0.78125
2017-03-02T17:35:06.872333: step 2135, loss 0.324193, acc 0.875
2017-03-02T17:35:06.942829: step 2136, loss 0.464338, acc 0.84375
2017-03-02T17:35:07.015341: step 2137, loss 0.302626, acc 0.890625
2017-03-02T17:35:07.094013: step 2138, loss 0.408367, acc 0.84375
2017-03-02T17:35:07.166949: step 2139, loss 0.37443, acc 0.875
2017-03-02T17:35:07.229628: step 2140, loss 0.192368, acc 0.953125
2017-03-02T17:35:07.307445: step 2141, loss 0.41001, acc 0.859375
2017-03-02T17:35:07.386040: step 2142, loss 0.388972, acc 0.859375
2017-03-02T17:35:07.459464: step 2143, loss 0.602345, acc 0.765625
2017-03-02T17:35:07.527482: step 2144, loss 0.338834, acc 0.859375
2017-03-02T17:35:07.606502: step 2145, loss 0.324719, acc 0.890625
2017-03-02T17:35:07.677292: step 2146, loss 0.399574, acc 0.890625
2017-03-02T17:35:07.750629: step 2147, loss 0.426409, acc 0.78125
2017-03-02T17:35:07.821660: step 2148, loss 0.448404, acc 0.828125
2017-03-02T17:35:07.891077: step 2149, loss 0.482056, acc 0.796875
2017-03-02T17:35:07.977422: step 2150, loss 0.548956, acc 0.796875
2017-03-02T17:35:08.048522: step 2151, loss 0.379833, acc 0.84375
2017-03-02T17:35:08.121727: step 2152, loss 0.493712, acc 0.84375
2017-03-02T17:35:08.191742: step 2153, loss 0.2574, acc 0.90625
2017-03-02T17:35:08.260682: step 2154, loss 0.408526, acc 0.859375
2017-03-02T17:35:08.345146: step 2155, loss 0.515591, acc 0.828125
2017-03-02T17:35:08.429630: step 2156, loss 0.324257, acc 0.75
2017-03-02T17:35:08.504833: step 2157, loss 0.408136, acc 0.859375
2017-03-02T17:35:08.576465: step 2158, loss 0.519637, acc 0.8125
2017-03-02T17:35:08.648758: step 2159, loss 0.258166, acc 0.890625
2017-03-02T17:35:08.731869: step 2160, loss 0.516997, acc 0.84375
2017-03-02T17:35:08.800510: step 2161, loss 0.285706, acc 0.890625
2017-03-02T17:35:08.876338: step 2162, loss 0.19376, acc 0.921875
2017-03-02T17:35:08.946506: step 2163, loss 0.216668, acc 0.921875
2017-03-02T17:35:09.017498: step 2164, loss 0.466332, acc 0.828125
2017-03-02T17:35:09.091608: step 2165, loss 0.384523, acc 0.890625
2017-03-02T17:35:09.163884: step 2166, loss 0.371848, acc 0.90625
2017-03-02T17:35:09.235540: step 2167, loss 0.324798, acc 0.859375
2017-03-02T17:35:09.307754: step 2168, loss 0.332482, acc 0.875
2017-03-02T17:35:09.368561: step 2169, loss 0.326106, acc 0.890625
2017-03-02T17:35:09.459054: step 2170, loss 0.311943, acc 0.859375
2017-03-02T17:35:09.529150: step 2171, loss 0.344953, acc 0.921875
2017-03-02T17:35:09.604932: step 2172, loss 0.265506, acc 0.90625
2017-03-02T17:35:09.675536: step 2173, loss 0.632945, acc 0.75
2017-03-02T17:35:09.754139: step 2174, loss 0.438886, acc 0.84375
2017-03-02T17:35:09.823979: step 2175, loss 0.564175, acc 0.796875
2017-03-02T17:35:09.896786: step 2176, loss 0.301065, acc 0.90625
2017-03-02T17:35:09.973668: step 2177, loss 0.20624, acc 0.953125
2017-03-02T17:35:10.052857: step 2178, loss 0.493258, acc 0.828125
2017-03-02T17:35:10.126752: step 2179, loss 0.398793, acc 0.859375
2017-03-02T17:35:10.201391: step 2180, loss 0.341462, acc 0.890625
2017-03-02T17:35:10.269265: step 2181, loss 0.301919, acc 0.875
2017-03-02T17:35:10.339234: step 2182, loss 0.258252, acc 0.90625
2017-03-02T17:35:10.424085: step 2183, loss 0.320223, acc 0.890625
2017-03-02T17:35:10.497812: step 2184, loss 0.413406, acc 0.875
2017-03-02T17:35:10.576629: step 2185, loss 0.51443, acc 0.828125
2017-03-02T17:35:10.654828: step 2186, loss 0.327317, acc 0.890625
2017-03-02T17:35:10.725646: step 2187, loss 0.356074, acc 0.875
2017-03-02T17:35:10.815786: step 2188, loss 0.308012, acc 0.953125
2017-03-02T17:35:10.888108: step 2189, loss 0.275677, acc 0.875
2017-03-02T17:35:10.957826: step 2190, loss 0.39147, acc 0.84375
2017-03-02T17:35:11.033502: step 2191, loss 0.396146, acc 0.859375
2017-03-02T17:35:11.105691: step 2192, loss 0.295734, acc 0.890625
2017-03-02T17:35:11.180586: step 2193, loss 0.271617, acc 0.875
2017-03-02T17:35:11.252916: step 2194, loss 0.363917, acc 0.828125
2017-03-02T17:35:11.327236: step 2195, loss 0.330956, acc 0.90625
2017-03-02T17:35:11.397439: step 2196, loss 0.335092, acc 0.796875
2017-03-02T17:35:11.473191: step 2197, loss 0.19955, acc 0.9375
2017-03-02T17:35:11.544314: step 2198, loss 0.577708, acc 0.796875
2017-03-02T17:35:11.611275: step 2199, loss 0.313615, acc 0.90625
2017-03-02T17:35:11.679040: step 2200, loss 0.249546, acc 0.90625

Evaluation:
2017-03-02T17:35:11.714579: step 2200, loss 0.894773, acc 0.682048

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2200

2017-03-02T17:35:12.207876: step 2201, loss 0.253463, acc 0.859375
2017-03-02T17:35:12.276579: step 2202, loss 0.370898, acc 0.84375
2017-03-02T17:35:12.345954: step 2203, loss 0.224914, acc 0.921875
2017-03-02T17:35:12.410697: step 2204, loss 0.388909, acc 0.828125
2017-03-02T17:35:12.485450: step 2205, loss 0.261899, acc 0.890625
2017-03-02T17:35:12.557528: step 2206, loss 0.403534, acc 0.875
2017-03-02T17:35:12.627048: step 2207, loss 0.400472, acc 0.875
2017-03-02T17:35:12.699552: step 2208, loss 0.502847, acc 0.78125
2017-03-02T17:35:12.772072: step 2209, loss 0.364546, acc 0.84375
2017-03-02T17:35:12.850135: step 2210, loss 0.286091, acc 0.890625
2017-03-02T17:35:12.925635: step 2211, loss 0.342384, acc 0.875
2017-03-02T17:35:13.001460: step 2212, loss 0.309104, acc 0.921875
2017-03-02T17:35:13.072847: step 2213, loss 0.271008, acc 0.921875
2017-03-02T17:35:13.168058: step 2214, loss 0.331087, acc 0.875
2017-03-02T17:35:13.241999: step 2215, loss 0.188841, acc 0.953125
2017-03-02T17:35:13.318908: step 2216, loss 0.338394, acc 0.90625
2017-03-02T17:35:13.387666: step 2217, loss 0.367636, acc 0.828125
2017-03-02T17:35:13.458067: step 2218, loss 0.360651, acc 0.90625
2017-03-02T17:35:13.531480: step 2219, loss 0.220879, acc 0.9375
2017-03-02T17:35:13.601825: step 2220, loss 0.23136, acc 0.890625
2017-03-02T17:35:13.671807: step 2221, loss 0.534648, acc 0.78125
2017-03-02T17:35:13.742272: step 2222, loss 0.328968, acc 0.890625
2017-03-02T17:35:13.817759: step 2223, loss 0.281823, acc 0.875
2017-03-02T17:35:13.905905: step 2224, loss 0.391042, acc 0.859375
2017-03-02T17:35:13.976618: step 2225, loss 0.337288, acc 0.875
2017-03-02T17:35:14.041800: step 2226, loss 0.289586, acc 0.90625
2017-03-02T17:35:14.119501: step 2227, loss 0.411314, acc 0.8125
2017-03-02T17:35:14.185812: step 2228, loss 0.507443, acc 0.75
2017-03-02T17:35:14.261500: step 2229, loss 0.434255, acc 0.90625
2017-03-02T17:35:14.338267: step 2230, loss 0.348293, acc 0.890625
2017-03-02T17:35:14.410165: step 2231, loss 0.489892, acc 0.828125
2017-03-02T17:35:14.475212: step 2232, loss 0.494264, acc 0.78125
2017-03-02T17:35:14.550428: step 2233, loss 0.330423, acc 0.875
2017-03-02T17:35:14.622763: step 2234, loss 0.252939, acc 0.875
2017-03-02T17:35:14.699226: step 2235, loss 0.396914, acc 0.875
2017-03-02T17:35:14.776287: step 2236, loss 0.278124, acc 0.859375
2017-03-02T17:35:14.851980: step 2237, loss 0.408185, acc 0.875
2017-03-02T17:35:14.925904: step 2238, loss 0.505777, acc 0.859375
2017-03-02T17:35:14.997670: step 2239, loss 0.377619, acc 0.859375
2017-03-02T17:35:15.069301: step 2240, loss 0.276608, acc 0.890625
2017-03-02T17:35:15.136254: step 2241, loss 0.297867, acc 0.921875
2017-03-02T17:35:15.222855: step 2242, loss 0.289455, acc 0.90625
2017-03-02T17:35:15.293622: step 2243, loss 0.215074, acc 0.90625
2017-03-02T17:35:15.369501: step 2244, loss 0.38283, acc 0.859375
2017-03-02T17:35:15.440112: step 2245, loss 0.286957, acc 0.921875
2017-03-02T17:35:15.511373: step 2246, loss 0.30436, acc 0.90625
2017-03-02T17:35:15.582428: step 2247, loss 0.301075, acc 0.90625
2017-03-02T17:35:15.666009: step 2248, loss 0.30137, acc 0.828125
2017-03-02T17:35:15.735980: step 2249, loss 0.447576, acc 0.84375
2017-03-02T17:35:15.798621: step 2250, loss 0.306276, acc 0.84375
2017-03-02T17:35:15.865398: step 2251, loss 0.571092, acc 0.75
2017-03-02T17:35:15.939017: step 2252, loss 0.24802, acc 0.9375
2017-03-02T17:35:16.010534: step 2253, loss 0.533982, acc 0.859375
2017-03-02T17:35:16.082405: step 2254, loss 0.270211, acc 0.921875
2017-03-02T17:35:16.156250: step 2255, loss 0.52859, acc 0.8125
2017-03-02T17:35:16.229089: step 2256, loss 0.447199, acc 0.875
2017-03-02T17:35:16.309335: step 2257, loss 0.347548, acc 0.859375
2017-03-02T17:35:16.389340: step 2258, loss 0.366639, acc 0.90625
2017-03-02T17:35:16.455727: step 2259, loss 0.485454, acc 0.859375
2017-03-02T17:35:16.525735: step 2260, loss 0.346409, acc 0.90625
2017-03-02T17:35:16.596510: step 2261, loss 0.478752, acc 0.78125
2017-03-02T17:35:16.669818: step 2262, loss 0.336756, acc 0.875
2017-03-02T17:35:16.741263: step 2263, loss 0.290262, acc 0.921875
2017-03-02T17:35:16.817190: step 2264, loss 0.361071, acc 0.921875
2017-03-02T17:35:16.889152: step 2265, loss 0.397078, acc 0.890625
2017-03-02T17:35:16.959633: step 2266, loss 0.391502, acc 0.859375
2017-03-02T17:35:17.032409: step 2267, loss 0.393647, acc 0.859375
2017-03-02T17:35:17.110002: step 2268, loss 0.395243, acc 0.875
2017-03-02T17:35:17.190592: step 2269, loss 0.556042, acc 0.828125
2017-03-02T17:35:17.266160: step 2270, loss 0.255863, acc 0.90625
2017-03-02T17:35:17.338116: step 2271, loss 0.30237, acc 0.859375
2017-03-02T17:35:17.414793: step 2272, loss 0.487952, acc 0.75
2017-03-02T17:35:17.499440: step 2273, loss 0.547422, acc 0.8125
2017-03-02T17:35:17.573858: step 2274, loss 0.503281, acc 0.78125
2017-03-02T17:35:17.654786: step 2275, loss 0.376443, acc 0.84375
2017-03-02T17:35:17.734224: step 2276, loss 0.259751, acc 0.90625
2017-03-02T17:35:17.807841: step 2277, loss 0.486503, acc 0.8125
2017-03-02T17:35:17.878169: step 2278, loss 0.258224, acc 0.921875
2017-03-02T17:35:17.957855: step 2279, loss 0.352674, acc 0.875
2017-03-02T17:35:18.030088: step 2280, loss 0.41101, acc 0.90625
2017-03-02T17:35:18.102990: step 2281, loss 0.42139, acc 0.8125
2017-03-02T17:35:18.183927: step 2282, loss 0.259516, acc 0.875
2017-03-02T17:35:18.261137: step 2283, loss 0.416224, acc 0.84375
2017-03-02T17:35:18.363829: step 2284, loss 0.447394, acc 0.78125
2017-03-02T17:35:18.437719: step 2285, loss 0.279675, acc 0.90625
2017-03-02T17:35:18.505621: step 2286, loss 0.316495, acc 0.875
2017-03-02T17:35:18.575848: step 2287, loss 0.397648, acc 0.921875
2017-03-02T17:35:18.649290: step 2288, loss 0.253728, acc 0.90625
2017-03-02T17:35:18.724781: step 2289, loss 0.31905, acc 0.90625
2017-03-02T17:35:18.802168: step 2290, loss 0.428683, acc 0.859375
2017-03-02T17:35:18.877186: step 2291, loss 0.397988, acc 0.90625
2017-03-02T17:35:18.951285: step 2292, loss 0.296025, acc 0.921875
2017-03-02T17:35:19.023272: step 2293, loss 0.366787, acc 0.84375
2017-03-02T17:35:19.100024: step 2294, loss 0.452072, acc 0.8125
2017-03-02T17:35:19.170777: step 2295, loss 0.311509, acc 0.859375
2017-03-02T17:35:19.241079: step 2296, loss 0.31054, acc 0.875
2017-03-02T17:35:19.307942: step 2297, loss 0.380847, acc 0.890625
2017-03-02T17:35:19.398869: step 2298, loss 0.334829, acc 0.875
2017-03-02T17:35:19.465500: step 2299, loss 0.540876, acc 0.84375
2017-03-02T17:35:19.550943: step 2300, loss 0.37348, acc 0.90625

Evaluation:
2017-03-02T17:35:19.584560: step 2300, loss 0.876676, acc 0.686373

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2300

2017-03-02T17:35:20.034487: step 2301, loss 0.4375, acc 0.828125
2017-03-02T17:35:20.115600: step 2302, loss 0.537881, acc 0.765625
2017-03-02T17:35:20.188378: step 2303, loss 0.450085, acc 0.78125
2017-03-02T17:35:20.261034: step 2304, loss 0.597025, acc 0.796875
2017-03-02T17:35:20.332361: step 2305, loss 0.176633, acc 0.96875
2017-03-02T17:35:20.407989: step 2306, loss 0.374518, acc 0.84375
2017-03-02T17:35:20.479015: step 2307, loss 0.434175, acc 0.890625
2017-03-02T17:35:20.546001: step 2308, loss 0.257372, acc 0.890625
2017-03-02T17:35:20.616003: step 2309, loss 0.454122, acc 0.8125
2017-03-02T17:35:20.683582: step 2310, loss 0.423137, acc 0.8125
2017-03-02T17:35:20.764874: step 2311, loss 0.388368, acc 0.84375
2017-03-02T17:35:20.849555: step 2312, loss 0.267763, acc 0.90625
2017-03-02T17:35:20.920505: step 2313, loss 0.416336, acc 0.8125
2017-03-02T17:35:20.995277: step 2314, loss 0.375754, acc 0.875
2017-03-02T17:35:21.074859: step 2315, loss 0.61971, acc 0.765625
2017-03-02T17:35:21.149040: step 2316, loss 0.31592, acc 0.90625
2017-03-02T17:35:21.222308: step 2317, loss 0.507233, acc 0.796875
2017-03-02T17:35:21.296059: step 2318, loss 0.471977, acc 0.84375
2017-03-02T17:35:21.371829: step 2319, loss 0.335677, acc 0.875
2017-03-02T17:35:21.445029: step 2320, loss 0.387174, acc 0.859375
2017-03-02T17:35:21.529715: step 2321, loss 0.327146, acc 0.875
2017-03-02T17:35:21.617517: step 2322, loss 0.483846, acc 0.859375
2017-03-02T17:35:21.690250: step 2323, loss 0.467822, acc 0.84375
2017-03-02T17:35:21.762122: step 2324, loss 0.489184, acc 0.765625
2017-03-02T17:35:21.835179: step 2325, loss 0.420673, acc 0.859375
2017-03-02T17:35:21.907468: step 2326, loss 0.472192, acc 0.796875
2017-03-02T17:35:21.979379: step 2327, loss 0.349689, acc 0.84375
2017-03-02T17:35:22.048240: step 2328, loss 0.388532, acc 0.890625
2017-03-02T17:35:22.119159: step 2329, loss 0.3749, acc 0.84375
2017-03-02T17:35:22.194102: step 2330, loss 0.34617, acc 0.90625
2017-03-02T17:35:22.265804: step 2331, loss 0.64954, acc 0.734375
2017-03-02T17:35:22.341587: step 2332, loss 0.297527, acc 0.859375
2017-03-02T17:35:22.404985: step 2333, loss 0.467933, acc 0.8125
2017-03-02T17:35:22.475278: step 2334, loss 0.373964, acc 0.875
2017-03-02T17:35:22.554542: step 2335, loss 0.428373, acc 0.890625
2017-03-02T17:35:22.624557: step 2336, loss 0.451485, acc 0.859375
2017-03-02T17:35:22.696045: step 2337, loss 0.332778, acc 0.828125
2017-03-02T17:35:22.761327: step 2338, loss 0.437808, acc 0.84375
2017-03-02T17:35:22.840375: step 2339, loss 0.32289, acc 0.875
2017-03-02T17:35:22.913831: step 2340, loss 0.30698, acc 0.90625
2017-03-02T17:35:22.987973: step 2341, loss 0.541879, acc 0.78125
2017-03-02T17:35:23.063584: step 2342, loss 0.406836, acc 0.875
2017-03-02T17:35:23.138381: step 2343, loss 0.352407, acc 0.8125
2017-03-02T17:35:23.211598: step 2344, loss 0.260128, acc 0.921875
2017-03-02T17:35:23.286607: step 2345, loss 0.477605, acc 0.875
2017-03-02T17:35:23.355023: step 2346, loss 0.319532, acc 0.875
2017-03-02T17:35:23.435736: step 2347, loss 0.604131, acc 0.8125
2017-03-02T17:35:23.504416: step 2348, loss 0.392551, acc 0.84375
2017-03-02T17:35:23.573915: step 2349, loss 0.33977, acc 0.890625
2017-03-02T17:35:23.644945: step 2350, loss 0.450974, acc 0.84375
2017-03-02T17:35:23.717975: step 2351, loss 0.503993, acc 0.84375
2017-03-02T17:35:23.788035: step 2352, loss 0.134861, acc 1
2017-03-02T17:35:23.856981: step 2353, loss 0.522166, acc 0.875
2017-03-02T17:35:23.939812: step 2354, loss 0.215748, acc 0.90625
2017-03-02T17:35:24.010834: step 2355, loss 0.327488, acc 0.90625
2017-03-02T17:35:24.081421: step 2356, loss 0.476221, acc 0.828125
2017-03-02T17:35:24.140152: step 2357, loss 0.361591, acc 0.859375
2017-03-02T17:35:24.214765: step 2358, loss 0.423629, acc 0.84375
2017-03-02T17:35:24.293697: step 2359, loss 0.39082, acc 0.859375
2017-03-02T17:35:24.367270: step 2360, loss 0.308669, acc 0.875
2017-03-02T17:35:24.440377: step 2361, loss 0.308316, acc 0.875
2017-03-02T17:35:24.510620: step 2362, loss 0.266542, acc 0.90625
2017-03-02T17:35:24.580593: step 2363, loss 0.337318, acc 0.859375
2017-03-02T17:35:24.652215: step 2364, loss 0.391982, acc 0.828125
2017-03-02T17:35:24.728540: step 2365, loss 0.455511, acc 0.796875
2017-03-02T17:35:24.813880: step 2366, loss 0.221062, acc 0.953125
2017-03-02T17:35:24.882645: step 2367, loss 0.308731, acc 0.90625
2017-03-02T17:35:24.955359: step 2368, loss 0.368364, acc 0.90625
2017-03-02T17:35:25.035864: step 2369, loss 0.262054, acc 0.890625
2017-03-02T17:35:25.119017: step 2370, loss 0.244832, acc 0.953125
2017-03-02T17:35:25.191666: step 2371, loss 0.316218, acc 0.875
2017-03-02T17:35:25.268853: step 2372, loss 0.191161, acc 0.921875
2017-03-02T17:35:25.344530: step 2373, loss 0.226012, acc 0.890625
2017-03-02T17:35:25.424796: step 2374, loss 0.27825, acc 0.890625
2017-03-02T17:35:25.495563: step 2375, loss 0.258951, acc 0.875
2017-03-02T17:35:25.561580: step 2376, loss 0.274483, acc 0.890625
2017-03-02T17:35:25.643549: step 2377, loss 0.195425, acc 0.9375
2017-03-02T17:35:25.747677: step 2378, loss 0.366393, acc 0.875
2017-03-02T17:35:25.818311: step 2379, loss 0.279539, acc 0.890625
2017-03-02T17:35:25.891110: step 2380, loss 0.135038, acc 0.984375
2017-03-02T17:35:25.988618: step 2381, loss 0.278099, acc 0.921875
2017-03-02T17:35:26.049751: step 2382, loss 0.436354, acc 0.8125
2017-03-02T17:35:26.123067: step 2383, loss 0.369203, acc 0.890625
2017-03-02T17:35:26.190489: step 2384, loss 0.282448, acc 0.90625
2017-03-02T17:35:26.260134: step 2385, loss 0.465008, acc 0.8125
2017-03-02T17:35:26.338309: step 2386, loss 0.357663, acc 0.875
2017-03-02T17:35:26.408387: step 2387, loss 0.418438, acc 0.875
2017-03-02T17:35:26.482504: step 2388, loss 0.323273, acc 0.90625
2017-03-02T17:35:26.560768: step 2389, loss 0.4271, acc 0.84375
2017-03-02T17:35:26.627731: step 2390, loss 0.165687, acc 0.9375
2017-03-02T17:35:26.696235: step 2391, loss 0.369807, acc 0.875
2017-03-02T17:35:26.770677: step 2392, loss 0.318364, acc 0.90625
2017-03-02T17:35:26.839452: step 2393, loss 0.231736, acc 0.90625
2017-03-02T17:35:26.908055: step 2394, loss 0.219896, acc 0.90625
2017-03-02T17:35:26.972119: step 2395, loss 0.242859, acc 0.875
2017-03-02T17:35:27.045252: step 2396, loss 0.358329, acc 0.875
2017-03-02T17:35:27.121912: step 2397, loss 0.318045, acc 0.875
2017-03-02T17:35:27.195085: step 2398, loss 0.359245, acc 0.859375
2017-03-02T17:35:27.272861: step 2399, loss 0.244772, acc 0.90625
2017-03-02T17:35:27.350708: step 2400, loss 0.33149, acc 0.875

Evaluation:
2017-03-02T17:35:27.381697: step 2400, loss 0.865504, acc 0.669791

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2400

2017-03-02T17:35:27.836905: step 2401, loss 0.327448, acc 0.84375
2017-03-02T17:35:27.907353: step 2402, loss 0.420276, acc 0.875
2017-03-02T17:35:27.987659: step 2403, loss 0.417797, acc 0.875
2017-03-02T17:35:28.060448: step 2404, loss 0.27297, acc 0.9375
2017-03-02T17:35:28.135111: step 2405, loss 0.285988, acc 0.90625
2017-03-02T17:35:28.216178: step 2406, loss 0.291261, acc 0.859375
2017-03-02T17:35:28.290465: step 2407, loss 0.213664, acc 0.921875
2017-03-02T17:35:28.363028: step 2408, loss 0.289283, acc 0.890625
2017-03-02T17:35:28.441367: step 2409, loss 0.344076, acc 0.921875
2017-03-02T17:35:28.522194: step 2410, loss 0.401557, acc 0.859375
2017-03-02T17:35:28.604276: step 2411, loss 0.439295, acc 0.84375
2017-03-02T17:35:28.674462: step 2412, loss 0.465891, acc 0.84375
2017-03-02T17:35:28.745130: step 2413, loss 0.301461, acc 0.859375
2017-03-02T17:35:28.816913: step 2414, loss 0.447265, acc 0.90625
2017-03-02T17:35:28.898347: step 2415, loss 0.215504, acc 0.9375
2017-03-02T17:35:28.973037: step 2416, loss 0.332667, acc 0.875
2017-03-02T17:35:29.038216: step 2417, loss 0.521853, acc 0.8125
2017-03-02T17:35:29.107055: step 2418, loss 0.219454, acc 0.9375
2017-03-02T17:35:29.184594: step 2419, loss 0.399197, acc 0.859375
2017-03-02T17:35:29.260693: step 2420, loss 0.337991, acc 0.8125
2017-03-02T17:35:29.330409: step 2421, loss 0.324094, acc 0.84375
2017-03-02T17:35:29.403436: step 2422, loss 0.353142, acc 0.859375
2017-03-02T17:35:29.474206: step 2423, loss 0.220007, acc 0.9375
2017-03-02T17:35:29.548438: step 2424, loss 0.261473, acc 0.953125
2017-03-02T17:35:29.620031: step 2425, loss 0.295118, acc 0.90625
2017-03-02T17:35:29.695071: step 2426, loss 0.542912, acc 0.796875
2017-03-02T17:35:29.769755: step 2427, loss 0.471117, acc 0.84375
2017-03-02T17:35:29.857208: step 2428, loss 0.290838, acc 0.859375
2017-03-02T17:35:29.929668: step 2429, loss 0.380688, acc 0.8125
2017-03-02T17:35:30.034062: step 2430, loss 0.239732, acc 0.96875
2017-03-02T17:35:30.107472: step 2431, loss 0.365528, acc 0.875
2017-03-02T17:35:30.176012: step 2432, loss 0.334062, acc 0.859375
2017-03-02T17:35:30.245968: step 2433, loss 0.256324, acc 0.921875
2017-03-02T17:35:30.316054: step 2434, loss 0.375915, acc 0.796875
2017-03-02T17:35:30.395507: step 2435, loss 0.355256, acc 0.890625
2017-03-02T17:35:30.468152: step 2436, loss 0.352775, acc 0.875
2017-03-02T17:35:30.543142: step 2437, loss 0.281997, acc 0.875
2017-03-02T17:35:30.618459: step 2438, loss 0.384163, acc 0.84375
2017-03-02T17:35:30.695283: step 2439, loss 0.309748, acc 0.921875
2017-03-02T17:35:30.764379: step 2440, loss 0.214547, acc 0.921875
2017-03-02T17:35:30.835453: step 2441, loss 0.567479, acc 0.78125
2017-03-02T17:35:30.908674: step 2442, loss 0.287644, acc 0.875
2017-03-02T17:35:30.988356: step 2443, loss 0.421319, acc 0.859375
2017-03-02T17:35:31.065449: step 2444, loss 0.355077, acc 0.875
2017-03-02T17:35:31.136675: step 2445, loss 0.43198, acc 0.859375
2017-03-02T17:35:31.208601: step 2446, loss 0.265274, acc 0.890625
2017-03-02T17:35:31.291843: step 2447, loss 0.320547, acc 0.90625
2017-03-02T17:35:31.362576: step 2448, loss 0.478201, acc 0.8125
2017-03-02T17:35:31.437498: step 2449, loss 0.244829, acc 0.9375
2017-03-02T17:35:31.506084: step 2450, loss 0.455968, acc 0.84375
2017-03-02T17:35:31.582353: step 2451, loss 0.264863, acc 0.90625
2017-03-02T17:35:31.658827: step 2452, loss 0.34257, acc 0.890625
2017-03-02T17:35:31.732279: step 2453, loss 0.290595, acc 0.9375
2017-03-02T17:35:31.799771: step 2454, loss 0.240134, acc 0.921875
2017-03-02T17:35:31.873585: step 2455, loss 0.369279, acc 0.828125
2017-03-02T17:35:31.947604: step 2456, loss 0.275634, acc 0.921875
2017-03-02T17:35:32.019244: step 2457, loss 0.283822, acc 0.890625
2017-03-02T17:35:32.091437: step 2458, loss 0.225039, acc 0.921875
2017-03-02T17:35:32.164794: step 2459, loss 0.466745, acc 0.859375
2017-03-02T17:35:32.236444: step 2460, loss 0.315062, acc 0.90625
2017-03-02T17:35:32.315992: step 2461, loss 0.318825, acc 0.859375
2017-03-02T17:35:32.381763: step 2462, loss 0.416839, acc 0.875
2017-03-02T17:35:32.456170: step 2463, loss 0.228192, acc 0.90625
2017-03-02T17:35:32.535067: step 2464, loss 0.244709, acc 0.90625
2017-03-02T17:35:32.609208: step 2465, loss 0.298436, acc 0.921875
2017-03-02T17:35:32.690132: step 2466, loss 0.304768, acc 0.890625
2017-03-02T17:35:32.758419: step 2467, loss 0.34255, acc 0.828125
2017-03-02T17:35:32.830135: step 2468, loss 0.302393, acc 0.875
2017-03-02T17:35:32.900562: step 2469, loss 0.4387, acc 0.859375
2017-03-02T17:35:32.969065: step 2470, loss 0.611859, acc 0.734375
2017-03-02T17:35:33.035089: step 2471, loss 0.303623, acc 0.875
2017-03-02T17:35:33.101282: step 2472, loss 0.286522, acc 0.921875
2017-03-02T17:35:33.178929: step 2473, loss 0.261411, acc 0.84375
2017-03-02T17:35:33.246753: step 2474, loss 0.448943, acc 0.78125
2017-03-02T17:35:33.322822: step 2475, loss 0.389421, acc 0.890625
2017-03-02T17:35:33.392929: step 2476, loss 0.264599, acc 0.921875
2017-03-02T17:35:33.462606: step 2477, loss 0.411514, acc 0.859375
2017-03-02T17:35:33.532605: step 2478, loss 0.574562, acc 0.796875
2017-03-02T17:35:33.606819: step 2479, loss 0.394231, acc 0.828125
2017-03-02T17:35:33.685576: step 2480, loss 0.323215, acc 0.890625
2017-03-02T17:35:33.754362: step 2481, loss 0.411396, acc 0.890625
2017-03-02T17:35:33.819665: step 2482, loss 0.361224, acc 0.84375
2017-03-02T17:35:33.888949: step 2483, loss 0.237752, acc 0.875
2017-03-02T17:35:33.963998: step 2484, loss 0.421436, acc 0.859375
2017-03-02T17:35:34.036874: step 2485, loss 0.25571, acc 0.90625
2017-03-02T17:35:34.111168: step 2486, loss 0.313347, acc 0.890625
2017-03-02T17:35:34.184391: step 2487, loss 0.293632, acc 0.890625
2017-03-02T17:35:34.257888: step 2488, loss 0.443787, acc 0.875
2017-03-02T17:35:34.327987: step 2489, loss 0.31173, acc 0.875
2017-03-02T17:35:34.404823: step 2490, loss 0.331546, acc 0.890625
2017-03-02T17:35:34.474019: step 2491, loss 0.197877, acc 0.890625
2017-03-02T17:35:34.544188: step 2492, loss 0.308989, acc 0.890625
2017-03-02T17:35:34.618155: step 2493, loss 0.477227, acc 0.8125
2017-03-02T17:35:34.688830: step 2494, loss 0.383904, acc 0.828125
2017-03-02T17:35:34.766787: step 2495, loss 0.202917, acc 0.9375
2017-03-02T17:35:34.840597: step 2496, loss 0.378916, acc 0.875
2017-03-02T17:35:34.917490: step 2497, loss 0.469808, acc 0.8125
2017-03-02T17:35:34.987753: step 2498, loss 0.430789, acc 0.84375
2017-03-02T17:35:35.059450: step 2499, loss 0.294959, acc 0.890625
2017-03-02T17:35:35.124845: step 2500, loss 0.40379, acc 0.828125

Evaluation:
2017-03-02T17:35:35.156230: step 2500, loss 0.882498, acc 0.687094

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2500

2017-03-02T17:35:35.591077: step 2501, loss 0.442001, acc 0.84375
2017-03-02T17:35:35.673044: step 2502, loss 0.486475, acc 0.828125
2017-03-02T17:35:35.749279: step 2503, loss 0.385338, acc 0.875
2017-03-02T17:35:35.814760: step 2504, loss 0.442016, acc 0.828125
2017-03-02T17:35:35.885199: step 2505, loss 0.415814, acc 0.84375
2017-03-02T17:35:35.961079: step 2506, loss 0.524123, acc 0.8125
2017-03-02T17:35:36.032136: step 2507, loss 0.320699, acc 0.875
2017-03-02T17:35:36.110971: step 2508, loss 0.243426, acc 0.890625
2017-03-02T17:35:36.191054: step 2509, loss 0.30103, acc 0.890625
2017-03-02T17:35:36.271419: step 2510, loss 0.477803, acc 0.859375
2017-03-02T17:35:36.347264: step 2511, loss 0.372143, acc 0.890625
2017-03-02T17:35:36.425578: step 2512, loss 0.347334, acc 0.890625
2017-03-02T17:35:36.493359: step 2513, loss 0.478033, acc 0.890625
2017-03-02T17:35:36.570149: step 2514, loss 0.217462, acc 0.875
2017-03-02T17:35:36.640451: step 2515, loss 0.272745, acc 0.953125
2017-03-02T17:35:36.713289: step 2516, loss 0.36254, acc 0.890625
2017-03-02T17:35:36.784645: step 2517, loss 0.333672, acc 0.875
2017-03-02T17:35:36.864700: step 2518, loss 0.439743, acc 0.796875
2017-03-02T17:35:36.937504: step 2519, loss 0.38999, acc 0.84375
2017-03-02T17:35:37.013802: step 2520, loss 0.359902, acc 0.859375
2017-03-02T17:35:37.090081: step 2521, loss 0.372623, acc 0.859375
2017-03-02T17:35:37.159261: step 2522, loss 0.504747, acc 0.84375
2017-03-02T17:35:37.230046: step 2523, loss 0.411945, acc 0.84375
2017-03-02T17:35:37.300502: step 2524, loss 0.293367, acc 0.890625
2017-03-02T17:35:37.373691: step 2525, loss 0.339064, acc 0.890625
2017-03-02T17:35:37.446274: step 2526, loss 0.473238, acc 0.796875
2017-03-02T17:35:37.525252: step 2527, loss 0.314335, acc 0.90625
2017-03-02T17:35:37.599945: step 2528, loss 0.350945, acc 0.859375
2017-03-02T17:35:37.675267: step 2529, loss 0.162673, acc 0.953125
2017-03-02T17:35:37.746310: step 2530, loss 0.234733, acc 0.90625
2017-03-02T17:35:37.809356: step 2531, loss 0.298201, acc 0.9375
2017-03-02T17:35:37.879127: step 2532, loss 0.244937, acc 0.9375
2017-03-02T17:35:37.945495: step 2533, loss 0.421738, acc 0.875
2017-03-02T17:35:38.015935: step 2534, loss 0.428852, acc 0.828125
2017-03-02T17:35:38.097247: step 2535, loss 0.354669, acc 0.859375
2017-03-02T17:35:38.167644: step 2536, loss 0.291706, acc 0.90625
2017-03-02T17:35:38.241347: step 2537, loss 0.273962, acc 0.875
2017-03-02T17:35:38.319109: step 2538, loss 0.32647, acc 0.875
2017-03-02T17:35:38.391079: step 2539, loss 0.410932, acc 0.859375
2017-03-02T17:35:38.462009: step 2540, loss 0.404135, acc 0.828125
2017-03-02T17:35:38.539545: step 2541, loss 0.341511, acc 0.875
2017-03-02T17:35:38.609278: step 2542, loss 0.434535, acc 0.859375
2017-03-02T17:35:38.683255: step 2543, loss 0.372631, acc 0.84375
2017-03-02T17:35:38.754228: step 2544, loss 0.35101, acc 0.9375
2017-03-02T17:35:38.827169: step 2545, loss 0.38572, acc 0.875
2017-03-02T17:35:38.899939: step 2546, loss 0.348422, acc 0.890625
2017-03-02T17:35:38.985858: step 2547, loss 0.20028, acc 0.9375
2017-03-02T17:35:39.055841: step 2548, loss 0.0471824, acc 1
2017-03-02T17:35:39.129132: step 2549, loss 0.454523, acc 0.828125
2017-03-02T17:35:39.203728: step 2550, loss 0.208838, acc 0.90625
2017-03-02T17:35:39.282112: step 2551, loss 0.198226, acc 0.9375
2017-03-02T17:35:39.349641: step 2552, loss 0.324733, acc 0.890625
2017-03-02T17:35:39.421531: step 2553, loss 0.416989, acc 0.875
2017-03-02T17:35:39.501548: step 2554, loss 0.180868, acc 0.9375
2017-03-02T17:35:39.579485: step 2555, loss 0.218421, acc 0.921875
2017-03-02T17:35:39.652739: step 2556, loss 0.248556, acc 0.890625
2017-03-02T17:35:39.725515: step 2557, loss 0.255749, acc 0.90625
2017-03-02T17:35:39.800150: step 2558, loss 0.426573, acc 0.84375
2017-03-02T17:35:39.872072: step 2559, loss 0.287236, acc 0.875
2017-03-02T17:35:39.944701: step 2560, loss 0.26082, acc 0.90625
2017-03-02T17:35:40.017415: step 2561, loss 0.405122, acc 0.859375
2017-03-02T17:35:40.086358: step 2562, loss 0.207073, acc 0.921875
2017-03-02T17:35:40.158664: step 2563, loss 0.264178, acc 0.875
2017-03-02T17:35:40.232186: step 2564, loss 0.358002, acc 0.90625
2017-03-02T17:35:40.303773: step 2565, loss 0.340058, acc 0.84375
2017-03-02T17:35:40.375995: step 2566, loss 0.236952, acc 0.921875
2017-03-02T17:35:40.447592: step 2567, loss 0.237384, acc 0.921875
2017-03-02T17:35:40.513718: step 2568, loss 0.284915, acc 0.875
2017-03-02T17:35:40.587184: step 2569, loss 0.34864, acc 0.84375
2017-03-02T17:35:40.658474: step 2570, loss 0.234103, acc 0.90625
2017-03-02T17:35:40.725302: step 2571, loss 0.439496, acc 0.84375
2017-03-02T17:35:40.796191: step 2572, loss 0.321812, acc 0.890625
2017-03-02T17:35:40.868204: step 2573, loss 0.282852, acc 0.90625
2017-03-02T17:35:40.938339: step 2574, loss 0.446851, acc 0.84375
2017-03-02T17:35:41.010722: step 2575, loss 0.513758, acc 0.828125
2017-03-02T17:35:41.083546: step 2576, loss 0.395485, acc 0.828125
2017-03-02T17:35:41.154958: step 2577, loss 0.317232, acc 0.90625
2017-03-02T17:35:41.229333: step 2578, loss 0.220029, acc 0.921875
2017-03-02T17:35:41.301546: step 2579, loss 0.350662, acc 0.859375
2017-03-02T17:35:41.373157: step 2580, loss 0.379586, acc 0.859375
2017-03-02T17:35:41.442697: step 2581, loss 0.438285, acc 0.890625
2017-03-02T17:35:41.513911: step 2582, loss 0.285188, acc 0.890625
2017-03-02T17:35:41.591556: step 2583, loss 0.50723, acc 0.828125
2017-03-02T17:35:41.667256: step 2584, loss 0.325092, acc 0.921875
2017-03-02T17:35:41.752005: step 2585, loss 0.349078, acc 0.828125
2017-03-02T17:35:41.831411: step 2586, loss 0.350596, acc 0.875
2017-03-02T17:35:41.907474: step 2587, loss 0.251602, acc 0.9375
2017-03-02T17:35:41.985194: step 2588, loss 0.270643, acc 0.90625
2017-03-02T17:35:42.053763: step 2589, loss 0.370642, acc 0.875
2017-03-02T17:35:42.125083: step 2590, loss 0.304702, acc 0.90625
2017-03-02T17:35:42.196540: step 2591, loss 0.222286, acc 0.90625
2017-03-02T17:35:42.277114: step 2592, loss 0.250158, acc 0.890625
2017-03-02T17:35:42.343254: step 2593, loss 0.293999, acc 0.921875
2017-03-02T17:35:42.414255: step 2594, loss 0.248653, acc 0.921875
2017-03-02T17:35:42.485814: step 2595, loss 0.413759, acc 0.875
2017-03-02T17:35:42.552526: step 2596, loss 0.191423, acc 0.9375
2017-03-02T17:35:42.621122: step 2597, loss 0.293621, acc 0.875
2017-03-02T17:35:42.691119: step 2598, loss 0.283753, acc 0.890625
2017-03-02T17:35:42.760719: step 2599, loss 0.317831, acc 0.875
2017-03-02T17:35:42.830331: step 2600, loss 0.339334, acc 0.890625

Evaluation:
2017-03-02T17:35:42.864687: step 2600, loss 0.956351, acc 0.68349

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2600

2017-03-02T17:35:43.295586: step 2601, loss 0.509868, acc 0.78125
2017-03-02T17:35:43.375927: step 2602, loss 0.516992, acc 0.78125
2017-03-02T17:35:43.458960: step 2603, loss 0.402734, acc 0.859375
2017-03-02T17:35:43.532886: step 2604, loss 0.199414, acc 0.953125
2017-03-02T17:35:43.606302: step 2605, loss 0.328534, acc 0.84375
2017-03-02T17:35:43.682254: step 2606, loss 0.353637, acc 0.875
2017-03-02T17:35:43.757577: step 2607, loss 0.41823, acc 0.890625
2017-03-02T17:35:43.830541: step 2608, loss 0.256287, acc 0.90625
2017-03-02T17:35:43.901150: step 2609, loss 0.346022, acc 0.890625
2017-03-02T17:35:43.972244: step 2610, loss 0.189029, acc 0.9375
2017-03-02T17:35:44.045879: step 2611, loss 0.199044, acc 0.984375
2017-03-02T17:35:44.114098: step 2612, loss 0.45947, acc 0.875
2017-03-02T17:35:44.187715: step 2613, loss 0.397706, acc 0.90625
2017-03-02T17:35:44.260889: step 2614, loss 0.420144, acc 0.84375
2017-03-02T17:35:44.335153: step 2615, loss 0.284706, acc 0.90625
2017-03-02T17:35:44.405329: step 2616, loss 0.294342, acc 0.890625
2017-03-02T17:35:44.480151: step 2617, loss 0.211863, acc 0.953125
2017-03-02T17:35:44.561391: step 2618, loss 0.295502, acc 0.90625
2017-03-02T17:35:44.637824: step 2619, loss 0.492376, acc 0.859375
2017-03-02T17:35:44.709426: step 2620, loss 0.301927, acc 0.921875
2017-03-02T17:35:44.789957: step 2621, loss 0.34121, acc 0.890625
2017-03-02T17:35:44.856963: step 2622, loss 0.218573, acc 0.921875
2017-03-02T17:35:44.937102: step 2623, loss 0.218175, acc 0.9375
2017-03-02T17:35:45.015106: step 2624, loss 0.199364, acc 0.921875
2017-03-02T17:35:45.088785: step 2625, loss 0.26321, acc 0.9375
2017-03-02T17:35:45.160385: step 2626, loss 0.188304, acc 0.921875
2017-03-02T17:35:45.239034: step 2627, loss 0.223802, acc 0.9375
2017-03-02T17:35:45.309496: step 2628, loss 0.208091, acc 0.96875
2017-03-02T17:35:45.380468: step 2629, loss 0.471399, acc 0.796875
2017-03-02T17:35:45.450702: step 2630, loss 0.293808, acc 0.90625
2017-03-02T17:35:45.515358: step 2631, loss 0.463639, acc 0.84375
2017-03-02T17:35:45.587228: step 2632, loss 0.279465, acc 0.875
2017-03-02T17:35:45.651468: step 2633, loss 0.402616, acc 0.890625
2017-03-02T17:35:45.730298: step 2634, loss 0.299871, acc 0.859375
2017-03-02T17:35:45.807350: step 2635, loss 0.345298, acc 0.875
2017-03-02T17:35:45.899609: step 2636, loss 0.344559, acc 0.84375
2017-03-02T17:35:45.971173: step 2637, loss 0.345882, acc 0.875
2017-03-02T17:35:46.044869: step 2638, loss 0.217433, acc 0.90625
2017-03-02T17:35:46.120698: step 2639, loss 0.262156, acc 0.90625
2017-03-02T17:35:46.190394: step 2640, loss 0.427908, acc 0.859375
2017-03-02T17:35:46.260495: step 2641, loss 0.264687, acc 0.875
2017-03-02T17:35:46.333375: step 2642, loss 0.214657, acc 0.921875
2017-03-02T17:35:46.414375: step 2643, loss 0.503738, acc 0.875
2017-03-02T17:35:46.490331: step 2644, loss 0.234099, acc 0.921875
2017-03-02T17:35:46.570974: step 2645, loss 0.427689, acc 0.796875
2017-03-02T17:35:46.644954: step 2646, loss 0.207157, acc 0.9375
2017-03-02T17:35:46.718829: step 2647, loss 0.33204, acc 0.90625
2017-03-02T17:35:46.793265: step 2648, loss 0.244766, acc 0.921875
2017-03-02T17:35:46.864736: step 2649, loss 0.405372, acc 0.84375
2017-03-02T17:35:46.935343: step 2650, loss 0.403134, acc 0.84375
2017-03-02T17:35:47.002849: step 2651, loss 0.406026, acc 0.828125
2017-03-02T17:35:47.078285: step 2652, loss 0.375938, acc 0.890625
2017-03-02T17:35:47.151597: step 2653, loss 0.251618, acc 0.921875
2017-03-02T17:35:47.222312: step 2654, loss 0.523463, acc 0.8125
2017-03-02T17:35:47.294505: step 2655, loss 0.177167, acc 0.921875
2017-03-02T17:35:47.370207: step 2656, loss 0.279491, acc 0.890625
2017-03-02T17:35:47.440414: step 2657, loss 0.232945, acc 0.921875
2017-03-02T17:35:47.510363: step 2658, loss 0.363237, acc 0.875
2017-03-02T17:35:47.576911: step 2659, loss 0.471655, acc 0.796875
2017-03-02T17:35:47.644585: step 2660, loss 0.281862, acc 0.875
2017-03-02T17:35:47.721779: step 2661, loss 0.230493, acc 0.9375
2017-03-02T17:35:47.791114: step 2662, loss 0.564135, acc 0.796875
2017-03-02T17:35:47.864227: step 2663, loss 0.249469, acc 0.9375
2017-03-02T17:35:47.936078: step 2664, loss 0.384134, acc 0.890625
2017-03-02T17:35:48.015853: step 2665, loss 0.229036, acc 0.9375
2017-03-02T17:35:48.091102: step 2666, loss 0.364166, acc 0.875
2017-03-02T17:35:48.162543: step 2667, loss 0.234311, acc 0.90625
2017-03-02T17:35:48.236493: step 2668, loss 0.357843, acc 0.84375
2017-03-02T17:35:48.301452: step 2669, loss 0.310992, acc 0.859375
2017-03-02T17:35:48.373271: step 2670, loss 0.314407, acc 0.875
2017-03-02T17:35:48.445881: step 2671, loss 0.235812, acc 0.875
2017-03-02T17:35:48.519324: step 2672, loss 0.313302, acc 0.84375
2017-03-02T17:35:48.593860: step 2673, loss 0.461951, acc 0.796875
2017-03-02T17:35:48.668605: step 2674, loss 0.309659, acc 0.859375
2017-03-02T17:35:48.740236: step 2675, loss 0.453458, acc 0.828125
2017-03-02T17:35:48.812824: step 2676, loss 0.186404, acc 0.921875
2017-03-02T17:35:48.885824: step 2677, loss 0.328044, acc 0.890625
2017-03-02T17:35:48.954582: step 2678, loss 0.256576, acc 0.890625
2017-03-02T17:35:49.023437: step 2679, loss 0.447842, acc 0.890625
2017-03-02T17:35:49.090565: step 2680, loss 0.297594, acc 0.859375
2017-03-02T17:35:49.159138: step 2681, loss 0.257501, acc 0.953125
2017-03-02T17:35:49.230912: step 2682, loss 0.285648, acc 0.90625
2017-03-02T17:35:49.310039: step 2683, loss 0.231965, acc 0.90625
2017-03-02T17:35:49.384815: step 2684, loss 0.234517, acc 0.890625
2017-03-02T17:35:49.462788: step 2685, loss 0.31622, acc 0.921875
2017-03-02T17:35:49.532409: step 2686, loss 0.196829, acc 0.9375
2017-03-02T17:35:49.608726: step 2687, loss 0.389195, acc 0.890625
2017-03-02T17:35:49.684416: step 2688, loss 0.26084, acc 0.875
2017-03-02T17:35:49.755811: step 2689, loss 0.524774, acc 0.828125
2017-03-02T17:35:49.828954: step 2690, loss 0.281888, acc 0.875
2017-03-02T17:35:49.901279: step 2691, loss 0.331559, acc 0.890625
2017-03-02T17:35:49.974067: step 2692, loss 0.290689, acc 0.890625
2017-03-02T17:35:50.043345: step 2693, loss 0.42784, acc 0.8125
2017-03-02T17:35:50.120288: step 2694, loss 0.306267, acc 0.859375
2017-03-02T17:35:50.189429: step 2695, loss 0.53161, acc 0.859375
2017-03-02T17:35:50.271454: step 2696, loss 0.393526, acc 0.875
2017-03-02T17:35:50.341635: step 2697, loss 0.163758, acc 0.953125
2017-03-02T17:35:50.406955: step 2698, loss 0.203652, acc 0.921875
2017-03-02T17:35:50.479495: step 2699, loss 0.310383, acc 0.90625
2017-03-02T17:35:50.550006: step 2700, loss 0.222479, acc 0.9375

Evaluation:
2017-03-02T17:35:50.585629: step 2700, loss 0.96497, acc 0.692141

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2700

2017-03-02T17:35:51.027969: step 2701, loss 0.213579, acc 0.890625
2017-03-02T17:35:51.097720: step 2702, loss 0.178706, acc 0.921875
2017-03-02T17:35:51.172464: step 2703, loss 0.350135, acc 0.84375
2017-03-02T17:35:51.245235: step 2704, loss 0.369879, acc 0.90625
2017-03-02T17:35:51.322706: step 2705, loss 0.328113, acc 0.890625
2017-03-02T17:35:51.394149: step 2706, loss 0.215881, acc 0.953125
2017-03-02T17:35:51.472361: step 2707, loss 0.434841, acc 0.84375
2017-03-02T17:35:51.548848: step 2708, loss 0.181117, acc 0.90625
2017-03-02T17:35:51.629168: step 2709, loss 0.2798, acc 0.890625
2017-03-02T17:35:51.703759: step 2710, loss 0.348081, acc 0.875
2017-03-02T17:35:51.781922: step 2711, loss 0.496747, acc 0.828125
2017-03-02T17:35:51.847038: step 2712, loss 0.23117, acc 0.90625
2017-03-02T17:35:51.918028: step 2713, loss 0.316644, acc 0.875
2017-03-02T17:35:51.991639: step 2714, loss 0.36606, acc 0.875
2017-03-02T17:35:52.064854: step 2715, loss 0.255894, acc 0.921875
2017-03-02T17:35:52.151944: step 2716, loss 0.463237, acc 0.859375
2017-03-02T17:35:52.226352: step 2717, loss 0.311916, acc 0.875
2017-03-02T17:35:52.297927: step 2718, loss 0.464009, acc 0.84375
2017-03-02T17:35:52.381753: step 2719, loss 0.567987, acc 0.875
2017-03-02T17:35:52.447819: step 2720, loss 0.272356, acc 0.90625
2017-03-02T17:35:52.516364: step 2721, loss 0.284318, acc 0.921875
2017-03-02T17:35:52.589098: step 2722, loss 0.323644, acc 0.875
2017-03-02T17:35:52.663274: step 2723, loss 0.117868, acc 0.953125
2017-03-02T17:35:52.734636: step 2724, loss 0.259806, acc 0.90625
2017-03-02T17:35:52.814933: step 2725, loss 0.147813, acc 0.953125
2017-03-02T17:35:52.887167: step 2726, loss 0.437387, acc 0.859375
2017-03-02T17:35:52.961716: step 2727, loss 0.316311, acc 0.828125
2017-03-02T17:35:53.034670: step 2728, loss 0.285721, acc 0.9375
2017-03-02T17:35:53.105378: step 2729, loss 0.411937, acc 0.8125
2017-03-02T17:35:53.189615: step 2730, loss 0.300537, acc 0.875
2017-03-02T17:35:53.261097: step 2731, loss 0.399336, acc 0.890625
2017-03-02T17:35:53.333404: step 2732, loss 0.256923, acc 0.921875
2017-03-02T17:35:53.409462: step 2733, loss 0.359614, acc 0.890625
2017-03-02T17:35:53.481762: step 2734, loss 0.150342, acc 0.9375
2017-03-02T17:35:53.558724: step 2735, loss 0.361147, acc 0.875
2017-03-02T17:35:53.632327: step 2736, loss 0.370621, acc 0.875
2017-03-02T17:35:53.704165: step 2737, loss 0.454603, acc 0.84375
2017-03-02T17:35:53.775129: step 2738, loss 0.437752, acc 0.859375
2017-03-02T17:35:53.846832: step 2739, loss 0.343316, acc 0.921875
2017-03-02T17:35:53.917920: step 2740, loss 0.472179, acc 0.828125
2017-03-02T17:35:53.993421: step 2741, loss 0.225937, acc 0.921875
2017-03-02T17:35:54.067363: step 2742, loss 0.476313, acc 0.828125
2017-03-02T17:35:54.138318: step 2743, loss 0.403412, acc 0.859375
2017-03-02T17:35:54.206206: step 2744, loss 0.35752, acc 0.75
2017-03-02T17:35:54.279428: step 2745, loss 0.317954, acc 0.875
2017-03-02T17:35:54.357255: step 2746, loss 0.407317, acc 0.875
2017-03-02T17:35:54.426085: step 2747, loss 0.359439, acc 0.84375
2017-03-02T17:35:54.502719: step 2748, loss 0.374234, acc 0.796875
2017-03-02T17:35:54.571267: step 2749, loss 0.287813, acc 0.875
2017-03-02T17:35:54.645838: step 2750, loss 0.352171, acc 0.859375
2017-03-02T17:35:54.718353: step 2751, loss 0.188881, acc 0.921875
2017-03-02T17:35:54.790959: step 2752, loss 0.478963, acc 0.796875
2017-03-02T17:35:54.865534: step 2753, loss 0.235163, acc 0.921875
2017-03-02T17:35:54.939700: step 2754, loss 0.214117, acc 0.953125
2017-03-02T17:35:55.011580: step 2755, loss 0.201233, acc 0.921875
2017-03-02T17:35:55.086165: step 2756, loss 0.266833, acc 0.921875
2017-03-02T17:35:55.154601: step 2757, loss 0.251767, acc 0.859375
2017-03-02T17:35:55.222158: step 2758, loss 0.255021, acc 0.921875
2017-03-02T17:35:55.281666: step 2759, loss 0.270305, acc 0.9375
2017-03-02T17:35:55.351638: step 2760, loss 0.346286, acc 0.890625
2017-03-02T17:35:55.432730: step 2761, loss 0.399345, acc 0.921875
2017-03-02T17:35:55.506656: step 2762, loss 0.316314, acc 0.890625
2017-03-02T17:35:55.575552: step 2763, loss 0.423848, acc 0.8125
2017-03-02T17:35:55.646259: step 2764, loss 0.321312, acc 0.921875
2017-03-02T17:35:55.719197: step 2765, loss 0.253552, acc 0.875
2017-03-02T17:35:55.798239: step 2766, loss 0.343972, acc 0.875
2017-03-02T17:35:55.870538: step 2767, loss 0.199755, acc 0.921875
2017-03-02T17:35:55.938809: step 2768, loss 0.303434, acc 0.90625
2017-03-02T17:35:56.011927: step 2769, loss 0.212962, acc 0.90625
2017-03-02T17:35:56.083335: step 2770, loss 0.367409, acc 0.890625
2017-03-02T17:35:56.161993: step 2771, loss 0.335138, acc 0.90625
2017-03-02T17:35:56.234532: step 2772, loss 0.263975, acc 0.921875
2017-03-02T17:35:56.308374: step 2773, loss 0.282196, acc 0.90625
2017-03-02T17:35:56.382548: step 2774, loss 0.296781, acc 0.875
2017-03-02T17:35:56.457513: step 2775, loss 0.308155, acc 0.875
2017-03-02T17:35:56.521559: step 2776, loss 0.264243, acc 0.890625
2017-03-02T17:35:56.585942: step 2777, loss 0.317439, acc 0.875
2017-03-02T17:35:56.657747: step 2778, loss 0.334011, acc 0.875
2017-03-02T17:35:56.735719: step 2779, loss 0.323018, acc 0.90625
2017-03-02T17:35:56.810071: step 2780, loss 0.352105, acc 0.875
2017-03-02T17:35:56.888971: step 2781, loss 0.230038, acc 0.9375
2017-03-02T17:35:56.973086: step 2782, loss 0.201391, acc 0.9375
2017-03-02T17:35:57.042608: step 2783, loss 0.331985, acc 0.90625
2017-03-02T17:35:57.113970: step 2784, loss 0.167067, acc 0.953125
2017-03-02T17:35:57.175744: step 2785, loss 0.316898, acc 0.921875
2017-03-02T17:35:57.241701: step 2786, loss 0.425763, acc 0.8125
2017-03-02T17:35:57.308498: step 2787, loss 0.405487, acc 0.875
2017-03-02T17:35:57.378301: step 2788, loss 0.389763, acc 0.84375
2017-03-02T17:35:57.450219: step 2789, loss 0.363198, acc 0.890625
2017-03-02T17:35:57.525504: step 2790, loss 0.278425, acc 0.890625
2017-03-02T17:35:57.604908: step 2791, loss 0.147652, acc 0.9375
2017-03-02T17:35:57.685646: step 2792, loss 0.257825, acc 0.90625
2017-03-02T17:35:57.761975: step 2793, loss 0.323013, acc 0.90625
2017-03-02T17:35:57.841091: step 2794, loss 0.216252, acc 0.953125
2017-03-02T17:35:57.910614: step 2795, loss 0.244325, acc 0.921875
2017-03-02T17:35:57.977472: step 2796, loss 0.309222, acc 0.890625
2017-03-02T17:35:58.048786: step 2797, loss 0.356942, acc 0.90625
2017-03-02T17:35:58.128525: step 2798, loss 0.202513, acc 0.90625
2017-03-02T17:35:58.207621: step 2799, loss 0.403015, acc 0.890625
2017-03-02T17:35:58.287449: step 2800, loss 0.309019, acc 0.828125

Evaluation:
2017-03-02T17:35:58.321344: step 2800, loss 0.913366, acc 0.677001

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2800

2017-03-02T17:35:58.774570: step 2801, loss 0.227514, acc 0.90625
2017-03-02T17:35:58.843424: step 2802, loss 0.285384, acc 0.875
2017-03-02T17:35:58.914243: step 2803, loss 0.290964, acc 0.875
2017-03-02T17:35:58.983858: step 2804, loss 0.177709, acc 0.9375
2017-03-02T17:35:59.054806: step 2805, loss 0.258477, acc 0.90625
2017-03-02T17:35:59.131885: step 2806, loss 0.397004, acc 0.859375
2017-03-02T17:35:59.204071: step 2807, loss 0.363806, acc 0.84375
2017-03-02T17:35:59.272529: step 2808, loss 0.148934, acc 0.96875
2017-03-02T17:35:59.339447: step 2809, loss 0.385239, acc 0.84375
2017-03-02T17:35:59.415705: step 2810, loss 0.283549, acc 0.921875
2017-03-02T17:35:59.493244: step 2811, loss 0.216301, acc 0.921875
2017-03-02T17:35:59.569508: step 2812, loss 0.282079, acc 0.90625
2017-03-02T17:35:59.648849: step 2813, loss 0.423366, acc 0.90625
2017-03-02T17:35:59.724416: step 2814, loss 0.258224, acc 0.890625
2017-03-02T17:35:59.802224: step 2815, loss 0.225056, acc 0.90625
2017-03-02T17:35:59.875332: step 2816, loss 0.27705, acc 0.90625
2017-03-02T17:35:59.941047: step 2817, loss 0.173263, acc 0.96875
2017-03-02T17:36:00.015091: step 2818, loss 0.314173, acc 0.875
2017-03-02T17:36:00.079093: step 2819, loss 0.46864, acc 0.859375
2017-03-02T17:36:00.154484: step 2820, loss 0.249988, acc 0.921875
2017-03-02T17:36:00.235293: step 2821, loss 0.285035, acc 0.90625
2017-03-02T17:36:00.320996: step 2822, loss 0.161781, acc 0.96875
2017-03-02T17:36:00.398727: step 2823, loss 0.323746, acc 0.859375
2017-03-02T17:36:00.472059: step 2824, loss 0.275734, acc 0.90625
2017-03-02T17:36:00.550467: step 2825, loss 0.362546, acc 0.859375
2017-03-02T17:36:00.627852: step 2826, loss 0.608413, acc 0.828125
2017-03-02T17:36:00.697206: step 2827, loss 0.255248, acc 0.890625
2017-03-02T17:36:00.766405: step 2828, loss 0.382528, acc 0.890625
2017-03-02T17:36:00.835533: step 2829, loss 0.102198, acc 1
2017-03-02T17:36:00.903628: step 2830, loss 0.297675, acc 0.921875
2017-03-02T17:36:00.979873: step 2831, loss 0.404227, acc 0.890625
2017-03-02T17:36:01.050290: step 2832, loss 0.367052, acc 0.859375
2017-03-02T17:36:01.114657: step 2833, loss 0.12397, acc 0.953125
2017-03-02T17:36:01.185391: step 2834, loss 0.335083, acc 0.90625
2017-03-02T17:36:01.255200: step 2835, loss 0.317871, acc 0.90625
2017-03-02T17:36:01.315809: step 2836, loss 0.338164, acc 0.875
2017-03-02T17:36:01.381988: step 2837, loss 0.367918, acc 0.890625
2017-03-02T17:36:01.454403: step 2838, loss 0.340133, acc 0.859375
2017-03-02T17:36:01.524720: step 2839, loss 0.266672, acc 0.9375
2017-03-02T17:36:01.601227: step 2840, loss 0.213215, acc 0.921875
2017-03-02T17:36:01.682979: step 2841, loss 0.406113, acc 0.890625
2017-03-02T17:36:01.769425: step 2842, loss 0.218283, acc 0.953125
2017-03-02T17:36:01.840644: step 2843, loss 0.313289, acc 0.890625
2017-03-02T17:36:01.918228: step 2844, loss 0.208192, acc 0.953125
2017-03-02T17:36:01.990077: step 2845, loss 0.323167, acc 0.875
2017-03-02T17:36:02.063200: step 2846, loss 0.220505, acc 0.9375
2017-03-02T17:36:02.131076: step 2847, loss 0.255997, acc 0.90625
2017-03-02T17:36:02.199591: step 2848, loss 0.255232, acc 0.90625
2017-03-02T17:36:02.282038: step 2849, loss 0.332432, acc 0.859375
2017-03-02T17:36:02.362862: step 2850, loss 0.326122, acc 0.875
2017-03-02T17:36:02.436539: step 2851, loss 0.363387, acc 0.890625
2017-03-02T17:36:02.504615: step 2852, loss 0.514952, acc 0.8125
2017-03-02T17:36:02.583424: step 2853, loss 0.258835, acc 0.875
2017-03-02T17:36:02.660292: step 2854, loss 0.183867, acc 0.953125
2017-03-02T17:36:02.725346: step 2855, loss 0.246344, acc 0.890625
2017-03-02T17:36:02.789392: step 2856, loss 0.3072, acc 0.890625
2017-03-02T17:36:02.865056: step 2857, loss 0.214666, acc 0.9375
2017-03-02T17:36:02.935066: step 2858, loss 0.243175, acc 0.90625
2017-03-02T17:36:02.997455: step 2859, loss 0.273712, acc 0.9375
2017-03-02T17:36:03.061095: step 2860, loss 0.476076, acc 0.875
2017-03-02T17:36:03.128014: step 2861, loss 0.266651, acc 0.90625
2017-03-02T17:36:03.199262: step 2862, loss 0.255063, acc 0.90625
2017-03-02T17:36:03.273003: step 2863, loss 0.180724, acc 0.953125
2017-03-02T17:36:03.344565: step 2864, loss 0.214185, acc 0.890625
2017-03-02T17:36:03.414877: step 2865, loss 0.268117, acc 0.890625
2017-03-02T17:36:03.482527: step 2866, loss 0.198159, acc 0.90625
2017-03-02T17:36:03.574596: step 2867, loss 0.22713, acc 0.890625
2017-03-02T17:36:03.652994: step 2868, loss 0.259771, acc 0.90625
2017-03-02T17:36:03.743464: step 2869, loss 0.283182, acc 0.875
2017-03-02T17:36:03.824416: step 2870, loss 0.265534, acc 0.890625
2017-03-02T17:36:03.903846: step 2871, loss 0.274106, acc 0.9375
2017-03-02T17:36:03.977429: step 2872, loss 0.187349, acc 0.953125
2017-03-02T17:36:04.049555: step 2873, loss 0.220756, acc 0.890625
2017-03-02T17:36:04.118247: step 2874, loss 0.198109, acc 0.9375
2017-03-02T17:36:04.183413: step 2875, loss 0.145815, acc 0.9375
2017-03-02T17:36:04.259422: step 2876, loss 0.394807, acc 0.875
2017-03-02T17:36:04.334018: step 2877, loss 0.352896, acc 0.84375
2017-03-02T17:36:04.403498: step 2878, loss 0.264068, acc 0.90625
2017-03-02T17:36:04.477707: step 2879, loss 0.398828, acc 0.84375
2017-03-02T17:36:04.551571: step 2880, loss 0.310922, acc 0.90625
2017-03-02T17:36:04.623167: step 2881, loss 0.405674, acc 0.828125
2017-03-02T17:36:04.695539: step 2882, loss 0.403613, acc 0.828125
2017-03-02T17:36:04.772625: step 2883, loss 0.232944, acc 0.9375
2017-03-02T17:36:04.839252: step 2884, loss 0.28171, acc 0.9375
2017-03-02T17:36:04.907887: step 2885, loss 0.23537, acc 0.96875
2017-03-02T17:36:04.973217: step 2886, loss 0.367515, acc 0.859375
2017-03-02T17:36:05.049006: step 2887, loss 0.425707, acc 0.84375
2017-03-02T17:36:05.122644: step 2888, loss 0.300029, acc 0.921875
2017-03-02T17:36:05.192437: step 2889, loss 0.246718, acc 0.9375
2017-03-02T17:36:05.265550: step 2890, loss 0.209271, acc 0.9375
2017-03-02T17:36:05.340240: step 2891, loss 0.337828, acc 0.90625
2017-03-02T17:36:05.418747: step 2892, loss 0.290954, acc 0.921875
2017-03-02T17:36:05.492547: step 2893, loss 0.2613, acc 0.921875
2017-03-02T17:36:05.565501: step 2894, loss 0.22899, acc 0.90625
2017-03-02T17:36:05.636935: step 2895, loss 0.278826, acc 0.921875
2017-03-02T17:36:05.709732: step 2896, loss 0.309541, acc 0.890625
2017-03-02T17:36:05.786999: step 2897, loss 0.269264, acc 0.90625
2017-03-02T17:36:05.868746: step 2898, loss 0.219053, acc 0.90625
2017-03-02T17:36:05.937218: step 2899, loss 0.265988, acc 0.890625
2017-03-02T17:36:06.022328: step 2900, loss 0.30272, acc 0.890625

Evaluation:
2017-03-02T17:36:06.054527: step 2900, loss 0.960166, acc 0.675559

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-2900

2017-03-02T17:36:06.505327: step 2901, loss 0.298858, acc 0.890625
2017-03-02T17:36:06.579960: step 2902, loss 0.288245, acc 0.90625
2017-03-02T17:36:06.656404: step 2903, loss 0.426754, acc 0.828125
2017-03-02T17:36:06.724522: step 2904, loss 0.265199, acc 0.890625
2017-03-02T17:36:06.798363: step 2905, loss 0.31088, acc 0.875
2017-03-02T17:36:06.874111: step 2906, loss 0.320303, acc 0.859375
2017-03-02T17:36:06.945724: step 2907, loss 0.313106, acc 0.859375
2017-03-02T17:36:07.013839: step 2908, loss 0.344194, acc 0.875
2017-03-02T17:36:07.085512: step 2909, loss 0.25561, acc 0.921875
2017-03-02T17:36:07.161571: step 2910, loss 0.334138, acc 0.859375
2017-03-02T17:36:07.237034: step 2911, loss 0.451593, acc 0.84375
2017-03-02T17:36:07.311290: step 2912, loss 0.310914, acc 0.90625
2017-03-02T17:36:07.384415: step 2913, loss 0.154529, acc 0.9375
2017-03-02T17:36:07.458640: step 2914, loss 0.227374, acc 0.890625
2017-03-02T17:36:07.541311: step 2915, loss 0.363375, acc 0.859375
2017-03-02T17:36:07.620012: step 2916, loss 0.315981, acc 0.90625
2017-03-02T17:36:07.691094: step 2917, loss 0.323925, acc 0.90625
2017-03-02T17:36:07.771758: step 2918, loss 0.330765, acc 0.890625
2017-03-02T17:36:07.847708: step 2919, loss 0.421299, acc 0.859375
2017-03-02T17:36:07.918808: step 2920, loss 0.249906, acc 0.90625
2017-03-02T17:36:07.999357: step 2921, loss 0.413729, acc 0.8125
2017-03-02T17:36:08.069846: step 2922, loss 0.256403, acc 0.890625
2017-03-02T17:36:08.141311: step 2923, loss 0.19239, acc 0.953125
2017-03-02T17:36:08.216557: step 2924, loss 0.269625, acc 0.90625
2017-03-02T17:36:08.298594: step 2925, loss 0.179786, acc 0.953125
2017-03-02T17:36:08.370128: step 2926, loss 0.132787, acc 0.984375
2017-03-02T17:36:08.443136: step 2927, loss 0.389426, acc 0.875
2017-03-02T17:36:08.517835: step 2928, loss 0.265014, acc 0.875
2017-03-02T17:36:08.589207: step 2929, loss 0.400369, acc 0.828125
2017-03-02T17:36:08.665239: step 2930, loss 0.313789, acc 0.875
2017-03-02T17:36:08.739379: step 2931, loss 0.30296, acc 0.890625
2017-03-02T17:36:08.813019: step 2932, loss 0.172479, acc 0.953125
2017-03-02T17:36:08.885989: step 2933, loss 0.297326, acc 0.875
2017-03-02T17:36:08.959915: step 2934, loss 0.212561, acc 0.921875
2017-03-02T17:36:09.041697: step 2935, loss 0.253097, acc 0.921875
2017-03-02T17:36:09.106181: step 2936, loss 0.427108, acc 0.890625
2017-03-02T17:36:09.179387: step 2937, loss 0.19224, acc 0.90625
2017-03-02T17:36:09.258950: step 2938, loss 0.234401, acc 0.921875
2017-03-02T17:36:09.331964: step 2939, loss 0.336831, acc 0.90625
2017-03-02T17:36:09.401355: step 2940, loss 0.0126826, acc 1
2017-03-02T17:36:09.474825: step 2941, loss 0.28027, acc 0.90625
2017-03-02T17:36:09.551401: step 2942, loss 0.292487, acc 0.875
2017-03-02T17:36:09.626834: step 2943, loss 0.268811, acc 0.921875
2017-03-02T17:36:09.706975: step 2944, loss 0.296257, acc 0.90625
2017-03-02T17:36:09.778909: step 2945, loss 0.319837, acc 0.890625
2017-03-02T17:36:09.860558: step 2946, loss 0.251799, acc 0.890625
2017-03-02T17:36:09.931753: step 2947, loss 0.306028, acc 0.9375
2017-03-02T17:36:10.017590: step 2948, loss 0.264629, acc 0.9375
2017-03-02T17:36:10.087234: step 2949, loss 0.176981, acc 0.96875
2017-03-02T17:36:10.160841: step 2950, loss 0.157958, acc 0.953125
2017-03-02T17:36:10.231824: step 2951, loss 0.348569, acc 0.859375
2017-03-02T17:36:10.307783: step 2952, loss 0.15939, acc 0.953125
2017-03-02T17:36:10.368800: step 2953, loss 0.275343, acc 0.890625
2017-03-02T17:36:10.448755: step 2954, loss 0.245619, acc 0.90625
2017-03-02T17:36:10.527693: step 2955, loss 0.473264, acc 0.890625
2017-03-02T17:36:10.599266: step 2956, loss 0.219029, acc 0.921875
2017-03-02T17:36:10.676745: step 2957, loss 0.288645, acc 0.90625
2017-03-02T17:36:10.763168: step 2958, loss 0.201594, acc 0.921875
2017-03-02T17:36:10.839327: step 2959, loss 0.235047, acc 0.9375
2017-03-02T17:36:10.913150: step 2960, loss 0.279219, acc 0.890625
2017-03-02T17:36:10.986906: step 2961, loss 0.320826, acc 0.875
2017-03-02T17:36:11.051259: step 2962, loss 0.301651, acc 0.921875
2017-03-02T17:36:11.119909: step 2963, loss 0.32781, acc 0.84375
2017-03-02T17:36:11.198024: step 2964, loss 0.209759, acc 0.921875
2017-03-02T17:36:11.270682: step 2965, loss 0.314251, acc 0.828125
2017-03-02T17:36:11.345449: step 2966, loss 0.158457, acc 0.953125
2017-03-02T17:36:11.420030: step 2967, loss 0.342603, acc 0.90625
2017-03-02T17:36:11.489633: step 2968, loss 0.258895, acc 0.890625
2017-03-02T17:36:11.571697: step 2969, loss 0.419392, acc 0.859375
2017-03-02T17:36:11.647702: step 2970, loss 0.188766, acc 0.921875
2017-03-02T17:36:11.716543: step 2971, loss 0.197342, acc 0.96875
2017-03-02T17:36:11.788761: step 2972, loss 0.207736, acc 0.90625
2017-03-02T17:36:11.872036: step 2973, loss 0.131815, acc 0.9375
2017-03-02T17:36:11.945298: step 2974, loss 0.213033, acc 0.921875
2017-03-02T17:36:12.024963: step 2975, loss 0.465375, acc 0.828125
2017-03-02T17:36:12.095284: step 2976, loss 0.230693, acc 0.9375
2017-03-02T17:36:12.175894: step 2977, loss 0.17813, acc 0.9375
2017-03-02T17:36:12.246608: step 2978, loss 0.371815, acc 0.90625
2017-03-02T17:36:12.317560: step 2979, loss 0.130041, acc 0.953125
2017-03-02T17:36:12.390079: step 2980, loss 0.305997, acc 0.90625
2017-03-02T17:36:12.458253: step 2981, loss 0.252214, acc 0.921875
2017-03-02T17:36:12.523532: step 2982, loss 0.221465, acc 0.90625
2017-03-02T17:36:12.592746: step 2983, loss 0.129181, acc 0.96875
2017-03-02T17:36:12.664518: step 2984, loss 0.372992, acc 0.90625
2017-03-02T17:36:12.735989: step 2985, loss 0.381004, acc 0.8125
2017-03-02T17:36:12.809458: step 2986, loss 0.383802, acc 0.859375
2017-03-02T17:36:12.884152: step 2987, loss 0.236746, acc 0.921875
2017-03-02T17:36:12.954874: step 2988, loss 0.208351, acc 0.921875
2017-03-02T17:36:13.021548: step 2989, loss 0.25819, acc 0.90625
2017-03-02T17:36:13.096068: step 2990, loss 0.209724, acc 0.921875
2017-03-02T17:36:13.172435: step 2991, loss 0.212518, acc 0.90625
2017-03-02T17:36:13.242243: step 2992, loss 0.376085, acc 0.84375
2017-03-02T17:36:13.313924: step 2993, loss 0.232031, acc 0.890625
2017-03-02T17:36:13.385138: step 2994, loss 0.104628, acc 0.96875
2017-03-02T17:36:13.457330: step 2995, loss 0.19782, acc 0.9375
2017-03-02T17:36:13.528741: step 2996, loss 0.334227, acc 0.859375
2017-03-02T17:36:13.603434: step 2997, loss 0.213512, acc 0.875
2017-03-02T17:36:13.681680: step 2998, loss 0.17348, acc 0.9375
2017-03-02T17:36:13.754980: step 2999, loss 0.266852, acc 0.890625
2017-03-02T17:36:13.825279: step 3000, loss 0.337296, acc 0.890625

Evaluation:
2017-03-02T17:36:13.858699: step 3000, loss 0.966683, acc 0.682769

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3000

2017-03-02T17:36:14.298904: step 3001, loss 0.46148, acc 0.84375
2017-03-02T17:36:14.372284: step 3002, loss 0.275738, acc 0.890625
2017-03-02T17:36:14.441401: step 3003, loss 0.275407, acc 0.875
2017-03-02T17:36:14.511932: step 3004, loss 0.317011, acc 0.90625
2017-03-02T17:36:14.584064: step 3005, loss 0.315589, acc 0.875
2017-03-02T17:36:14.658585: step 3006, loss 0.178995, acc 0.953125
2017-03-02T17:36:14.738673: step 3007, loss 0.43017, acc 0.8125
2017-03-02T17:36:14.813088: step 3008, loss 0.293608, acc 0.84375
2017-03-02T17:36:14.896477: step 3009, loss 0.118755, acc 0.984375
2017-03-02T17:36:14.969018: step 3010, loss 0.247831, acc 0.90625
2017-03-02T17:36:15.042352: step 3011, loss 0.415541, acc 0.828125
2017-03-02T17:36:15.120484: step 3012, loss 0.239446, acc 0.890625
2017-03-02T17:36:15.196379: step 3013, loss 0.309637, acc 0.90625
2017-03-02T17:36:15.267900: step 3014, loss 0.412714, acc 0.84375
2017-03-02T17:36:15.339962: step 3015, loss 0.41182, acc 0.875
2017-03-02T17:36:15.415730: step 3016, loss 0.315873, acc 0.859375
2017-03-02T17:36:15.487171: step 3017, loss 0.491327, acc 0.8125
2017-03-02T17:36:15.560314: step 3018, loss 0.504664, acc 0.859375
2017-03-02T17:36:15.632881: step 3019, loss 0.309384, acc 0.875
2017-03-02T17:36:15.709435: step 3020, loss 0.153025, acc 0.9375
2017-03-02T17:36:15.786197: step 3021, loss 0.227974, acc 0.921875
2017-03-02T17:36:15.857647: step 3022, loss 0.236573, acc 0.9375
2017-03-02T17:36:15.936874: step 3023, loss 0.182546, acc 0.953125
2017-03-02T17:36:16.010598: step 3024, loss 0.349163, acc 0.875
2017-03-02T17:36:16.083014: step 3025, loss 0.293861, acc 0.84375
2017-03-02T17:36:16.163825: step 3026, loss 0.201608, acc 0.921875
2017-03-02T17:36:16.239345: step 3027, loss 0.329056, acc 0.9375
2017-03-02T17:36:16.312685: step 3028, loss 0.11331, acc 0.953125
2017-03-02T17:36:16.393701: step 3029, loss 0.211976, acc 0.9375
2017-03-02T17:36:16.476199: step 3030, loss 0.262315, acc 0.90625
2017-03-02T17:36:16.539367: step 3031, loss 0.206557, acc 0.9375
2017-03-02T17:36:16.615543: step 3032, loss 0.289861, acc 0.9375
2017-03-02T17:36:16.684392: step 3033, loss 0.292615, acc 0.890625
2017-03-02T17:36:16.758650: step 3034, loss 0.276484, acc 0.875
2017-03-02T17:36:16.831358: step 3035, loss 0.241905, acc 0.890625
2017-03-02T17:36:16.909163: step 3036, loss 0.177265, acc 0.96875
2017-03-02T17:36:16.981589: step 3037, loss 0.383647, acc 0.828125
2017-03-02T17:36:17.051448: step 3038, loss 0.255738, acc 0.890625
2017-03-02T17:36:17.129872: step 3039, loss 0.25928, acc 0.9375
2017-03-02T17:36:17.197697: step 3040, loss 0.165583, acc 0.953125
2017-03-02T17:36:17.274459: step 3041, loss 0.271045, acc 0.9375
2017-03-02T17:36:17.346413: step 3042, loss 0.254765, acc 0.90625
2017-03-02T17:36:17.413675: step 3043, loss 0.512676, acc 0.796875
2017-03-02T17:36:17.486997: step 3044, loss 0.366257, acc 0.875
2017-03-02T17:36:17.565605: step 3045, loss 0.364813, acc 0.859375
2017-03-02T17:36:17.632694: step 3046, loss 0.196003, acc 0.953125
2017-03-02T17:36:17.704949: step 3047, loss 0.349113, acc 0.875
2017-03-02T17:36:17.779119: step 3048, loss 0.267144, acc 0.90625
2017-03-02T17:36:17.851330: step 3049, loss 0.336002, acc 0.90625
2017-03-02T17:36:17.930293: step 3050, loss 0.365276, acc 0.875
2017-03-02T17:36:18.020572: step 3051, loss 0.261344, acc 0.890625
2017-03-02T17:36:18.086558: step 3052, loss 0.275304, acc 0.859375
2017-03-02T17:36:18.160186: step 3053, loss 0.507312, acc 0.84375
2017-03-02T17:36:18.233155: step 3054, loss 0.578639, acc 0.8125
2017-03-02T17:36:18.304405: step 3055, loss 0.259961, acc 0.90625
2017-03-02T17:36:18.379376: step 3056, loss 0.317471, acc 0.90625
2017-03-02T17:36:18.453949: step 3057, loss 0.347199, acc 0.921875
2017-03-02T17:36:18.524834: step 3058, loss 0.279944, acc 0.90625
2017-03-02T17:36:18.597597: step 3059, loss 0.3001, acc 0.90625
2017-03-02T17:36:18.668298: step 3060, loss 0.273067, acc 0.921875
2017-03-02T17:36:18.735119: step 3061, loss 0.19994, acc 0.9375
2017-03-02T17:36:18.808760: step 3062, loss 0.175762, acc 0.9375
2017-03-02T17:36:18.880550: step 3063, loss 0.306584, acc 0.859375
2017-03-02T17:36:18.964225: step 3064, loss 0.291074, acc 0.875
2017-03-02T17:36:19.038041: step 3065, loss 0.345837, acc 0.921875
2017-03-02T17:36:19.109781: step 3066, loss 0.312848, acc 0.921875
2017-03-02T17:36:19.178206: step 3067, loss 0.239073, acc 0.9375
2017-03-02T17:36:19.250410: step 3068, loss 0.486023, acc 0.796875
2017-03-02T17:36:19.329813: step 3069, loss 0.283229, acc 0.875
2017-03-02T17:36:19.400026: step 3070, loss 0.278233, acc 0.921875
2017-03-02T17:36:19.486049: step 3071, loss 0.386585, acc 0.875
2017-03-02T17:36:19.547109: step 3072, loss 0.410702, acc 0.8125
2017-03-02T17:36:19.618868: step 3073, loss 0.156095, acc 0.96875
2017-03-02T17:36:19.687077: step 3074, loss 0.300626, acc 0.90625
2017-03-02T17:36:19.758126: step 3075, loss 0.309749, acc 0.890625
2017-03-02T17:36:19.829841: step 3076, loss 0.206498, acc 0.921875
2017-03-02T17:36:19.906210: step 3077, loss 0.14978, acc 0.953125
2017-03-02T17:36:19.976526: step 3078, loss 0.393106, acc 0.875
2017-03-02T17:36:20.051023: step 3079, loss 0.401382, acc 0.875
2017-03-02T17:36:20.121029: step 3080, loss 0.468314, acc 0.90625
2017-03-02T17:36:20.194709: step 3081, loss 0.271837, acc 0.921875
2017-03-02T17:36:20.264813: step 3082, loss 0.414035, acc 0.875
2017-03-02T17:36:20.332905: step 3083, loss 0.264275, acc 0.890625
2017-03-02T17:36:20.408618: step 3084, loss 0.310241, acc 0.875
2017-03-02T17:36:20.479858: step 3085, loss 0.239716, acc 0.890625
2017-03-02T17:36:20.552240: step 3086, loss 0.223624, acc 0.875
2017-03-02T17:36:20.632638: step 3087, loss 0.187281, acc 0.921875
2017-03-02T17:36:20.704200: step 3088, loss 0.270257, acc 0.890625
2017-03-02T17:36:20.775476: step 3089, loss 0.498085, acc 0.8125
2017-03-02T17:36:20.848679: step 3090, loss 0.257067, acc 0.921875
2017-03-02T17:36:20.919647: step 3091, loss 0.294177, acc 0.921875
2017-03-02T17:36:20.991378: step 3092, loss 0.451068, acc 0.859375
2017-03-02T17:36:21.063692: step 3093, loss 0.241084, acc 0.921875
2017-03-02T17:36:21.136351: step 3094, loss 0.24926, acc 0.9375
2017-03-02T17:36:21.207758: step 3095, loss 0.357379, acc 0.84375
2017-03-02T17:36:21.281502: step 3096, loss 0.348969, acc 0.875
2017-03-02T17:36:21.354469: step 3097, loss 0.314536, acc 0.890625
2017-03-02T17:36:21.423331: step 3098, loss 0.399842, acc 0.859375
2017-03-02T17:36:21.500231: step 3099, loss 0.291969, acc 0.921875
2017-03-02T17:36:21.572434: step 3100, loss 0.266807, acc 0.90625

Evaluation:
2017-03-02T17:36:21.606532: step 3100, loss 0.960227, acc 0.674838

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3100

2017-03-02T17:36:22.063690: step 3101, loss 0.368272, acc 0.875
2017-03-02T17:36:22.133181: step 3102, loss 0.316064, acc 0.890625
2017-03-02T17:36:22.207671: step 3103, loss 0.19392, acc 0.953125
2017-03-02T17:36:22.276391: step 3104, loss 0.38371, acc 0.859375
2017-03-02T17:36:22.347505: step 3105, loss 0.188293, acc 0.921875
2017-03-02T17:36:22.418988: step 3106, loss 0.270142, acc 0.859375
2017-03-02T17:36:22.493050: step 3107, loss 0.245511, acc 0.9375
2017-03-02T17:36:22.565063: step 3108, loss 0.326777, acc 0.859375
2017-03-02T17:36:22.635628: step 3109, loss 0.324404, acc 0.90625
2017-03-02T17:36:22.714343: step 3110, loss 0.3884, acc 0.875
2017-03-02T17:36:22.790849: step 3111, loss 0.25564, acc 0.890625
2017-03-02T17:36:22.885686: step 3112, loss 0.253185, acc 0.9375
2017-03-02T17:36:22.951109: step 3113, loss 0.116554, acc 0.984375
2017-03-02T17:36:23.031756: step 3114, loss 0.289696, acc 0.875
2017-03-02T17:36:23.111930: step 3115, loss 0.428406, acc 0.84375
2017-03-02T17:36:23.180481: step 3116, loss 0.338622, acc 0.890625
2017-03-02T17:36:23.251855: step 3117, loss 0.162066, acc 0.921875
2017-03-02T17:36:23.324145: step 3118, loss 0.365386, acc 0.890625
2017-03-02T17:36:23.391299: step 3119, loss 0.127458, acc 0.96875
2017-03-02T17:36:23.456396: step 3120, loss 0.330426, acc 0.90625
2017-03-02T17:36:23.528520: step 3121, loss 0.265083, acc 0.875
2017-03-02T17:36:23.596860: step 3122, loss 0.285385, acc 0.890625
2017-03-02T17:36:23.673611: step 3123, loss 0.373344, acc 0.875
2017-03-02T17:36:23.749920: step 3124, loss 0.19741, acc 0.90625
2017-03-02T17:36:23.820311: step 3125, loss 0.293284, acc 0.90625
2017-03-02T17:36:23.889942: step 3126, loss 0.172362, acc 0.921875
2017-03-02T17:36:23.962927: step 3127, loss 0.187464, acc 0.953125
2017-03-02T17:36:24.028342: step 3128, loss 0.421628, acc 0.90625
2017-03-02T17:36:24.101454: step 3129, loss 0.328583, acc 0.921875
2017-03-02T17:36:24.177078: step 3130, loss 0.150481, acc 0.96875
2017-03-02T17:36:24.245994: step 3131, loss 0.257718, acc 0.890625
2017-03-02T17:36:24.318850: step 3132, loss 0.334904, acc 0.84375
2017-03-02T17:36:24.391870: step 3133, loss 0.283656, acc 0.90625
2017-03-02T17:36:24.466278: step 3134, loss 0.227641, acc 0.875
2017-03-02T17:36:24.543373: step 3135, loss 0.488981, acc 0.859375
2017-03-02T17:36:24.609099: step 3136, loss 0.944396, acc 0.75
2017-03-02T17:36:24.670575: step 3137, loss 0.324158, acc 0.875
2017-03-02T17:36:24.742778: step 3138, loss 0.402862, acc 0.859375
2017-03-02T17:36:24.807336: step 3139, loss 0.316865, acc 0.890625
2017-03-02T17:36:24.888188: step 3140, loss 0.411488, acc 0.859375
2017-03-02T17:36:24.961101: step 3141, loss 0.215945, acc 0.921875
2017-03-02T17:36:25.036369: step 3142, loss 0.202627, acc 0.921875
2017-03-02T17:36:25.105126: step 3143, loss 0.627288, acc 0.796875
2017-03-02T17:36:25.181874: step 3144, loss 0.301555, acc 0.953125
2017-03-02T17:36:25.252718: step 3145, loss 0.305912, acc 0.90625
2017-03-02T17:36:25.322816: step 3146, loss 0.225394, acc 0.921875
2017-03-02T17:36:25.391906: step 3147, loss 0.210634, acc 0.9375
2017-03-02T17:36:25.467441: step 3148, loss 0.266882, acc 0.890625
2017-03-02T17:36:25.537631: step 3149, loss 0.350974, acc 0.890625
2017-03-02T17:36:25.611164: step 3150, loss 0.578972, acc 0.8125
2017-03-02T17:36:25.678671: step 3151, loss 0.127998, acc 0.9375
2017-03-02T17:36:25.744875: step 3152, loss 0.432657, acc 0.90625
2017-03-02T17:36:25.815691: step 3153, loss 0.21613, acc 0.890625
2017-03-02T17:36:25.888741: step 3154, loss 0.214808, acc 0.875
2017-03-02T17:36:25.962546: step 3155, loss 0.412284, acc 0.890625
2017-03-02T17:36:26.037191: step 3156, loss 0.316654, acc 0.875
2017-03-02T17:36:26.115698: step 3157, loss 0.398027, acc 0.921875
2017-03-02T17:36:26.183071: step 3158, loss 0.214697, acc 0.90625
2017-03-02T17:36:26.253353: step 3159, loss 0.330715, acc 0.859375
2017-03-02T17:36:26.330244: step 3160, loss 0.298761, acc 0.84375
2017-03-02T17:36:26.407092: step 3161, loss 0.159886, acc 0.96875
2017-03-02T17:36:26.479227: step 3162, loss 0.261012, acc 0.921875
2017-03-02T17:36:26.544626: step 3163, loss 0.24915, acc 0.96875
2017-03-02T17:36:26.620816: step 3164, loss 0.302583, acc 0.875
2017-03-02T17:36:26.692046: step 3165, loss 0.255861, acc 0.9375
2017-03-02T17:36:26.765852: step 3166, loss 0.202905, acc 0.890625
2017-03-02T17:36:26.836101: step 3167, loss 0.379898, acc 0.890625
2017-03-02T17:36:26.911357: step 3168, loss 0.300711, acc 0.890625
2017-03-02T17:36:26.983984: step 3169, loss 0.336386, acc 0.90625
2017-03-02T17:36:27.059266: step 3170, loss 0.278265, acc 0.890625
2017-03-02T17:36:27.127132: step 3171, loss 0.194864, acc 0.921875
2017-03-02T17:36:27.198999: step 3172, loss 0.480179, acc 0.859375
2017-03-02T17:36:27.277067: step 3173, loss 0.330319, acc 0.875
2017-03-02T17:36:27.343779: step 3174, loss 0.229899, acc 0.890625
2017-03-02T17:36:27.426532: step 3175, loss 0.200999, acc 0.90625
2017-03-02T17:36:27.491511: step 3176, loss 0.210755, acc 0.984375
2017-03-02T17:36:27.561968: step 3177, loss 0.3284, acc 0.90625
2017-03-02T17:36:27.629412: step 3178, loss 0.195325, acc 0.9375
2017-03-02T17:36:27.701145: step 3179, loss 0.184364, acc 0.921875
2017-03-02T17:36:27.775760: step 3180, loss 0.284548, acc 0.9375
2017-03-02T17:36:27.843480: step 3181, loss 0.309351, acc 0.921875
2017-03-02T17:36:27.916373: step 3182, loss 0.444155, acc 0.890625
2017-03-02T17:36:27.986860: step 3183, loss 0.418786, acc 0.890625
2017-03-02T17:36:28.060081: step 3184, loss 0.19292, acc 0.9375
2017-03-02T17:36:28.143369: step 3185, loss 0.125051, acc 0.96875
2017-03-02T17:36:28.218018: step 3186, loss 0.201164, acc 0.921875
2017-03-02T17:36:28.296713: step 3187, loss 0.211554, acc 0.90625
2017-03-02T17:36:28.367667: step 3188, loss 0.233273, acc 0.9375
2017-03-02T17:36:28.435336: step 3189, loss 0.0970729, acc 0.984375
2017-03-02T17:36:28.509067: step 3190, loss 0.166892, acc 0.9375
2017-03-02T17:36:28.583008: step 3191, loss 0.2761, acc 0.921875
2017-03-02T17:36:28.654075: step 3192, loss 0.320886, acc 0.90625
2017-03-02T17:36:28.723191: step 3193, loss 0.38381, acc 0.859375
2017-03-02T17:36:28.797346: step 3194, loss 0.347476, acc 0.890625
2017-03-02T17:36:28.866409: step 3195, loss 0.168087, acc 0.921875
2017-03-02T17:36:28.936148: step 3196, loss 0.350696, acc 0.84375
2017-03-02T17:36:29.015462: step 3197, loss 0.468857, acc 0.875
2017-03-02T17:36:29.091050: step 3198, loss 0.534047, acc 0.84375
2017-03-02T17:36:29.168874: step 3199, loss 0.20843, acc 0.890625
2017-03-02T17:36:29.237460: step 3200, loss 0.166889, acc 0.953125

Evaluation:
2017-03-02T17:36:29.275394: step 3200, loss 0.943817, acc 0.679885

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3200

2017-03-02T17:36:29.706869: step 3201, loss 0.269051, acc 0.90625
2017-03-02T17:36:29.782560: step 3202, loss 0.239881, acc 0.890625
2017-03-02T17:36:29.863933: step 3203, loss 0.276184, acc 0.90625
2017-03-02T17:36:29.955708: step 3204, loss 0.269537, acc 0.921875
2017-03-02T17:36:30.028992: step 3205, loss 0.423477, acc 0.859375
2017-03-02T17:36:30.095560: step 3206, loss 0.198105, acc 0.9375
2017-03-02T17:36:30.169097: step 3207, loss 0.331008, acc 0.890625
2017-03-02T17:36:30.242489: step 3208, loss 0.25681, acc 0.90625
2017-03-02T17:36:30.311661: step 3209, loss 0.106322, acc 0.953125
2017-03-02T17:36:30.387601: step 3210, loss 0.252969, acc 0.90625
2017-03-02T17:36:30.456692: step 3211, loss 0.253595, acc 0.921875
2017-03-02T17:36:30.540591: step 3212, loss 0.193419, acc 0.9375
2017-03-02T17:36:30.619454: step 3213, loss 0.250546, acc 0.921875
2017-03-02T17:36:30.695023: step 3214, loss 0.24691, acc 0.9375
2017-03-02T17:36:30.763622: step 3215, loss 0.205762, acc 0.953125
2017-03-02T17:36:30.836836: step 3216, loss 0.179603, acc 0.921875
2017-03-02T17:36:30.909292: step 3217, loss 0.278862, acc 0.90625
2017-03-02T17:36:30.975542: step 3218, loss 0.359796, acc 0.84375
2017-03-02T17:36:31.047521: step 3219, loss 0.260743, acc 0.859375
2017-03-02T17:36:31.121532: step 3220, loss 0.275779, acc 0.921875
2017-03-02T17:36:31.201194: step 3221, loss 0.225646, acc 0.953125
2017-03-02T17:36:31.278276: step 3222, loss 0.214773, acc 0.90625
2017-03-02T17:36:31.353627: step 3223, loss 0.161114, acc 0.90625
2017-03-02T17:36:31.424118: step 3224, loss 0.137785, acc 0.953125
2017-03-02T17:36:31.497826: step 3225, loss 0.441388, acc 0.84375
2017-03-02T17:36:31.568454: step 3226, loss 0.28372, acc 0.90625
2017-03-02T17:36:31.635394: step 3227, loss 0.171333, acc 0.9375
2017-03-02T17:36:31.705634: step 3228, loss 0.159066, acc 0.953125
2017-03-02T17:36:31.780350: step 3229, loss 0.170307, acc 0.953125
2017-03-02T17:36:31.845480: step 3230, loss 0.26818, acc 0.890625
2017-03-02T17:36:31.918318: step 3231, loss 0.309357, acc 0.859375
2017-03-02T17:36:31.996745: step 3232, loss 0.249673, acc 0.953125
2017-03-02T17:36:32.072314: step 3233, loss 0.155606, acc 0.96875
2017-03-02T17:36:32.147724: step 3234, loss 0.328662, acc 0.890625
2017-03-02T17:36:32.226343: step 3235, loss 0.307938, acc 0.921875
2017-03-02T17:36:32.296572: step 3236, loss 0.47556, acc 0.84375
2017-03-02T17:36:32.369592: step 3237, loss 0.350381, acc 0.890625
2017-03-02T17:36:32.438995: step 3238, loss 0.0955814, acc 0.984375
2017-03-02T17:36:32.513657: step 3239, loss 0.336063, acc 0.921875
2017-03-02T17:36:32.590445: step 3240, loss 0.288296, acc 0.890625
2017-03-02T17:36:32.661588: step 3241, loss 0.284774, acc 0.90625
2017-03-02T17:36:32.736202: step 3242, loss 0.323923, acc 0.875
2017-03-02T17:36:32.803588: step 3243, loss 0.308496, acc 0.875
2017-03-02T17:36:32.875462: step 3244, loss 0.209463, acc 0.921875
2017-03-02T17:36:32.944778: step 3245, loss 0.185285, acc 0.9375
2017-03-02T17:36:33.015785: step 3246, loss 0.36843, acc 0.90625
2017-03-02T17:36:33.089758: step 3247, loss 0.275951, acc 0.859375
2017-03-02T17:36:33.167073: step 3248, loss 0.491101, acc 0.859375
2017-03-02T17:36:33.251566: step 3249, loss 0.273248, acc 0.890625
2017-03-02T17:36:33.340006: step 3250, loss 0.197941, acc 0.9375
2017-03-02T17:36:33.413106: step 3251, loss 0.244822, acc 0.90625
2017-03-02T17:36:33.483122: step 3252, loss 0.156226, acc 0.96875
2017-03-02T17:36:33.547855: step 3253, loss 0.376948, acc 0.890625
2017-03-02T17:36:33.612652: step 3254, loss 0.332368, acc 0.9375
2017-03-02T17:36:33.693006: step 3255, loss 0.289116, acc 0.921875
2017-03-02T17:36:33.768863: step 3256, loss 0.168277, acc 0.9375
2017-03-02T17:36:33.833977: step 3257, loss 0.169574, acc 0.90625
2017-03-02T17:36:33.909742: step 3258, loss 0.28944, acc 0.90625
2017-03-02T17:36:33.978790: step 3259, loss 0.196759, acc 0.921875
2017-03-02T17:36:34.047984: step 3260, loss 0.123506, acc 0.984375
2017-03-02T17:36:34.129441: step 3261, loss 0.286344, acc 0.875
2017-03-02T17:36:34.203829: step 3262, loss 0.377352, acc 0.84375
2017-03-02T17:36:34.273611: step 3263, loss 0.223878, acc 0.90625
2017-03-02T17:36:34.347302: step 3264, loss 0.238293, acc 0.90625
2017-03-02T17:36:34.419574: step 3265, loss 0.161743, acc 0.921875
2017-03-02T17:36:34.496934: step 3266, loss 0.263783, acc 0.9375
2017-03-02T17:36:34.570318: step 3267, loss 0.235351, acc 0.890625
2017-03-02T17:36:34.647985: step 3268, loss 0.174321, acc 0.9375
2017-03-02T17:36:34.719108: step 3269, loss 0.390134, acc 0.84375
2017-03-02T17:36:34.824458: step 3270, loss 0.312568, acc 0.890625
2017-03-02T17:36:34.898358: step 3271, loss 0.279687, acc 0.890625
2017-03-02T17:36:34.966400: step 3272, loss 0.203518, acc 0.9375
2017-03-02T17:36:35.040776: step 3273, loss 0.253541, acc 0.921875
2017-03-02T17:36:35.116114: step 3274, loss 0.303513, acc 0.890625
2017-03-02T17:36:35.183006: step 3275, loss 0.157249, acc 0.953125
2017-03-02T17:36:35.252820: step 3276, loss 0.362814, acc 0.890625
2017-03-02T17:36:35.324605: step 3277, loss 0.254751, acc 0.9375
2017-03-02T17:36:35.395803: step 3278, loss 0.389721, acc 0.828125
2017-03-02T17:36:35.469365: step 3279, loss 0.402985, acc 0.859375
2017-03-02T17:36:35.530900: step 3280, loss 0.379659, acc 0.84375
2017-03-02T17:36:35.614526: step 3281, loss 0.304146, acc 0.859375
2017-03-02T17:36:35.684780: step 3282, loss 0.204575, acc 0.9375
2017-03-02T17:36:35.757444: step 3283, loss 0.38632, acc 0.875
2017-03-02T17:36:35.849702: step 3284, loss 0.2052, acc 0.921875
2017-03-02T17:36:35.913848: step 3285, loss 0.226381, acc 0.921875
2017-03-02T17:36:35.983629: step 3286, loss 0.496926, acc 0.765625
2017-03-02T17:36:36.053010: step 3287, loss 0.237495, acc 0.90625
2017-03-02T17:36:36.120220: step 3288, loss 0.28066, acc 0.9375
2017-03-02T17:36:36.192033: step 3289, loss 0.243352, acc 0.875
2017-03-02T17:36:36.267442: step 3290, loss 0.291486, acc 0.890625
2017-03-02T17:36:36.338280: step 3291, loss 0.244198, acc 0.921875
2017-03-02T17:36:36.415917: step 3292, loss 0.187958, acc 0.9375
2017-03-02T17:36:36.488368: step 3293, loss 0.139495, acc 0.96875
2017-03-02T17:36:36.558990: step 3294, loss 0.353238, acc 0.8125
2017-03-02T17:36:36.630868: step 3295, loss 0.34101, acc 0.875
2017-03-02T17:36:36.709059: step 3296, loss 0.312843, acc 0.921875
2017-03-02T17:36:36.783849: step 3297, loss 0.285139, acc 0.921875
2017-03-02T17:36:36.858162: step 3298, loss 0.285301, acc 0.859375
2017-03-02T17:36:36.937384: step 3299, loss 0.257455, acc 0.890625
2017-03-02T17:36:37.010310: step 3300, loss 0.355624, acc 0.890625

Evaluation:
2017-03-02T17:36:37.045593: step 3300, loss 0.969092, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3300

2017-03-02T17:36:37.503943: step 3301, loss 0.434647, acc 0.875
2017-03-02T17:36:37.576882: step 3302, loss 0.284196, acc 0.890625
2017-03-02T17:36:37.658149: step 3303, loss 0.166242, acc 0.953125
2017-03-02T17:36:37.741343: step 3304, loss 0.264654, acc 0.9375
2017-03-02T17:36:37.807977: step 3305, loss 0.246038, acc 0.890625
2017-03-02T17:36:37.896861: step 3306, loss 0.35369, acc 0.890625
2017-03-02T17:36:37.972831: step 3307, loss 0.339199, acc 0.84375
2017-03-02T17:36:38.043509: step 3308, loss 0.442872, acc 0.890625
2017-03-02T17:36:38.111005: step 3309, loss 0.083505, acc 0.984375
2017-03-02T17:36:38.201776: step 3310, loss 0.24078, acc 0.90625
2017-03-02T17:36:38.271355: step 3311, loss 0.232186, acc 0.90625
2017-03-02T17:36:38.344628: step 3312, loss 0.234486, acc 0.890625
2017-03-02T17:36:38.415590: step 3313, loss 0.348281, acc 0.859375
2017-03-02T17:36:38.486716: step 3314, loss 0.221746, acc 0.921875
2017-03-02T17:36:38.561044: step 3315, loss 0.464329, acc 0.875
2017-03-02T17:36:38.632895: step 3316, loss 0.290801, acc 0.890625
2017-03-02T17:36:38.703298: step 3317, loss 0.304828, acc 0.890625
2017-03-02T17:36:38.768878: step 3318, loss 0.396917, acc 0.859375
2017-03-02T17:36:38.830595: step 3319, loss 0.38105, acc 0.875
2017-03-02T17:36:38.896808: step 3320, loss 0.213092, acc 0.890625
2017-03-02T17:36:38.965827: step 3321, loss 0.400196, acc 0.8125
2017-03-02T17:36:39.039720: step 3322, loss 0.238316, acc 0.9375
2017-03-02T17:36:39.111676: step 3323, loss 0.399355, acc 0.84375
2017-03-02T17:36:39.179814: step 3324, loss 0.502318, acc 0.859375
2017-03-02T17:36:39.261496: step 3325, loss 0.643536, acc 0.765625
2017-03-02T17:36:39.338512: step 3326, loss 0.441876, acc 0.875
2017-03-02T17:36:39.409761: step 3327, loss 0.272437, acc 0.890625
2017-03-02T17:36:39.482571: step 3328, loss 0.183918, acc 0.9375
2017-03-02T17:36:39.559554: step 3329, loss 0.24342, acc 0.875
2017-03-02T17:36:39.632522: step 3330, loss 0.481895, acc 0.8125
2017-03-02T17:36:39.717381: step 3331, loss 0.34188, acc 0.890625
2017-03-02T17:36:39.782604: step 3332, loss 1.17603, acc 0.75
2017-03-02T17:36:39.869313: step 3333, loss 0.361653, acc 0.90625
2017-03-02T17:36:39.942803: step 3334, loss 0.112949, acc 0.984375
2017-03-02T17:36:40.012066: step 3335, loss 0.376396, acc 0.875
2017-03-02T17:36:40.097340: step 3336, loss 0.243207, acc 0.921875
2017-03-02T17:36:40.177430: step 3337, loss 0.539132, acc 0.8125
2017-03-02T17:36:40.247102: step 3338, loss 0.248618, acc 0.921875
2017-03-02T17:36:40.322302: step 3339, loss 0.240941, acc 0.90625
2017-03-02T17:36:40.403309: step 3340, loss 0.189502, acc 0.96875
2017-03-02T17:36:40.469116: step 3341, loss 0.180099, acc 0.953125
2017-03-02T17:36:40.541754: step 3342, loss 0.544686, acc 0.859375
2017-03-02T17:36:40.607762: step 3343, loss 0.248772, acc 0.921875
2017-03-02T17:36:40.683699: step 3344, loss 0.242008, acc 0.921875
2017-03-02T17:36:40.757587: step 3345, loss 0.237553, acc 0.9375
2017-03-02T17:36:40.832491: step 3346, loss 0.332664, acc 0.84375
2017-03-02T17:36:40.902479: step 3347, loss 0.197995, acc 0.921875
2017-03-02T17:36:40.975243: step 3348, loss 0.25218, acc 0.921875
2017-03-02T17:36:41.044043: step 3349, loss 0.17243, acc 0.953125
2017-03-02T17:36:41.112287: step 3350, loss 0.185837, acc 0.90625
2017-03-02T17:36:41.179329: step 3351, loss 0.314362, acc 0.921875
2017-03-02T17:36:41.255092: step 3352, loss 0.183657, acc 0.921875
2017-03-02T17:36:41.319800: step 3353, loss 0.243313, acc 0.953125
2017-03-02T17:36:41.392918: step 3354, loss 0.342416, acc 0.875
2017-03-02T17:36:41.475214: step 3355, loss 0.369344, acc 0.875
2017-03-02T17:36:41.548889: step 3356, loss 0.356688, acc 0.859375
2017-03-02T17:36:41.620624: step 3357, loss 0.248345, acc 0.953125
2017-03-02T17:36:41.695219: step 3358, loss 0.159345, acc 0.96875
2017-03-02T17:36:41.764382: step 3359, loss 0.207333, acc 0.9375
2017-03-02T17:36:41.826544: step 3360, loss 0.155785, acc 0.9375
2017-03-02T17:36:41.902299: step 3361, loss 0.307886, acc 0.90625
2017-03-02T17:36:41.971245: step 3362, loss 0.178497, acc 0.953125
2017-03-02T17:36:42.048635: step 3363, loss 0.326221, acc 0.921875
2017-03-02T17:36:42.121024: step 3364, loss 0.208006, acc 0.921875
2017-03-02T17:36:42.187254: step 3365, loss 0.293039, acc 0.875
2017-03-02T17:36:42.255867: step 3366, loss 0.224408, acc 0.921875
2017-03-02T17:36:42.339673: step 3367, loss 0.204128, acc 0.921875
2017-03-02T17:36:42.410133: step 3368, loss 0.161191, acc 0.9375
2017-03-02T17:36:42.484298: step 3369, loss 0.258671, acc 0.921875
2017-03-02T17:36:42.555521: step 3370, loss 0.298043, acc 0.890625
2017-03-02T17:36:42.628110: step 3371, loss 0.289736, acc 0.90625
2017-03-02T17:36:42.701528: step 3372, loss 0.368121, acc 0.84375
2017-03-02T17:36:42.780437: step 3373, loss 0.311088, acc 0.890625
2017-03-02T17:36:42.852116: step 3374, loss 0.190907, acc 0.9375
2017-03-02T17:36:42.923352: step 3375, loss 0.247395, acc 0.90625
2017-03-02T17:36:42.999988: step 3376, loss 0.224693, acc 0.90625
2017-03-02T17:36:43.065651: step 3377, loss 0.229065, acc 0.90625
2017-03-02T17:36:43.142599: step 3378, loss 0.237348, acc 0.890625
2017-03-02T17:36:43.215989: step 3379, loss 0.289791, acc 0.921875
2017-03-02T17:36:43.282604: step 3380, loss 0.286144, acc 0.890625
2017-03-02T17:36:43.357476: step 3381, loss 0.205718, acc 0.953125
2017-03-02T17:36:43.429558: step 3382, loss 0.223508, acc 0.96875
2017-03-02T17:36:43.502398: step 3383, loss 0.247064, acc 0.90625
2017-03-02T17:36:43.574505: step 3384, loss 0.179569, acc 0.9375
2017-03-02T17:36:43.639484: step 3385, loss 0.252793, acc 0.921875
2017-03-02T17:36:43.714109: step 3386, loss 0.192684, acc 0.921875
2017-03-02T17:36:43.790415: step 3387, loss 0.244128, acc 0.921875
2017-03-02T17:36:43.864744: step 3388, loss 0.232919, acc 0.890625
2017-03-02T17:36:43.934369: step 3389, loss 0.267267, acc 0.890625
2017-03-02T17:36:44.013768: step 3390, loss 0.361367, acc 0.84375
2017-03-02T17:36:44.094679: step 3391, loss 0.248868, acc 0.90625
2017-03-02T17:36:44.165376: step 3392, loss 0.308228, acc 0.890625
2017-03-02T17:36:44.240031: step 3393, loss 0.287766, acc 0.890625
2017-03-02T17:36:44.315772: step 3394, loss 0.303189, acc 0.875
2017-03-02T17:36:44.381829: step 3395, loss 0.198367, acc 0.890625
2017-03-02T17:36:44.457156: step 3396, loss 0.192231, acc 0.953125
2017-03-02T17:36:44.543142: step 3397, loss 0.310666, acc 0.859375
2017-03-02T17:36:44.610969: step 3398, loss 0.274504, acc 0.890625
2017-03-02T17:36:44.681013: step 3399, loss 0.277295, acc 0.890625
2017-03-02T17:36:44.765618: step 3400, loss 0.313962, acc 0.875

Evaluation:
2017-03-02T17:36:44.799026: step 3400, loss 0.977832, acc 0.684932

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3400

2017-03-02T17:36:45.280055: step 3401, loss 0.335843, acc 0.890625
2017-03-02T17:36:45.348636: step 3402, loss 0.235036, acc 0.875
2017-03-02T17:36:45.420480: step 3403, loss 0.267989, acc 0.90625
2017-03-02T17:36:45.500614: step 3404, loss 0.317229, acc 0.90625
2017-03-02T17:36:45.577525: step 3405, loss 0.380879, acc 0.875
2017-03-02T17:36:45.649686: step 3406, loss 0.335625, acc 0.875
2017-03-02T17:36:45.760466: step 3407, loss 0.230062, acc 0.890625
2017-03-02T17:36:45.840913: step 3408, loss 0.274427, acc 0.921875
2017-03-02T17:36:45.909746: step 3409, loss 0.380822, acc 0.875
2017-03-02T17:36:45.986052: step 3410, loss 0.127871, acc 0.953125
2017-03-02T17:36:46.061723: step 3411, loss 0.256234, acc 0.90625
2017-03-02T17:36:46.130939: step 3412, loss 0.225265, acc 0.9375
2017-03-02T17:36:46.202382: step 3413, loss 0.228724, acc 0.9375
2017-03-02T17:36:46.268519: step 3414, loss 0.125411, acc 0.9375
2017-03-02T17:36:46.335340: step 3415, loss 0.262079, acc 0.921875
2017-03-02T17:36:46.420196: step 3416, loss 0.304122, acc 0.875
2017-03-02T17:36:46.493297: step 3417, loss 0.222019, acc 0.921875
2017-03-02T17:36:46.566336: step 3418, loss 0.25744, acc 0.890625
2017-03-02T17:36:46.638909: step 3419, loss 0.321563, acc 0.921875
2017-03-02T17:36:46.710163: step 3420, loss 0.179592, acc 0.953125
2017-03-02T17:36:46.788592: step 3421, loss 0.190917, acc 0.921875
2017-03-02T17:36:46.859169: step 3422, loss 0.187419, acc 0.921875
2017-03-02T17:36:46.933927: step 3423, loss 0.250736, acc 0.953125
2017-03-02T17:36:47.008640: step 3424, loss 0.414659, acc 0.859375
2017-03-02T17:36:47.107742: step 3425, loss 0.213397, acc 0.921875
2017-03-02T17:36:47.177861: step 3426, loss 0.364173, acc 0.828125
2017-03-02T17:36:47.247937: step 3427, loss 0.278684, acc 0.90625
2017-03-02T17:36:47.318596: step 3428, loss 0.285959, acc 0.875
2017-03-02T17:36:47.398452: step 3429, loss 0.307642, acc 0.890625
2017-03-02T17:36:47.471750: step 3430, loss 0.138545, acc 0.9375
2017-03-02T17:36:47.545487: step 3431, loss 0.266197, acc 0.921875
2017-03-02T17:36:47.620700: step 3432, loss 0.259546, acc 0.875
2017-03-02T17:36:47.699631: step 3433, loss 0.284027, acc 0.859375
2017-03-02T17:36:47.769884: step 3434, loss 0.183853, acc 0.953125
2017-03-02T17:36:47.843041: step 3435, loss 0.224091, acc 0.953125
2017-03-02T17:36:47.918974: step 3436, loss 0.200161, acc 0.921875
2017-03-02T17:36:47.995199: step 3437, loss 0.158056, acc 0.9375
2017-03-02T17:36:48.062238: step 3438, loss 0.153243, acc 0.953125
2017-03-02T17:36:48.138048: step 3439, loss 0.127655, acc 0.984375
2017-03-02T17:36:48.219157: step 3440, loss 0.17796, acc 0.953125
2017-03-02T17:36:48.286220: step 3441, loss 0.20674, acc 0.953125
2017-03-02T17:36:48.364983: step 3442, loss 0.192262, acc 0.921875
2017-03-02T17:36:48.435168: step 3443, loss 0.290794, acc 0.90625
2017-03-02T17:36:48.514534: step 3444, loss 0.355858, acc 0.90625
2017-03-02T17:36:48.592501: step 3445, loss 0.335249, acc 0.90625
2017-03-02T17:36:48.667903: step 3446, loss 0.268413, acc 0.90625
2017-03-02T17:36:48.738030: step 3447, loss 0.31792, acc 0.875
2017-03-02T17:36:48.813061: step 3448, loss 0.288246, acc 0.90625
2017-03-02T17:36:48.884144: step 3449, loss 0.300606, acc 0.921875
2017-03-02T17:36:48.955538: step 3450, loss 0.227971, acc 0.921875
2017-03-02T17:36:49.026892: step 3451, loss 0.219578, acc 0.9375
2017-03-02T17:36:49.100483: step 3452, loss 0.236522, acc 0.921875
2017-03-02T17:36:49.172304: step 3453, loss 0.280928, acc 0.921875
2017-03-02T17:36:49.244190: step 3454, loss 0.295081, acc 0.890625
2017-03-02T17:36:49.315670: step 3455, loss 0.223133, acc 0.9375
2017-03-02T17:36:49.397364: step 3456, loss 0.196134, acc 0.921875
2017-03-02T17:36:49.470806: step 3457, loss 0.276307, acc 0.9375
2017-03-02T17:36:49.541565: step 3458, loss 0.249377, acc 0.890625
2017-03-02T17:36:49.613170: step 3459, loss 0.34087, acc 0.890625
2017-03-02T17:36:49.691051: step 3460, loss 0.271872, acc 0.90625
2017-03-02T17:36:49.767334: step 3461, loss 0.169257, acc 0.96875
2017-03-02T17:36:49.835512: step 3462, loss 0.318342, acc 0.890625
2017-03-02T17:36:49.921973: step 3463, loss 0.25495, acc 0.859375
2017-03-02T17:36:49.995903: step 3464, loss 0.348803, acc 0.921875
2017-03-02T17:36:50.068587: step 3465, loss 0.22627, acc 0.921875
2017-03-02T17:36:50.140024: step 3466, loss 0.350508, acc 0.84375
2017-03-02T17:36:50.203231: step 3467, loss 0.298987, acc 0.90625
2017-03-02T17:36:50.274193: step 3468, loss 0.179313, acc 0.9375
2017-03-02T17:36:50.345031: step 3469, loss 0.240874, acc 0.921875
2017-03-02T17:36:50.415742: step 3470, loss 0.161806, acc 0.953125
2017-03-02T17:36:50.493801: step 3471, loss 0.205412, acc 0.90625
2017-03-02T17:36:50.607468: step 3472, loss 0.304391, acc 0.90625
2017-03-02T17:36:50.680054: step 3473, loss 0.352864, acc 0.9375
2017-03-02T17:36:50.754549: step 3474, loss 0.271285, acc 0.90625
2017-03-02T17:36:50.827969: step 3475, loss 0.397143, acc 0.828125
2017-03-02T17:36:50.909647: step 3476, loss 0.203072, acc 0.9375
2017-03-02T17:36:50.978817: step 3477, loss 0.224879, acc 0.921875
2017-03-02T17:36:51.062985: step 3478, loss 0.219046, acc 0.9375
2017-03-02T17:36:51.138074: step 3479, loss 0.310619, acc 0.921875
2017-03-02T17:36:51.213730: step 3480, loss 0.277417, acc 0.890625
2017-03-02T17:36:51.289942: step 3481, loss 0.321139, acc 0.875
2017-03-02T17:36:51.354988: step 3482, loss 0.133912, acc 0.953125
2017-03-02T17:36:51.427177: step 3483, loss 0.243931, acc 0.890625
2017-03-02T17:36:51.496652: step 3484, loss 0.285111, acc 0.875
2017-03-02T17:36:51.570322: step 3485, loss 0.350155, acc 0.890625
2017-03-02T17:36:51.641523: step 3486, loss 0.350571, acc 0.890625
2017-03-02T17:36:51.721442: step 3487, loss 0.417491, acc 0.8125
2017-03-02T17:36:51.795381: step 3488, loss 0.156645, acc 0.96875
2017-03-02T17:36:51.862715: step 3489, loss 0.402404, acc 0.875
2017-03-02T17:36:51.933419: step 3490, loss 0.398299, acc 0.890625
2017-03-02T17:36:52.005058: step 3491, loss 0.0901558, acc 0.984375
2017-03-02T17:36:52.071525: step 3492, loss 0.313768, acc 0.875
2017-03-02T17:36:52.145832: step 3493, loss 0.317244, acc 0.875
2017-03-02T17:36:52.226086: step 3494, loss 0.288682, acc 0.875
2017-03-02T17:36:52.292377: step 3495, loss 0.261315, acc 0.90625
2017-03-02T17:36:52.364003: step 3496, loss 0.310829, acc 0.890625
2017-03-02T17:36:52.449261: step 3497, loss 0.245795, acc 0.921875
2017-03-02T17:36:52.524302: step 3498, loss 0.260137, acc 0.859375
2017-03-02T17:36:52.601227: step 3499, loss 0.264284, acc 0.890625
2017-03-02T17:36:52.676292: step 3500, loss 0.218738, acc 0.90625

Evaluation:
2017-03-02T17:36:52.716667: step 3500, loss 1.01148, acc 0.674838

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3500

2017-03-02T17:36:53.163110: step 3501, loss 0.362255, acc 0.875
2017-03-02T17:36:53.236849: step 3502, loss 0.199011, acc 0.890625
2017-03-02T17:36:53.308534: step 3503, loss 0.322684, acc 0.90625
2017-03-02T17:36:53.379541: step 3504, loss 0.194089, acc 0.90625
2017-03-02T17:36:53.454740: step 3505, loss 0.285717, acc 0.921875
2017-03-02T17:36:53.527550: step 3506, loss 0.165508, acc 0.9375
2017-03-02T17:36:53.602043: step 3507, loss 0.289154, acc 0.890625
2017-03-02T17:36:53.671344: step 3508, loss 0.201258, acc 0.90625
2017-03-02T17:36:53.742082: step 3509, loss 0.2143, acc 0.9375
2017-03-02T17:36:53.811324: step 3510, loss 0.195954, acc 0.921875
2017-03-02T17:36:53.880695: step 3511, loss 0.275025, acc 0.890625
2017-03-02T17:36:53.969750: step 3512, loss 0.224222, acc 0.921875
2017-03-02T17:36:54.052114: step 3513, loss 0.33699, acc 0.859375
2017-03-02T17:36:54.131580: step 3514, loss 0.183454, acc 0.90625
2017-03-02T17:36:54.204329: step 3515, loss 0.21664, acc 0.921875
2017-03-02T17:36:54.278141: step 3516, loss 0.218521, acc 0.9375
2017-03-02T17:36:54.352054: step 3517, loss 0.127131, acc 0.96875
2017-03-02T17:36:54.432685: step 3518, loss 0.209286, acc 0.890625
2017-03-02T17:36:54.500669: step 3519, loss 0.301332, acc 0.90625
2017-03-02T17:36:54.600549: step 3520, loss 0.181681, acc 0.9375
2017-03-02T17:36:54.686173: step 3521, loss 0.282669, acc 0.921875
2017-03-02T17:36:54.761373: step 3522, loss 0.281987, acc 0.90625
2017-03-02T17:36:54.834269: step 3523, loss 0.183114, acc 0.9375
2017-03-02T17:36:54.903293: step 3524, loss 0.425333, acc 0.890625
2017-03-02T17:36:54.971790: step 3525, loss 0.194486, acc 0.9375
2017-03-02T17:36:55.039619: step 3526, loss 0.304429, acc 0.90625
2017-03-02T17:36:55.105557: step 3527, loss 0.348937, acc 0.875
2017-03-02T17:36:55.181071: step 3528, loss 2.05182, acc 0.75
2017-03-02T17:36:55.256129: step 3529, loss 0.147588, acc 0.96875
2017-03-02T17:36:55.325393: step 3530, loss 0.266588, acc 0.890625
2017-03-02T17:36:55.398742: step 3531, loss 0.207761, acc 0.921875
2017-03-02T17:36:55.473730: step 3532, loss 0.199488, acc 0.9375
2017-03-02T17:36:55.542613: step 3533, loss 0.144587, acc 0.953125
2017-03-02T17:36:55.615760: step 3534, loss 0.464699, acc 0.84375
2017-03-02T17:36:55.692217: step 3535, loss 0.300624, acc 0.9375
2017-03-02T17:36:55.760319: step 3536, loss 0.339149, acc 0.859375
2017-03-02T17:36:55.830474: step 3537, loss 0.170868, acc 0.953125
2017-03-02T17:36:55.903599: step 3538, loss 0.364645, acc 0.90625
2017-03-02T17:36:55.970261: step 3539, loss 0.165356, acc 0.9375
2017-03-02T17:36:56.053391: step 3540, loss 0.293131, acc 0.890625
2017-03-02T17:36:56.132724: step 3541, loss 0.373224, acc 0.9375
2017-03-02T17:36:56.194289: step 3542, loss 0.344545, acc 0.890625
2017-03-02T17:36:56.266806: step 3543, loss 0.252501, acc 0.90625
2017-03-02T17:36:56.336136: step 3544, loss 0.211444, acc 0.953125
2017-03-02T17:36:56.403079: step 3545, loss 0.14482, acc 0.953125
2017-03-02T17:36:56.485805: step 3546, loss 0.2123, acc 0.953125
2017-03-02T17:36:56.555155: step 3547, loss 0.275899, acc 0.90625
2017-03-02T17:36:56.622815: step 3548, loss 0.250861, acc 0.890625
2017-03-02T17:36:56.699935: step 3549, loss 0.169562, acc 0.96875
2017-03-02T17:36:56.772856: step 3550, loss 0.190938, acc 0.90625
2017-03-02T17:36:56.844112: step 3551, loss 0.291808, acc 0.90625
2017-03-02T17:36:56.921867: step 3552, loss 0.194576, acc 0.90625
2017-03-02T17:36:56.994333: step 3553, loss 0.117397, acc 0.96875
2017-03-02T17:36:57.065540: step 3554, loss 0.152507, acc 0.9375
2017-03-02T17:36:57.146084: step 3555, loss 0.16343, acc 0.96875
2017-03-02T17:36:57.219755: step 3556, loss 0.157896, acc 0.9375
2017-03-02T17:36:57.285261: step 3557, loss 0.20858, acc 0.921875
2017-03-02T17:36:57.352613: step 3558, loss 0.286316, acc 0.890625
2017-03-02T17:36:57.423474: step 3559, loss 0.476676, acc 0.921875
2017-03-02T17:36:57.514289: step 3560, loss 0.23129, acc 0.875
2017-03-02T17:36:57.582937: step 3561, loss 0.135568, acc 0.96875
2017-03-02T17:36:57.659068: step 3562, loss 0.380612, acc 0.828125
2017-03-02T17:36:57.727268: step 3563, loss 0.264701, acc 0.90625
2017-03-02T17:36:57.797742: step 3564, loss 0.186507, acc 0.953125
2017-03-02T17:36:57.869344: step 3565, loss 0.215669, acc 0.921875
2017-03-02T17:36:57.940678: step 3566, loss 0.27472, acc 0.875
2017-03-02T17:36:58.016897: step 3567, loss 0.348011, acc 0.859375
2017-03-02T17:36:58.096868: step 3568, loss 0.145152, acc 0.9375
2017-03-02T17:36:58.174042: step 3569, loss 0.154175, acc 0.9375
2017-03-02T17:36:58.253098: step 3570, loss 0.34542, acc 0.859375
2017-03-02T17:36:58.324990: step 3571, loss 0.225218, acc 0.90625
2017-03-02T17:36:58.394899: step 3572, loss 0.177705, acc 0.953125
2017-03-02T17:36:58.468806: step 3573, loss 0.211501, acc 0.890625
2017-03-02T17:36:58.556775: step 3574, loss 0.356468, acc 0.890625
2017-03-02T17:36:58.632557: step 3575, loss 0.526702, acc 0.859375
2017-03-02T17:36:58.712033: step 3576, loss 0.524367, acc 0.890625
2017-03-02T17:36:58.783163: step 3577, loss 0.406857, acc 0.875
2017-03-02T17:36:58.847216: step 3578, loss 0.228474, acc 0.90625
2017-03-02T17:36:58.924545: step 3579, loss 0.247141, acc 0.90625
2017-03-02T17:36:59.001324: step 3580, loss 0.136442, acc 0.953125
2017-03-02T17:36:59.071420: step 3581, loss 0.14312, acc 0.953125
2017-03-02T17:36:59.144195: step 3582, loss 0.237243, acc 0.921875
2017-03-02T17:36:59.215919: step 3583, loss 0.213887, acc 0.9375
2017-03-02T17:36:59.285097: step 3584, loss 0.22077, acc 0.890625
2017-03-02T17:36:59.357750: step 3585, loss 0.169989, acc 0.921875
2017-03-02T17:36:59.430150: step 3586, loss 0.316883, acc 0.890625
2017-03-02T17:36:59.501436: step 3587, loss 0.252699, acc 0.921875
2017-03-02T17:36:59.574626: step 3588, loss 0.427535, acc 0.875
2017-03-02T17:36:59.650223: step 3589, loss 0.203735, acc 0.921875
2017-03-02T17:36:59.741906: step 3590, loss 0.0994852, acc 0.96875
2017-03-02T17:36:59.814668: step 3591, loss 0.433984, acc 0.875
2017-03-02T17:36:59.878974: step 3592, loss 0.472003, acc 0.875
2017-03-02T17:36:59.947873: step 3593, loss 0.369919, acc 0.859375
2017-03-02T17:37:00.020703: step 3594, loss 0.386666, acc 0.90625
2017-03-02T17:37:00.090043: step 3595, loss 0.233032, acc 0.9375
2017-03-02T17:37:00.155299: step 3596, loss 0.364226, acc 0.890625
2017-03-02T17:37:00.222728: step 3597, loss 0.285962, acc 0.890625
2017-03-02T17:37:00.301205: step 3598, loss 0.326278, acc 0.859375
2017-03-02T17:37:00.380012: step 3599, loss 0.234028, acc 0.921875
2017-03-02T17:37:00.448163: step 3600, loss 0.132005, acc 0.9375

Evaluation:
2017-03-02T17:37:00.488873: step 3600, loss 0.995275, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3600

2017-03-02T17:37:00.945673: step 3601, loss 0.150702, acc 0.9375
2017-03-02T17:37:01.030928: step 3602, loss 0.195366, acc 0.90625
2017-03-02T17:37:01.107369: step 3603, loss 0.244218, acc 0.9375
2017-03-02T17:37:01.188625: step 3604, loss 0.208823, acc 0.921875
2017-03-02T17:37:01.268488: step 3605, loss 0.252391, acc 0.921875
2017-03-02T17:37:01.339377: step 3606, loss 0.253366, acc 0.921875
2017-03-02T17:37:01.415778: step 3607, loss 0.292855, acc 0.90625
2017-03-02T17:37:01.487523: step 3608, loss 0.271484, acc 0.921875
2017-03-02T17:37:01.553578: step 3609, loss 0.314523, acc 0.875
2017-03-02T17:37:01.625356: step 3610, loss 0.300022, acc 0.921875
2017-03-02T17:37:01.688319: step 3611, loss 0.198272, acc 0.90625
2017-03-02T17:37:01.758869: step 3612, loss 0.23767, acc 0.9375
2017-03-02T17:37:01.830851: step 3613, loss 0.346286, acc 0.875
2017-03-02T17:37:01.898893: step 3614, loss 0.14837, acc 0.953125
2017-03-02T17:37:01.960199: step 3615, loss 0.152667, acc 0.921875
2017-03-02T17:37:02.034957: step 3616, loss 0.365533, acc 0.859375
2017-03-02T17:37:02.105231: step 3617, loss 0.187, acc 0.921875
2017-03-02T17:37:02.173956: step 3618, loss 0.423618, acc 0.859375
2017-03-02T17:37:02.246723: step 3619, loss 0.204744, acc 0.9375
2017-03-02T17:37:02.320962: step 3620, loss 0.254142, acc 0.9375
2017-03-02T17:37:02.393210: step 3621, loss 0.186331, acc 0.90625
2017-03-02T17:37:02.465951: step 3622, loss 0.220875, acc 0.9375
2017-03-02T17:37:02.541575: step 3623, loss 0.26997, acc 0.921875
2017-03-02T17:37:02.612357: step 3624, loss 0.145723, acc 0.9375
2017-03-02T17:37:02.685751: step 3625, loss 0.394572, acc 0.90625
2017-03-02T17:37:02.752875: step 3626, loss 0.318009, acc 0.890625
2017-03-02T17:37:02.828486: step 3627, loss 0.370738, acc 0.90625
2017-03-02T17:37:02.900812: step 3628, loss 0.263898, acc 0.921875
2017-03-02T17:37:02.974562: step 3629, loss 0.207138, acc 0.9375
2017-03-02T17:37:03.042798: step 3630, loss 0.188816, acc 0.921875
2017-03-02T17:37:03.134415: step 3631, loss 0.340899, acc 0.890625
2017-03-02T17:37:03.210654: step 3632, loss 0.397263, acc 0.875
2017-03-02T17:37:03.281836: step 3633, loss 0.160422, acc 0.9375
2017-03-02T17:37:03.352519: step 3634, loss 0.288471, acc 0.921875
2017-03-02T17:37:03.426861: step 3635, loss 0.169378, acc 0.921875
2017-03-02T17:37:03.500799: step 3636, loss 0.157097, acc 0.953125
2017-03-02T17:37:03.572030: step 3637, loss 0.262652, acc 0.9375
2017-03-02T17:37:03.647868: step 3638, loss 0.236338, acc 0.90625
2017-03-02T17:37:03.728929: step 3639, loss 0.197566, acc 0.953125
2017-03-02T17:37:03.801507: step 3640, loss 0.200804, acc 0.90625
2017-03-02T17:37:03.869588: step 3641, loss 0.436021, acc 0.859375
2017-03-02T17:37:03.940274: step 3642, loss 0.264375, acc 0.875
2017-03-02T17:37:04.043844: step 3643, loss 0.167729, acc 0.921875
2017-03-02T17:37:04.117746: step 3644, loss 0.157914, acc 0.9375
2017-03-02T17:37:04.194417: step 3645, loss 0.335392, acc 0.875
2017-03-02T17:37:04.267092: step 3646, loss 0.195578, acc 0.921875
2017-03-02T17:37:04.334752: step 3647, loss 0.197552, acc 0.953125
2017-03-02T17:37:04.406805: step 3648, loss 0.233208, acc 0.90625
2017-03-02T17:37:04.484196: step 3649, loss 0.275901, acc 0.9375
2017-03-02T17:37:04.551295: step 3650, loss 0.286261, acc 0.90625
2017-03-02T17:37:04.625146: step 3651, loss 0.350275, acc 0.90625
2017-03-02T17:37:04.698963: step 3652, loss 0.198424, acc 0.9375
2017-03-02T17:37:04.767560: step 3653, loss 0.285855, acc 0.890625
2017-03-02T17:37:04.843690: step 3654, loss 0.455106, acc 0.84375
2017-03-02T17:37:04.909198: step 3655, loss 0.245553, acc 0.90625
2017-03-02T17:37:04.974931: step 3656, loss 0.216214, acc 0.9375
2017-03-02T17:37:05.047107: step 3657, loss 0.24562, acc 0.9375
2017-03-02T17:37:05.119962: step 3658, loss 0.145708, acc 0.953125
2017-03-02T17:37:05.196915: step 3659, loss 0.193245, acc 0.890625
2017-03-02T17:37:05.269994: step 3660, loss 0.220038, acc 0.9375
2017-03-02T17:37:05.342915: step 3661, loss 0.228821, acc 0.875
2017-03-02T17:37:05.409949: step 3662, loss 0.228848, acc 0.921875
2017-03-02T17:37:05.485743: step 3663, loss 0.203851, acc 0.921875
2017-03-02T17:37:05.558633: step 3664, loss 0.209312, acc 0.9375
2017-03-02T17:37:05.629299: step 3665, loss 0.26875, acc 0.890625
2017-03-02T17:37:05.705240: step 3666, loss 0.377386, acc 0.828125
2017-03-02T17:37:05.776773: step 3667, loss 0.230454, acc 0.921875
2017-03-02T17:37:05.848466: step 3668, loss 0.273237, acc 0.90625
2017-03-02T17:37:05.924333: step 3669, loss 0.370318, acc 0.859375
2017-03-02T17:37:05.997002: step 3670, loss 0.107432, acc 0.953125
2017-03-02T17:37:06.073363: step 3671, loss 0.21538, acc 0.9375
2017-03-02T17:37:06.139129: step 3672, loss 0.236994, acc 0.953125
2017-03-02T17:37:06.209116: step 3673, loss 0.296546, acc 0.921875
2017-03-02T17:37:06.287483: step 3674, loss 0.20835, acc 0.921875
2017-03-02T17:37:06.356074: step 3675, loss 0.347236, acc 0.859375
2017-03-02T17:37:06.429771: step 3676, loss 0.395036, acc 0.859375
2017-03-02T17:37:06.508458: step 3677, loss 0.403391, acc 0.859375
2017-03-02T17:37:06.576821: step 3678, loss 0.378809, acc 0.890625
2017-03-02T17:37:06.645070: step 3679, loss 0.370331, acc 0.90625
2017-03-02T17:37:06.716958: step 3680, loss 0.145942, acc 0.953125
2017-03-02T17:37:06.797084: step 3681, loss 0.15479, acc 0.9375
2017-03-02T17:37:06.869427: step 3682, loss 0.226897, acc 0.9375
2017-03-02T17:37:06.946602: step 3683, loss 0.359245, acc 0.859375
2017-03-02T17:37:07.022279: step 3684, loss 0.375754, acc 0.90625
2017-03-02T17:37:07.097382: step 3685, loss 0.383129, acc 0.84375
2017-03-02T17:37:07.167215: step 3686, loss 0.296225, acc 0.90625
2017-03-02T17:37:07.242039: step 3687, loss 0.331674, acc 0.875
2017-03-02T17:37:07.314443: step 3688, loss 0.273248, acc 0.875
2017-03-02T17:37:07.386636: step 3689, loss 0.372739, acc 0.859375
2017-03-02T17:37:07.460809: step 3690, loss 0.40764, acc 0.890625
2017-03-02T17:37:07.532980: step 3691, loss 0.334479, acc 0.890625
2017-03-02T17:37:07.605913: step 3692, loss 0.349089, acc 0.828125
2017-03-02T17:37:07.676382: step 3693, loss 0.279288, acc 0.859375
2017-03-02T17:37:07.748888: step 3694, loss 0.240678, acc 0.921875
2017-03-02T17:37:07.819350: step 3695, loss 0.249759, acc 0.890625
2017-03-02T17:37:07.898242: step 3696, loss 0.296036, acc 0.921875
2017-03-02T17:37:07.971080: step 3697, loss 0.240681, acc 0.890625
2017-03-02T17:37:08.036991: step 3698, loss 0.307429, acc 0.90625
2017-03-02T17:37:08.110119: step 3699, loss 0.22846, acc 0.890625
2017-03-02T17:37:08.187554: step 3700, loss 0.185028, acc 0.96875

Evaluation:
2017-03-02T17:37:08.214922: step 3700, loss 1.0214, acc 0.668349

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3700

2017-03-02T17:37:08.673543: step 3701, loss 0.257101, acc 0.90625
2017-03-02T17:37:08.751253: step 3702, loss 0.287738, acc 0.875
2017-03-02T17:37:08.825969: step 3703, loss 0.148033, acc 0.9375
2017-03-02T17:37:08.894989: step 3704, loss 0.273587, acc 0.90625
2017-03-02T17:37:08.979614: step 3705, loss 0.525113, acc 0.828125
2017-03-02T17:37:09.051837: step 3706, loss 0.242445, acc 0.921875
2017-03-02T17:37:09.134712: step 3707, loss 0.31836, acc 0.875
2017-03-02T17:37:09.210163: step 3708, loss 0.232265, acc 0.90625
2017-03-02T17:37:09.288447: step 3709, loss 0.153662, acc 0.96875
2017-03-02T17:37:09.359021: step 3710, loss 0.271261, acc 0.90625
2017-03-02T17:37:09.429076: step 3711, loss 0.110752, acc 0.96875
2017-03-02T17:37:09.501600: step 3712, loss 0.207969, acc 0.921875
2017-03-02T17:37:09.571375: step 3713, loss 0.198394, acc 0.921875
2017-03-02T17:37:09.643390: step 3714, loss 0.385768, acc 0.8125
2017-03-02T17:37:09.724581: step 3715, loss 0.277819, acc 0.921875
2017-03-02T17:37:09.791830: step 3716, loss 0.190279, acc 0.9375
2017-03-02T17:37:09.865984: step 3717, loss 0.161615, acc 0.921875
2017-03-02T17:37:09.934830: step 3718, loss 0.310648, acc 0.875
2017-03-02T17:37:10.010595: step 3719, loss 0.316474, acc 0.90625
2017-03-02T17:37:10.082362: step 3720, loss 0.244372, acc 0.921875
2017-03-02T17:37:10.157373: step 3721, loss 0.368306, acc 0.90625
2017-03-02T17:37:10.233453: step 3722, loss 0.269711, acc 0.921875
2017-03-02T17:37:10.302180: step 3723, loss 0.181969, acc 0.9375
2017-03-02T17:37:10.376862: step 3724, loss 0.0789116, acc 1
2017-03-02T17:37:10.446484: step 3725, loss 0.339559, acc 0.90625
2017-03-02T17:37:10.522005: step 3726, loss 0.164042, acc 0.96875
2017-03-02T17:37:10.591882: step 3727, loss 0.161102, acc 0.9375
2017-03-02T17:37:10.669067: step 3728, loss 0.186213, acc 0.9375
2017-03-02T17:37:10.743624: step 3729, loss 0.204223, acc 0.953125
2017-03-02T17:37:10.814240: step 3730, loss 0.164055, acc 0.953125
2017-03-02T17:37:10.895639: step 3731, loss 0.146356, acc 0.921875
2017-03-02T17:37:10.980055: step 3732, loss 0.33334, acc 0.890625
2017-03-02T17:37:11.048856: step 3733, loss 0.238872, acc 0.953125
2017-03-02T17:37:11.119670: step 3734, loss 0.286194, acc 0.921875
2017-03-02T17:37:11.190576: step 3735, loss 0.332982, acc 0.859375
2017-03-02T17:37:11.267488: step 3736, loss 0.310617, acc 0.890625
2017-03-02T17:37:11.341879: step 3737, loss 0.325214, acc 0.890625
2017-03-02T17:37:11.414939: step 3738, loss 0.123894, acc 0.953125
2017-03-02T17:37:11.486419: step 3739, loss 0.375453, acc 0.90625
2017-03-02T17:37:11.561085: step 3740, loss 0.131326, acc 0.9375
2017-03-02T17:37:11.632958: step 3741, loss 0.158866, acc 0.984375
2017-03-02T17:37:11.717683: step 3742, loss 0.221996, acc 0.921875
2017-03-02T17:37:11.791897: step 3743, loss 0.178227, acc 0.96875
2017-03-02T17:37:11.869418: step 3744, loss 0.155903, acc 0.953125
2017-03-02T17:37:11.944784: step 3745, loss 0.0426388, acc 1
2017-03-02T17:37:12.021025: step 3746, loss 0.246185, acc 0.953125
2017-03-02T17:37:12.088538: step 3747, loss 0.34681, acc 0.921875
2017-03-02T17:37:12.159250: step 3748, loss 0.238616, acc 0.921875
2017-03-02T17:37:12.248202: step 3749, loss 0.196428, acc 0.90625
2017-03-02T17:37:12.324537: step 3750, loss 0.218607, acc 0.90625
2017-03-02T17:37:12.390424: step 3751, loss 0.276187, acc 0.921875
2017-03-02T17:37:12.465416: step 3752, loss 0.183051, acc 0.9375
2017-03-02T17:37:12.541934: step 3753, loss 0.260049, acc 0.921875
2017-03-02T17:37:12.609368: step 3754, loss 0.104415, acc 0.921875
2017-03-02T17:37:12.679951: step 3755, loss 0.158903, acc 0.9375
2017-03-02T17:37:12.749375: step 3756, loss 0.135758, acc 0.96875
2017-03-02T17:37:12.818996: step 3757, loss 0.315293, acc 0.90625
2017-03-02T17:37:12.890899: step 3758, loss 0.165786, acc 0.96875
2017-03-02T17:37:12.961519: step 3759, loss 0.0818303, acc 0.984375
2017-03-02T17:37:13.026494: step 3760, loss 0.366209, acc 0.890625
2017-03-02T17:37:13.095977: step 3761, loss 0.198478, acc 0.921875
2017-03-02T17:37:13.170587: step 3762, loss 0.262854, acc 0.875
2017-03-02T17:37:13.237857: step 3763, loss 0.240068, acc 0.9375
2017-03-02T17:37:13.304630: step 3764, loss 0.242056, acc 0.9375
2017-03-02T17:37:13.380877: step 3765, loss 0.417881, acc 0.828125
2017-03-02T17:37:13.461921: step 3766, loss 0.286891, acc 0.9375
2017-03-02T17:37:13.529534: step 3767, loss 0.311699, acc 0.890625
2017-03-02T17:37:13.601797: step 3768, loss 0.17555, acc 0.953125
2017-03-02T17:37:13.673352: step 3769, loss 0.238238, acc 0.890625
2017-03-02T17:37:13.744329: step 3770, loss 0.338931, acc 0.90625
2017-03-02T17:37:13.819660: step 3771, loss 0.336459, acc 0.921875
2017-03-02T17:37:13.895458: step 3772, loss 0.22958, acc 0.921875
2017-03-02T17:37:14.006874: step 3773, loss 0.336503, acc 0.890625
2017-03-02T17:37:14.090033: step 3774, loss 0.176328, acc 0.9375
2017-03-02T17:37:14.163442: step 3775, loss 0.25329, acc 0.90625
2017-03-02T17:37:14.241594: step 3776, loss 0.153198, acc 0.953125
2017-03-02T17:37:14.322844: step 3777, loss 0.126149, acc 0.953125
2017-03-02T17:37:14.395263: step 3778, loss 0.17266, acc 0.9375
2017-03-02T17:37:14.473138: step 3779, loss 0.228309, acc 0.90625
2017-03-02T17:37:14.550322: step 3780, loss 0.210353, acc 0.921875
2017-03-02T17:37:14.632418: step 3781, loss 0.170971, acc 0.921875
2017-03-02T17:37:14.707152: step 3782, loss 0.332782, acc 0.875
2017-03-02T17:37:14.780434: step 3783, loss 0.382371, acc 0.875
2017-03-02T17:37:14.855175: step 3784, loss 0.160093, acc 0.953125
2017-03-02T17:37:14.931832: step 3785, loss 0.158069, acc 0.9375
2017-03-02T17:37:15.003280: step 3786, loss 0.217622, acc 0.9375
2017-03-02T17:37:15.077379: step 3787, loss 0.235476, acc 0.921875
2017-03-02T17:37:15.150902: step 3788, loss 0.229711, acc 0.921875
2017-03-02T17:37:15.221303: step 3789, loss 0.128072, acc 0.953125
2017-03-02T17:37:15.291409: step 3790, loss 0.270388, acc 0.921875
2017-03-02T17:37:15.363876: step 3791, loss 0.14988, acc 0.953125
2017-03-02T17:37:15.430975: step 3792, loss 0.252784, acc 0.890625
2017-03-02T17:37:15.492645: step 3793, loss 0.205016, acc 0.9375
2017-03-02T17:37:15.566193: step 3794, loss 0.292774, acc 0.890625
2017-03-02T17:37:15.648401: step 3795, loss 0.444895, acc 0.828125
2017-03-02T17:37:15.729480: step 3796, loss 0.170789, acc 0.9375
2017-03-02T17:37:15.796021: step 3797, loss 0.186072, acc 0.890625
2017-03-02T17:37:15.871043: step 3798, loss 0.245364, acc 0.90625
2017-03-02T17:37:15.939599: step 3799, loss 0.371028, acc 0.859375
2017-03-02T17:37:16.013356: step 3800, loss 0.298384, acc 0.875

Evaluation:
2017-03-02T17:37:16.041877: step 3800, loss 1.05346, acc 0.671233

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3800

2017-03-02T17:37:16.490384: step 3801, loss 0.218631, acc 0.9375
2017-03-02T17:37:16.555679: step 3802, loss 0.193189, acc 0.90625
2017-03-02T17:37:16.627331: step 3803, loss 0.0920579, acc 0.984375
2017-03-02T17:37:16.705506: step 3804, loss 0.220582, acc 0.9375
2017-03-02T17:37:16.778150: step 3805, loss 0.184762, acc 0.921875
2017-03-02T17:37:16.853121: step 3806, loss 0.19547, acc 0.9375
2017-03-02T17:37:16.933636: step 3807, loss 0.310015, acc 0.890625
2017-03-02T17:37:17.007618: step 3808, loss 0.176652, acc 0.953125
2017-03-02T17:37:17.079896: step 3809, loss 0.217045, acc 0.90625
2017-03-02T17:37:17.156077: step 3810, loss 0.508816, acc 0.828125
2017-03-02T17:37:17.223416: step 3811, loss 0.23223, acc 0.90625
2017-03-02T17:37:17.298567: step 3812, loss 0.188096, acc 0.96875
2017-03-02T17:37:17.375816: step 3813, loss 0.159596, acc 0.921875
2017-03-02T17:37:17.447620: step 3814, loss 0.0956185, acc 0.96875
2017-03-02T17:37:17.523132: step 3815, loss 0.316288, acc 0.90625
2017-03-02T17:37:17.595963: step 3816, loss 0.383839, acc 0.859375
2017-03-02T17:37:17.669794: step 3817, loss 0.164278, acc 0.921875
2017-03-02T17:37:17.741636: step 3818, loss 0.309141, acc 0.890625
2017-03-02T17:37:17.815316: step 3819, loss 0.171504, acc 0.9375
2017-03-02T17:37:17.889459: step 3820, loss 0.236887, acc 0.921875
2017-03-02T17:37:17.958453: step 3821, loss 0.191929, acc 0.921875
2017-03-02T17:37:18.037161: step 3822, loss 0.328322, acc 0.875
2017-03-02T17:37:18.102406: step 3823, loss 0.237133, acc 0.90625
2017-03-02T17:37:18.174202: step 3824, loss 0.317989, acc 0.90625
2017-03-02T17:37:18.255127: step 3825, loss 0.32886, acc 0.890625
2017-03-02T17:37:18.328168: step 3826, loss 0.168148, acc 0.921875
2017-03-02T17:37:18.402000: step 3827, loss 0.272744, acc 0.90625
2017-03-02T17:37:18.472405: step 3828, loss 0.276199, acc 0.875
2017-03-02T17:37:18.542701: step 3829, loss 0.327021, acc 0.875
2017-03-02T17:37:18.616183: step 3830, loss 0.109363, acc 0.984375
2017-03-02T17:37:18.689824: step 3831, loss 0.181432, acc 0.9375
2017-03-02T17:37:18.761478: step 3832, loss 0.125634, acc 0.9375
2017-03-02T17:37:18.830949: step 3833, loss 0.283912, acc 0.875
2017-03-02T17:37:18.905270: step 3834, loss 0.35484, acc 0.875
2017-03-02T17:37:18.977731: step 3835, loss 0.36036, acc 0.890625
2017-03-02T17:37:19.048832: step 3836, loss 0.279582, acc 0.921875
2017-03-02T17:37:19.122800: step 3837, loss 0.226457, acc 0.921875
2017-03-02T17:37:19.192069: step 3838, loss 0.187014, acc 0.953125
2017-03-02T17:37:19.267252: step 3839, loss 0.27925, acc 0.890625
2017-03-02T17:37:19.343708: step 3840, loss 0.258676, acc 0.890625
2017-03-02T17:37:19.416557: step 3841, loss 0.282746, acc 0.9375
2017-03-02T17:37:19.491593: step 3842, loss 0.342555, acc 0.875
2017-03-02T17:37:19.565627: step 3843, loss 0.206491, acc 0.9375
2017-03-02T17:37:19.639837: step 3844, loss 0.301081, acc 0.890625
2017-03-02T17:37:19.716691: step 3845, loss 0.324304, acc 0.890625
2017-03-02T17:37:19.798294: step 3846, loss 0.332751, acc 0.859375
2017-03-02T17:37:19.872980: step 3847, loss 0.243616, acc 0.90625
2017-03-02T17:37:19.948356: step 3848, loss 0.190539, acc 0.96875
2017-03-02T17:37:20.022368: step 3849, loss 0.163584, acc 0.9375
2017-03-02T17:37:20.093695: step 3850, loss 0.152643, acc 0.953125
2017-03-02T17:37:20.173189: step 3851, loss 0.146016, acc 0.953125
2017-03-02T17:37:20.240747: step 3852, loss 0.199131, acc 0.921875
2017-03-02T17:37:20.308122: step 3853, loss 0.239328, acc 0.921875
2017-03-02T17:37:20.387070: step 3854, loss 0.333463, acc 0.90625
2017-03-02T17:37:20.458059: step 3855, loss 0.177552, acc 0.9375
2017-03-02T17:37:20.555126: step 3856, loss 0.198942, acc 0.9375
2017-03-02T17:37:20.624670: step 3857, loss 0.264837, acc 0.921875
2017-03-02T17:37:20.700770: step 3858, loss 0.230005, acc 0.890625
2017-03-02T17:37:20.774066: step 3859, loss 0.237712, acc 0.90625
2017-03-02T17:37:20.842600: step 3860, loss 0.215716, acc 0.953125
2017-03-02T17:37:20.911684: step 3861, loss 0.260379, acc 0.921875
2017-03-02T17:37:20.985292: step 3862, loss 0.109914, acc 0.96875
2017-03-02T17:37:21.053444: step 3863, loss 0.304727, acc 0.8125
2017-03-02T17:37:21.125790: step 3864, loss 0.2365, acc 0.921875
2017-03-02T17:37:21.196918: step 3865, loss 0.139087, acc 0.9375
2017-03-02T17:37:21.268192: step 3866, loss 0.161413, acc 0.9375
2017-03-02T17:37:21.348387: step 3867, loss 0.139677, acc 0.921875
2017-03-02T17:37:21.427225: step 3868, loss 0.300887, acc 0.859375
2017-03-02T17:37:21.495071: step 3869, loss 0.142024, acc 0.9375
2017-03-02T17:37:21.567442: step 3870, loss 0.23756, acc 0.9375
2017-03-02T17:37:21.647309: step 3871, loss 0.128258, acc 0.921875
2017-03-02T17:37:21.717689: step 3872, loss 0.22492, acc 0.921875
2017-03-02T17:37:21.797516: step 3873, loss 0.220659, acc 0.890625
2017-03-02T17:37:21.875596: step 3874, loss 0.179808, acc 0.953125
2017-03-02T17:37:21.953220: step 3875, loss 0.285081, acc 0.890625
2017-03-02T17:37:22.026220: step 3876, loss 0.236194, acc 0.921875
2017-03-02T17:37:22.105012: step 3877, loss 0.195718, acc 0.921875
2017-03-02T17:37:22.171162: step 3878, loss 0.200317, acc 0.921875
2017-03-02T17:37:22.244093: step 3879, loss 0.258003, acc 0.890625
2017-03-02T17:37:22.321429: step 3880, loss 0.351313, acc 0.890625
2017-03-02T17:37:22.396765: step 3881, loss 0.207477, acc 0.921875
2017-03-02T17:37:22.481437: step 3882, loss 0.287779, acc 0.859375
2017-03-02T17:37:22.552895: step 3883, loss 0.193269, acc 0.9375
2017-03-02T17:37:22.622706: step 3884, loss 0.304662, acc 0.890625
2017-03-02T17:37:22.696879: step 3885, loss 0.19229, acc 0.921875
2017-03-02T17:37:22.771670: step 3886, loss 0.166313, acc 0.953125
2017-03-02T17:37:22.837404: step 3887, loss 0.13039, acc 0.96875
2017-03-02T17:37:22.911264: step 3888, loss 0.31892, acc 0.890625
2017-03-02T17:37:22.992982: step 3889, loss 0.392466, acc 0.859375
2017-03-02T17:37:23.069319: step 3890, loss 0.222247, acc 0.90625
2017-03-02T17:37:23.141177: step 3891, loss 0.504803, acc 0.875
2017-03-02T17:37:23.215163: step 3892, loss 0.148712, acc 0.96875
2017-03-02T17:37:23.288542: step 3893, loss 0.231067, acc 0.875
2017-03-02T17:37:23.358192: step 3894, loss 0.182671, acc 0.9375
2017-03-02T17:37:23.424987: step 3895, loss 0.276832, acc 0.890625
2017-03-02T17:37:23.491432: step 3896, loss 0.169838, acc 0.953125
2017-03-02T17:37:23.564048: step 3897, loss 0.353435, acc 0.9375
2017-03-02T17:37:23.638709: step 3898, loss 0.244364, acc 0.90625
2017-03-02T17:37:23.710452: step 3899, loss 0.149796, acc 0.96875
2017-03-02T17:37:23.779330: step 3900, loss 0.324388, acc 0.859375

Evaluation:
2017-03-02T17:37:23.811409: step 3900, loss 1.04454, acc 0.658976

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-3900

2017-03-02T17:37:24.272629: step 3901, loss 0.17489, acc 0.9375
2017-03-02T17:37:24.343466: step 3902, loss 0.349447, acc 0.875
2017-03-02T17:37:24.413415: step 3903, loss 0.219588, acc 0.890625
2017-03-02T17:37:24.484792: step 3904, loss 0.193464, acc 0.921875
2017-03-02T17:37:24.561147: step 3905, loss 0.351707, acc 0.84375
2017-03-02T17:37:24.639134: step 3906, loss 0.362643, acc 0.859375
2017-03-02T17:37:24.714318: step 3907, loss 0.226373, acc 0.921875
2017-03-02T17:37:24.786753: step 3908, loss 0.238503, acc 0.9375
2017-03-02T17:37:24.857453: step 3909, loss 0.318416, acc 0.859375
2017-03-02T17:37:24.931511: step 3910, loss 0.30697, acc 0.875
2017-03-02T17:37:25.005305: step 3911, loss 0.307325, acc 0.875
2017-03-02T17:37:25.079723: step 3912, loss 0.274591, acc 0.90625
2017-03-02T17:37:25.148484: step 3913, loss 0.227081, acc 0.875
2017-03-02T17:37:25.219437: step 3914, loss 0.301569, acc 0.9375
2017-03-02T17:37:25.290685: step 3915, loss 0.304209, acc 0.875
2017-03-02T17:37:25.362952: step 3916, loss 0.209002, acc 0.9375
2017-03-02T17:37:25.432825: step 3917, loss 0.174357, acc 0.953125
2017-03-02T17:37:25.504573: step 3918, loss 0.149804, acc 0.9375
2017-03-02T17:37:25.588429: step 3919, loss 0.254499, acc 0.890625
2017-03-02T17:37:25.656855: step 3920, loss 0.0576224, acc 1
2017-03-02T17:37:25.728820: step 3921, loss 0.34511, acc 0.890625
2017-03-02T17:37:25.804171: step 3922, loss 0.191427, acc 0.90625
2017-03-02T17:37:25.904119: step 3923, loss 0.34688, acc 0.890625
2017-03-02T17:37:25.996118: step 3924, loss 0.192403, acc 0.890625
2017-03-02T17:37:26.055528: step 3925, loss 0.357714, acc 0.875
2017-03-02T17:37:26.132922: step 3926, loss 0.192501, acc 0.921875
2017-03-02T17:37:26.212776: step 3927, loss 0.291127, acc 0.921875
2017-03-02T17:37:26.277569: step 3928, loss 0.14369, acc 0.984375
2017-03-02T17:37:26.387402: step 3929, loss 0.230393, acc 0.921875
2017-03-02T17:37:26.461878: step 3930, loss 0.147907, acc 0.921875
2017-03-02T17:37:26.534379: step 3931, loss 0.364594, acc 0.890625
2017-03-02T17:37:26.615309: step 3932, loss 0.177127, acc 0.953125
2017-03-02T17:37:26.684462: step 3933, loss 0.103359, acc 0.953125
2017-03-02T17:37:26.754761: step 3934, loss 0.198234, acc 0.921875
2017-03-02T17:37:26.827752: step 3935, loss 0.176567, acc 0.921875
2017-03-02T17:37:26.896959: step 3936, loss 0.190625, acc 0.953125
2017-03-02T17:37:26.970782: step 3937, loss 0.481178, acc 0.890625
2017-03-02T17:37:27.043739: step 3938, loss 0.172551, acc 0.9375
2017-03-02T17:37:27.109948: step 3939, loss 0.237082, acc 0.90625
2017-03-02T17:37:27.182537: step 3940, loss 0.204207, acc 0.9375
2017-03-02T17:37:27.268907: step 3941, loss 0.29036, acc 0.90625
2017-03-02T17:37:27.335621: step 3942, loss 0.203139, acc 0.953125
2017-03-02T17:37:27.409226: step 3943, loss 0.133652, acc 0.96875
2017-03-02T17:37:27.484115: step 3944, loss 0.118389, acc 0.96875
2017-03-02T17:37:27.561639: step 3945, loss 0.277589, acc 0.875
2017-03-02T17:37:27.632698: step 3946, loss 0.163193, acc 0.9375
2017-03-02T17:37:27.702422: step 3947, loss 0.35126, acc 0.921875
2017-03-02T17:37:27.769536: step 3948, loss 0.238715, acc 0.90625
2017-03-02T17:37:27.842443: step 3949, loss 0.277265, acc 0.921875
2017-03-02T17:37:27.919752: step 3950, loss 0.249601, acc 0.921875
2017-03-02T17:37:27.990534: step 3951, loss 0.262117, acc 0.90625
2017-03-02T17:37:28.062132: step 3952, loss 0.172172, acc 0.953125
2017-03-02T17:37:28.139057: step 3953, loss 0.492035, acc 0.859375
2017-03-02T17:37:28.207166: step 3954, loss 0.348971, acc 0.859375
2017-03-02T17:37:28.277920: step 3955, loss 0.269333, acc 0.90625
2017-03-02T17:37:28.357612: step 3956, loss 0.128195, acc 0.953125
2017-03-02T17:37:28.436444: step 3957, loss 0.219659, acc 0.9375
2017-03-02T17:37:28.506211: step 3958, loss 0.22547, acc 0.90625
2017-03-02T17:37:28.584901: step 3959, loss 0.179692, acc 0.953125
2017-03-02T17:37:28.662383: step 3960, loss 0.202548, acc 0.9375
2017-03-02T17:37:28.734504: step 3961, loss 0.17856, acc 0.921875
2017-03-02T17:37:28.809427: step 3962, loss 0.162012, acc 0.9375
2017-03-02T17:37:28.883154: step 3963, loss 0.199593, acc 0.921875
2017-03-02T17:37:28.962484: step 3964, loss 0.193922, acc 0.90625
2017-03-02T17:37:29.034655: step 3965, loss 0.173668, acc 0.921875
2017-03-02T17:37:29.108592: step 3966, loss 0.218186, acc 0.90625
2017-03-02T17:37:29.189683: step 3967, loss 0.301685, acc 0.921875
2017-03-02T17:37:29.266263: step 3968, loss 0.424431, acc 0.859375
2017-03-02T17:37:29.333268: step 3969, loss 0.335323, acc 0.90625
2017-03-02T17:37:29.405149: step 3970, loss 0.29333, acc 0.890625
2017-03-02T17:37:29.481715: step 3971, loss 0.1408, acc 0.96875
2017-03-02T17:37:29.549412: step 3972, loss 0.264285, acc 0.90625
2017-03-02T17:37:29.630367: step 3973, loss 0.229533, acc 0.9375
2017-03-02T17:37:29.705356: step 3974, loss 0.225464, acc 0.90625
2017-03-02T17:37:29.773581: step 3975, loss 0.163353, acc 0.9375
2017-03-02T17:37:29.853729: step 3976, loss 0.411474, acc 0.890625
2017-03-02T17:37:29.942128: step 3977, loss 0.241859, acc 0.90625
2017-03-02T17:37:30.009109: step 3978, loss 0.164522, acc 0.9375
2017-03-02T17:37:30.086515: step 3979, loss 0.367086, acc 0.875
2017-03-02T17:37:30.162797: step 3980, loss 0.215291, acc 0.921875
2017-03-02T17:37:30.232968: step 3981, loss 0.122398, acc 0.96875
2017-03-02T17:37:30.304323: step 3982, loss 0.0960916, acc 0.984375
2017-03-02T17:37:30.384770: step 3983, loss 0.137338, acc 0.984375
2017-03-02T17:37:30.450753: step 3984, loss 0.153453, acc 0.9375
2017-03-02T17:37:30.521381: step 3985, loss 0.471136, acc 0.8125
2017-03-02T17:37:30.599057: step 3986, loss 0.123952, acc 0.96875
2017-03-02T17:37:30.667383: step 3987, loss 0.471254, acc 0.890625
2017-03-02T17:37:30.738693: step 3988, loss 0.256225, acc 0.890625
2017-03-02T17:37:30.809013: step 3989, loss 0.254161, acc 0.90625
2017-03-02T17:37:30.880110: step 3990, loss 0.186364, acc 0.921875
2017-03-02T17:37:30.952491: step 3991, loss 0.21896, acc 0.953125
2017-03-02T17:37:31.029232: step 3992, loss 0.273647, acc 0.875
2017-03-02T17:37:31.100976: step 3993, loss 0.343477, acc 0.828125
2017-03-02T17:37:31.174158: step 3994, loss 0.226814, acc 0.875
2017-03-02T17:37:31.267752: step 3995, loss 0.234504, acc 0.921875
2017-03-02T17:37:31.345944: step 3996, loss 0.218196, acc 0.9375
2017-03-02T17:37:31.421948: step 3997, loss 0.189952, acc 0.953125
2017-03-02T17:37:31.490476: step 3998, loss 0.264004, acc 0.90625
2017-03-02T17:37:31.566162: step 3999, loss 0.239032, acc 0.921875
2017-03-02T17:37:31.639142: step 4000, loss 0.20389, acc 0.9375

Evaluation:
2017-03-02T17:37:31.666334: step 4000, loss 1.05514, acc 0.67628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4000

2017-03-02T17:37:32.121399: step 4001, loss 0.348997, acc 0.890625
2017-03-02T17:37:32.191017: step 4002, loss 0.227654, acc 0.953125
2017-03-02T17:37:32.267294: step 4003, loss 0.226667, acc 0.90625
2017-03-02T17:37:32.338271: step 4004, loss 0.258761, acc 0.921875
2017-03-02T17:37:32.407453: step 4005, loss 0.203968, acc 0.9375
2017-03-02T17:37:32.477749: step 4006, loss 0.246427, acc 0.921875
2017-03-02T17:37:32.549774: step 4007, loss 0.373069, acc 0.875
2017-03-02T17:37:32.629529: step 4008, loss 0.405408, acc 0.859375
2017-03-02T17:37:32.686400: step 4009, loss 0.181335, acc 0.953125
2017-03-02T17:37:32.758254: step 4010, loss 0.13803, acc 0.9375
2017-03-02T17:37:32.825459: step 4011, loss 0.129341, acc 0.96875
2017-03-02T17:37:32.902314: step 4012, loss 0.361065, acc 0.875
2017-03-02T17:37:32.980666: step 4013, loss 0.29222, acc 0.890625
2017-03-02T17:37:33.053092: step 4014, loss 0.249254, acc 0.921875
2017-03-02T17:37:33.129601: step 4015, loss 0.208102, acc 0.9375
2017-03-02T17:37:33.199868: step 4016, loss 0.370782, acc 0.859375
2017-03-02T17:37:33.271303: step 4017, loss 0.244191, acc 0.90625
2017-03-02T17:37:33.344887: step 4018, loss 0.357369, acc 0.875
2017-03-02T17:37:33.415734: step 4019, loss 0.156203, acc 0.9375
2017-03-02T17:37:33.512115: step 4020, loss 0.154958, acc 0.953125
2017-03-02T17:37:33.569772: step 4021, loss 0.111167, acc 0.953125
2017-03-02T17:37:33.638825: step 4022, loss 0.2968, acc 0.875
2017-03-02T17:37:33.710973: step 4023, loss 0.236558, acc 0.921875
2017-03-02T17:37:33.783617: step 4024, loss 0.335911, acc 0.890625
2017-03-02T17:37:33.857280: step 4025, loss 0.121514, acc 0.953125
2017-03-02T17:37:33.930569: step 4026, loss 0.263483, acc 0.859375
2017-03-02T17:37:34.004682: step 4027, loss 0.130398, acc 0.96875
2017-03-02T17:37:34.074844: step 4028, loss 0.18208, acc 0.921875
2017-03-02T17:37:34.144318: step 4029, loss 0.234583, acc 0.921875
2017-03-02T17:37:34.215363: step 4030, loss 0.193537, acc 0.9375
2017-03-02T17:37:34.304748: step 4031, loss 0.284479, acc 0.90625
2017-03-02T17:37:34.376932: step 4032, loss 0.137895, acc 0.921875
2017-03-02T17:37:34.447362: step 4033, loss 0.180592, acc 0.921875
2017-03-02T17:37:34.519491: step 4034, loss 0.177897, acc 0.9375
2017-03-02T17:37:34.593434: step 4035, loss 0.159837, acc 0.9375
2017-03-02T17:37:34.669714: step 4036, loss 0.190252, acc 0.90625
2017-03-02T17:37:34.747262: step 4037, loss 0.413427, acc 0.859375
2017-03-02T17:37:34.830594: step 4038, loss 0.210847, acc 0.9375
2017-03-02T17:37:34.905151: step 4039, loss 0.179007, acc 0.953125
2017-03-02T17:37:34.972834: step 4040, loss 0.37683, acc 0.921875
2017-03-02T17:37:35.044351: step 4041, loss 0.336801, acc 0.859375
2017-03-02T17:37:35.115455: step 4042, loss 0.281904, acc 0.890625
2017-03-02T17:37:35.189896: step 4043, loss 0.205424, acc 0.9375
2017-03-02T17:37:35.286956: step 4044, loss 0.179509, acc 0.921875
2017-03-02T17:37:35.359283: step 4045, loss 0.283734, acc 0.921875
2017-03-02T17:37:35.435178: step 4046, loss 0.349886, acc 0.875
2017-03-02T17:37:35.511150: step 4047, loss 0.219506, acc 0.921875
2017-03-02T17:37:35.586498: step 4048, loss 0.125049, acc 0.953125
2017-03-02T17:37:35.652801: step 4049, loss 0.123136, acc 0.9375
2017-03-02T17:37:35.726955: step 4050, loss 0.2173, acc 0.90625
2017-03-02T17:37:35.814754: step 4051, loss 0.210809, acc 0.921875
2017-03-02T17:37:35.881643: step 4052, loss 0.390598, acc 0.859375
2017-03-02T17:37:35.958183: step 4053, loss 0.252942, acc 0.90625
2017-03-02T17:37:36.031729: step 4054, loss 0.111528, acc 0.96875
2017-03-02T17:37:36.102026: step 4055, loss 0.313496, acc 0.859375
2017-03-02T17:37:36.172705: step 4056, loss 0.141034, acc 0.9375
2017-03-02T17:37:36.247683: step 4057, loss 0.137958, acc 0.953125
2017-03-02T17:37:36.320749: step 4058, loss 0.184802, acc 0.9375
2017-03-02T17:37:36.394094: step 4059, loss 0.214142, acc 0.921875
2017-03-02T17:37:36.473575: step 4060, loss 0.385231, acc 0.90625
2017-03-02T17:37:36.546230: step 4061, loss 0.258534, acc 0.875
2017-03-02T17:37:36.618721: step 4062, loss 0.241048, acc 0.890625
2017-03-02T17:37:36.698917: step 4063, loss 0.428093, acc 0.875
2017-03-02T17:37:36.770817: step 4064, loss 0.189255, acc 0.953125
2017-03-02T17:37:36.845267: step 4065, loss 0.166642, acc 0.96875
2017-03-02T17:37:36.919211: step 4066, loss 0.259903, acc 0.921875
2017-03-02T17:37:36.984163: step 4067, loss 0.538763, acc 0.890625
2017-03-02T17:37:37.064368: step 4068, loss 0.37348, acc 0.84375
2017-03-02T17:37:37.140270: step 4069, loss 0.286775, acc 0.890625
2017-03-02T17:37:37.210468: step 4070, loss 0.303677, acc 0.875
2017-03-02T17:37:37.294869: step 4071, loss 0.225087, acc 0.890625
2017-03-02T17:37:37.378693: step 4072, loss 0.165109, acc 0.953125
2017-03-02T17:37:37.447229: step 4073, loss 0.438775, acc 0.84375
2017-03-02T17:37:37.520573: step 4074, loss 0.296621, acc 0.890625
2017-03-02T17:37:37.590993: step 4075, loss 0.200988, acc 0.921875
2017-03-02T17:37:37.666065: step 4076, loss 0.361707, acc 0.859375
2017-03-02T17:37:37.744042: step 4077, loss 0.211277, acc 0.921875
2017-03-02T17:37:37.814936: step 4078, loss 0.264357, acc 0.921875
2017-03-02T17:37:37.888759: step 4079, loss 0.224971, acc 0.9375
2017-03-02T17:37:37.960330: step 4080, loss 0.33615, acc 0.90625
2017-03-02T17:37:38.038042: step 4081, loss 0.408401, acc 0.875
2017-03-02T17:37:38.118498: step 4082, loss 0.203709, acc 0.90625
2017-03-02T17:37:38.191933: step 4083, loss 0.271306, acc 0.90625
2017-03-02T17:37:38.261150: step 4084, loss 0.361113, acc 0.890625
2017-03-02T17:37:38.336182: step 4085, loss 0.261986, acc 0.890625
2017-03-02T17:37:38.407864: step 4086, loss 0.422882, acc 0.875
2017-03-02T17:37:38.483124: step 4087, loss 0.200574, acc 0.9375
2017-03-02T17:37:38.557201: step 4088, loss 0.165071, acc 0.984375
2017-03-02T17:37:38.633892: step 4089, loss 0.456275, acc 0.8125
2017-03-02T17:37:38.706118: step 4090, loss 0.230581, acc 0.921875
2017-03-02T17:37:38.792290: step 4091, loss 0.250713, acc 0.9375
2017-03-02T17:37:38.863842: step 4092, loss 0.178471, acc 0.9375
2017-03-02T17:37:38.927341: step 4093, loss 0.213354, acc 0.90625
2017-03-02T17:37:39.000916: step 4094, loss 0.213244, acc 0.90625
2017-03-02T17:37:39.077459: step 4095, loss 0.146586, acc 0.96875
2017-03-02T17:37:39.144477: step 4096, loss 0.238905, acc 0.90625
2017-03-02T17:37:39.206747: step 4097, loss 0.211563, acc 0.90625
2017-03-02T17:37:39.278184: step 4098, loss 0.210688, acc 0.9375
2017-03-02T17:37:39.356220: step 4099, loss 0.217499, acc 0.921875
2017-03-02T17:37:39.428212: step 4100, loss 0.214066, acc 0.9375

Evaluation:
2017-03-02T17:37:39.461778: step 4100, loss 1.07282, acc 0.678443

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4100

2017-03-02T17:37:39.941696: step 4101, loss 0.220097, acc 0.921875
2017-03-02T17:37:40.004975: step 4102, loss 0.175042, acc 0.9375
2017-03-02T17:37:40.075363: step 4103, loss 0.250195, acc 0.90625
2017-03-02T17:37:40.148229: step 4104, loss 0.244051, acc 0.90625
2017-03-02T17:37:40.216990: step 4105, loss 0.420851, acc 0.859375
2017-03-02T17:37:40.290890: step 4106, loss 0.316242, acc 0.921875
2017-03-02T17:37:40.360997: step 4107, loss 0.232481, acc 0.859375
2017-03-02T17:37:40.432248: step 4108, loss 0.138335, acc 0.9375
2017-03-02T17:37:40.504806: step 4109, loss 0.145066, acc 0.953125
2017-03-02T17:37:40.580024: step 4110, loss 0.239182, acc 0.90625
2017-03-02T17:37:40.652774: step 4111, loss 0.338492, acc 0.90625
2017-03-02T17:37:40.738050: step 4112, loss 0.201873, acc 0.890625
2017-03-02T17:37:40.812083: step 4113, loss 0.178818, acc 0.953125
2017-03-02T17:37:40.879483: step 4114, loss 0.350638, acc 0.875
2017-03-02T17:37:40.950853: step 4115, loss 0.302243, acc 0.890625
2017-03-02T17:37:41.019499: step 4116, loss 0.0926204, acc 1
2017-03-02T17:37:41.094508: step 4117, loss 0.0968177, acc 0.96875
2017-03-02T17:37:41.168666: step 4118, loss 0.35883, acc 0.890625
2017-03-02T17:37:41.239502: step 4119, loss 0.0962856, acc 0.953125
2017-03-02T17:37:41.312070: step 4120, loss 0.305126, acc 0.875
2017-03-02T17:37:41.382893: step 4121, loss 0.154854, acc 0.96875
2017-03-02T17:37:41.457081: step 4122, loss 0.259254, acc 0.90625
2017-03-02T17:37:41.540442: step 4123, loss 0.149084, acc 0.9375
2017-03-02T17:37:41.614996: step 4124, loss 0.400843, acc 0.875
2017-03-02T17:37:41.681570: step 4125, loss 0.179261, acc 0.953125
2017-03-02T17:37:41.756553: step 4126, loss 0.212948, acc 0.953125
2017-03-02T17:37:41.823934: step 4127, loss 0.0729436, acc 0.984375
2017-03-02T17:37:41.895575: step 4128, loss 0.23089, acc 0.921875
2017-03-02T17:37:41.975974: step 4129, loss 0.346363, acc 0.890625
2017-03-02T17:37:42.051248: step 4130, loss 0.117312, acc 0.953125
2017-03-02T17:37:42.123060: step 4131, loss 0.104193, acc 0.96875
2017-03-02T17:37:42.185549: step 4132, loss 0.189656, acc 0.953125
2017-03-02T17:37:42.252050: step 4133, loss 0.240872, acc 0.9375
2017-03-02T17:37:42.324660: step 4134, loss 0.355992, acc 0.890625
2017-03-02T17:37:42.399474: step 4135, loss 0.130675, acc 0.9375
2017-03-02T17:37:42.470571: step 4136, loss 0.0997657, acc 0.96875
2017-03-02T17:37:42.552399: step 4137, loss 0.272742, acc 0.921875
2017-03-02T17:37:42.624941: step 4138, loss 0.403416, acc 0.875
2017-03-02T17:37:42.692836: step 4139, loss 0.232438, acc 0.921875
2017-03-02T17:37:42.763333: step 4140, loss 0.247702, acc 0.90625
2017-03-02T17:37:42.838698: step 4141, loss 0.197391, acc 0.9375
2017-03-02T17:37:42.906597: step 4142, loss 0.122509, acc 0.96875
2017-03-02T17:37:42.980140: step 4143, loss 0.158969, acc 0.953125
2017-03-02T17:37:43.053836: step 4144, loss 0.165335, acc 0.921875
2017-03-02T17:37:43.123023: step 4145, loss 0.226363, acc 0.890625
2017-03-02T17:37:43.193507: step 4146, loss 0.196346, acc 0.9375
2017-03-02T17:37:43.277800: step 4147, loss 0.20809, acc 0.953125
2017-03-02T17:37:43.366808: step 4148, loss 0.145859, acc 0.9375
2017-03-02T17:37:43.440905: step 4149, loss 0.0921153, acc 0.984375
2017-03-02T17:37:43.506445: step 4150, loss 0.309395, acc 0.84375
2017-03-02T17:37:43.579065: step 4151, loss 0.232961, acc 0.90625
2017-03-02T17:37:43.650896: step 4152, loss 0.218052, acc 0.90625
2017-03-02T17:37:43.719757: step 4153, loss 0.16692, acc 0.9375
2017-03-02T17:37:43.794213: step 4154, loss 0.220847, acc 0.9375
2017-03-02T17:37:43.867224: step 4155, loss 0.302544, acc 0.90625
2017-03-02T17:37:43.939867: step 4156, loss 0.181411, acc 0.9375
2017-03-02T17:37:44.009154: step 4157, loss 0.326712, acc 0.90625
2017-03-02T17:37:44.080543: step 4158, loss 0.292061, acc 0.875
2017-03-02T17:37:44.155881: step 4159, loss 0.346625, acc 0.890625
2017-03-02T17:37:44.226971: step 4160, loss 0.0707764, acc 0.984375
2017-03-02T17:37:44.302211: step 4161, loss 0.13717, acc 0.9375
2017-03-02T17:37:44.387561: step 4162, loss 0.178217, acc 0.90625
2017-03-02T17:37:44.458465: step 4163, loss 0.161961, acc 0.921875
2017-03-02T17:37:44.529027: step 4164, loss 0.181051, acc 0.953125
2017-03-02T17:37:44.603762: step 4165, loss 0.210902, acc 0.90625
2017-03-02T17:37:44.679469: step 4166, loss 0.127906, acc 0.96875
2017-03-02T17:37:44.751974: step 4167, loss 0.329328, acc 0.875
2017-03-02T17:37:44.823995: step 4168, loss 0.275654, acc 0.921875
2017-03-02T17:37:44.896875: step 4169, loss 0.0922355, acc 0.953125
2017-03-02T17:37:44.973244: step 4170, loss 0.315967, acc 0.859375
2017-03-02T17:37:45.043470: step 4171, loss 0.33444, acc 0.84375
2017-03-02T17:37:45.115648: step 4172, loss 0.287434, acc 0.859375
2017-03-02T17:37:45.186349: step 4173, loss 0.271924, acc 0.875
2017-03-02T17:37:45.262052: step 4174, loss 0.208917, acc 0.921875
2017-03-02T17:37:45.333489: step 4175, loss 0.167604, acc 0.953125
2017-03-02T17:37:45.417949: step 4176, loss 0.260771, acc 0.890625
2017-03-02T17:37:45.494956: step 4177, loss 0.185581, acc 0.96875
2017-03-02T17:37:45.571515: step 4178, loss 0.224702, acc 0.921875
2017-03-02T17:37:45.644447: step 4179, loss 0.167536, acc 0.953125
2017-03-02T17:37:45.718478: step 4180, loss 0.260399, acc 0.890625
2017-03-02T17:37:45.794667: step 4181, loss 0.202132, acc 0.953125
2017-03-02T17:37:45.871631: step 4182, loss 0.356539, acc 0.859375
2017-03-02T17:37:45.939418: step 4183, loss 0.373385, acc 0.875
2017-03-02T17:37:46.013715: step 4184, loss 0.207776, acc 0.90625
2017-03-02T17:37:46.086639: step 4185, loss 0.197982, acc 0.9375
2017-03-02T17:37:46.154569: step 4186, loss 0.231839, acc 0.90625
2017-03-02T17:37:46.233227: step 4187, loss 0.270423, acc 0.921875
2017-03-02T17:37:46.296835: step 4188, loss 0.286873, acc 0.90625
2017-03-02T17:37:46.364944: step 4189, loss 0.20909, acc 0.90625
2017-03-02T17:37:46.440658: step 4190, loss 0.157656, acc 0.953125
2017-03-02T17:37:46.514970: step 4191, loss 0.129081, acc 0.953125
2017-03-02T17:37:46.586900: step 4192, loss 0.22342, acc 0.90625
2017-03-02T17:37:46.659875: step 4193, loss 0.12587, acc 0.953125
2017-03-02T17:37:46.734482: step 4194, loss 0.230193, acc 0.921875
2017-03-02T17:37:46.801197: step 4195, loss 0.224092, acc 0.90625
2017-03-02T17:37:46.869755: step 4196, loss 0.283998, acc 0.90625
2017-03-02T17:37:46.941212: step 4197, loss 0.135536, acc 0.984375
2017-03-02T17:37:47.011553: step 4198, loss 0.15851, acc 0.921875
2017-03-02T17:37:47.083503: step 4199, loss 0.216559, acc 0.90625
2017-03-02T17:37:47.152745: step 4200, loss 0.152377, acc 0.953125

Evaluation:
2017-03-02T17:37:47.178734: step 4200, loss 1.06746, acc 0.678443

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4200

2017-03-02T17:37:47.617072: step 4201, loss 0.101282, acc 0.984375
2017-03-02T17:37:47.679098: step 4202, loss 0.25046, acc 0.9375
2017-03-02T17:37:47.750101: step 4203, loss 0.173183, acc 0.921875
2017-03-02T17:37:47.814723: step 4204, loss 0.22331, acc 0.890625
2017-03-02T17:37:47.884433: step 4205, loss 0.248979, acc 0.90625
2017-03-02T17:37:47.955645: step 4206, loss 0.237873, acc 0.890625
2017-03-02T17:37:48.029097: step 4207, loss 0.416068, acc 0.9375
2017-03-02T17:37:48.094495: step 4208, loss 0.0966364, acc 0.96875
2017-03-02T17:37:48.163105: step 4209, loss 0.260633, acc 0.921875
2017-03-02T17:37:48.231836: step 4210, loss 0.2584, acc 0.890625
2017-03-02T17:37:48.301888: step 4211, loss 0.182325, acc 0.9375
2017-03-02T17:37:48.373872: step 4212, loss 0.212986, acc 0.9375
2017-03-02T17:37:48.441529: step 4213, loss 0.492322, acc 0.84375
2017-03-02T17:37:48.515692: step 4214, loss 0.345821, acc 0.890625
2017-03-02T17:37:48.581988: step 4215, loss 0.165389, acc 0.953125
2017-03-02T17:37:48.647837: step 4216, loss 0.165653, acc 0.9375
2017-03-02T17:37:48.720768: step 4217, loss 0.240421, acc 0.90625
2017-03-02T17:37:48.786650: step 4218, loss 0.14101, acc 0.96875
2017-03-02T17:37:48.855562: step 4219, loss 0.289999, acc 0.875
2017-03-02T17:37:48.926631: step 4220, loss 0.345182, acc 0.875
2017-03-02T17:37:48.999864: step 4221, loss 0.120558, acc 0.9375
2017-03-02T17:37:49.067023: step 4222, loss 0.163544, acc 0.984375
2017-03-02T17:37:49.130219: step 4223, loss 0.376707, acc 0.859375
2017-03-02T17:37:49.196407: step 4224, loss 0.131248, acc 0.96875
2017-03-02T17:37:49.265948: step 4225, loss 0.34094, acc 0.875
2017-03-02T17:37:49.336178: step 4226, loss 0.365447, acc 0.890625
2017-03-02T17:37:49.394671: step 4227, loss 0.232586, acc 0.90625
2017-03-02T17:37:49.465764: step 4228, loss 0.229049, acc 0.90625
2017-03-02T17:37:49.523989: step 4229, loss 0.16504, acc 0.9375
2017-03-02T17:37:49.583909: step 4230, loss 0.130613, acc 0.984375
2017-03-02T17:37:49.651692: step 4231, loss 0.207322, acc 0.921875
2017-03-02T17:37:49.722901: step 4232, loss 0.360136, acc 0.828125
2017-03-02T17:37:49.781634: step 4233, loss 0.391686, acc 0.890625
2017-03-02T17:37:49.852825: step 4234, loss 0.308864, acc 0.875
2017-03-02T17:37:49.923870: step 4235, loss 0.325338, acc 0.859375
2017-03-02T17:37:50.002441: step 4236, loss 0.246948, acc 0.890625
2017-03-02T17:37:50.073752: step 4237, loss 0.276098, acc 0.890625
2017-03-02T17:37:50.140997: step 4238, loss 0.212154, acc 0.921875
2017-03-02T17:37:50.206552: step 4239, loss 0.245347, acc 0.90625
2017-03-02T17:37:50.272566: step 4240, loss 0.128949, acc 0.96875
2017-03-02T17:37:50.342700: step 4241, loss 0.336655, acc 0.890625
2017-03-02T17:37:50.414522: step 4242, loss 0.265522, acc 0.90625
2017-03-02T17:37:50.478043: step 4243, loss 0.0779301, acc 0.96875
2017-03-02T17:37:50.542225: step 4244, loss 0.175724, acc 0.953125
2017-03-02T17:37:50.604417: step 4245, loss 0.178901, acc 0.953125
2017-03-02T17:37:50.670781: step 4246, loss 0.186694, acc 0.96875
2017-03-02T17:37:50.741887: step 4247, loss 0.276543, acc 0.890625
2017-03-02T17:37:50.807577: step 4248, loss 0.226173, acc 0.9375
2017-03-02T17:37:50.887225: step 4249, loss 0.243031, acc 0.90625
2017-03-02T17:37:50.951218: step 4250, loss 0.181822, acc 0.953125
2017-03-02T17:37:51.018068: step 4251, loss 0.248714, acc 0.890625
2017-03-02T17:37:51.083740: step 4252, loss 0.128226, acc 0.96875
2017-03-02T17:37:51.150337: step 4253, loss 0.19306, acc 0.90625
2017-03-02T17:37:51.220527: step 4254, loss 0.33512, acc 0.921875
2017-03-02T17:37:51.281692: step 4255, loss 0.178286, acc 0.953125
2017-03-02T17:37:51.348372: step 4256, loss 0.191248, acc 0.90625
2017-03-02T17:37:51.418803: step 4257, loss 0.366434, acc 0.859375
2017-03-02T17:37:51.478549: step 4258, loss 0.256622, acc 0.9375
2017-03-02T17:37:51.545389: step 4259, loss 0.241468, acc 0.9375
2017-03-02T17:37:51.616059: step 4260, loss 0.155799, acc 0.9375
2017-03-02T17:37:51.688997: step 4261, loss 0.244201, acc 0.9375
2017-03-02T17:37:51.756178: step 4262, loss 0.151773, acc 0.9375
2017-03-02T17:37:51.828056: step 4263, loss 0.25687, acc 0.890625
2017-03-02T17:37:51.893586: step 4264, loss 0.204492, acc 0.921875
2017-03-02T17:37:51.960772: step 4265, loss 0.19789, acc 0.890625
2017-03-02T17:37:52.028295: step 4266, loss 0.266386, acc 0.890625
2017-03-02T17:37:52.093383: step 4267, loss 0.289729, acc 0.875
2017-03-02T17:37:52.163033: step 4268, loss 0.277145, acc 0.890625
2017-03-02T17:37:52.231893: step 4269, loss 0.260209, acc 0.921875
2017-03-02T17:37:52.292958: step 4270, loss 0.136651, acc 0.96875
2017-03-02T17:37:52.355920: step 4271, loss 0.368767, acc 0.890625
2017-03-02T17:37:52.423131: step 4272, loss 0.320901, acc 0.921875
2017-03-02T17:37:52.492872: step 4273, loss 0.316486, acc 0.859375
2017-03-02T17:37:52.563911: step 4274, loss 0.270133, acc 0.90625
2017-03-02T17:37:52.635106: step 4275, loss 0.156631, acc 0.96875
2017-03-02T17:37:52.704241: step 4276, loss 0.0999288, acc 0.9375
2017-03-02T17:37:52.770470: step 4277, loss 0.377241, acc 0.890625
2017-03-02T17:37:52.840201: step 4278, loss 0.111723, acc 0.96875
2017-03-02T17:37:52.909963: step 4279, loss 0.313737, acc 0.890625
2017-03-02T17:37:52.980854: step 4280, loss 0.19413, acc 0.90625
2017-03-02T17:37:53.045339: step 4281, loss 0.342461, acc 0.890625
2017-03-02T17:37:53.111383: step 4282, loss 0.312903, acc 0.890625
2017-03-02T17:37:53.184579: step 4283, loss 0.143176, acc 0.9375
2017-03-02T17:37:53.246911: step 4284, loss 0.295756, acc 0.875
2017-03-02T17:37:53.308107: step 4285, loss 0.208026, acc 0.921875
2017-03-02T17:37:53.377556: step 4286, loss 0.164881, acc 0.953125
2017-03-02T17:37:53.442230: step 4287, loss 0.300738, acc 0.890625
2017-03-02T17:37:53.508032: step 4288, loss 0.217676, acc 0.9375
2017-03-02T17:37:53.579834: step 4289, loss 0.246361, acc 0.921875
2017-03-02T17:37:53.641942: step 4290, loss 0.36631, acc 0.828125
2017-03-02T17:37:53.707503: step 4291, loss 0.222754, acc 0.9375
2017-03-02T17:37:53.775887: step 4292, loss 0.28797, acc 0.890625
2017-03-02T17:37:53.842268: step 4293, loss 0.276257, acc 0.890625
2017-03-02T17:37:53.909413: step 4294, loss 0.423164, acc 0.859375
2017-03-02T17:37:53.965286: step 4295, loss 0.159282, acc 0.9375
2017-03-02T17:37:54.030262: step 4296, loss 0.308635, acc 0.890625
2017-03-02T17:37:54.094237: step 4297, loss 0.28337, acc 0.875
2017-03-02T17:37:54.165631: step 4298, loss 0.134024, acc 0.9375
2017-03-02T17:37:54.233324: step 4299, loss 0.272132, acc 0.90625
2017-03-02T17:37:54.302528: step 4300, loss 0.186878, acc 0.953125

Evaluation:
2017-03-02T17:37:54.329249: step 4300, loss 1.06599, acc 0.663302

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4300

2017-03-02T17:37:58.099136: step 4301, loss 0.315119, acc 0.890625
2017-03-02T17:37:58.169745: step 4302, loss 0.404297, acc 0.84375
2017-03-02T17:37:58.236869: step 4303, loss 0.242991, acc 0.921875
2017-03-02T17:37:58.303158: step 4304, loss 0.266289, acc 0.921875
2017-03-02T17:37:58.365562: step 4305, loss 0.192267, acc 0.921875
2017-03-02T17:37:58.429577: step 4306, loss 0.32853, acc 0.859375
2017-03-02T17:37:58.494924: step 4307, loss 0.232378, acc 0.921875
2017-03-02T17:37:58.563005: step 4308, loss 0.316759, acc 0.890625
2017-03-02T17:37:58.632997: step 4309, loss 0.344863, acc 0.875
2017-03-02T17:37:58.699467: step 4310, loss 0.298675, acc 0.90625
2017-03-02T17:37:58.768556: step 4311, loss 0.399847, acc 0.84375
2017-03-02T17:37:58.834920: step 4312, loss 0.289373, acc 1
2017-03-02T17:37:58.909086: step 4313, loss 0.0987068, acc 0.96875
2017-03-02T17:37:58.970059: step 4314, loss 0.246776, acc 0.90625
2017-03-02T17:37:59.034631: step 4315, loss 0.202567, acc 0.921875
2017-03-02T17:37:59.105200: step 4316, loss 0.155715, acc 0.9375
2017-03-02T17:37:59.168553: step 4317, loss 0.224379, acc 0.9375
2017-03-02T17:37:59.236096: step 4318, loss 0.121506, acc 0.984375
2017-03-02T17:37:59.299580: step 4319, loss 0.165863, acc 0.921875
2017-03-02T17:37:59.365260: step 4320, loss 0.407116, acc 0.859375
2017-03-02T17:37:59.430787: step 4321, loss 0.359225, acc 0.90625
2017-03-02T17:37:59.501175: step 4322, loss 0.132111, acc 0.953125
2017-03-02T17:37:59.569982: step 4323, loss 0.178794, acc 0.9375
2017-03-02T17:37:59.644048: step 4324, loss 0.078921, acc 0.96875
2017-03-02T17:37:59.709110: step 4325, loss 0.276588, acc 0.890625
2017-03-02T17:37:59.776794: step 4326, loss 0.198761, acc 0.90625
2017-03-02T17:37:59.846246: step 4327, loss 0.142303, acc 0.90625
2017-03-02T17:37:59.912851: step 4328, loss 0.139133, acc 0.953125
2017-03-02T17:37:59.980644: step 4329, loss 0.212701, acc 0.90625
2017-03-02T17:38:00.050990: step 4330, loss 0.191141, acc 0.953125
2017-03-02T17:38:00.115824: step 4331, loss 0.100523, acc 0.96875
2017-03-02T17:38:00.184514: step 4332, loss 0.15786, acc 0.921875
2017-03-02T17:38:00.251723: step 4333, loss 0.323969, acc 0.921875
2017-03-02T17:38:00.318559: step 4334, loss 0.310094, acc 0.890625
2017-03-02T17:38:00.388601: step 4335, loss 0.162837, acc 0.9375
2017-03-02T17:38:00.455509: step 4336, loss 0.332985, acc 0.875
2017-03-02T17:38:00.523355: step 4337, loss 0.228421, acc 0.921875
2017-03-02T17:38:00.588997: step 4338, loss 0.202846, acc 0.9375
2017-03-02T17:38:00.662166: step 4339, loss 0.237817, acc 0.90625
2017-03-02T17:38:00.735124: step 4340, loss 0.268174, acc 0.9375
2017-03-02T17:38:00.802841: step 4341, loss 0.26208, acc 0.875
2017-03-02T17:38:00.871947: step 4342, loss 0.210751, acc 0.90625
2017-03-02T17:38:00.942008: step 4343, loss 0.118587, acc 0.953125
2017-03-02T17:38:01.007464: step 4344, loss 0.0728534, acc 0.984375
2017-03-02T17:38:01.075104: step 4345, loss 0.318506, acc 0.890625
2017-03-02T17:38:01.145270: step 4346, loss 0.0849703, acc 0.96875
2017-03-02T17:38:01.215398: step 4347, loss 0.189363, acc 0.90625
2017-03-02T17:38:01.285848: step 4348, loss 0.148823, acc 0.953125
2017-03-02T17:38:01.359539: step 4349, loss 0.196729, acc 0.953125
2017-03-02T17:38:01.428569: step 4350, loss 0.0820498, acc 0.96875
2017-03-02T17:38:01.495353: step 4351, loss 0.178134, acc 0.953125
2017-03-02T17:38:01.573248: step 4352, loss 0.177545, acc 0.921875
2017-03-02T17:38:01.641425: step 4353, loss 0.186438, acc 0.921875
2017-03-02T17:38:01.705219: step 4354, loss 0.21978, acc 0.90625
2017-03-02T17:38:01.771249: step 4355, loss 0.167539, acc 0.921875
2017-03-02T17:38:01.840362: step 4356, loss 0.160598, acc 0.921875
2017-03-02T17:38:01.910875: step 4357, loss 0.197413, acc 0.921875
2017-03-02T17:38:01.979680: step 4358, loss 0.380631, acc 0.921875
2017-03-02T17:38:02.046126: step 4359, loss 0.162707, acc 0.9375
2017-03-02T17:38:02.115322: step 4360, loss 0.289118, acc 0.890625
2017-03-02T17:38:02.187468: step 4361, loss 0.298038, acc 0.875
2017-03-02T17:38:02.256235: step 4362, loss 0.203142, acc 0.921875
2017-03-02T17:38:02.324636: step 4363, loss 0.386429, acc 0.890625
2017-03-02T17:38:02.393547: step 4364, loss 0.135649, acc 0.96875
2017-03-02T17:38:02.467631: step 4365, loss 0.190164, acc 0.953125
2017-03-02T17:38:02.535419: step 4366, loss 0.350599, acc 0.890625
2017-03-02T17:38:02.605330: step 4367, loss 0.228193, acc 0.90625
2017-03-02T17:38:02.678485: step 4368, loss 0.167342, acc 0.953125
2017-03-02T17:38:02.749977: step 4369, loss 0.258266, acc 0.90625
2017-03-02T17:38:02.821493: step 4370, loss 0.229196, acc 0.921875
2017-03-02T17:38:02.898297: step 4371, loss 0.199908, acc 0.9375
2017-03-02T17:38:02.962596: step 4372, loss 0.31649, acc 0.875
2017-03-02T17:38:03.033097: step 4373, loss 0.335564, acc 0.890625
2017-03-02T17:38:03.101989: step 4374, loss 0.168403, acc 0.921875
2017-03-02T17:38:03.171827: step 4375, loss 0.2249, acc 0.921875
2017-03-02T17:38:03.244287: step 4376, loss 0.20034, acc 0.875
2017-03-02T17:38:03.311150: step 4377, loss 0.202262, acc 0.9375
2017-03-02T17:38:03.367427: step 4378, loss 0.0859879, acc 0.984375
2017-03-02T17:38:03.439267: step 4379, loss 0.290834, acc 0.921875
2017-03-02T17:38:03.510362: step 4380, loss 0.22478, acc 0.90625
2017-03-02T17:38:03.574698: step 4381, loss 0.275015, acc 0.921875
2017-03-02T17:38:03.642863: step 4382, loss 0.176008, acc 0.953125
2017-03-02T17:38:03.708527: step 4383, loss 0.234811, acc 0.9375
2017-03-02T17:38:03.775750: step 4384, loss 0.140855, acc 0.90625
2017-03-02T17:38:03.851031: step 4385, loss 0.273822, acc 0.859375
2017-03-02T17:38:03.917083: step 4386, loss 0.15234, acc 0.921875
2017-03-02T17:38:03.988236: step 4387, loss 0.192084, acc 0.96875
2017-03-02T17:38:04.051176: step 4388, loss 0.207002, acc 0.890625
2017-03-02T17:38:04.116805: step 4389, loss 0.19274, acc 0.90625
2017-03-02T17:38:04.181573: step 4390, loss 0.255876, acc 0.890625
2017-03-02T17:38:04.251547: step 4391, loss 0.434769, acc 0.84375
2017-03-02T17:38:04.317947: step 4392, loss 0.301506, acc 0.875
2017-03-02T17:38:04.387744: step 4393, loss 0.129734, acc 0.953125
2017-03-02T17:38:04.460474: step 4394, loss 0.244529, acc 0.90625
2017-03-02T17:38:04.517450: step 4395, loss 0.29402, acc 0.90625
2017-03-02T17:38:04.583355: step 4396, loss 0.144085, acc 0.9375
2017-03-02T17:38:04.650300: step 4397, loss 0.206271, acc 0.90625
2017-03-02T17:38:04.716116: step 4398, loss 0.194182, acc 0.9375
2017-03-02T17:38:04.768456: step 4399, loss 0.234244, acc 0.90625
2017-03-02T17:38:04.821527: step 4400, loss 0.212821, acc 0.890625

Evaluation:
2017-03-02T17:38:04.849697: step 4400, loss 1.08702, acc 0.67628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4400

2017-03-02T17:38:05.300500: step 4401, loss 0.215495, acc 0.921875
2017-03-02T17:38:05.369319: step 4402, loss 0.219057, acc 0.921875
2017-03-02T17:38:05.439779: step 4403, loss 0.33475, acc 0.9375
2017-03-02T17:38:05.508288: step 4404, loss 0.198664, acc 0.921875
2017-03-02T17:38:05.576841: step 4405, loss 0.152312, acc 0.9375
2017-03-02T17:38:05.659529: step 4406, loss 0.532985, acc 0.84375
2017-03-02T17:38:05.727474: step 4407, loss 0.20121, acc 0.875
2017-03-02T17:38:05.794459: step 4408, loss 0.152733, acc 0.9375
2017-03-02T17:38:05.865491: step 4409, loss 0.364892, acc 0.84375
2017-03-02T17:38:05.932564: step 4410, loss 0.142803, acc 0.921875
2017-03-02T17:38:06.002158: step 4411, loss 0.127153, acc 0.953125
2017-03-02T17:38:06.069201: step 4412, loss 0.361868, acc 0.859375
2017-03-02T17:38:06.139566: step 4413, loss 0.1452, acc 0.921875
2017-03-02T17:38:06.209667: step 4414, loss 0.18214, acc 0.9375
2017-03-02T17:38:06.279106: step 4415, loss 0.325091, acc 0.859375
2017-03-02T17:38:06.350369: step 4416, loss 0.293, acc 0.921875
2017-03-02T17:38:06.417562: step 4417, loss 0.318688, acc 0.890625
2017-03-02T17:38:06.484614: step 4418, loss 0.367375, acc 0.90625
2017-03-02T17:38:06.554874: step 4419, loss 0.252482, acc 0.921875
2017-03-02T17:38:06.628074: step 4420, loss 0.384324, acc 0.90625
2017-03-02T17:38:06.692161: step 4421, loss 0.201554, acc 0.921875
2017-03-02T17:38:06.759321: step 4422, loss 0.162504, acc 0.9375
2017-03-02T17:38:06.831815: step 4423, loss 0.310231, acc 0.875
2017-03-02T17:38:06.899737: step 4424, loss 0.167658, acc 0.921875
2017-03-02T17:38:06.966813: step 4425, loss 0.330701, acc 0.90625
2017-03-02T17:38:07.033176: step 4426, loss 0.179988, acc 0.9375
2017-03-02T17:38:07.101513: step 4427, loss 0.187176, acc 0.921875
2017-03-02T17:38:07.162847: step 4428, loss 0.30533, acc 0.84375
2017-03-02T17:38:07.229845: step 4429, loss 0.260553, acc 0.890625
2017-03-02T17:38:07.301419: step 4430, loss 0.158065, acc 0.9375
2017-03-02T17:38:07.372865: step 4431, loss 0.200034, acc 0.9375
2017-03-02T17:38:07.443680: step 4432, loss 0.348265, acc 0.859375
2017-03-02T17:38:07.514845: step 4433, loss 0.133499, acc 0.9375
2017-03-02T17:38:07.586087: step 4434, loss 0.248484, acc 0.890625
2017-03-02T17:38:07.649204: step 4435, loss 0.390146, acc 0.890625
2017-03-02T17:38:07.722020: step 4436, loss 0.208536, acc 0.90625
2017-03-02T17:38:07.789232: step 4437, loss 0.19522, acc 0.921875
2017-03-02T17:38:07.858467: step 4438, loss 0.169327, acc 0.90625
2017-03-02T17:38:07.930571: step 4439, loss 0.191858, acc 0.9375
2017-03-02T17:38:08.002289: step 4440, loss 0.109505, acc 0.984375
2017-03-02T17:38:08.071952: step 4441, loss 0.279615, acc 0.890625
2017-03-02T17:38:08.140963: step 4442, loss 0.320787, acc 0.921875
2017-03-02T17:38:08.212440: step 4443, loss 0.0908922, acc 0.984375
2017-03-02T17:38:08.285942: step 4444, loss 0.205587, acc 0.9375
2017-03-02T17:38:08.356437: step 4445, loss 0.275137, acc 0.921875
2017-03-02T17:38:08.423651: step 4446, loss 0.199916, acc 0.9375
2017-03-02T17:38:08.503264: step 4447, loss 0.22375, acc 0.953125
2017-03-02T17:38:08.577910: step 4448, loss 0.2415, acc 0.890625
2017-03-02T17:38:08.647295: step 4449, loss 0.189512, acc 0.9375
2017-03-02T17:38:08.712366: step 4450, loss 0.353974, acc 0.90625
2017-03-02T17:38:08.775227: step 4451, loss 0.187826, acc 0.9375
2017-03-02T17:38:08.851876: step 4452, loss 0.178204, acc 0.921875
2017-03-02T17:38:08.922449: step 4453, loss 0.183951, acc 0.9375
2017-03-02T17:38:08.986618: step 4454, loss 0.309333, acc 0.859375
2017-03-02T17:38:09.056160: step 4455, loss 0.327599, acc 0.890625
2017-03-02T17:38:09.127115: step 4456, loss 0.153945, acc 0.921875
2017-03-02T17:38:09.198512: step 4457, loss 0.137907, acc 0.921875
2017-03-02T17:38:09.263771: step 4458, loss 0.173765, acc 0.90625
2017-03-02T17:38:09.325822: step 4459, loss 0.296923, acc 0.890625
2017-03-02T17:38:09.394569: step 4460, loss 0.330389, acc 0.875
2017-03-02T17:38:09.462645: step 4461, loss 0.141816, acc 0.96875
2017-03-02T17:38:09.531311: step 4462, loss 0.334476, acc 0.9375
2017-03-02T17:38:09.595961: step 4463, loss 0.0954719, acc 0.984375
2017-03-02T17:38:09.665995: step 4464, loss 0.1261, acc 0.96875
2017-03-02T17:38:09.732615: step 4465, loss 0.112352, acc 0.953125
2017-03-02T17:38:09.803438: step 4466, loss 0.323904, acc 0.890625
2017-03-02T17:38:09.870619: step 4467, loss 0.241444, acc 0.921875
2017-03-02T17:38:09.938966: step 4468, loss 0.232215, acc 0.90625
2017-03-02T17:38:10.007989: step 4469, loss 0.350764, acc 0.875
2017-03-02T17:38:10.074340: step 4470, loss 0.308497, acc 0.890625
2017-03-02T17:38:10.143893: step 4471, loss 0.161045, acc 0.921875
2017-03-02T17:38:10.215209: step 4472, loss 0.154747, acc 0.921875
2017-03-02T17:38:10.285436: step 4473, loss 0.155854, acc 0.921875
2017-03-02T17:38:10.357202: step 4474, loss 0.215114, acc 0.9375
2017-03-02T17:38:10.428194: step 4475, loss 0.164445, acc 0.921875
2017-03-02T17:38:10.497254: step 4476, loss 0.46082, acc 0.890625
2017-03-02T17:38:10.566356: step 4477, loss 0.153468, acc 0.953125
2017-03-02T17:38:10.640392: step 4478, loss 0.19486, acc 0.953125
2017-03-02T17:38:10.709333: step 4479, loss 0.14448, acc 0.96875
2017-03-02T17:38:10.783578: step 4480, loss 0.295701, acc 0.90625
2017-03-02T17:38:10.860811: step 4481, loss 0.250876, acc 0.890625
2017-03-02T17:38:10.921180: step 4482, loss 0.258247, acc 0.875
2017-03-02T17:38:10.988883: step 4483, loss 0.360892, acc 0.859375
2017-03-02T17:38:11.056012: step 4484, loss 0.201928, acc 0.953125
2017-03-02T17:38:11.120062: step 4485, loss 0.305462, acc 0.921875
2017-03-02T17:38:11.197119: step 4486, loss 0.197302, acc 0.921875
2017-03-02T17:38:11.267289: step 4487, loss 0.13743, acc 0.9375
2017-03-02T17:38:11.335331: step 4488, loss 0.213095, acc 0.90625
2017-03-02T17:38:11.397949: step 4489, loss 0.157219, acc 0.953125
2017-03-02T17:38:11.465485: step 4490, loss 0.158689, acc 0.921875
2017-03-02T17:38:11.534389: step 4491, loss 0.167734, acc 0.921875
2017-03-02T17:38:11.620850: step 4492, loss 0.353928, acc 0.875
2017-03-02T17:38:11.687577: step 4493, loss 0.166143, acc 0.9375
2017-03-02T17:38:11.753027: step 4494, loss 0.111383, acc 0.9375
2017-03-02T17:38:11.824050: step 4495, loss 0.163789, acc 0.921875
2017-03-02T17:38:11.886979: step 4496, loss 0.230816, acc 0.953125
2017-03-02T17:38:11.956220: step 4497, loss 0.31803, acc 0.890625
2017-03-02T17:38:12.029996: step 4498, loss 0.171914, acc 0.953125
2017-03-02T17:38:12.102400: step 4499, loss 0.188833, acc 0.875
2017-03-02T17:38:12.171540: step 4500, loss 0.18269, acc 0.9375

Evaluation:
2017-03-02T17:38:12.199807: step 4500, loss 1.11132, acc 0.682048

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4500

2017-03-02T17:38:12.660830: step 4501, loss 0.217148, acc 0.9375
2017-03-02T17:38:12.726664: step 4502, loss 0.336217, acc 0.890625
2017-03-02T17:38:12.793892: step 4503, loss 0.23444, acc 0.875
2017-03-02T17:38:12.861019: step 4504, loss 0.294202, acc 0.890625
2017-03-02T17:38:12.934217: step 4505, loss 0.197488, acc 0.921875
2017-03-02T17:38:13.000511: step 4506, loss 0.173229, acc 0.921875
2017-03-02T17:38:13.071812: step 4507, loss 0.322179, acc 0.875
2017-03-02T17:38:13.138056: step 4508, loss 0.0123086, acc 1
2017-03-02T17:38:13.207499: step 4509, loss 0.166231, acc 0.9375
2017-03-02T17:38:13.272525: step 4510, loss 0.116998, acc 0.9375
2017-03-02T17:38:13.342164: step 4511, loss 0.141003, acc 0.96875
2017-03-02T17:38:13.409761: step 4512, loss 0.241041, acc 0.921875
2017-03-02T17:38:13.481377: step 4513, loss 0.22674, acc 0.90625
2017-03-02T17:38:13.551902: step 4514, loss 0.255253, acc 0.953125
2017-03-02T17:38:13.621447: step 4515, loss 0.115203, acc 0.9375
2017-03-02T17:38:13.687543: step 4516, loss 0.120124, acc 1
2017-03-02T17:38:13.755497: step 4517, loss 0.0914523, acc 0.984375
2017-03-02T17:38:13.819899: step 4518, loss 0.14235, acc 0.953125
2017-03-02T17:38:13.888024: step 4519, loss 0.159876, acc 0.953125
2017-03-02T17:38:13.954876: step 4520, loss 0.1051, acc 0.96875
2017-03-02T17:38:14.021450: step 4521, loss 0.166858, acc 0.921875
2017-03-02T17:38:14.091535: step 4522, loss 0.101111, acc 0.96875
2017-03-02T17:38:14.159880: step 4523, loss 0.215902, acc 0.921875
2017-03-02T17:38:14.227158: step 4524, loss 0.125685, acc 0.96875
2017-03-02T17:38:14.292900: step 4525, loss 0.184573, acc 0.921875
2017-03-02T17:38:14.364789: step 4526, loss 0.237377, acc 0.90625
2017-03-02T17:38:14.431366: step 4527, loss 0.150272, acc 0.9375
2017-03-02T17:38:14.499539: step 4528, loss 0.306232, acc 0.890625
2017-03-02T17:38:14.569513: step 4529, loss 0.230491, acc 0.953125
2017-03-02T17:38:14.642578: step 4530, loss 0.258101, acc 0.9375
2017-03-02T17:38:14.717895: step 4531, loss 0.146841, acc 0.953125
2017-03-02T17:38:14.790873: step 4532, loss 0.190286, acc 0.921875
2017-03-02T17:38:14.859943: step 4533, loss 0.0829007, acc 0.984375
2017-03-02T17:38:14.930643: step 4534, loss 0.299138, acc 0.890625
2017-03-02T17:38:14.998457: step 4535, loss 0.371847, acc 0.890625
2017-03-02T17:38:15.064833: step 4536, loss 0.14054, acc 0.953125
2017-03-02T17:38:15.134553: step 4537, loss 0.175014, acc 0.9375
2017-03-02T17:38:15.207891: step 4538, loss 0.164131, acc 0.984375
2017-03-02T17:38:15.275575: step 4539, loss 0.166676, acc 0.9375
2017-03-02T17:38:15.342875: step 4540, loss 0.302791, acc 0.90625
2017-03-02T17:38:15.412362: step 4541, loss 0.157562, acc 0.9375
2017-03-02T17:38:15.482271: step 4542, loss 0.312478, acc 0.859375
2017-03-02T17:38:15.549068: step 4543, loss 0.22648, acc 0.90625
2017-03-02T17:38:15.614558: step 4544, loss 0.270915, acc 0.875
2017-03-02T17:38:15.682798: step 4545, loss 0.15664, acc 0.953125
2017-03-02T17:38:15.741871: step 4546, loss 0.252348, acc 0.90625
2017-03-02T17:38:15.809915: step 4547, loss 0.218004, acc 0.890625
2017-03-02T17:38:15.877468: step 4548, loss 0.207876, acc 0.9375
2017-03-02T17:38:15.944848: step 4549, loss 0.23525, acc 0.921875
2017-03-02T17:38:16.009673: step 4550, loss 0.244612, acc 0.90625
2017-03-02T17:38:16.077059: step 4551, loss 0.229045, acc 0.9375
2017-03-02T17:38:16.142213: step 4552, loss 0.111919, acc 0.953125
2017-03-02T17:38:16.205728: step 4553, loss 0.184689, acc 0.953125
2017-03-02T17:38:16.277281: step 4554, loss 0.291625, acc 0.875
2017-03-02T17:38:16.352307: step 4555, loss 0.237468, acc 0.9375
2017-03-02T17:38:16.417479: step 4556, loss 0.167412, acc 0.9375
2017-03-02T17:38:16.489144: step 4557, loss 0.397022, acc 0.859375
2017-03-02T17:38:16.555491: step 4558, loss 0.199077, acc 0.96875
2017-03-02T17:38:16.623327: step 4559, loss 0.171999, acc 0.921875
2017-03-02T17:38:16.693967: step 4560, loss 0.149023, acc 0.9375
2017-03-02T17:38:16.759481: step 4561, loss 0.344097, acc 0.875
2017-03-02T17:38:16.827884: step 4562, loss 0.269884, acc 0.90625
2017-03-02T17:38:16.896031: step 4563, loss 0.267126, acc 0.921875
2017-03-02T17:38:16.962276: step 4564, loss 0.383651, acc 0.828125
2017-03-02T17:38:17.034661: step 4565, loss 0.209151, acc 0.921875
2017-03-02T17:38:17.097716: step 4566, loss 0.188683, acc 0.921875
2017-03-02T17:38:17.165934: step 4567, loss 0.312427, acc 0.9375
2017-03-02T17:38:17.241174: step 4568, loss 0.198876, acc 0.9375
2017-03-02T17:38:17.309057: step 4569, loss 0.218678, acc 0.9375
2017-03-02T17:38:17.382177: step 4570, loss 0.217155, acc 0.890625
2017-03-02T17:38:17.451696: step 4571, loss 0.29537, acc 0.90625
2017-03-02T17:38:17.523038: step 4572, loss 0.162629, acc 0.921875
2017-03-02T17:38:17.586405: step 4573, loss 0.19174, acc 0.9375
2017-03-02T17:38:17.654653: step 4574, loss 0.46859, acc 0.8125
2017-03-02T17:38:17.716545: step 4575, loss 0.171502, acc 0.90625
2017-03-02T17:38:17.785275: step 4576, loss 0.220001, acc 0.90625
2017-03-02T17:38:17.851127: step 4577, loss 0.214873, acc 0.953125
2017-03-02T17:38:17.919400: step 4578, loss 0.194311, acc 0.921875
2017-03-02T17:38:17.983027: step 4579, loss 0.299451, acc 0.890625
2017-03-02T17:38:18.052226: step 4580, loss 0.351817, acc 0.890625
2017-03-02T17:38:18.123557: step 4581, loss 0.136131, acc 0.96875
2017-03-02T17:38:18.189297: step 4582, loss 0.170458, acc 0.953125
2017-03-02T17:38:18.261484: step 4583, loss 0.190491, acc 0.890625
2017-03-02T17:38:18.332376: step 4584, loss 0.232726, acc 0.90625
2017-03-02T17:38:18.402089: step 4585, loss 0.212192, acc 0.90625
2017-03-02T17:38:18.473195: step 4586, loss 0.203985, acc 0.9375
2017-03-02T17:38:18.543565: step 4587, loss 0.320072, acc 0.90625
2017-03-02T17:38:18.616328: step 4588, loss 0.148938, acc 0.9375
2017-03-02T17:38:18.686648: step 4589, loss 0.241497, acc 0.90625
2017-03-02T17:38:18.758987: step 4590, loss 0.166681, acc 0.9375
2017-03-02T17:38:18.826501: step 4591, loss 0.105764, acc 0.96875
2017-03-02T17:38:18.902287: step 4592, loss 0.284606, acc 0.890625
2017-03-02T17:38:18.967314: step 4593, loss 0.258866, acc 0.890625
2017-03-02T17:38:19.036046: step 4594, loss 0.33477, acc 0.921875
2017-03-02T17:38:19.105774: step 4595, loss 0.128143, acc 0.953125
2017-03-02T17:38:19.171663: step 4596, loss 0.217546, acc 0.890625
2017-03-02T17:38:19.241942: step 4597, loss 0.188332, acc 0.921875
2017-03-02T17:38:19.307320: step 4598, loss 0.204462, acc 0.9375
2017-03-02T17:38:19.367768: step 4599, loss 0.384054, acc 0.84375
2017-03-02T17:38:19.434974: step 4600, loss 0.138336, acc 0.953125

Evaluation:
2017-03-02T17:38:19.463367: step 4600, loss 1.11637, acc 0.679885

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4600

2017-03-02T17:38:19.964752: step 4601, loss 0.0952774, acc 0.953125
2017-03-02T17:38:20.035295: step 4602, loss 0.249965, acc 0.921875
2017-03-02T17:38:20.105845: step 4603, loss 0.183572, acc 0.9375
2017-03-02T17:38:20.168794: step 4604, loss 0.237767, acc 0.90625
2017-03-02T17:38:20.238189: step 4605, loss 0.140348, acc 0.96875
2017-03-02T17:38:20.308728: step 4606, loss 0.295654, acc 0.859375
2017-03-02T17:38:20.380704: step 4607, loss 0.20625, acc 0.9375
2017-03-02T17:38:20.451738: step 4608, loss 0.206057, acc 0.890625
2017-03-02T17:38:20.512880: step 4609, loss 0.21348, acc 0.9375
2017-03-02T17:38:20.580047: step 4610, loss 0.227613, acc 0.921875
2017-03-02T17:38:20.647227: step 4611, loss 0.248176, acc 0.890625
2017-03-02T17:38:20.717124: step 4612, loss 0.302559, acc 0.890625
2017-03-02T17:38:20.787643: step 4613, loss 0.305866, acc 0.890625
2017-03-02T17:38:20.859897: step 4614, loss 0.279368, acc 0.9375
2017-03-02T17:38:20.930676: step 4615, loss 0.186563, acc 0.921875
2017-03-02T17:38:20.999691: step 4616, loss 0.0959738, acc 0.96875
2017-03-02T17:38:21.079747: step 4617, loss 0.0988189, acc 0.953125
2017-03-02T17:38:21.148362: step 4618, loss 0.140092, acc 0.96875
2017-03-02T17:38:21.217137: step 4619, loss 0.468865, acc 0.84375
2017-03-02T17:38:21.281688: step 4620, loss 0.15446, acc 0.953125
2017-03-02T17:38:21.344951: step 4621, loss 0.233646, acc 0.953125
2017-03-02T17:38:21.413018: step 4622, loss 0.360932, acc 0.859375
2017-03-02T17:38:21.490558: step 4623, loss 0.141951, acc 0.9375
2017-03-02T17:38:21.559363: step 4624, loss 0.171734, acc 0.90625
2017-03-02T17:38:21.624123: step 4625, loss 0.26876, acc 0.90625
2017-03-02T17:38:21.688309: step 4626, loss 0.238723, acc 0.890625
2017-03-02T17:38:21.759040: step 4627, loss 0.243179, acc 0.859375
2017-03-02T17:38:21.831301: step 4628, loss 0.255193, acc 0.9375
2017-03-02T17:38:21.893004: step 4629, loss 0.360542, acc 0.84375
2017-03-02T17:38:21.960337: step 4630, loss 0.227733, acc 0.953125
2017-03-02T17:38:22.025852: step 4631, loss 0.251403, acc 0.890625
2017-03-02T17:38:22.094468: step 4632, loss 0.168586, acc 0.90625
2017-03-02T17:38:22.165195: step 4633, loss 0.137414, acc 0.9375
2017-03-02T17:38:22.228911: step 4634, loss 0.0774224, acc 0.984375
2017-03-02T17:38:22.298492: step 4635, loss 0.365427, acc 0.859375
2017-03-02T17:38:22.365048: step 4636, loss 0.223231, acc 0.859375
2017-03-02T17:38:22.428826: step 4637, loss 0.299548, acc 0.890625
2017-03-02T17:38:22.495528: step 4638, loss 0.11652, acc 0.953125
2017-03-02T17:38:22.560091: step 4639, loss 0.305897, acc 0.90625
2017-03-02T17:38:22.631671: step 4640, loss 0.185543, acc 0.90625
2017-03-02T17:38:22.698435: step 4641, loss 0.254568, acc 0.875
2017-03-02T17:38:22.791843: step 4642, loss 0.211436, acc 0.890625
2017-03-02T17:38:22.863707: step 4643, loss 0.214594, acc 0.90625
2017-03-02T17:38:22.932909: step 4644, loss 0.14591, acc 0.9375
2017-03-02T17:38:22.998008: step 4645, loss 0.198162, acc 0.90625
2017-03-02T17:38:23.070366: step 4646, loss 0.282618, acc 0.953125
2017-03-02T17:38:23.138371: step 4647, loss 0.297215, acc 0.9375
2017-03-02T17:38:23.208832: step 4648, loss 0.376745, acc 0.859375
2017-03-02T17:38:23.278414: step 4649, loss 0.294801, acc 0.875
2017-03-02T17:38:23.344302: step 4650, loss 0.154934, acc 0.921875
2017-03-02T17:38:23.413681: step 4651, loss 0.140531, acc 0.953125
2017-03-02T17:38:23.480944: step 4652, loss 0.316333, acc 0.890625
2017-03-02T17:38:23.545988: step 4653, loss 0.17749, acc 0.921875
2017-03-02T17:38:23.612521: step 4654, loss 0.148596, acc 0.9375
2017-03-02T17:38:23.681147: step 4655, loss 0.171326, acc 0.9375
2017-03-02T17:38:23.745289: step 4656, loss 0.237249, acc 0.890625
2017-03-02T17:38:23.812885: step 4657, loss 0.280001, acc 0.890625
2017-03-02T17:38:23.879365: step 4658, loss 0.229021, acc 0.921875
2017-03-02T17:38:23.947557: step 4659, loss 0.131236, acc 0.96875
2017-03-02T17:38:24.013808: step 4660, loss 0.244526, acc 0.9375
2017-03-02T17:38:24.085048: step 4661, loss 0.299316, acc 0.859375
2017-03-02T17:38:24.150503: step 4662, loss 0.159906, acc 0.921875
2017-03-02T17:38:24.219076: step 4663, loss 0.25933, acc 0.9375
2017-03-02T17:38:24.290083: step 4664, loss 0.237859, acc 0.9375
2017-03-02T17:38:24.364827: step 4665, loss 0.368562, acc 0.890625
2017-03-02T17:38:24.431089: step 4666, loss 0.337961, acc 0.859375
2017-03-02T17:38:24.499005: step 4667, loss 0.244317, acc 0.90625
2017-03-02T17:38:24.566989: step 4668, loss 0.217754, acc 0.921875
2017-03-02T17:38:24.639759: step 4669, loss 0.0896329, acc 0.96875
2017-03-02T17:38:24.719382: step 4670, loss 0.114647, acc 0.953125
2017-03-02T17:38:24.788964: step 4671, loss 0.152633, acc 0.96875
2017-03-02T17:38:24.858024: step 4672, loss 0.340288, acc 0.84375
2017-03-02T17:38:24.936454: step 4673, loss 0.171779, acc 0.953125
2017-03-02T17:38:25.003062: step 4674, loss 0.182817, acc 0.921875
2017-03-02T17:38:25.069053: step 4675, loss 0.19926, acc 0.921875
2017-03-02T17:38:25.134849: step 4676, loss 0.274549, acc 0.890625
2017-03-02T17:38:25.210121: step 4677, loss 0.176773, acc 0.921875
2017-03-02T17:38:25.277710: step 4678, loss 0.366418, acc 0.875
2017-03-02T17:38:25.353454: step 4679, loss 0.293653, acc 0.90625
2017-03-02T17:38:25.421663: step 4680, loss 0.299524, acc 0.828125
2017-03-02T17:38:25.487158: step 4681, loss 0.296888, acc 0.9375
2017-03-02T17:38:25.556409: step 4682, loss 0.201342, acc 0.921875
2017-03-02T17:38:25.623214: step 4683, loss 0.207798, acc 0.9375
2017-03-02T17:38:25.685912: step 4684, loss 0.401782, acc 0.890625
2017-03-02T17:38:25.755198: step 4685, loss 0.297981, acc 0.9375
2017-03-02T17:38:25.824071: step 4686, loss 0.152171, acc 0.9375
2017-03-02T17:38:25.892795: step 4687, loss 0.299218, acc 0.875
2017-03-02T17:38:25.960419: step 4688, loss 0.265026, acc 0.90625
2017-03-02T17:38:26.033937: step 4689, loss 0.303467, acc 0.875
2017-03-02T17:38:26.106499: step 4690, loss 0.216562, acc 0.90625
2017-03-02T17:38:26.181662: step 4691, loss 0.262271, acc 0.859375
2017-03-02T17:38:26.251157: step 4692, loss 0.131924, acc 0.9375
2017-03-02T17:38:26.318329: step 4693, loss 0.224242, acc 0.921875
2017-03-02T17:38:26.390753: step 4694, loss 0.251579, acc 0.90625
2017-03-02T17:38:26.465492: step 4695, loss 0.216036, acc 0.9375
2017-03-02T17:38:26.536481: step 4696, loss 0.301804, acc 0.875
2017-03-02T17:38:26.604681: step 4697, loss 0.1181, acc 0.953125
2017-03-02T17:38:26.662446: step 4698, loss 0.250184, acc 0.90625
2017-03-02T17:38:26.729753: step 4699, loss 0.254413, acc 0.921875
2017-03-02T17:38:26.794675: step 4700, loss 0.226276, acc 0.90625

Evaluation:
2017-03-02T17:38:26.819937: step 4700, loss 1.08498, acc 0.679164

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4700

2017-03-02T17:38:27.266573: step 4701, loss 0.317977, acc 0.890625
2017-03-02T17:38:27.338259: step 4702, loss 0.254488, acc 0.90625
2017-03-02T17:38:27.405833: step 4703, loss 0.113399, acc 0.96875
2017-03-02T17:38:27.466828: step 4704, loss 0.0126544, acc 1
2017-03-02T17:38:27.535576: step 4705, loss 0.142545, acc 0.953125
2017-03-02T17:38:27.604445: step 4706, loss 0.201538, acc 0.9375
2017-03-02T17:38:27.676028: step 4707, loss 0.162307, acc 0.9375
2017-03-02T17:38:27.732946: step 4708, loss 0.307388, acc 0.84375
2017-03-02T17:38:27.799351: step 4709, loss 0.433598, acc 0.84375
2017-03-02T17:38:27.868849: step 4710, loss 0.296748, acc 0.890625
2017-03-02T17:38:27.934026: step 4711, loss 0.197228, acc 0.953125
2017-03-02T17:38:28.001581: step 4712, loss 0.165836, acc 0.9375
2017-03-02T17:38:28.070884: step 4713, loss 0.213017, acc 0.921875
2017-03-02T17:38:28.142046: step 4714, loss 0.231856, acc 0.875
2017-03-02T17:38:28.203417: step 4715, loss 0.221631, acc 0.90625
2017-03-02T17:38:28.272007: step 4716, loss 0.156332, acc 0.921875
2017-03-02T17:38:28.338853: step 4717, loss 0.278532, acc 0.890625
2017-03-02T17:38:28.405353: step 4718, loss 0.182155, acc 0.921875
2017-03-02T17:38:28.478069: step 4719, loss 0.129221, acc 0.9375
2017-03-02T17:38:28.566416: step 4720, loss 0.170328, acc 0.9375
2017-03-02T17:38:28.635579: step 4721, loss 0.236367, acc 0.90625
2017-03-02T17:38:28.705555: step 4722, loss 0.221284, acc 0.90625
2017-03-02T17:38:28.773680: step 4723, loss 0.0990727, acc 0.984375
2017-03-02T17:38:28.845293: step 4724, loss 0.106028, acc 0.96875
2017-03-02T17:38:28.914226: step 4725, loss 0.156717, acc 0.96875
2017-03-02T17:38:28.982955: step 4726, loss 0.187808, acc 0.953125
2017-03-02T17:38:29.053070: step 4727, loss 0.142684, acc 0.953125
2017-03-02T17:38:29.117351: step 4728, loss 0.223235, acc 0.90625
2017-03-02T17:38:29.186823: step 4729, loss 0.243624, acc 0.921875
2017-03-02T17:38:29.253263: step 4730, loss 0.185661, acc 0.9375
2017-03-02T17:38:29.322875: step 4731, loss 0.247858, acc 0.90625
2017-03-02T17:38:29.392036: step 4732, loss 0.247903, acc 0.921875
2017-03-02T17:38:29.460588: step 4733, loss 0.263181, acc 0.9375
2017-03-02T17:38:29.525852: step 4734, loss 0.231914, acc 0.90625
2017-03-02T17:38:29.593633: step 4735, loss 0.116539, acc 0.953125
2017-03-02T17:38:29.662680: step 4736, loss 0.160896, acc 0.96875
2017-03-02T17:38:29.738014: step 4737, loss 0.438622, acc 0.859375
2017-03-02T17:38:29.808768: step 4738, loss 0.186796, acc 0.921875
2017-03-02T17:38:29.885614: step 4739, loss 0.270371, acc 0.90625
2017-03-02T17:38:29.955142: step 4740, loss 0.379708, acc 0.921875
2017-03-02T17:38:30.027521: step 4741, loss 0.151281, acc 0.953125
2017-03-02T17:38:30.097620: step 4742, loss 0.142042, acc 0.9375
2017-03-02T17:38:30.171899: step 4743, loss 0.218075, acc 0.9375
2017-03-02T17:38:30.237733: step 4744, loss 0.175064, acc 0.921875
2017-03-02T17:38:30.298983: step 4745, loss 0.245787, acc 0.9375
2017-03-02T17:38:30.365570: step 4746, loss 0.203819, acc 0.90625
2017-03-02T17:38:30.428926: step 4747, loss 0.149654, acc 0.9375
2017-03-02T17:38:30.499426: step 4748, loss 0.201828, acc 0.953125
2017-03-02T17:38:30.567022: step 4749, loss 0.182311, acc 0.90625
2017-03-02T17:38:30.638356: step 4750, loss 0.178939, acc 0.90625
2017-03-02T17:38:30.707670: step 4751, loss 0.524189, acc 0.859375
2017-03-02T17:38:30.775343: step 4752, loss 0.178358, acc 0.953125
2017-03-02T17:38:30.844687: step 4753, loss 0.270379, acc 0.921875
2017-03-02T17:38:30.912171: step 4754, loss 0.127111, acc 0.921875
2017-03-02T17:38:30.980809: step 4755, loss 0.159568, acc 0.9375
2017-03-02T17:38:31.050757: step 4756, loss 0.284357, acc 0.890625
2017-03-02T17:38:31.122854: step 4757, loss 0.266951, acc 0.921875
2017-03-02T17:38:31.197464: step 4758, loss 0.160963, acc 0.921875
2017-03-02T17:38:31.263950: step 4759, loss 0.27305, acc 0.890625
2017-03-02T17:38:31.336798: step 4760, loss 0.141973, acc 0.96875
2017-03-02T17:38:31.404499: step 4761, loss 0.185759, acc 0.9375
2017-03-02T17:38:31.475798: step 4762, loss 0.143452, acc 0.96875
2017-03-02T17:38:31.542836: step 4763, loss 0.190628, acc 0.9375
2017-03-02T17:38:31.612009: step 4764, loss 0.15922, acc 0.953125
2017-03-02T17:38:31.679124: step 4765, loss 0.180053, acc 0.9375
2017-03-02T17:38:31.751800: step 4766, loss 0.273539, acc 0.875
2017-03-02T17:38:31.818442: step 4767, loss 0.238115, acc 0.890625
2017-03-02T17:38:31.889092: step 4768, loss 0.116481, acc 0.984375
2017-03-02T17:38:31.960715: step 4769, loss 0.351754, acc 0.859375
2017-03-02T17:38:32.033337: step 4770, loss 0.350404, acc 0.875
2017-03-02T17:38:32.105487: step 4771, loss 0.144153, acc 0.953125
2017-03-02T17:38:32.176073: step 4772, loss 0.253737, acc 0.875
2017-03-02T17:38:32.240315: step 4773, loss 0.240642, acc 0.90625
2017-03-02T17:38:32.309642: step 4774, loss 0.277441, acc 0.90625
2017-03-02T17:38:32.378410: step 4775, loss 0.199258, acc 0.921875
2017-03-02T17:38:32.449736: step 4776, loss 0.200884, acc 0.9375
2017-03-02T17:38:32.518489: step 4777, loss 0.204584, acc 0.90625
2017-03-02T17:38:32.584869: step 4778, loss 0.179217, acc 0.9375
2017-03-02T17:38:32.645678: step 4779, loss 0.189962, acc 0.890625
2017-03-02T17:38:32.720862: step 4780, loss 0.268929, acc 0.921875
2017-03-02T17:38:32.786681: step 4781, loss 0.354707, acc 0.859375
2017-03-02T17:38:32.855157: step 4782, loss 0.167581, acc 0.921875
2017-03-02T17:38:32.926302: step 4783, loss 0.221096, acc 0.953125
2017-03-02T17:38:32.992771: step 4784, loss 0.114642, acc 0.953125
2017-03-02T17:38:33.068222: step 4785, loss 0.357071, acc 0.859375
2017-03-02T17:38:33.137609: step 4786, loss 0.234451, acc 0.890625
2017-03-02T17:38:33.205832: step 4787, loss 0.166626, acc 0.90625
2017-03-02T17:38:33.271583: step 4788, loss 0.277169, acc 0.859375
2017-03-02T17:38:33.341899: step 4789, loss 0.283047, acc 0.90625
2017-03-02T17:38:33.403106: step 4790, loss 0.29086, acc 0.9375
2017-03-02T17:38:33.472586: step 4791, loss 0.352303, acc 0.859375
2017-03-02T17:38:33.543806: step 4792, loss 0.219695, acc 0.890625
2017-03-02T17:38:33.614058: step 4793, loss 0.209777, acc 0.90625
2017-03-02T17:38:33.680826: step 4794, loss 0.13073, acc 0.9375
2017-03-02T17:38:33.750331: step 4795, loss 0.234767, acc 0.890625
2017-03-02T17:38:33.819325: step 4796, loss 0.291722, acc 0.890625
2017-03-02T17:38:33.884206: step 4797, loss 0.152984, acc 0.953125
2017-03-02T17:38:33.952952: step 4798, loss 0.17544, acc 0.953125
2017-03-02T17:38:34.021736: step 4799, loss 0.266566, acc 0.890625
2017-03-02T17:38:34.088571: step 4800, loss 0.0865712, acc 0.984375

Evaluation:
2017-03-02T17:38:34.115618: step 4800, loss 1.09279, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4800

2017-03-02T17:38:34.561739: step 4801, loss 0.145049, acc 0.9375
2017-03-02T17:38:34.628731: step 4802, loss 0.213701, acc 0.921875
2017-03-02T17:38:34.698326: step 4803, loss 0.19153, acc 0.890625
2017-03-02T17:38:34.768571: step 4804, loss 0.221154, acc 0.890625
2017-03-02T17:38:34.847241: step 4805, loss 0.276504, acc 0.9375
2017-03-02T17:38:34.917725: step 4806, loss 0.250997, acc 0.90625
2017-03-02T17:38:34.986169: step 4807, loss 0.210363, acc 0.921875
2017-03-02T17:38:35.055604: step 4808, loss 0.194879, acc 0.90625
2017-03-02T17:38:35.131156: step 4809, loss 0.201325, acc 0.9375
2017-03-02T17:38:35.207730: step 4810, loss 0.148445, acc 0.96875
2017-03-02T17:38:35.276745: step 4811, loss 0.400766, acc 0.875
2017-03-02T17:38:35.341463: step 4812, loss 0.110862, acc 0.984375
2017-03-02T17:38:35.412956: step 4813, loss 0.24381, acc 0.90625
2017-03-02T17:38:35.478309: step 4814, loss 0.169788, acc 0.90625
2017-03-02T17:38:35.543198: step 4815, loss 0.13638, acc 0.96875
2017-03-02T17:38:35.611431: step 4816, loss 0.143676, acc 0.96875
2017-03-02T17:38:35.689436: step 4817, loss 0.108052, acc 0.984375
2017-03-02T17:38:35.755214: step 4818, loss 0.234568, acc 0.90625
2017-03-02T17:38:35.832594: step 4819, loss 0.243834, acc 0.90625
2017-03-02T17:38:35.899244: step 4820, loss 0.204917, acc 0.9375
2017-03-02T17:38:35.968472: step 4821, loss 0.236923, acc 0.90625
2017-03-02T17:38:36.038784: step 4822, loss 0.386983, acc 0.890625
2017-03-02T17:38:36.108824: step 4823, loss 0.266499, acc 0.890625
2017-03-02T17:38:36.171970: step 4824, loss 0.125671, acc 0.9375
2017-03-02T17:38:36.242432: step 4825, loss 0.112293, acc 0.984375
2017-03-02T17:38:36.309004: step 4826, loss 0.181335, acc 0.921875
2017-03-02T17:38:36.376359: step 4827, loss 0.127231, acc 0.953125
2017-03-02T17:38:36.446142: step 4828, loss 0.176905, acc 0.921875
2017-03-02T17:38:36.516762: step 4829, loss 0.234341, acc 0.953125
2017-03-02T17:38:36.585127: step 4830, loss 0.213319, acc 0.90625
2017-03-02T17:38:36.655487: step 4831, loss 0.28932, acc 0.921875
2017-03-02T17:38:36.725900: step 4832, loss 0.229901, acc 0.90625
2017-03-02T17:38:36.793999: step 4833, loss 0.116476, acc 0.953125
2017-03-02T17:38:36.856996: step 4834, loss 0.205873, acc 0.921875
2017-03-02T17:38:36.928797: step 4835, loss 0.301223, acc 0.90625
2017-03-02T17:38:36.999777: step 4836, loss 0.289605, acc 0.890625
2017-03-02T17:38:37.071416: step 4837, loss 0.375952, acc 0.890625
2017-03-02T17:38:37.153616: step 4838, loss 0.231403, acc 0.953125
2017-03-02T17:38:37.222115: step 4839, loss 0.225706, acc 0.90625
2017-03-02T17:38:37.291137: step 4840, loss 0.268624, acc 0.890625
2017-03-02T17:38:37.360296: step 4841, loss 0.283977, acc 0.859375
2017-03-02T17:38:37.426663: step 4842, loss 0.298261, acc 0.84375
2017-03-02T17:38:37.493649: step 4843, loss 0.113818, acc 0.9375
2017-03-02T17:38:37.559173: step 4844, loss 0.242252, acc 0.90625
2017-03-02T17:38:37.626475: step 4845, loss 0.249148, acc 0.921875
2017-03-02T17:38:37.695190: step 4846, loss 0.229376, acc 0.921875
2017-03-02T17:38:37.759152: step 4847, loss 0.167281, acc 0.921875
2017-03-02T17:38:37.829734: step 4848, loss 0.163533, acc 0.90625
2017-03-02T17:38:37.900525: step 4849, loss 0.320231, acc 0.90625
2017-03-02T17:38:37.970972: step 4850, loss 0.151366, acc 0.953125
2017-03-02T17:38:38.042287: step 4851, loss 0.0531786, acc 0.984375
2017-03-02T17:38:38.109736: step 4852, loss 0.232671, acc 0.921875
2017-03-02T17:38:38.180837: step 4853, loss 0.163789, acc 0.9375
2017-03-02T17:38:38.254265: step 4854, loss 0.156027, acc 0.921875
2017-03-02T17:38:38.320386: step 4855, loss 0.0956529, acc 0.96875
2017-03-02T17:38:38.390340: step 4856, loss 0.131943, acc 0.953125
2017-03-02T17:38:38.458540: step 4857, loss 0.275329, acc 0.90625
2017-03-02T17:38:38.527129: step 4858, loss 0.280057, acc 0.90625
2017-03-02T17:38:38.595554: step 4859, loss 0.0524034, acc 0.96875
2017-03-02T17:38:38.662500: step 4860, loss 0.279823, acc 0.90625
2017-03-02T17:38:38.737812: step 4861, loss 0.496215, acc 0.84375
2017-03-02T17:38:38.805968: step 4862, loss 0.303317, acc 0.890625
2017-03-02T17:38:38.873145: step 4863, loss 0.207657, acc 0.90625
2017-03-02T17:38:38.942157: step 4864, loss 0.206779, acc 0.953125
2017-03-02T17:38:39.007583: step 4865, loss 0.166854, acc 0.921875
2017-03-02T17:38:39.077293: step 4866, loss 0.4129, acc 0.84375
2017-03-02T17:38:39.143979: step 4867, loss 0.187209, acc 0.890625
2017-03-02T17:38:39.205233: step 4868, loss 0.261435, acc 0.90625
2017-03-02T17:38:39.270287: step 4869, loss 0.0983914, acc 0.96875
2017-03-02T17:38:39.337372: step 4870, loss 0.0754166, acc 0.984375
2017-03-02T17:38:39.405704: step 4871, loss 0.238402, acc 0.921875
2017-03-02T17:38:39.473278: step 4872, loss 0.1803, acc 0.921875
2017-03-02T17:38:39.544604: step 4873, loss 0.333893, acc 0.890625
2017-03-02T17:38:39.614398: step 4874, loss 0.220562, acc 0.921875
2017-03-02T17:38:39.674359: step 4875, loss 0.228067, acc 0.921875
2017-03-02T17:38:39.746498: step 4876, loss 0.135353, acc 0.921875
2017-03-02T17:38:39.816970: step 4877, loss 0.16892, acc 0.953125
2017-03-02T17:38:39.881823: step 4878, loss 0.217047, acc 0.9375
2017-03-02T17:38:39.946270: step 4879, loss 0.166567, acc 0.9375
2017-03-02T17:38:40.021524: step 4880, loss 0.312574, acc 0.890625
2017-03-02T17:38:40.086235: step 4881, loss 0.397576, acc 0.875
2017-03-02T17:38:40.154581: step 4882, loss 0.277495, acc 0.921875
2017-03-02T17:38:40.222224: step 4883, loss 0.243443, acc 0.890625
2017-03-02T17:38:40.287099: step 4884, loss 0.348988, acc 0.875
2017-03-02T17:38:40.357275: step 4885, loss 0.099904, acc 0.953125
2017-03-02T17:38:40.420999: step 4886, loss 0.357365, acc 0.9375
2017-03-02T17:38:40.494632: step 4887, loss 0.311149, acc 0.890625
2017-03-02T17:38:40.563490: step 4888, loss 0.200558, acc 0.921875
2017-03-02T17:38:40.631150: step 4889, loss 0.1136, acc 0.96875
2017-03-02T17:38:40.698691: step 4890, loss 0.326486, acc 0.875
2017-03-02T17:38:40.767494: step 4891, loss 0.125763, acc 0.9375
2017-03-02T17:38:40.838603: step 4892, loss 0.148933, acc 0.953125
2017-03-02T17:38:40.909627: step 4893, loss 0.295426, acc 0.921875
2017-03-02T17:38:40.976253: step 4894, loss 0.17823, acc 0.90625
2017-03-02T17:38:41.046884: step 4895, loss 0.232536, acc 0.890625
2017-03-02T17:38:41.108320: step 4896, loss 0.222121, acc 0.921875
2017-03-02T17:38:41.176978: step 4897, loss 0.269995, acc 0.890625
2017-03-02T17:38:41.249496: step 4898, loss 0.238093, acc 0.875
2017-03-02T17:38:41.318511: step 4899, loss 0.209806, acc 0.9375
2017-03-02T17:38:41.384576: step 4900, loss 0.0404382, acc 1

Evaluation:
2017-03-02T17:38:41.409889: step 4900, loss 1.10432, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-4900

2017-03-02T17:38:41.876946: step 4901, loss 0.126101, acc 0.953125
2017-03-02T17:38:41.945649: step 4902, loss 0.116084, acc 0.953125
2017-03-02T17:38:42.018799: step 4903, loss 0.300125, acc 0.921875
2017-03-02T17:38:42.088131: step 4904, loss 0.135485, acc 0.953125
2017-03-02T17:38:42.154355: step 4905, loss 0.157488, acc 0.9375
2017-03-02T17:38:42.222161: step 4906, loss 0.13644, acc 0.9375
2017-03-02T17:38:42.289688: step 4907, loss 0.2283, acc 0.90625
2017-03-02T17:38:42.353755: step 4908, loss 0.277683, acc 0.875
2017-03-02T17:38:42.421554: step 4909, loss 0.19899, acc 0.921875
2017-03-02T17:38:42.486922: step 4910, loss 0.217007, acc 0.90625
2017-03-02T17:38:42.563428: step 4911, loss 0.174765, acc 0.953125
2017-03-02T17:38:42.638096: step 4912, loss 0.135116, acc 0.953125
2017-03-02T17:38:42.708401: step 4913, loss 0.317257, acc 0.90625
2017-03-02T17:38:42.776549: step 4914, loss 0.122561, acc 0.953125
2017-03-02T17:38:42.844940: step 4915, loss 0.231636, acc 0.921875
2017-03-02T17:38:42.915756: step 4916, loss 0.20722, acc 0.90625
2017-03-02T17:38:42.986609: step 4917, loss 0.144649, acc 0.9375
2017-03-02T17:38:43.057791: step 4918, loss 0.238077, acc 0.921875
2017-03-02T17:38:43.120182: step 4919, loss 0.166539, acc 0.953125
2017-03-02T17:38:43.196922: step 4920, loss 0.161654, acc 0.921875
2017-03-02T17:38:43.270549: step 4921, loss 0.189627, acc 0.9375
2017-03-02T17:38:43.336469: step 4922, loss 0.178252, acc 0.96875
2017-03-02T17:38:43.404783: step 4923, loss 0.206627, acc 0.9375
2017-03-02T17:38:43.472807: step 4924, loss 0.157259, acc 0.9375
2017-03-02T17:38:43.542133: step 4925, loss 0.163925, acc 0.921875
2017-03-02T17:38:43.607612: step 4926, loss 0.157283, acc 0.9375
2017-03-02T17:38:43.672868: step 4927, loss 0.183372, acc 0.921875
2017-03-02T17:38:43.745500: step 4928, loss 0.303093, acc 0.890625
2017-03-02T17:38:43.815569: step 4929, loss 0.153825, acc 0.96875
2017-03-02T17:38:43.885048: step 4930, loss 0.219588, acc 0.921875
2017-03-02T17:38:43.949692: step 4931, loss 0.187443, acc 0.890625
2017-03-02T17:38:44.013376: step 4932, loss 0.142822, acc 0.953125
2017-03-02T17:38:44.079090: step 4933, loss 0.253039, acc 0.9375
2017-03-02T17:38:44.142437: step 4934, loss 0.206656, acc 0.953125
2017-03-02T17:38:44.216735: step 4935, loss 0.132612, acc 0.921875
2017-03-02T17:38:44.286822: step 4936, loss 0.195781, acc 0.953125
2017-03-02T17:38:44.349483: step 4937, loss 0.127608, acc 0.9375
2017-03-02T17:38:44.418247: step 4938, loss 0.228658, acc 0.90625
2017-03-02T17:38:44.487756: step 4939, loss 0.322229, acc 0.90625
2017-03-02T17:38:44.556073: step 4940, loss 0.176792, acc 0.953125
2017-03-02T17:38:44.625793: step 4941, loss 0.188949, acc 0.921875
2017-03-02T17:38:44.697438: step 4942, loss 0.195886, acc 0.921875
2017-03-02T17:38:44.766986: step 4943, loss 0.374143, acc 0.890625
2017-03-02T17:38:44.836191: step 4944, loss 0.217933, acc 0.953125
2017-03-02T17:38:44.903329: step 4945, loss 0.147521, acc 0.953125
2017-03-02T17:38:44.971170: step 4946, loss 0.135343, acc 0.953125
2017-03-02T17:38:45.040798: step 4947, loss 0.168112, acc 0.921875
2017-03-02T17:38:45.108737: step 4948, loss 0.24338, acc 0.921875
2017-03-02T17:38:45.177028: step 4949, loss 0.167423, acc 0.96875
2017-03-02T17:38:45.252566: step 4950, loss 0.145366, acc 0.9375
2017-03-02T17:38:45.320515: step 4951, loss 0.126655, acc 0.953125
2017-03-02T17:38:45.381487: step 4952, loss 0.112978, acc 0.984375
2017-03-02T17:38:45.453997: step 4953, loss 0.242677, acc 0.875
2017-03-02T17:38:45.523267: step 4954, loss 0.647088, acc 0.796875
2017-03-02T17:38:45.592530: step 4955, loss 0.49422, acc 0.796875
2017-03-02T17:38:45.665277: step 4956, loss 0.266921, acc 0.890625
2017-03-02T17:38:45.740451: step 4957, loss 0.194235, acc 0.921875
2017-03-02T17:38:45.810316: step 4958, loss 0.336711, acc 0.890625
2017-03-02T17:38:45.879627: step 4959, loss 0.286213, acc 0.875
2017-03-02T17:38:45.950103: step 4960, loss 0.22623, acc 0.890625
2017-03-02T17:38:46.018681: step 4961, loss 0.306098, acc 0.875
2017-03-02T17:38:46.083469: step 4962, loss 0.19785, acc 0.90625
2017-03-02T17:38:46.157797: step 4963, loss 0.081696, acc 0.984375
2017-03-02T17:38:46.221936: step 4964, loss 0.256558, acc 0.921875
2017-03-02T17:38:46.290244: step 4965, loss 0.15594, acc 0.921875
2017-03-02T17:38:46.354953: step 4966, loss 0.177408, acc 0.921875
2017-03-02T17:38:46.423748: step 4967, loss 0.271265, acc 0.890625
2017-03-02T17:38:46.492719: step 4968, loss 0.123728, acc 0.953125
2017-03-02T17:38:46.563163: step 4969, loss 0.244658, acc 0.90625
2017-03-02T17:38:46.628362: step 4970, loss 0.205245, acc 0.890625
2017-03-02T17:38:46.697781: step 4971, loss 0.244965, acc 0.890625
2017-03-02T17:38:46.767789: step 4972, loss 0.242431, acc 0.859375
2017-03-02T17:38:46.835074: step 4973, loss 0.201449, acc 0.90625
2017-03-02T17:38:46.902948: step 4974, loss 0.119195, acc 0.953125
2017-03-02T17:38:46.973370: step 4975, loss 0.273992, acc 0.90625
2017-03-02T17:38:47.042032: step 4976, loss 0.118807, acc 0.984375
2017-03-02T17:38:47.112469: step 4977, loss 0.193344, acc 0.875
2017-03-02T17:38:47.180459: step 4978, loss 0.172339, acc 0.953125
2017-03-02T17:38:47.250764: step 4979, loss 0.203867, acc 0.921875
2017-03-02T17:38:47.319848: step 4980, loss 0.301388, acc 0.90625
2017-03-02T17:38:47.391348: step 4981, loss 0.421586, acc 0.84375
2017-03-02T17:38:47.462711: step 4982, loss 0.180981, acc 0.96875
2017-03-02T17:38:47.539414: step 4983, loss 0.274243, acc 0.875
2017-03-02T17:38:47.605027: step 4984, loss 0.20781, acc 0.921875
2017-03-02T17:38:47.664252: step 4985, loss 0.186869, acc 0.9375
2017-03-02T17:38:47.732542: step 4986, loss 0.154896, acc 0.953125
2017-03-02T17:38:47.799260: step 4987, loss 0.13476, acc 0.953125
2017-03-02T17:38:47.879867: step 4988, loss 0.162937, acc 0.921875
2017-03-02T17:38:47.948720: step 4989, loss 0.182366, acc 0.921875
2017-03-02T17:38:48.016822: step 4990, loss 0.307474, acc 0.875
2017-03-02T17:38:48.085241: step 4991, loss 0.115834, acc 0.953125
2017-03-02T17:38:48.155190: step 4992, loss 0.22217, acc 0.921875
2017-03-02T17:38:48.225524: step 4993, loss 0.045143, acc 1
2017-03-02T17:38:48.293668: step 4994, loss 0.300785, acc 0.890625
2017-03-02T17:38:48.359761: step 4995, loss 0.247501, acc 0.90625
2017-03-02T17:38:48.431843: step 4996, loss 0.170797, acc 0.96875
2017-03-02T17:38:48.499245: step 4997, loss 0.154373, acc 0.953125
2017-03-02T17:38:48.565748: step 4998, loss 0.120008, acc 0.9375
2017-03-02T17:38:48.632207: step 4999, loss 0.147138, acc 0.921875
2017-03-02T17:38:48.704797: step 5000, loss 0.152036, acc 0.9375

Evaluation:
2017-03-02T17:38:48.732192: step 5000, loss 1.18079, acc 0.685652

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5000

2017-03-02T17:38:49.165665: step 5001, loss 0.289232, acc 0.90625
2017-03-02T17:38:49.232962: step 5002, loss 0.141767, acc 0.953125
2017-03-02T17:38:49.309110: step 5003, loss 0.176641, acc 0.90625
2017-03-02T17:38:49.375359: step 5004, loss 0.298562, acc 0.875
2017-03-02T17:38:49.440742: step 5005, loss 0.428763, acc 0.890625
2017-03-02T17:38:49.510126: step 5006, loss 0.388054, acc 0.859375
2017-03-02T17:38:49.587255: step 5007, loss 0.144354, acc 0.953125
2017-03-02T17:38:49.657626: step 5008, loss 0.190301, acc 0.890625
2017-03-02T17:38:49.736411: step 5009, loss 0.395135, acc 0.890625
2017-03-02T17:38:49.806706: step 5010, loss 0.130215, acc 0.96875
2017-03-02T17:38:49.878016: step 5011, loss 0.301396, acc 0.90625
2017-03-02T17:38:49.947682: step 5012, loss 0.235521, acc 0.90625
2017-03-02T17:38:50.022012: step 5013, loss 0.186053, acc 0.953125
2017-03-02T17:38:50.088997: step 5014, loss 0.208818, acc 0.90625
2017-03-02T17:38:50.155594: step 5015, loss 0.192009, acc 0.9375
2017-03-02T17:38:50.226877: step 5016, loss 0.185926, acc 0.90625
2017-03-02T17:38:50.290331: step 5017, loss 0.183246, acc 0.9375
2017-03-02T17:38:50.359262: step 5018, loss 0.143502, acc 0.953125
2017-03-02T17:38:50.428088: step 5019, loss 0.166676, acc 0.921875
2017-03-02T17:38:50.498671: step 5020, loss 0.279937, acc 0.90625
2017-03-02T17:38:50.563992: step 5021, loss 0.256769, acc 0.9375
2017-03-02T17:38:50.635268: step 5022, loss 0.187721, acc 0.921875
2017-03-02T17:38:50.705308: step 5023, loss 0.146849, acc 0.96875
2017-03-02T17:38:50.777676: step 5024, loss 0.163051, acc 0.9375
2017-03-02T17:38:50.843969: step 5025, loss 0.228096, acc 0.9375
2017-03-02T17:38:50.910627: step 5026, loss 0.204266, acc 0.90625
2017-03-02T17:38:50.981542: step 5027, loss 0.0643846, acc 1
2017-03-02T17:38:51.045957: step 5028, loss 0.214647, acc 0.90625
2017-03-02T17:38:51.112878: step 5029, loss 0.1979, acc 0.953125
2017-03-02T17:38:51.170490: step 5030, loss 0.187426, acc 0.953125
2017-03-02T17:38:51.241398: step 5031, loss 0.128618, acc 0.953125
2017-03-02T17:38:51.303185: step 5032, loss 0.150664, acc 0.890625
2017-03-02T17:38:51.374533: step 5033, loss 0.224599, acc 0.953125
2017-03-02T17:38:51.440849: step 5034, loss 0.169939, acc 0.9375
2017-03-02T17:38:51.508500: step 5035, loss 0.253398, acc 0.890625
2017-03-02T17:38:51.576884: step 5036, loss 0.246105, acc 0.9375
2017-03-02T17:38:51.643866: step 5037, loss 0.282023, acc 0.875
2017-03-02T17:38:51.711609: step 5038, loss 0.357813, acc 0.859375
2017-03-02T17:38:51.783249: step 5039, loss 0.196073, acc 0.953125
2017-03-02T17:38:51.851857: step 5040, loss 0.15801, acc 0.9375
2017-03-02T17:38:51.929145: step 5041, loss 0.0969243, acc 0.953125
2017-03-02T17:38:51.999171: step 5042, loss 0.239486, acc 0.890625
2017-03-02T17:38:52.070216: step 5043, loss 0.172455, acc 0.90625
2017-03-02T17:38:52.136568: step 5044, loss 0.130258, acc 0.953125
2017-03-02T17:38:52.204931: step 5045, loss 0.130282, acc 0.953125
2017-03-02T17:38:52.279146: step 5046, loss 0.285594, acc 0.921875
2017-03-02T17:38:52.348787: step 5047, loss 0.215261, acc 0.921875
2017-03-02T17:38:52.415865: step 5048, loss 0.10146, acc 0.984375
2017-03-02T17:38:52.481922: step 5049, loss 0.195584, acc 0.90625
2017-03-02T17:38:52.547693: step 5050, loss 0.225031, acc 0.921875
2017-03-02T17:38:52.613967: step 5051, loss 0.179161, acc 0.9375
2017-03-02T17:38:52.681942: step 5052, loss 0.14953, acc 0.9375
2017-03-02T17:38:52.748806: step 5053, loss 0.216239, acc 0.890625
2017-03-02T17:38:52.817251: step 5054, loss 0.37102, acc 0.84375
2017-03-02T17:38:52.885532: step 5055, loss 0.462514, acc 0.859375
2017-03-02T17:38:52.952368: step 5056, loss 0.363439, acc 0.890625
2017-03-02T17:38:53.021213: step 5057, loss 0.359777, acc 0.890625
2017-03-02T17:38:53.094322: step 5058, loss 0.234421, acc 0.90625
2017-03-02T17:38:53.165727: step 5059, loss 0.149212, acc 0.96875
2017-03-02T17:38:53.230510: step 5060, loss 0.17979, acc 0.921875
2017-03-02T17:38:53.297985: step 5061, loss 0.280746, acc 0.921875
2017-03-02T17:38:53.367386: step 5062, loss 0.181251, acc 0.9375
2017-03-02T17:38:53.437642: step 5063, loss 0.109684, acc 0.953125
2017-03-02T17:38:53.503321: step 5064, loss 0.260408, acc 0.921875
2017-03-02T17:38:53.568058: step 5065, loss 0.270644, acc 0.921875
2017-03-02T17:38:53.640105: step 5066, loss 0.163223, acc 0.9375
2017-03-02T17:38:53.715422: step 5067, loss 0.229534, acc 0.90625
2017-03-02T17:38:53.786701: step 5068, loss 0.277341, acc 0.890625
2017-03-02T17:38:53.856700: step 5069, loss 0.362173, acc 0.875
2017-03-02T17:38:53.918286: step 5070, loss 0.229806, acc 0.90625
2017-03-02T17:38:53.990069: step 5071, loss 0.267112, acc 0.890625
2017-03-02T17:38:54.056228: step 5072, loss 0.124976, acc 0.9375
2017-03-02T17:38:54.127263: step 5073, loss 0.211357, acc 0.90625
2017-03-02T17:38:54.199585: step 5074, loss 0.139525, acc 0.953125
2017-03-02T17:38:54.270281: step 5075, loss 0.177446, acc 0.9375
2017-03-02T17:38:54.344828: step 5076, loss 0.236036, acc 0.921875
2017-03-02T17:38:54.410006: step 5077, loss 0.47271, acc 0.8125
2017-03-02T17:38:54.476578: step 5078, loss 0.343704, acc 0.875
2017-03-02T17:38:54.540485: step 5079, loss 0.141474, acc 0.9375
2017-03-02T17:38:54.607497: step 5080, loss 0.0666451, acc 1
2017-03-02T17:38:54.676062: step 5081, loss 0.29327, acc 0.90625
2017-03-02T17:38:54.746340: step 5082, loss 0.213759, acc 0.921875
2017-03-02T17:38:54.815983: step 5083, loss 0.211911, acc 0.90625
2017-03-02T17:38:54.884537: step 5084, loss 0.273494, acc 0.90625
2017-03-02T17:38:54.954273: step 5085, loss 0.601747, acc 0.828125
2017-03-02T17:38:55.024611: step 5086, loss 0.321536, acc 0.84375
2017-03-02T17:38:55.096443: step 5087, loss 0.216694, acc 0.90625
2017-03-02T17:38:55.167868: step 5088, loss 0.2123, acc 0.90625
2017-03-02T17:38:55.238498: step 5089, loss 0.356083, acc 0.890625
2017-03-02T17:38:55.313131: step 5090, loss 0.152398, acc 0.90625
2017-03-02T17:38:55.376987: step 5091, loss 0.18573, acc 0.9375
2017-03-02T17:38:55.442897: step 5092, loss 0.335659, acc 0.890625
2017-03-02T17:38:55.513785: step 5093, loss 0.197481, acc 0.9375
2017-03-02T17:38:55.568410: step 5094, loss 0.289114, acc 0.890625
2017-03-02T17:38:55.635950: step 5095, loss 0.209571, acc 0.9375
2017-03-02T17:38:55.696234: step 5096, loss 0.285004, acc 0.75
2017-03-02T17:38:55.769226: step 5097, loss 0.202523, acc 0.9375
2017-03-02T17:38:55.837519: step 5098, loss 0.116877, acc 0.96875
2017-03-02T17:38:55.901144: step 5099, loss 0.195592, acc 0.90625
2017-03-02T17:38:55.973973: step 5100, loss 0.283992, acc 0.890625

Evaluation:
2017-03-02T17:38:56.003667: step 5100, loss 1.1346, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5100

2017-03-02T17:38:56.431668: step 5101, loss 0.140573, acc 0.921875
2017-03-02T17:38:56.494057: step 5102, loss 0.108574, acc 0.984375
2017-03-02T17:38:56.561936: step 5103, loss 0.186181, acc 0.90625
2017-03-02T17:38:56.634343: step 5104, loss 0.216002, acc 0.90625
2017-03-02T17:38:56.703170: step 5105, loss 0.105796, acc 0.9375
2017-03-02T17:38:56.768863: step 5106, loss 0.272389, acc 0.859375
2017-03-02T17:38:56.840053: step 5107, loss 0.151897, acc 0.921875
2017-03-02T17:38:56.905381: step 5108, loss 0.283973, acc 0.890625
2017-03-02T17:38:56.984504: step 5109, loss 0.229912, acc 0.921875
2017-03-02T17:38:57.051833: step 5110, loss 0.166855, acc 0.953125
2017-03-02T17:38:57.118846: step 5111, loss 0.115095, acc 0.96875
2017-03-02T17:38:57.199470: step 5112, loss 0.385433, acc 0.875
2017-03-02T17:38:57.269289: step 5113, loss 0.121342, acc 0.953125
2017-03-02T17:38:57.336554: step 5114, loss 0.313531, acc 0.84375
2017-03-02T17:38:57.403447: step 5115, loss 0.138993, acc 0.9375
2017-03-02T17:38:57.471291: step 5116, loss 0.247374, acc 0.9375
2017-03-02T17:38:57.542448: step 5117, loss 0.29831, acc 0.890625
2017-03-02T17:38:57.611534: step 5118, loss 0.104175, acc 0.96875
2017-03-02T17:38:57.679333: step 5119, loss 0.154092, acc 0.96875
2017-03-02T17:38:57.746542: step 5120, loss 0.236136, acc 0.890625
2017-03-02T17:38:57.814552: step 5121, loss 0.216547, acc 0.90625
2017-03-02T17:38:57.881041: step 5122, loss 0.289264, acc 0.890625
2017-03-02T17:38:57.944803: step 5123, loss 0.237632, acc 0.890625
2017-03-02T17:38:58.012338: step 5124, loss 0.28293, acc 0.921875
2017-03-02T17:38:58.078084: step 5125, loss 0.209421, acc 0.9375
2017-03-02T17:38:58.141618: step 5126, loss 0.13198, acc 0.9375
2017-03-02T17:38:58.210612: step 5127, loss 0.22488, acc 0.90625
2017-03-02T17:38:58.273456: step 5128, loss 0.252651, acc 0.875
2017-03-02T17:38:58.337486: step 5129, loss 0.200676, acc 0.9375
2017-03-02T17:38:58.398017: step 5130, loss 0.136965, acc 0.953125
2017-03-02T17:38:58.466727: step 5131, loss 0.148375, acc 0.90625
2017-03-02T17:38:58.537312: step 5132, loss 0.237326, acc 0.890625
2017-03-02T17:38:58.600922: step 5133, loss 0.265595, acc 0.890625
2017-03-02T17:38:58.664849: step 5134, loss 0.129154, acc 0.9375
2017-03-02T17:38:58.734857: step 5135, loss 0.16371, acc 0.9375
2017-03-02T17:38:58.797844: step 5136, loss 0.203451, acc 0.90625
2017-03-02T17:38:58.864903: step 5137, loss 0.267113, acc 0.875
2017-03-02T17:38:58.935308: step 5138, loss 0.199793, acc 0.921875
2017-03-02T17:38:59.003212: step 5139, loss 0.16032, acc 0.921875
2017-03-02T17:38:59.069892: step 5140, loss 0.0824763, acc 0.984375
2017-03-02T17:38:59.137155: step 5141, loss 0.46045, acc 0.890625
2017-03-02T17:38:59.203406: step 5142, loss 0.206642, acc 0.890625
2017-03-02T17:38:59.266926: step 5143, loss 0.135773, acc 0.953125
2017-03-02T17:38:59.336669: step 5144, loss 0.275895, acc 0.90625
2017-03-02T17:38:59.409461: step 5145, loss 0.177139, acc 0.921875
2017-03-02T17:38:59.483579: step 5146, loss 0.089643, acc 0.953125
2017-03-02T17:38:59.546945: step 5147, loss 0.103594, acc 0.953125
2017-03-02T17:38:59.615986: step 5148, loss 0.34752, acc 0.9375
2017-03-02T17:38:59.682241: step 5149, loss 0.314702, acc 0.90625
2017-03-02T17:38:59.746656: step 5150, loss 0.307687, acc 0.90625
2017-03-02T17:38:59.815999: step 5151, loss 0.214507, acc 0.953125
2017-03-02T17:38:59.883517: step 5152, loss 0.131796, acc 0.953125
2017-03-02T17:38:59.952447: step 5153, loss 0.269654, acc 0.890625
2017-03-02T17:39:00.019204: step 5154, loss 0.248983, acc 0.890625
2017-03-02T17:39:00.086258: step 5155, loss 0.277191, acc 0.921875
2017-03-02T17:39:00.151518: step 5156, loss 0.184562, acc 0.953125
2017-03-02T17:39:00.213803: step 5157, loss 0.291103, acc 0.921875
2017-03-02T17:39:00.281462: step 5158, loss 0.115611, acc 0.953125
2017-03-02T17:39:00.352727: step 5159, loss 0.157982, acc 0.96875
2017-03-02T17:39:00.420635: step 5160, loss 0.173755, acc 0.921875
2017-03-02T17:39:00.486035: step 5161, loss 0.151795, acc 0.953125
2017-03-02T17:39:00.551860: step 5162, loss 0.271728, acc 0.90625
2017-03-02T17:39:00.619961: step 5163, loss 0.112133, acc 0.9375
2017-03-02T17:39:00.683729: step 5164, loss 0.171839, acc 0.9375
2017-03-02T17:39:00.752464: step 5165, loss 0.374884, acc 0.875
2017-03-02T17:39:00.821935: step 5166, loss 0.191532, acc 0.921875
2017-03-02T17:39:00.890986: step 5167, loss 0.161855, acc 0.921875
2017-03-02T17:39:00.956552: step 5168, loss 0.258133, acc 0.90625
2017-03-02T17:39:01.020727: step 5169, loss 0.240069, acc 0.90625
2017-03-02T17:39:01.085947: step 5170, loss 0.295671, acc 0.875
2017-03-02T17:39:01.154412: step 5171, loss 0.163726, acc 0.953125
2017-03-02T17:39:01.219962: step 5172, loss 0.204786, acc 0.90625
2017-03-02T17:39:01.288079: step 5173, loss 0.224047, acc 0.890625
2017-03-02T17:39:01.355292: step 5174, loss 0.0800296, acc 1
2017-03-02T17:39:01.430698: step 5175, loss 0.121419, acc 0.953125
2017-03-02T17:39:01.496556: step 5176, loss 0.10726, acc 0.953125
2017-03-02T17:39:01.560796: step 5177, loss 0.15106, acc 0.9375
2017-03-02T17:39:01.638758: step 5178, loss 0.0740506, acc 0.96875
2017-03-02T17:39:01.711468: step 5179, loss 0.0947158, acc 0.96875
2017-03-02T17:39:01.777597: step 5180, loss 0.232145, acc 0.90625
2017-03-02T17:39:01.847244: step 5181, loss 0.165745, acc 0.953125
2017-03-02T17:39:01.914179: step 5182, loss 0.211363, acc 0.921875
2017-03-02T17:39:01.984194: step 5183, loss 0.135595, acc 0.9375
2017-03-02T17:39:02.063553: step 5184, loss 0.286957, acc 0.875
2017-03-02T17:39:02.150447: step 5185, loss 0.292136, acc 0.90625
2017-03-02T17:39:02.217508: step 5186, loss 0.156608, acc 0.921875
2017-03-02T17:39:02.284097: step 5187, loss 0.245138, acc 0.921875
2017-03-02T17:39:02.357098: step 5188, loss 0.194838, acc 0.90625
2017-03-02T17:39:02.418223: step 5189, loss 0.257386, acc 0.859375
2017-03-02T17:39:02.490648: step 5190, loss 0.101569, acc 0.984375
2017-03-02T17:39:02.566512: step 5191, loss 0.0951876, acc 0.9375
2017-03-02T17:39:02.637523: step 5192, loss 0.1755, acc 0.921875
2017-03-02T17:39:02.702990: step 5193, loss 0.123001, acc 0.921875
2017-03-02T17:39:02.768116: step 5194, loss 0.355333, acc 0.90625
2017-03-02T17:39:02.832469: step 5195, loss 0.293396, acc 0.90625
2017-03-02T17:39:02.897447: step 5196, loss 0.364019, acc 0.921875
2017-03-02T17:39:02.961526: step 5197, loss 0.360712, acc 0.90625
2017-03-02T17:39:03.029014: step 5198, loss 0.236723, acc 0.9375
2017-03-02T17:39:03.100960: step 5199, loss 0.101204, acc 0.96875
2017-03-02T17:39:03.172153: step 5200, loss 0.256336, acc 0.921875

Evaluation:
2017-03-02T17:39:03.200846: step 5200, loss 1.13928, acc 0.680606

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5200

2017-03-02T17:39:03.641378: step 5201, loss 0.268112, acc 0.890625
2017-03-02T17:39:03.707232: step 5202, loss 0.176737, acc 0.953125
2017-03-02T17:39:03.773486: step 5203, loss 0.183277, acc 0.921875
2017-03-02T17:39:03.836477: step 5204, loss 0.120069, acc 0.96875
2017-03-02T17:39:03.913353: step 5205, loss 0.145953, acc 0.953125
2017-03-02T17:39:03.987326: step 5206, loss 0.161532, acc 0.921875
2017-03-02T17:39:04.058838: step 5207, loss 0.257036, acc 0.890625
2017-03-02T17:39:04.126756: step 5208, loss 0.221198, acc 0.921875
2017-03-02T17:39:04.191721: step 5209, loss 0.286513, acc 0.921875
2017-03-02T17:39:04.263576: step 5210, loss 0.174105, acc 0.921875
2017-03-02T17:39:04.330152: step 5211, loss 0.222311, acc 0.921875
2017-03-02T17:39:04.396462: step 5212, loss 0.420892, acc 0.875
2017-03-02T17:39:04.454150: step 5213, loss 0.120261, acc 0.96875
2017-03-02T17:39:04.527185: step 5214, loss 0.170179, acc 0.90625
2017-03-02T17:39:04.601034: step 5215, loss 0.242192, acc 0.921875
2017-03-02T17:39:04.674957: step 5216, loss 0.251778, acc 0.875
2017-03-02T17:39:04.742279: step 5217, loss 0.272661, acc 0.921875
2017-03-02T17:39:04.814766: step 5218, loss 0.157888, acc 0.953125
2017-03-02T17:39:04.882503: step 5219, loss 0.264382, acc 0.890625
2017-03-02T17:39:04.949885: step 5220, loss 0.0927971, acc 1
2017-03-02T17:39:05.019862: step 5221, loss 0.438768, acc 0.828125
2017-03-02T17:39:05.095414: step 5222, loss 0.119723, acc 0.953125
2017-03-02T17:39:05.161860: step 5223, loss 0.227482, acc 0.9375
2017-03-02T17:39:05.226943: step 5224, loss 0.171384, acc 0.9375
2017-03-02T17:39:05.295997: step 5225, loss 0.210457, acc 0.921875
2017-03-02T17:39:05.364210: step 5226, loss 0.225438, acc 0.9375
2017-03-02T17:39:05.430369: step 5227, loss 0.269963, acc 0.90625
2017-03-02T17:39:05.498630: step 5228, loss 0.234423, acc 0.90625
2017-03-02T17:39:05.570593: step 5229, loss 0.232056, acc 0.921875
2017-03-02T17:39:05.636307: step 5230, loss 0.114903, acc 0.953125
2017-03-02T17:39:05.702836: step 5231, loss 0.239385, acc 0.9375
2017-03-02T17:39:05.772467: step 5232, loss 0.281864, acc 0.859375
2017-03-02T17:39:05.831669: step 5233, loss 0.253299, acc 0.859375
2017-03-02T17:39:05.898213: step 5234, loss 0.213851, acc 0.9375
2017-03-02T17:39:05.963228: step 5235, loss 0.208237, acc 0.953125
2017-03-02T17:39:06.029064: step 5236, loss 0.377146, acc 0.890625
2017-03-02T17:39:06.104195: step 5237, loss 0.240979, acc 0.90625
2017-03-02T17:39:06.170638: step 5238, loss 0.213242, acc 0.90625
2017-03-02T17:39:06.242434: step 5239, loss 0.136721, acc 0.953125
2017-03-02T17:39:06.308840: step 5240, loss 0.256621, acc 0.90625
2017-03-02T17:39:06.374153: step 5241, loss 0.257987, acc 0.875
2017-03-02T17:39:06.451755: step 5242, loss 0.241295, acc 0.921875
2017-03-02T17:39:06.521451: step 5243, loss 0.194401, acc 0.90625
2017-03-02T17:39:06.590717: step 5244, loss 0.383249, acc 0.859375
2017-03-02T17:39:06.657463: step 5245, loss 0.39513, acc 0.90625
2017-03-02T17:39:06.728521: step 5246, loss 0.232249, acc 0.921875
2017-03-02T17:39:06.797838: step 5247, loss 0.212968, acc 0.875
2017-03-02T17:39:06.869358: step 5248, loss 0.196897, acc 0.9375
2017-03-02T17:39:06.938641: step 5249, loss 0.207077, acc 0.9375
2017-03-02T17:39:07.001329: step 5250, loss 0.170829, acc 0.921875
2017-03-02T17:39:07.070800: step 5251, loss 0.115669, acc 0.953125
2017-03-02T17:39:07.138137: step 5252, loss 0.277159, acc 0.90625
2017-03-02T17:39:07.205767: step 5253, loss 0.318664, acc 0.859375
2017-03-02T17:39:07.276426: step 5254, loss 0.133878, acc 0.921875
2017-03-02T17:39:07.347765: step 5255, loss 0.193756, acc 0.921875
2017-03-02T17:39:07.414040: step 5256, loss 0.230131, acc 0.9375
2017-03-02T17:39:07.482183: step 5257, loss 0.149394, acc 0.9375
2017-03-02T17:39:07.552095: step 5258, loss 0.516935, acc 0.84375
2017-03-02T17:39:07.624490: step 5259, loss 0.127626, acc 0.984375
2017-03-02T17:39:07.701381: step 5260, loss 0.316935, acc 0.859375
2017-03-02T17:39:07.770346: step 5261, loss 0.229056, acc 0.90625
2017-03-02T17:39:07.837424: step 5262, loss 0.139684, acc 0.921875
2017-03-02T17:39:07.903522: step 5263, loss 0.0957309, acc 0.984375
2017-03-02T17:39:07.972898: step 5264, loss 0.269418, acc 0.921875
2017-03-02T17:39:08.040818: step 5265, loss 0.0606203, acc 1
2017-03-02T17:39:08.107784: step 5266, loss 0.253863, acc 0.9375
2017-03-02T17:39:08.173189: step 5267, loss 0.249094, acc 0.921875
2017-03-02T17:39:08.239711: step 5268, loss 0.186712, acc 0.90625
2017-03-02T17:39:08.310794: step 5269, loss 0.213366, acc 0.921875
2017-03-02T17:39:08.383308: step 5270, loss 0.230861, acc 0.921875
2017-03-02T17:39:08.452242: step 5271, loss 0.13067, acc 0.953125
2017-03-02T17:39:08.520219: step 5272, loss 0.220487, acc 0.953125
2017-03-02T17:39:08.587140: step 5273, loss 0.159985, acc 0.9375
2017-03-02T17:39:08.653659: step 5274, loss 0.200794, acc 0.921875
2017-03-02T17:39:08.712781: step 5275, loss 0.109375, acc 0.96875
2017-03-02T17:39:08.782682: step 5276, loss 0.157418, acc 0.921875
2017-03-02T17:39:08.850557: step 5277, loss 0.319073, acc 0.890625
2017-03-02T17:39:08.924453: step 5278, loss 0.136179, acc 0.953125
2017-03-02T17:39:09.002347: step 5279, loss 0.178134, acc 0.9375
2017-03-02T17:39:09.070626: step 5280, loss 0.165937, acc 0.9375
2017-03-02T17:39:09.137587: step 5281, loss 0.151548, acc 0.921875
2017-03-02T17:39:09.207497: step 5282, loss 0.170599, acc 0.9375
2017-03-02T17:39:09.278114: step 5283, loss 0.230591, acc 0.921875
2017-03-02T17:39:09.338543: step 5284, loss 0.206322, acc 0.90625
2017-03-02T17:39:09.411184: step 5285, loss 0.346817, acc 0.90625
2017-03-02T17:39:09.484518: step 5286, loss 0.168508, acc 0.9375
2017-03-02T17:39:09.557023: step 5287, loss 0.15978, acc 0.9375
2017-03-02T17:39:09.621790: step 5288, loss 0.316726, acc 0.90625
2017-03-02T17:39:09.688622: step 5289, loss 0.254259, acc 0.890625
2017-03-02T17:39:09.757806: step 5290, loss 0.201798, acc 0.90625
2017-03-02T17:39:09.824876: step 5291, loss 0.247679, acc 0.890625
2017-03-02T17:39:09.892634: step 5292, loss 0.27497, acc 0.75
2017-03-02T17:39:09.962250: step 5293, loss 0.248995, acc 0.90625
2017-03-02T17:39:10.026476: step 5294, loss 0.240685, acc 0.890625
2017-03-02T17:39:10.094284: step 5295, loss 0.115492, acc 0.953125
2017-03-02T17:39:10.167492: step 5296, loss 0.28995, acc 0.90625
2017-03-02T17:39:10.236034: step 5297, loss 0.0943434, acc 0.953125
2017-03-02T17:39:10.304153: step 5298, loss 0.105943, acc 0.953125
2017-03-02T17:39:10.371428: step 5299, loss 0.193807, acc 0.90625
2017-03-02T17:39:10.438717: step 5300, loss 0.139002, acc 0.953125

Evaluation:
2017-03-02T17:39:10.465551: step 5300, loss 1.15525, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5300

2017-03-02T17:39:10.951880: step 5301, loss 0.136101, acc 0.953125
2017-03-02T17:39:11.015855: step 5302, loss 0.147249, acc 0.9375
2017-03-02T17:39:11.080877: step 5303, loss 0.157045, acc 0.921875
2017-03-02T17:39:11.148665: step 5304, loss 0.240349, acc 0.90625
2017-03-02T17:39:11.216047: step 5305, loss 0.176693, acc 0.921875
2017-03-02T17:39:11.283792: step 5306, loss 0.144766, acc 0.953125
2017-03-02T17:39:11.354768: step 5307, loss 0.307074, acc 0.875
2017-03-02T17:39:11.427028: step 5308, loss 0.129374, acc 0.96875
2017-03-02T17:39:11.496449: step 5309, loss 0.26656, acc 0.921875
2017-03-02T17:39:11.563278: step 5310, loss 0.185865, acc 0.921875
2017-03-02T17:39:11.631440: step 5311, loss 0.245605, acc 0.890625
2017-03-02T17:39:11.700968: step 5312, loss 0.304001, acc 0.859375
2017-03-02T17:39:11.789431: step 5313, loss 0.183106, acc 0.921875
2017-03-02T17:39:11.861596: step 5314, loss 0.134416, acc 0.9375
2017-03-02T17:39:11.931703: step 5315, loss 0.155496, acc 0.953125
2017-03-02T17:39:11.995446: step 5316, loss 0.226924, acc 0.921875
2017-03-02T17:39:12.061579: step 5317, loss 0.195977, acc 0.9375
2017-03-02T17:39:12.130097: step 5318, loss 0.216556, acc 0.890625
2017-03-02T17:39:12.197313: step 5319, loss 0.199441, acc 0.90625
2017-03-02T17:39:12.268565: step 5320, loss 0.207649, acc 0.921875
2017-03-02T17:39:12.339780: step 5321, loss 0.0583347, acc 0.984375
2017-03-02T17:39:12.409611: step 5322, loss 0.204411, acc 0.953125
2017-03-02T17:39:12.478204: step 5323, loss 0.169757, acc 0.9375
2017-03-02T17:39:12.542666: step 5324, loss 0.261078, acc 0.875
2017-03-02T17:39:12.610149: step 5325, loss 0.214578, acc 0.9375
2017-03-02T17:39:12.680304: step 5326, loss 0.131543, acc 0.921875
2017-03-02T17:39:12.753374: step 5327, loss 0.196961, acc 0.921875
2017-03-02T17:39:12.833557: step 5328, loss 0.137675, acc 0.9375
2017-03-02T17:39:12.901081: step 5329, loss 0.270814, acc 0.890625
2017-03-02T17:39:12.966807: step 5330, loss 0.193638, acc 0.9375
2017-03-02T17:39:13.036362: step 5331, loss 0.156326, acc 0.96875
2017-03-02T17:39:13.106108: step 5332, loss 0.196586, acc 0.96875
2017-03-02T17:39:13.175927: step 5333, loss 0.10951, acc 0.9375
2017-03-02T17:39:13.241532: step 5334, loss 0.132288, acc 0.9375
2017-03-02T17:39:13.326249: step 5335, loss 0.221739, acc 0.90625
2017-03-02T17:39:13.392920: step 5336, loss 0.174359, acc 0.921875
2017-03-02T17:39:13.460396: step 5337, loss 0.158996, acc 0.9375
2017-03-02T17:39:13.535219: step 5338, loss 0.154895, acc 0.953125
2017-03-02T17:39:13.596072: step 5339, loss 0.120891, acc 0.953125
2017-03-02T17:39:13.662048: step 5340, loss 0.146322, acc 0.90625
2017-03-02T17:39:13.726034: step 5341, loss 0.253275, acc 0.875
2017-03-02T17:39:13.792836: step 5342, loss 0.204061, acc 0.921875
2017-03-02T17:39:13.863080: step 5343, loss 0.133076, acc 0.953125
2017-03-02T17:39:13.933970: step 5344, loss 0.0555317, acc 0.984375
2017-03-02T17:39:14.005645: step 5345, loss 0.221675, acc 0.921875
2017-03-02T17:39:14.074603: step 5346, loss 0.238971, acc 0.890625
2017-03-02T17:39:14.142405: step 5347, loss 0.229539, acc 0.921875
2017-03-02T17:39:14.208124: step 5348, loss 0.114072, acc 0.953125
2017-03-02T17:39:14.275105: step 5349, loss 0.284349, acc 0.921875
2017-03-02T17:39:14.345390: step 5350, loss 0.24274, acc 0.921875
2017-03-02T17:39:14.413457: step 5351, loss 0.213941, acc 0.90625
2017-03-02T17:39:14.481789: step 5352, loss 0.226786, acc 0.921875
2017-03-02T17:39:14.549431: step 5353, loss 0.317036, acc 0.859375
2017-03-02T17:39:14.616818: step 5354, loss 0.173499, acc 0.9375
2017-03-02T17:39:14.684419: step 5355, loss 0.124171, acc 0.9375
2017-03-02T17:39:14.755618: step 5356, loss 0.112627, acc 0.96875
2017-03-02T17:39:14.825622: step 5357, loss 0.256496, acc 0.890625
2017-03-02T17:39:14.894021: step 5358, loss 0.276506, acc 0.90625
2017-03-02T17:39:14.957257: step 5359, loss 0.344564, acc 0.84375
2017-03-02T17:39:15.025177: step 5360, loss 0.249216, acc 0.90625
2017-03-02T17:39:15.090974: step 5361, loss 0.186495, acc 0.96875
2017-03-02T17:39:15.158889: step 5362, loss 0.219339, acc 0.890625
2017-03-02T17:39:15.220394: step 5363, loss 0.142094, acc 0.953125
2017-03-02T17:39:15.289500: step 5364, loss 0.197278, acc 0.890625
2017-03-02T17:39:15.353945: step 5365, loss 0.200645, acc 0.90625
2017-03-02T17:39:15.419337: step 5366, loss 0.114793, acc 0.96875
2017-03-02T17:39:15.485919: step 5367, loss 0.0906846, acc 0.953125
2017-03-02T17:39:15.556848: step 5368, loss 0.126849, acc 0.953125
2017-03-02T17:39:15.624610: step 5369, loss 0.300569, acc 0.890625
2017-03-02T17:39:15.690086: step 5370, loss 0.224278, acc 0.90625
2017-03-02T17:39:15.760912: step 5371, loss 0.351081, acc 0.90625
2017-03-02T17:39:15.829461: step 5372, loss 0.51092, acc 0.84375
2017-03-02T17:39:15.895043: step 5373, loss 0.18535, acc 0.953125
2017-03-02T17:39:15.959344: step 5374, loss 0.298561, acc 0.875
2017-03-02T17:39:16.026661: step 5375, loss 0.245287, acc 0.921875
2017-03-02T17:39:16.094899: step 5376, loss 0.103782, acc 0.984375
2017-03-02T17:39:16.159038: step 5377, loss 0.2893, acc 0.859375
2017-03-02T17:39:16.225688: step 5378, loss 0.23889, acc 0.890625
2017-03-02T17:39:16.292481: step 5379, loss 0.313821, acc 0.875
2017-03-02T17:39:16.359549: step 5380, loss 0.291563, acc 0.890625
2017-03-02T17:39:16.420240: step 5381, loss 0.081898, acc 0.984375
2017-03-02T17:39:16.485150: step 5382, loss 0.327615, acc 0.890625
2017-03-02T17:39:16.552968: step 5383, loss 0.270861, acc 0.859375
2017-03-02T17:39:16.618486: step 5384, loss 0.194415, acc 0.921875
2017-03-02T17:39:16.690518: step 5385, loss 0.230205, acc 0.90625
2017-03-02T17:39:16.754884: step 5386, loss 0.226315, acc 0.921875
2017-03-02T17:39:16.818758: step 5387, loss 0.152357, acc 0.9375
2017-03-02T17:39:16.886891: step 5388, loss 0.256061, acc 0.890625
2017-03-02T17:39:16.959442: step 5389, loss 0.134115, acc 0.9375
2017-03-02T17:39:17.022021: step 5390, loss 0.174203, acc 0.921875
2017-03-02T17:39:17.093502: step 5391, loss 0.123439, acc 0.96875
2017-03-02T17:39:17.164105: step 5392, loss 0.218436, acc 0.890625
2017-03-02T17:39:17.234949: step 5393, loss 0.203231, acc 0.96875
2017-03-02T17:39:17.307654: step 5394, loss 0.0927972, acc 0.984375
2017-03-02T17:39:17.375648: step 5395, loss 0.255563, acc 0.90625
2017-03-02T17:39:17.454178: step 5396, loss 0.181638, acc 0.9375
2017-03-02T17:39:17.524073: step 5397, loss 0.102345, acc 0.984375
2017-03-02T17:39:17.594577: step 5398, loss 0.304292, acc 0.921875
2017-03-02T17:39:17.665880: step 5399, loss 0.349356, acc 0.921875
2017-03-02T17:39:17.737463: step 5400, loss 0.223891, acc 0.90625

Evaluation:
2017-03-02T17:39:17.765824: step 5400, loss 1.13451, acc 0.678443

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5400

2017-03-02T17:39:18.204959: step 5401, loss 0.181993, acc 0.921875
2017-03-02T17:39:18.264825: step 5402, loss 0.156736, acc 0.921875
2017-03-02T17:39:18.330950: step 5403, loss 0.263573, acc 0.890625
2017-03-02T17:39:18.399424: step 5404, loss 0.129847, acc 0.9375
2017-03-02T17:39:18.472753: step 5405, loss 0.253676, acc 0.90625
2017-03-02T17:39:18.537256: step 5406, loss 0.253333, acc 0.890625
2017-03-02T17:39:18.604339: step 5407, loss 0.112118, acc 0.96875
2017-03-02T17:39:18.668587: step 5408, loss 0.173893, acc 0.90625
2017-03-02T17:39:18.733508: step 5409, loss 0.212437, acc 0.90625
2017-03-02T17:39:18.798285: step 5410, loss 0.179298, acc 0.953125
2017-03-02T17:39:18.862862: step 5411, loss 0.369421, acc 0.859375
2017-03-02T17:39:18.925279: step 5412, loss 0.145292, acc 0.921875
2017-03-02T17:39:18.994401: step 5413, loss 0.06566, acc 0.984375
2017-03-02T17:39:19.055707: step 5414, loss 0.262481, acc 0.921875
2017-03-02T17:39:19.122739: step 5415, loss 0.220444, acc 0.921875
2017-03-02T17:39:19.197796: step 5416, loss 0.21623, acc 0.90625
2017-03-02T17:39:19.262889: step 5417, loss 0.100652, acc 0.96875
2017-03-02T17:39:19.333837: step 5418, loss 0.314111, acc 0.859375
2017-03-02T17:39:19.399874: step 5419, loss 0.27313, acc 0.875
2017-03-02T17:39:19.470338: step 5420, loss 0.16553, acc 0.890625
2017-03-02T17:39:19.537918: step 5421, loss 0.30506, acc 0.890625
2017-03-02T17:39:19.615248: step 5422, loss 0.338181, acc 0.921875
2017-03-02T17:39:19.677705: step 5423, loss 0.227701, acc 0.921875
2017-03-02T17:39:19.747648: step 5424, loss 0.214902, acc 0.9375
2017-03-02T17:39:19.818779: step 5425, loss 0.229215, acc 0.90625
2017-03-02T17:39:19.885213: step 5426, loss 0.159118, acc 0.921875
2017-03-02T17:39:19.948639: step 5427, loss 0.267581, acc 0.890625
2017-03-02T17:39:20.020487: step 5428, loss 0.108613, acc 0.96875
2017-03-02T17:39:20.092100: step 5429, loss 0.27901, acc 0.875
2017-03-02T17:39:20.163798: step 5430, loss 0.390767, acc 0.875
2017-03-02T17:39:20.233947: step 5431, loss 0.126359, acc 0.953125
2017-03-02T17:39:20.300207: step 5432, loss 0.0696247, acc 0.984375
2017-03-02T17:39:20.366241: step 5433, loss 0.194899, acc 0.921875
2017-03-02T17:39:20.442140: step 5434, loss 0.402125, acc 0.90625
2017-03-02T17:39:20.509885: step 5435, loss 0.155566, acc 0.953125
2017-03-02T17:39:20.571816: step 5436, loss 0.224404, acc 0.890625
2017-03-02T17:39:20.641729: step 5437, loss 0.183641, acc 0.9375
2017-03-02T17:39:20.702831: step 5438, loss 0.207111, acc 0.875
2017-03-02T17:39:20.777010: step 5439, loss 0.113567, acc 0.953125
2017-03-02T17:39:20.851664: step 5440, loss 0.265988, acc 0.875
2017-03-02T17:39:20.907636: step 5441, loss 0.210681, acc 0.9375
2017-03-02T17:39:20.977879: step 5442, loss 0.245035, acc 0.921875
2017-03-02T17:39:21.043019: step 5443, loss 0.22522, acc 0.90625
2017-03-02T17:39:21.108535: step 5444, loss 0.322173, acc 0.875
2017-03-02T17:39:21.174979: step 5445, loss 0.255899, acc 0.90625
2017-03-02T17:39:21.240708: step 5446, loss 0.220973, acc 0.9375
2017-03-02T17:39:21.313893: step 5447, loss 0.148007, acc 0.9375
2017-03-02T17:39:21.383571: step 5448, loss 0.249377, acc 0.90625
2017-03-02T17:39:21.455486: step 5449, loss 0.209303, acc 0.921875
2017-03-02T17:39:21.524945: step 5450, loss 0.271578, acc 0.890625
2017-03-02T17:39:21.593315: step 5451, loss 0.322938, acc 0.875
2017-03-02T17:39:21.664250: step 5452, loss 0.170461, acc 0.921875
2017-03-02T17:39:21.729969: step 5453, loss 0.171844, acc 0.890625
2017-03-02T17:39:21.793314: step 5454, loss 0.206745, acc 0.921875
2017-03-02T17:39:21.868755: step 5455, loss 0.201527, acc 0.9375
2017-03-02T17:39:21.937866: step 5456, loss 0.207196, acc 0.921875
2017-03-02T17:39:22.004176: step 5457, loss 0.217023, acc 0.921875
2017-03-02T17:39:22.072499: step 5458, loss 0.457172, acc 0.90625
2017-03-02T17:39:22.145051: step 5459, loss 0.13768, acc 0.9375
2017-03-02T17:39:22.218820: step 5460, loss 0.25731, acc 0.875
2017-03-02T17:39:22.288906: step 5461, loss 0.195996, acc 0.921875
2017-03-02T17:39:22.352594: step 5462, loss 0.303648, acc 0.90625
2017-03-02T17:39:22.416348: step 5463, loss 0.0926861, acc 0.96875
2017-03-02T17:39:22.488181: step 5464, loss 0.250825, acc 0.921875
2017-03-02T17:39:22.555960: step 5465, loss 0.283596, acc 0.859375
2017-03-02T17:39:22.624233: step 5466, loss 0.240289, acc 0.90625
2017-03-02T17:39:22.682656: step 5467, loss 0.0961674, acc 0.953125
2017-03-02T17:39:22.757430: step 5468, loss 0.0779174, acc 0.96875
2017-03-02T17:39:22.826931: step 5469, loss 0.301533, acc 0.890625
2017-03-02T17:39:22.893198: step 5470, loss 0.183393, acc 0.90625
2017-03-02T17:39:22.961543: step 5471, loss 0.124699, acc 0.96875
2017-03-02T17:39:23.029700: step 5472, loss 0.136142, acc 0.953125
2017-03-02T17:39:23.095525: step 5473, loss 0.12808, acc 0.953125
2017-03-02T17:39:23.173022: step 5474, loss 0.148598, acc 0.921875
2017-03-02T17:39:23.237970: step 5475, loss 0.0853339, acc 0.984375
2017-03-02T17:39:23.303149: step 5476, loss 0.248739, acc 0.9375
2017-03-02T17:39:23.370384: step 5477, loss 0.195119, acc 0.953125
2017-03-02T17:39:23.436517: step 5478, loss 0.296262, acc 0.90625
2017-03-02T17:39:23.506518: step 5479, loss 0.253581, acc 0.921875
2017-03-02T17:39:23.579344: step 5480, loss 0.125394, acc 0.953125
2017-03-02T17:39:23.643738: step 5481, loss 0.277069, acc 0.90625
2017-03-02T17:39:23.728095: step 5482, loss 0.199451, acc 0.921875
2017-03-02T17:39:23.798282: step 5483, loss 0.255079, acc 0.90625
2017-03-02T17:39:23.869016: step 5484, loss 0.23749, acc 0.9375
2017-03-02T17:39:23.929698: step 5485, loss 0.271621, acc 0.90625
2017-03-02T17:39:23.998930: step 5486, loss 0.321556, acc 0.859375
2017-03-02T17:39:24.062342: step 5487, loss 0.392435, acc 0.90625
2017-03-02T17:39:24.125300: step 5488, loss 0.231269, acc 1
2017-03-02T17:39:24.204281: step 5489, loss 0.130925, acc 0.96875
2017-03-02T17:39:24.270963: step 5490, loss 0.153843, acc 0.953125
2017-03-02T17:39:24.332364: step 5491, loss 0.102002, acc 0.984375
2017-03-02T17:39:24.396818: step 5492, loss 0.173682, acc 0.953125
2017-03-02T17:39:24.463970: step 5493, loss 0.159271, acc 0.921875
2017-03-02T17:39:24.533527: step 5494, loss 0.185161, acc 0.875
2017-03-02T17:39:24.602747: step 5495, loss 0.137473, acc 0.9375
2017-03-02T17:39:24.678580: step 5496, loss 0.337618, acc 0.875
2017-03-02T17:39:24.752582: step 5497, loss 0.254246, acc 0.921875
2017-03-02T17:39:24.819729: step 5498, loss 0.166484, acc 0.9375
2017-03-02T17:39:24.896115: step 5499, loss 0.10585, acc 0.984375
2017-03-02T17:39:24.979735: step 5500, loss 0.172372, acc 0.921875

Evaluation:
2017-03-02T17:39:25.009456: step 5500, loss 1.16533, acc 0.67628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5500

2017-03-02T17:39:25.459833: step 5501, loss 0.221147, acc 0.953125
2017-03-02T17:39:25.523785: step 5502, loss 0.160833, acc 0.921875
2017-03-02T17:39:25.590278: step 5503, loss 0.217672, acc 0.9375
2017-03-02T17:39:25.665965: step 5504, loss 0.0687521, acc 0.984375
2017-03-02T17:39:25.736725: step 5505, loss 0.0301265, acc 1
2017-03-02T17:39:25.803493: step 5506, loss 0.0858647, acc 0.984375
2017-03-02T17:39:25.873168: step 5507, loss 0.0969533, acc 0.96875
2017-03-02T17:39:25.934712: step 5508, loss 0.303969, acc 0.875
2017-03-02T17:39:26.000817: step 5509, loss 0.346813, acc 0.875
2017-03-02T17:39:26.067983: step 5510, loss 0.253643, acc 0.875
2017-03-02T17:39:26.134152: step 5511, loss 0.184912, acc 0.9375
2017-03-02T17:39:26.200183: step 5512, loss 0.139431, acc 0.9375
2017-03-02T17:39:26.266157: step 5513, loss 0.209536, acc 0.921875
2017-03-02T17:39:26.324449: step 5514, loss 0.0879341, acc 0.96875
2017-03-02T17:39:26.391741: step 5515, loss 0.237949, acc 0.9375
2017-03-02T17:39:26.461055: step 5516, loss 0.30523, acc 0.890625
2017-03-02T17:39:26.526527: step 5517, loss 0.125263, acc 0.953125
2017-03-02T17:39:26.587668: step 5518, loss 0.0948211, acc 0.96875
2017-03-02T17:39:26.649995: step 5519, loss 0.138995, acc 0.953125
2017-03-02T17:39:26.718263: step 5520, loss 0.155437, acc 0.9375
2017-03-02T17:39:26.787274: step 5521, loss 0.144267, acc 0.9375
2017-03-02T17:39:26.855224: step 5522, loss 0.2404, acc 0.921875
2017-03-02T17:39:26.921858: step 5523, loss 0.127902, acc 0.921875
2017-03-02T17:39:26.990140: step 5524, loss 0.257508, acc 0.859375
2017-03-02T17:39:27.055234: step 5525, loss 0.0954281, acc 0.96875
2017-03-02T17:39:27.119923: step 5526, loss 0.126682, acc 0.953125
2017-03-02T17:39:27.188814: step 5527, loss 0.111931, acc 0.953125
2017-03-02T17:39:27.254584: step 5528, loss 0.214376, acc 0.921875
2017-03-02T17:39:27.320454: step 5529, loss 0.254784, acc 0.9375
2017-03-02T17:39:27.387857: step 5530, loss 0.279468, acc 0.890625
2017-03-02T17:39:27.458767: step 5531, loss 0.0826311, acc 0.984375
2017-03-02T17:39:27.522667: step 5532, loss 0.155677, acc 0.921875
2017-03-02T17:39:27.588179: step 5533, loss 0.133419, acc 0.9375
2017-03-02T17:39:27.656622: step 5534, loss 0.237843, acc 0.90625
2017-03-02T17:39:27.724347: step 5535, loss 0.221346, acc 0.9375
2017-03-02T17:39:27.794582: step 5536, loss 0.162246, acc 0.953125
2017-03-02T17:39:27.862338: step 5537, loss 0.175276, acc 0.96875
2017-03-02T17:39:27.930986: step 5538, loss 0.0467352, acc 0.984375
2017-03-02T17:39:27.999422: step 5539, loss 0.193891, acc 0.9375
2017-03-02T17:39:28.072441: step 5540, loss 0.22053, acc 0.96875
2017-03-02T17:39:28.147212: step 5541, loss 0.275325, acc 0.9375
2017-03-02T17:39:28.217632: step 5542, loss 0.153647, acc 0.921875
2017-03-02T17:39:28.289055: step 5543, loss 0.0959778, acc 0.96875
2017-03-02T17:39:28.357853: step 5544, loss 0.233795, acc 0.90625
2017-03-02T17:39:28.423756: step 5545, loss 0.248791, acc 0.875
2017-03-02T17:39:28.492425: step 5546, loss 0.237887, acc 0.890625
2017-03-02T17:39:28.564025: step 5547, loss 0.0450255, acc 1
2017-03-02T17:39:28.633537: step 5548, loss 0.228093, acc 0.9375
2017-03-02T17:39:28.698981: step 5549, loss 0.149665, acc 0.9375
2017-03-02T17:39:28.768201: step 5550, loss 0.278354, acc 0.875
2017-03-02T17:39:28.836965: step 5551, loss 0.148225, acc 0.921875
2017-03-02T17:39:28.904692: step 5552, loss 0.174099, acc 0.875
2017-03-02T17:39:28.971993: step 5553, loss 0.223453, acc 0.90625
2017-03-02T17:39:29.040330: step 5554, loss 0.0942493, acc 0.96875
2017-03-02T17:39:29.106457: step 5555, loss 0.267193, acc 0.875
2017-03-02T17:39:29.178235: step 5556, loss 0.262032, acc 0.90625
2017-03-02T17:39:29.255819: step 5557, loss 0.140987, acc 0.96875
2017-03-02T17:39:29.325595: step 5558, loss 0.272736, acc 0.890625
2017-03-02T17:39:29.393170: step 5559, loss 0.148411, acc 0.953125
2017-03-02T17:39:29.461367: step 5560, loss 0.190626, acc 0.921875
2017-03-02T17:39:29.528643: step 5561, loss 0.281966, acc 0.90625
2017-03-02T17:39:29.597091: step 5562, loss 0.181522, acc 0.96875
2017-03-02T17:39:29.666926: step 5563, loss 0.158164, acc 0.921875
2017-03-02T17:39:29.732111: step 5564, loss 0.175376, acc 0.953125
2017-03-02T17:39:29.802714: step 5565, loss 0.2264, acc 0.953125
2017-03-02T17:39:29.870550: step 5566, loss 0.186381, acc 0.890625
2017-03-02T17:39:29.943010: step 5567, loss 0.185337, acc 0.953125
2017-03-02T17:39:30.000819: step 5568, loss 0.314388, acc 0.90625
2017-03-02T17:39:30.067009: step 5569, loss 0.358531, acc 0.90625
2017-03-02T17:39:30.134454: step 5570, loss 0.18062, acc 0.90625
2017-03-02T17:39:30.191183: step 5571, loss 0.342228, acc 0.890625
2017-03-02T17:39:30.290059: step 5572, loss 0.193663, acc 0.953125
2017-03-02T17:39:30.355387: step 5573, loss 0.143939, acc 0.90625
2017-03-02T17:39:30.422802: step 5574, loss 0.192719, acc 0.921875
2017-03-02T17:39:30.486557: step 5575, loss 0.220245, acc 0.90625
2017-03-02T17:39:30.554576: step 5576, loss 0.21682, acc 0.921875
2017-03-02T17:39:30.622366: step 5577, loss 0.1008, acc 0.96875
2017-03-02T17:39:30.702749: step 5578, loss 0.329999, acc 0.875
2017-03-02T17:39:30.771339: step 5579, loss 0.263357, acc 0.84375
2017-03-02T17:39:30.843005: step 5580, loss 0.151788, acc 0.9375
2017-03-02T17:39:30.909142: step 5581, loss 0.11839, acc 0.953125
2017-03-02T17:39:30.976128: step 5582, loss 0.230022, acc 0.90625
2017-03-02T17:39:31.041851: step 5583, loss 0.241107, acc 0.890625
2017-03-02T17:39:31.110972: step 5584, loss 0.115351, acc 0.96875
2017-03-02T17:39:31.180863: step 5585, loss 0.233827, acc 0.90625
2017-03-02T17:39:31.257501: step 5586, loss 0.12233, acc 0.9375
2017-03-02T17:39:31.324564: step 5587, loss 0.160652, acc 0.9375
2017-03-02T17:39:31.399955: step 5588, loss 0.146382, acc 0.921875
2017-03-02T17:39:31.467506: step 5589, loss 0.097062, acc 0.96875
2017-03-02T17:39:31.537664: step 5590, loss 0.292337, acc 0.921875
2017-03-02T17:39:31.603952: step 5591, loss 0.216432, acc 0.90625
2017-03-02T17:39:31.672390: step 5592, loss 0.240888, acc 0.90625
2017-03-02T17:39:31.734139: step 5593, loss 0.105229, acc 0.984375
2017-03-02T17:39:31.794816: step 5594, loss 0.133266, acc 0.9375
2017-03-02T17:39:31.867982: step 5595, loss 0.33225, acc 0.90625
2017-03-02T17:39:31.937681: step 5596, loss 0.235481, acc 0.921875
2017-03-02T17:39:32.008660: step 5597, loss 0.17473, acc 0.921875
2017-03-02T17:39:32.075830: step 5598, loss 0.276427, acc 0.890625
2017-03-02T17:39:32.143613: step 5599, loss 0.37815, acc 0.796875
2017-03-02T17:39:32.216380: step 5600, loss 0.0858677, acc 0.96875

Evaluation:
2017-03-02T17:39:32.242249: step 5600, loss 1.15754, acc 0.66907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5600

2017-03-02T17:39:32.673014: step 5601, loss 0.166513, acc 0.9375
2017-03-02T17:39:32.736664: step 5602, loss 0.333619, acc 0.859375
2017-03-02T17:39:32.827838: step 5603, loss 0.158829, acc 0.9375
2017-03-02T17:39:32.894358: step 5604, loss 0.233867, acc 0.921875
2017-03-02T17:39:32.979231: step 5605, loss 0.287328, acc 0.875
2017-03-02T17:39:33.050069: step 5606, loss 0.394674, acc 0.859375
2017-03-02T17:39:33.126530: step 5607, loss 0.184225, acc 0.9375
2017-03-02T17:39:33.195704: step 5608, loss 0.189338, acc 0.9375
2017-03-02T17:39:33.268573: step 5609, loss 0.351506, acc 0.890625
2017-03-02T17:39:33.338747: step 5610, loss 0.123535, acc 0.953125
2017-03-02T17:39:33.409901: step 5611, loss 0.0752001, acc 0.984375
2017-03-02T17:39:33.474687: step 5612, loss 0.0601954, acc 1
2017-03-02T17:39:33.544953: step 5613, loss 0.320568, acc 0.890625
2017-03-02T17:39:33.613928: step 5614, loss 0.285909, acc 0.90625
2017-03-02T17:39:33.680784: step 5615, loss 0.294766, acc 0.90625
2017-03-02T17:39:33.746492: step 5616, loss 0.202586, acc 0.875
2017-03-02T17:39:33.814214: step 5617, loss 0.340012, acc 0.875
2017-03-02T17:39:33.878876: step 5618, loss 0.295768, acc 0.859375
2017-03-02T17:39:33.949492: step 5619, loss 0.171978, acc 0.921875
2017-03-02T17:39:34.017737: step 5620, loss 0.127204, acc 0.953125
2017-03-02T17:39:34.084230: step 5621, loss 0.0609627, acc 0.984375
2017-03-02T17:39:34.156026: step 5622, loss 0.246426, acc 0.875
2017-03-02T17:39:34.222618: step 5623, loss 0.232996, acc 0.890625
2017-03-02T17:39:34.290995: step 5624, loss 0.205497, acc 0.890625
2017-03-02T17:39:34.361149: step 5625, loss 0.124963, acc 0.953125
2017-03-02T17:39:34.433940: step 5626, loss 0.163108, acc 0.921875
2017-03-02T17:39:34.503949: step 5627, loss 0.229511, acc 0.890625
2017-03-02T17:39:34.575122: step 5628, loss 0.206986, acc 0.921875
2017-03-02T17:39:34.654537: step 5629, loss 0.1762, acc 0.9375
2017-03-02T17:39:34.729978: step 5630, loss 0.190173, acc 0.921875
2017-03-02T17:39:34.804986: step 5631, loss 0.140603, acc 0.9375
2017-03-02T17:39:34.879979: step 5632, loss 0.259766, acc 0.890625
2017-03-02T17:39:34.963245: step 5633, loss 0.236961, acc 0.90625
2017-03-02T17:39:35.038143: step 5634, loss 0.16681, acc 0.9375
2017-03-02T17:39:35.114592: step 5635, loss 0.268989, acc 0.875
2017-03-02T17:39:35.187206: step 5636, loss 0.13611, acc 0.953125
2017-03-02T17:39:35.258414: step 5637, loss 0.199108, acc 0.921875
2017-03-02T17:39:35.333603: step 5638, loss 0.268404, acc 0.921875
2017-03-02T17:39:35.411059: step 5639, loss 0.217106, acc 0.9375
2017-03-02T17:39:35.491360: step 5640, loss 0.258571, acc 0.921875
2017-03-02T17:39:35.562063: step 5641, loss 0.25341, acc 0.859375
2017-03-02T17:39:35.632880: step 5642, loss 0.238259, acc 0.875
2017-03-02T17:39:35.702220: step 5643, loss 0.315073, acc 0.90625
2017-03-02T17:39:35.778365: step 5644, loss 0.240456, acc 0.90625
2017-03-02T17:39:35.850149: step 5645, loss 0.179128, acc 0.9375
2017-03-02T17:39:35.921275: step 5646, loss 0.143259, acc 0.9375
2017-03-02T17:39:35.997000: step 5647, loss 0.206622, acc 0.9375
2017-03-02T17:39:36.074490: step 5648, loss 0.22945, acc 0.875
2017-03-02T17:39:36.162019: step 5649, loss 0.313396, acc 0.875
2017-03-02T17:39:36.241230: step 5650, loss 0.228558, acc 0.9375
2017-03-02T17:39:36.314285: step 5651, loss 0.24963, acc 0.90625
2017-03-02T17:39:36.387219: step 5652, loss 0.233432, acc 0.921875
2017-03-02T17:39:36.469603: step 5653, loss 0.332493, acc 0.890625
2017-03-02T17:39:36.537247: step 5654, loss 0.11441, acc 0.953125
2017-03-02T17:39:36.605941: step 5655, loss 0.388211, acc 0.859375
2017-03-02T17:39:36.673922: step 5656, loss 0.314769, acc 0.90625
2017-03-02T17:39:36.747054: step 5657, loss 0.150414, acc 0.96875
2017-03-02T17:39:36.826244: step 5658, loss 0.204318, acc 0.90625
2017-03-02T17:39:36.894782: step 5659, loss 0.0627109, acc 0.984375
2017-03-02T17:39:36.966330: step 5660, loss 0.164713, acc 0.921875
2017-03-02T17:39:37.039130: step 5661, loss 0.379827, acc 0.875
2017-03-02T17:39:37.110844: step 5662, loss 0.353416, acc 0.84375
2017-03-02T17:39:37.187749: step 5663, loss 0.271847, acc 0.875
2017-03-02T17:39:37.255529: step 5664, loss 0.131638, acc 0.9375
2017-03-02T17:39:37.327216: step 5665, loss 0.355681, acc 0.859375
2017-03-02T17:39:37.406089: step 5666, loss 0.216067, acc 0.890625
2017-03-02T17:39:37.480576: step 5667, loss 0.129909, acc 0.953125
2017-03-02T17:39:37.559095: step 5668, loss 0.210326, acc 0.953125
2017-03-02T17:39:37.656554: step 5669, loss 0.221833, acc 0.890625
2017-03-02T17:39:37.723722: step 5670, loss 0.181454, acc 0.9375
2017-03-02T17:39:37.794222: step 5671, loss 0.264445, acc 0.953125
2017-03-02T17:39:37.867771: step 5672, loss 0.160025, acc 0.921875
2017-03-02T17:39:37.941179: step 5673, loss 0.224558, acc 0.875
2017-03-02T17:39:38.034903: step 5674, loss 0.245012, acc 0.921875
2017-03-02T17:39:38.112924: step 5675, loss 0.233318, acc 0.875
2017-03-02T17:39:38.186933: step 5676, loss 0.261328, acc 0.921875
2017-03-02T17:39:38.259457: step 5677, loss 0.301775, acc 0.890625
2017-03-02T17:39:38.334035: step 5678, loss 0.254987, acc 0.890625
2017-03-02T17:39:38.410119: step 5679, loss 0.139097, acc 0.921875
2017-03-02T17:39:38.479396: step 5680, loss 0.0705065, acc 0.984375
2017-03-02T17:39:38.552687: step 5681, loss 0.232739, acc 0.953125
2017-03-02T17:39:38.623378: step 5682, loss 0.13032, acc 0.953125
2017-03-02T17:39:38.690461: step 5683, loss 0.281259, acc 0.890625
2017-03-02T17:39:38.761088: step 5684, loss 0.0252522, acc 1
2017-03-02T17:39:38.842995: step 5685, loss 0.113294, acc 0.984375
2017-03-02T17:39:38.911721: step 5686, loss 0.239767, acc 0.90625
2017-03-02T17:39:38.983688: step 5687, loss 0.10021, acc 0.96875
2017-03-02T17:39:39.064234: step 5688, loss 0.0915297, acc 0.953125
2017-03-02T17:39:39.146176: step 5689, loss 0.13588, acc 0.953125
2017-03-02T17:39:39.220863: step 5690, loss 0.143789, acc 0.953125
2017-03-02T17:39:39.287658: step 5691, loss 0.244625, acc 0.921875
2017-03-02T17:39:39.361048: step 5692, loss 0.124809, acc 0.953125
2017-03-02T17:39:39.438256: step 5693, loss 0.305751, acc 0.84375
2017-03-02T17:39:39.512310: step 5694, loss 0.140161, acc 0.953125
2017-03-02T17:39:39.585565: step 5695, loss 0.187161, acc 0.9375
2017-03-02T17:39:39.649937: step 5696, loss 0.0741581, acc 0.984375
2017-03-02T17:39:39.721070: step 5697, loss 0.238366, acc 0.921875
2017-03-02T17:39:39.794211: step 5698, loss 0.146219, acc 0.953125
2017-03-02T17:39:39.868243: step 5699, loss 0.272049, acc 0.921875
2017-03-02T17:39:39.944494: step 5700, loss 0.157773, acc 0.953125

Evaluation:
2017-03-02T17:39:39.975189: step 5700, loss 1.15427, acc 0.666186

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5700

2017-03-02T17:39:40.430179: step 5701, loss 0.19146, acc 0.9375
2017-03-02T17:39:40.506175: step 5702, loss 0.369696, acc 0.84375
2017-03-02T17:39:40.577874: step 5703, loss 0.0982694, acc 0.953125
2017-03-02T17:39:40.648073: step 5704, loss 0.169844, acc 0.9375
2017-03-02T17:39:40.721671: step 5705, loss 0.183568, acc 0.921875
2017-03-02T17:39:40.794657: step 5706, loss 0.188781, acc 0.90625
2017-03-02T17:39:40.870689: step 5707, loss 0.251662, acc 0.890625
2017-03-02T17:39:40.948918: step 5708, loss 0.422418, acc 0.859375
2017-03-02T17:39:41.018859: step 5709, loss 0.17075, acc 0.9375
2017-03-02T17:39:41.106282: step 5710, loss 0.282663, acc 0.84375
2017-03-02T17:39:41.186094: step 5711, loss 0.148464, acc 0.96875
2017-03-02T17:39:41.262977: step 5712, loss 0.313321, acc 0.875
2017-03-02T17:39:41.336881: step 5713, loss 0.326474, acc 0.859375
2017-03-02T17:39:41.407808: step 5714, loss 0.129996, acc 0.96875
2017-03-02T17:39:41.479194: step 5715, loss 0.132378, acc 0.9375
2017-03-02T17:39:41.554390: step 5716, loss 0.197515, acc 0.921875
2017-03-02T17:39:41.627326: step 5717, loss 0.168264, acc 0.9375
2017-03-02T17:39:41.701287: step 5718, loss 0.224963, acc 0.921875
2017-03-02T17:39:41.772956: step 5719, loss 0.136094, acc 0.9375
2017-03-02T17:39:41.855575: step 5720, loss 0.221581, acc 0.9375
2017-03-02T17:39:41.934151: step 5721, loss 0.153915, acc 0.984375
2017-03-02T17:39:42.004005: step 5722, loss 0.0803305, acc 0.96875
2017-03-02T17:39:42.073314: step 5723, loss 0.156861, acc 0.9375
2017-03-02T17:39:42.150931: step 5724, loss 0.115821, acc 0.953125
2017-03-02T17:39:42.226163: step 5725, loss 0.345604, acc 0.90625
2017-03-02T17:39:42.312361: step 5726, loss 0.13319, acc 0.953125
2017-03-02T17:39:42.392759: step 5727, loss 0.216534, acc 0.921875
2017-03-02T17:39:42.459422: step 5728, loss 0.109822, acc 0.96875
2017-03-02T17:39:42.532962: step 5729, loss 0.286999, acc 0.921875
2017-03-02T17:39:42.601941: step 5730, loss 0.145189, acc 0.9375
2017-03-02T17:39:42.674325: step 5731, loss 0.129884, acc 0.9375
2017-03-02T17:39:42.738244: step 5732, loss 0.224783, acc 0.890625
2017-03-02T17:39:42.811887: step 5733, loss 0.187951, acc 0.921875
2017-03-02T17:39:42.884459: step 5734, loss 0.301638, acc 0.875
2017-03-02T17:39:42.952437: step 5735, loss 0.174546, acc 0.9375
2017-03-02T17:39:43.054531: step 5736, loss 0.203202, acc 0.921875
2017-03-02T17:39:43.121044: step 5737, loss 0.197979, acc 0.921875
2017-03-02T17:39:43.205724: step 5738, loss 0.11334, acc 0.953125
2017-03-02T17:39:43.279300: step 5739, loss 0.155756, acc 0.953125
2017-03-02T17:39:43.344271: step 5740, loss 0.15412, acc 0.953125
2017-03-02T17:39:43.411499: step 5741, loss 0.221422, acc 0.90625
2017-03-02T17:39:43.484956: step 5742, loss 0.135779, acc 0.921875
2017-03-02T17:39:43.555541: step 5743, loss 0.177761, acc 0.90625
2017-03-02T17:39:43.658888: step 5744, loss 0.144147, acc 0.953125
2017-03-02T17:39:43.735649: step 5745, loss 0.269958, acc 0.890625
2017-03-02T17:39:43.807433: step 5746, loss 0.141348, acc 0.921875
2017-03-02T17:39:43.881177: step 5747, loss 0.203842, acc 0.875
2017-03-02T17:39:43.956613: step 5748, loss 0.124966, acc 0.953125
2017-03-02T17:39:44.025673: step 5749, loss 0.155019, acc 0.953125
2017-03-02T17:39:44.093631: step 5750, loss 0.284976, acc 0.890625
2017-03-02T17:39:44.166030: step 5751, loss 0.241111, acc 0.921875
2017-03-02T17:39:44.240598: step 5752, loss 0.172069, acc 0.90625
2017-03-02T17:39:44.315290: step 5753, loss 0.28993, acc 0.90625
2017-03-02T17:39:44.390743: step 5754, loss 0.2331, acc 0.890625
2017-03-02T17:39:44.465558: step 5755, loss 0.110204, acc 0.96875
2017-03-02T17:39:44.537193: step 5756, loss 0.152725, acc 0.9375
2017-03-02T17:39:44.611644: step 5757, loss 0.126398, acc 0.9375
2017-03-02T17:39:44.683695: step 5758, loss 0.301207, acc 0.90625
2017-03-02T17:39:44.753824: step 5759, loss 0.271438, acc 0.9375
2017-03-02T17:39:44.826799: step 5760, loss 0.245202, acc 0.921875
2017-03-02T17:39:44.898468: step 5761, loss 0.118367, acc 0.96875
2017-03-02T17:39:44.969555: step 5762, loss 0.145946, acc 0.921875
2017-03-02T17:39:45.046381: step 5763, loss 0.252095, acc 0.9375
2017-03-02T17:39:45.125497: step 5764, loss 0.202371, acc 0.90625
2017-03-02T17:39:45.194729: step 5765, loss 0.304898, acc 0.890625
2017-03-02T17:39:45.269143: step 5766, loss 0.247858, acc 0.890625
2017-03-02T17:39:45.341902: step 5767, loss 0.28099, acc 0.890625
2017-03-02T17:39:45.410518: step 5768, loss 0.112873, acc 0.921875
2017-03-02T17:39:45.487654: step 5769, loss 0.247221, acc 0.90625
2017-03-02T17:39:45.559123: step 5770, loss 0.162639, acc 0.921875
2017-03-02T17:39:45.631575: step 5771, loss 0.169078, acc 0.921875
2017-03-02T17:39:45.701908: step 5772, loss 0.323371, acc 0.875
2017-03-02T17:39:45.788018: step 5773, loss 0.0883751, acc 0.984375
2017-03-02T17:39:45.859559: step 5774, loss 0.477349, acc 0.828125
2017-03-02T17:39:45.929193: step 5775, loss 0.229861, acc 0.9375
2017-03-02T17:39:46.006661: step 5776, loss 0.154511, acc 0.9375
2017-03-02T17:39:46.077392: step 5777, loss 0.344527, acc 0.875
2017-03-02T17:39:46.164026: step 5778, loss 0.199088, acc 0.9375
2017-03-02T17:39:46.242849: step 5779, loss 0.185757, acc 0.9375
2017-03-02T17:39:46.316397: step 5780, loss 0.22878, acc 0.921875
2017-03-02T17:39:46.386938: step 5781, loss 0.226649, acc 0.921875
2017-03-02T17:39:46.463704: step 5782, loss 0.160161, acc 0.921875
2017-03-02T17:39:46.531924: step 5783, loss 0.168261, acc 0.9375
2017-03-02T17:39:46.604798: step 5784, loss 0.15601, acc 0.953125
2017-03-02T17:39:46.676335: step 5785, loss 0.246877, acc 0.890625
2017-03-02T17:39:46.742404: step 5786, loss 0.0974642, acc 0.984375
2017-03-02T17:39:46.809295: step 5787, loss 0.495755, acc 0.890625
2017-03-02T17:39:46.880708: step 5788, loss 0.225249, acc 0.890625
2017-03-02T17:39:46.951014: step 5789, loss 0.306564, acc 0.90625
2017-03-02T17:39:47.030522: step 5790, loss 0.14546, acc 0.9375
2017-03-02T17:39:47.116611: step 5791, loss 0.284925, acc 0.890625
2017-03-02T17:39:47.195318: step 5792, loss 0.265097, acc 0.890625
2017-03-02T17:39:47.271191: step 5793, loss 0.140738, acc 0.953125
2017-03-02T17:39:47.348055: step 5794, loss 0.218484, acc 0.921875
2017-03-02T17:39:47.414919: step 5795, loss 0.234116, acc 0.90625
2017-03-02T17:39:47.481681: step 5796, loss 0.277978, acc 0.859375
2017-03-02T17:39:47.551777: step 5797, loss 0.328846, acc 0.875
2017-03-02T17:39:47.631419: step 5798, loss 0.295312, acc 0.890625
2017-03-02T17:39:47.703731: step 5799, loss 0.136202, acc 0.921875
2017-03-02T17:39:47.776078: step 5800, loss 0.056891, acc 1

Evaluation:
2017-03-02T17:39:47.813096: step 5800, loss 1.17288, acc 0.670512

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5800

2017-03-02T17:39:48.334390: step 5801, loss 0.128851, acc 0.96875
2017-03-02T17:39:48.406136: step 5802, loss 0.303371, acc 0.890625
2017-03-02T17:39:48.484605: step 5803, loss 0.445912, acc 0.890625
2017-03-02T17:39:48.560128: step 5804, loss 0.0885412, acc 0.953125
2017-03-02T17:39:48.632045: step 5805, loss 0.0968248, acc 0.9375
2017-03-02T17:39:48.720601: step 5806, loss 0.141282, acc 0.921875
2017-03-02T17:39:48.787190: step 5807, loss 0.208466, acc 0.859375
2017-03-02T17:39:48.846205: step 5808, loss 0.134113, acc 0.9375
2017-03-02T17:39:48.920558: step 5809, loss 0.174028, acc 0.953125
2017-03-02T17:39:48.988828: step 5810, loss 0.265691, acc 0.921875
2017-03-02T17:39:49.059378: step 5811, loss 0.176746, acc 0.921875
2017-03-02T17:39:49.132216: step 5812, loss 0.266348, acc 0.90625
2017-03-02T17:39:49.224479: step 5813, loss 0.3359, acc 0.90625
2017-03-02T17:39:49.295088: step 5814, loss 0.163731, acc 0.953125
2017-03-02T17:39:49.369012: step 5815, loss 0.20619, acc 0.875
2017-03-02T17:39:49.441368: step 5816, loss 0.142202, acc 0.953125
2017-03-02T17:39:49.510496: step 5817, loss 0.174153, acc 0.9375
2017-03-02T17:39:49.576250: step 5818, loss 0.275057, acc 0.90625
2017-03-02T17:39:49.647132: step 5819, loss 0.177141, acc 0.953125
2017-03-02T17:39:49.723010: step 5820, loss 0.157158, acc 0.9375
2017-03-02T17:39:49.801815: step 5821, loss 0.143971, acc 0.9375
2017-03-02T17:39:49.892775: step 5822, loss 0.136143, acc 0.953125
2017-03-02T17:39:49.960208: step 5823, loss 0.141431, acc 0.9375
2017-03-02T17:39:50.032415: step 5824, loss 0.115653, acc 0.953125
2017-03-02T17:39:50.104631: step 5825, loss 0.249391, acc 0.90625
2017-03-02T17:39:50.172341: step 5826, loss 0.170253, acc 0.9375
2017-03-02T17:39:50.241815: step 5827, loss 0.299049, acc 0.875
2017-03-02T17:39:50.328918: step 5828, loss 0.220575, acc 0.9375
2017-03-02T17:39:50.408427: step 5829, loss 0.288246, acc 0.875
2017-03-02T17:39:50.479279: step 5830, loss 0.152632, acc 0.921875
2017-03-02T17:39:50.551822: step 5831, loss 0.188764, acc 0.921875
2017-03-02T17:39:50.616386: step 5832, loss 0.171124, acc 0.9375
2017-03-02T17:39:50.688774: step 5833, loss 0.348374, acc 0.859375
2017-03-02T17:39:50.768583: step 5834, loss 0.157646, acc 0.953125
2017-03-02T17:39:50.832294: step 5835, loss 0.171153, acc 0.953125
2017-03-02T17:39:50.898015: step 5836, loss 0.144356, acc 0.9375
2017-03-02T17:39:50.970956: step 5837, loss 0.269362, acc 0.921875
2017-03-02T17:39:51.051916: step 5838, loss 0.259245, acc 0.890625
2017-03-02T17:39:51.126927: step 5839, loss 0.300671, acc 0.875
2017-03-02T17:39:51.205008: step 5840, loss 0.4239, acc 0.84375
2017-03-02T17:39:51.283074: step 5841, loss 0.228621, acc 0.859375
2017-03-02T17:39:51.356911: step 5842, loss 0.202087, acc 0.921875
2017-03-02T17:39:51.427851: step 5843, loss 0.244201, acc 0.90625
2017-03-02T17:39:51.498310: step 5844, loss 0.234489, acc 0.9375
2017-03-02T17:39:51.567241: step 5845, loss 0.128485, acc 0.9375
2017-03-02T17:39:51.639627: step 5846, loss 0.152026, acc 0.921875
2017-03-02T17:39:51.713617: step 5847, loss 0.25693, acc 0.859375
2017-03-02T17:39:51.785079: step 5848, loss 0.205573, acc 0.9375
2017-03-02T17:39:51.863300: step 5849, loss 0.301673, acc 0.875
2017-03-02T17:39:51.944778: step 5850, loss 0.278798, acc 0.875
2017-03-02T17:39:52.012717: step 5851, loss 0.140489, acc 0.921875
2017-03-02T17:39:52.085010: step 5852, loss 0.184321, acc 0.953125
2017-03-02T17:39:52.164024: step 5853, loss 0.251175, acc 0.921875
2017-03-02T17:39:52.227998: step 5854, loss 0.103119, acc 0.953125
2017-03-02T17:39:52.296004: step 5855, loss 0.153983, acc 0.953125
2017-03-02T17:39:52.379475: step 5856, loss 0.211943, acc 0.9375
2017-03-02T17:39:52.457198: step 5857, loss 0.154024, acc 0.96875
2017-03-02T17:39:52.536764: step 5858, loss 0.151244, acc 0.921875
2017-03-02T17:39:52.613860: step 5859, loss 0.21981, acc 0.890625
2017-03-02T17:39:52.686970: step 5860, loss 0.236222, acc 0.890625
2017-03-02T17:39:52.768635: step 5861, loss 0.224266, acc 0.921875
2017-03-02T17:39:52.841351: step 5862, loss 0.161885, acc 0.9375
2017-03-02T17:39:52.908705: step 5863, loss 0.162715, acc 0.96875
2017-03-02T17:39:52.974700: step 5864, loss 0.145137, acc 0.953125
2017-03-02T17:39:53.047372: step 5865, loss 0.175127, acc 0.90625
2017-03-02T17:39:53.117845: step 5866, loss 0.158022, acc 0.9375
2017-03-02T17:39:53.200339: step 5867, loss 0.363755, acc 0.90625
2017-03-02T17:39:53.273124: step 5868, loss 0.263415, acc 0.890625
2017-03-02T17:39:53.357925: step 5869, loss 0.10573, acc 0.96875
2017-03-02T17:39:53.434588: step 5870, loss 0.225248, acc 0.921875
2017-03-02T17:39:53.504687: step 5871, loss 0.226905, acc 0.921875
2017-03-02T17:39:53.583511: step 5872, loss 0.191201, acc 0.90625
2017-03-02T17:39:53.652388: step 5873, loss 0.239395, acc 0.90625
2017-03-02T17:39:53.747690: step 5874, loss 0.220295, acc 0.9375
2017-03-02T17:39:53.819367: step 5875, loss 0.309391, acc 0.921875
2017-03-02T17:39:53.886701: step 5876, loss 0.180597, acc 0.9375
2017-03-02T17:39:53.958776: step 5877, loss 0.162665, acc 0.953125
2017-03-02T17:39:54.030861: step 5878, loss 0.18073, acc 0.890625
2017-03-02T17:39:54.106749: step 5879, loss 0.112273, acc 0.953125
2017-03-02T17:39:54.185423: step 5880, loss 0.202485, acc 0.75
2017-03-02T17:39:54.267122: step 5881, loss 0.0959478, acc 0.96875
2017-03-02T17:39:54.339378: step 5882, loss 0.274063, acc 0.90625
2017-03-02T17:39:54.417181: step 5883, loss 0.236231, acc 0.90625
2017-03-02T17:39:54.487532: step 5884, loss 0.201767, acc 0.921875
2017-03-02T17:39:54.566728: step 5885, loss 0.0682449, acc 0.984375
2017-03-02T17:39:54.650992: step 5886, loss 0.295121, acc 0.875
2017-03-02T17:39:54.723574: step 5887, loss 0.119384, acc 0.953125
2017-03-02T17:39:54.799764: step 5888, loss 0.199581, acc 0.90625
2017-03-02T17:39:54.876774: step 5889, loss 0.12233, acc 0.953125
2017-03-02T17:39:54.948557: step 5890, loss 0.269338, acc 0.921875
2017-03-02T17:39:55.024162: step 5891, loss 0.212881, acc 0.90625
2017-03-02T17:39:55.098118: step 5892, loss 0.163548, acc 0.9375
2017-03-02T17:39:55.170710: step 5893, loss 0.21196, acc 0.9375
2017-03-02T17:39:55.242742: step 5894, loss 0.164657, acc 0.9375
2017-03-02T17:39:55.314939: step 5895, loss 0.139904, acc 0.953125
2017-03-02T17:39:55.388025: step 5896, loss 0.0893427, acc 0.953125
2017-03-02T17:39:55.455217: step 5897, loss 0.208324, acc 0.9375
2017-03-02T17:39:55.529059: step 5898, loss 0.0698567, acc 0.96875
2017-03-02T17:39:55.601235: step 5899, loss 0.138738, acc 0.96875
2017-03-02T17:39:55.671116: step 5900, loss 0.222569, acc 0.890625

Evaluation:
2017-03-02T17:39:55.708355: step 5900, loss 1.19389, acc 0.67628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-5900

2017-03-02T17:39:56.180468: step 5901, loss 0.0952654, acc 0.96875
2017-03-02T17:39:56.253215: step 5902, loss 0.417932, acc 0.890625
2017-03-02T17:39:56.317839: step 5903, loss 0.0614809, acc 0.984375
2017-03-02T17:39:56.392465: step 5904, loss 0.277068, acc 0.90625
2017-03-02T17:39:56.463587: step 5905, loss 0.122954, acc 0.953125
2017-03-02T17:39:56.536398: step 5906, loss 0.14564, acc 0.9375
2017-03-02T17:39:56.621667: step 5907, loss 0.107582, acc 0.96875
2017-03-02T17:39:56.699757: step 5908, loss 0.125658, acc 0.953125
2017-03-02T17:39:56.773122: step 5909, loss 0.289539, acc 0.859375
2017-03-02T17:39:56.847686: step 5910, loss 0.130084, acc 0.953125
2017-03-02T17:39:56.920846: step 5911, loss 0.202815, acc 0.921875
2017-03-02T17:39:56.990769: step 5912, loss 0.134714, acc 0.90625
2017-03-02T17:39:57.065697: step 5913, loss 0.0953188, acc 0.953125
2017-03-02T17:39:57.138637: step 5914, loss 0.117706, acc 0.984375
2017-03-02T17:39:57.211321: step 5915, loss 0.238336, acc 0.90625
2017-03-02T17:39:57.283157: step 5916, loss 0.171515, acc 0.953125
2017-03-02T17:39:57.354742: step 5917, loss 0.244329, acc 0.953125
2017-03-02T17:39:57.429033: step 5918, loss 0.141352, acc 0.9375
2017-03-02T17:39:57.505730: step 5919, loss 0.13205, acc 0.9375
2017-03-02T17:39:57.581028: step 5920, loss 0.12666, acc 0.9375
2017-03-02T17:39:57.657434: step 5921, loss 0.226075, acc 0.921875
2017-03-02T17:39:57.727240: step 5922, loss 0.170114, acc 0.9375
2017-03-02T17:39:57.799584: step 5923, loss 0.190714, acc 0.90625
2017-03-02T17:39:57.869986: step 5924, loss 0.1728, acc 0.953125
2017-03-02T17:39:57.952644: step 5925, loss 0.199073, acc 0.90625
2017-03-02T17:39:58.027074: step 5926, loss 0.227459, acc 0.9375
2017-03-02T17:39:58.092622: step 5927, loss 0.235729, acc 0.921875
2017-03-02T17:39:58.168694: step 5928, loss 0.119064, acc 0.96875
2017-03-02T17:39:58.243498: step 5929, loss 0.243255, acc 0.890625
2017-03-02T17:39:58.310120: step 5930, loss 0.240543, acc 0.875
2017-03-02T17:39:58.379925: step 5931, loss 0.238475, acc 0.90625
2017-03-02T17:39:58.445274: step 5932, loss 0.238731, acc 0.890625
2017-03-02T17:39:58.517296: step 5933, loss 0.207165, acc 0.921875
2017-03-02T17:39:58.592916: step 5934, loss 0.43695, acc 0.84375
2017-03-02T17:39:58.668399: step 5935, loss 0.191683, acc 0.921875
2017-03-02T17:39:58.743110: step 5936, loss 0.189055, acc 0.9375
2017-03-02T17:39:58.815882: step 5937, loss 0.0916346, acc 0.984375
2017-03-02T17:39:58.890924: step 5938, loss 0.222804, acc 0.9375
2017-03-02T17:39:58.961251: step 5939, loss 0.297306, acc 0.90625
2017-03-02T17:39:59.039069: step 5940, loss 0.214959, acc 0.90625
2017-03-02T17:39:59.121305: step 5941, loss 0.259715, acc 0.859375
2017-03-02T17:39:59.194459: step 5942, loss 0.17158, acc 0.90625
2017-03-02T17:39:59.269413: step 5943, loss 0.125227, acc 0.984375
2017-03-02T17:39:59.342910: step 5944, loss 0.118803, acc 0.9375
2017-03-02T17:39:59.416066: step 5945, loss 0.103936, acc 0.953125
2017-03-02T17:39:59.487054: step 5946, loss 0.157919, acc 0.921875
2017-03-02T17:39:59.559569: step 5947, loss 0.315464, acc 0.859375
2017-03-02T17:39:59.632901: step 5948, loss 0.177012, acc 0.921875
2017-03-02T17:39:59.717059: step 5949, loss 0.102148, acc 0.96875
2017-03-02T17:39:59.788552: step 5950, loss 0.249411, acc 0.90625
2017-03-02T17:39:59.859149: step 5951, loss 0.150813, acc 0.9375
2017-03-02T17:39:59.944862: step 5952, loss 0.256456, acc 0.953125
2017-03-02T17:40:00.033762: step 5953, loss 0.329082, acc 0.875
2017-03-02T17:40:00.122006: step 5954, loss 0.0743043, acc 0.96875
2017-03-02T17:40:00.194314: step 5955, loss 0.197332, acc 0.921875
2017-03-02T17:40:00.270566: step 5956, loss 0.328387, acc 0.890625
2017-03-02T17:40:00.341263: step 5957, loss 0.357638, acc 0.875
2017-03-02T17:40:00.409716: step 5958, loss 0.139, acc 0.9375
2017-03-02T17:40:00.475526: step 5959, loss 0.184754, acc 0.953125
2017-03-02T17:40:00.548551: step 5960, loss 0.225234, acc 0.921875
2017-03-02T17:40:00.618567: step 5961, loss 0.138986, acc 0.96875
2017-03-02T17:40:00.691371: step 5962, loss 0.237898, acc 0.890625
2017-03-02T17:40:00.769709: step 5963, loss 0.143013, acc 0.921875
2017-03-02T17:40:00.845063: step 5964, loss 0.111287, acc 0.9375
2017-03-02T17:40:00.917774: step 5965, loss 0.183103, acc 0.9375
2017-03-02T17:40:00.998644: step 5966, loss 0.111615, acc 0.953125
2017-03-02T17:40:01.067051: step 5967, loss 0.298583, acc 0.921875
2017-03-02T17:40:01.133758: step 5968, loss 0.194175, acc 0.90625
2017-03-02T17:40:01.218608: step 5969, loss 0.0643065, acc 0.984375
2017-03-02T17:40:01.290676: step 5970, loss 0.164776, acc 0.9375
2017-03-02T17:40:01.363450: step 5971, loss 0.178829, acc 0.921875
2017-03-02T17:40:01.436069: step 5972, loss 0.197859, acc 0.9375
2017-03-02T17:40:01.508083: step 5973, loss 0.199903, acc 0.890625
2017-03-02T17:40:01.577350: step 5974, loss 0.144835, acc 0.9375
2017-03-02T17:40:01.649164: step 5975, loss 0.130545, acc 0.9375
2017-03-02T17:40:01.727350: step 5976, loss 0.301387, acc 0.90625
2017-03-02T17:40:01.793800: step 5977, loss 0.189087, acc 0.90625
2017-03-02T17:40:01.870413: step 5978, loss 0.183933, acc 0.90625
2017-03-02T17:40:01.938376: step 5979, loss 0.131661, acc 0.96875
2017-03-02T17:40:02.009545: step 5980, loss 0.195237, acc 0.921875
2017-03-02T17:40:02.083414: step 5981, loss 0.219117, acc 0.921875
2017-03-02T17:40:02.157737: step 5982, loss 0.201439, acc 0.921875
2017-03-02T17:40:02.233615: step 5983, loss 0.238806, acc 0.890625
2017-03-02T17:40:02.306810: step 5984, loss 0.21789, acc 0.953125
2017-03-02T17:40:02.380938: step 5985, loss 0.170652, acc 0.953125
2017-03-02T17:40:02.452166: step 5986, loss 0.0809759, acc 0.984375
2017-03-02T17:40:02.518196: step 5987, loss 0.146491, acc 0.9375
2017-03-02T17:40:02.588677: step 5988, loss 0.31856, acc 0.890625
2017-03-02T17:40:02.675253: step 5989, loss 0.214579, acc 0.921875
2017-03-02T17:40:02.750210: step 5990, loss 0.141767, acc 0.953125
2017-03-02T17:40:02.823918: step 5991, loss 0.176893, acc 0.921875
2017-03-02T17:40:02.901284: step 5992, loss 0.236207, acc 0.921875
2017-03-02T17:40:02.974904: step 5993, loss 0.203334, acc 0.921875
2017-03-02T17:40:03.056176: step 5994, loss 0.162669, acc 0.9375
2017-03-02T17:40:03.124968: step 5995, loss 0.248861, acc 0.953125
2017-03-02T17:40:03.194018: step 5996, loss 0.270806, acc 0.890625
2017-03-02T17:40:03.270462: step 5997, loss 0.271175, acc 0.90625
2017-03-02T17:40:03.343482: step 5998, loss 0.308053, acc 0.875
2017-03-02T17:40:03.416862: step 5999, loss 0.129087, acc 0.9375
2017-03-02T17:40:03.490454: step 6000, loss 0.0936678, acc 1

Evaluation:
2017-03-02T17:40:03.523805: step 6000, loss 1.18083, acc 0.674117

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6000

2017-03-02T17:40:04.012869: step 6001, loss 0.164841, acc 0.953125
2017-03-02T17:40:04.085710: step 6002, loss 0.142701, acc 0.9375
2017-03-02T17:40:04.165949: step 6003, loss 0.198284, acc 0.953125
2017-03-02T17:40:04.256700: step 6004, loss 0.272064, acc 0.9375
2017-03-02T17:40:04.329025: step 6005, loss 0.235343, acc 0.859375
2017-03-02T17:40:04.402377: step 6006, loss 0.229793, acc 0.90625
2017-03-02T17:40:04.481323: step 6007, loss 0.212962, acc 0.9375
2017-03-02T17:40:04.550224: step 6008, loss 0.257103, acc 0.875
2017-03-02T17:40:04.623080: step 6009, loss 0.101317, acc 0.96875
2017-03-02T17:40:04.691962: step 6010, loss 0.389318, acc 0.859375
2017-03-02T17:40:04.763576: step 6011, loss 0.226979, acc 0.9375
2017-03-02T17:40:04.840973: step 6012, loss 0.279686, acc 0.875
2017-03-02T17:40:04.914280: step 6013, loss 0.162646, acc 0.921875
2017-03-02T17:40:04.984774: step 6014, loss 0.167955, acc 0.90625
2017-03-02T17:40:05.058008: step 6015, loss 0.289356, acc 0.90625
2017-03-02T17:40:05.129565: step 6016, loss 0.27745, acc 0.859375
2017-03-02T17:40:05.204453: step 6017, loss 0.284932, acc 0.875
2017-03-02T17:40:05.282147: step 6018, loss 0.221681, acc 0.921875
2017-03-02T17:40:05.383628: step 6019, loss 0.308516, acc 0.921875
2017-03-02T17:40:05.459633: step 6020, loss 0.130876, acc 0.9375
2017-03-02T17:40:05.534334: step 6021, loss 0.283376, acc 0.890625
2017-03-02T17:40:05.607556: step 6022, loss 0.241938, acc 0.90625
2017-03-02T17:40:05.678978: step 6023, loss 0.307897, acc 0.890625
2017-03-02T17:40:05.748201: step 6024, loss 0.146235, acc 0.96875
2017-03-02T17:40:05.820055: step 6025, loss 0.24486, acc 0.9375
2017-03-02T17:40:05.890885: step 6026, loss 0.265469, acc 0.890625
2017-03-02T17:40:05.966461: step 6027, loss 0.359583, acc 0.90625
2017-03-02T17:40:06.037957: step 6028, loss 0.129026, acc 0.953125
2017-03-02T17:40:06.110990: step 6029, loss 0.152932, acc 0.9375
2017-03-02T17:40:06.180470: step 6030, loss 0.26077, acc 0.90625
2017-03-02T17:40:06.257391: step 6031, loss 0.205631, acc 0.953125
2017-03-02T17:40:06.331199: step 6032, loss 0.168968, acc 0.9375
2017-03-02T17:40:06.403667: step 6033, loss 0.211278, acc 0.890625
2017-03-02T17:40:06.477939: step 6034, loss 0.231409, acc 0.875
2017-03-02T17:40:06.559962: step 6035, loss 0.18265, acc 0.921875
2017-03-02T17:40:06.630130: step 6036, loss 0.235854, acc 0.90625
2017-03-02T17:40:06.705639: step 6037, loss 0.199303, acc 0.90625
2017-03-02T17:40:06.776670: step 6038, loss 0.124404, acc 0.96875
2017-03-02T17:40:06.844724: step 6039, loss 0.226629, acc 0.921875
2017-03-02T17:40:06.928793: step 6040, loss 0.305437, acc 0.875
2017-03-02T17:40:07.016496: step 6041, loss 0.351007, acc 0.875
2017-03-02T17:40:07.102417: step 6042, loss 0.148398, acc 0.9375
2017-03-02T17:40:07.171395: step 6043, loss 0.163123, acc 0.9375
2017-03-02T17:40:07.238583: step 6044, loss 0.230733, acc 0.9375
2017-03-02T17:40:07.310487: step 6045, loss 0.195365, acc 0.953125
2017-03-02T17:40:07.383568: step 6046, loss 0.20797, acc 0.875
2017-03-02T17:40:07.467438: step 6047, loss 0.0878782, acc 1
2017-03-02T17:40:07.547971: step 6048, loss 0.189535, acc 0.921875
2017-03-02T17:40:07.621652: step 6049, loss 0.1584, acc 0.953125
2017-03-02T17:40:07.695414: step 6050, loss 0.273224, acc 0.90625
2017-03-02T17:40:07.776204: step 6051, loss 0.190088, acc 0.921875
2017-03-02T17:40:07.859883: step 6052, loss 0.0990762, acc 0.984375
2017-03-02T17:40:07.926527: step 6053, loss 0.229511, acc 0.9375
2017-03-02T17:40:07.989348: step 6054, loss 0.318877, acc 0.859375
2017-03-02T17:40:08.065896: step 6055, loss 0.262696, acc 0.921875
2017-03-02T17:40:08.149375: step 6056, loss 0.367285, acc 0.84375
2017-03-02T17:40:08.226609: step 6057, loss 0.258939, acc 0.84375
2017-03-02T17:40:08.297835: step 6058, loss 0.128055, acc 0.90625
2017-03-02T17:40:08.372815: step 6059, loss 0.174624, acc 0.90625
2017-03-02T17:40:08.443231: step 6060, loss 0.157603, acc 0.953125
2017-03-02T17:40:08.518883: step 6061, loss 0.347606, acc 0.875
2017-03-02T17:40:08.589156: step 6062, loss 0.0873315, acc 0.953125
2017-03-02T17:40:08.660826: step 6063, loss 0.296436, acc 0.90625
2017-03-02T17:40:08.733051: step 6064, loss 0.186743, acc 0.9375
2017-03-02T17:40:08.811352: step 6065, loss 0.211927, acc 0.90625
2017-03-02T17:40:08.886474: step 6066, loss 0.176514, acc 0.890625
2017-03-02T17:40:08.961919: step 6067, loss 0.329461, acc 0.875
2017-03-02T17:40:09.035391: step 6068, loss 0.155098, acc 0.953125
2017-03-02T17:40:09.103943: step 6069, loss 0.304153, acc 0.875
2017-03-02T17:40:09.181581: step 6070, loss 0.2651, acc 0.921875
2017-03-02T17:40:09.258667: step 6071, loss 0.143054, acc 0.953125
2017-03-02T17:40:09.332277: step 6072, loss 0.31164, acc 0.890625
2017-03-02T17:40:09.388150: step 6073, loss 0.156702, acc 0.90625
2017-03-02T17:40:09.464591: step 6074, loss 0.201147, acc 0.9375
2017-03-02T17:40:09.537379: step 6075, loss 0.220501, acc 0.890625
2017-03-02T17:40:09.602177: step 6076, loss 0.0249619, acc 1
2017-03-02T17:40:09.681435: step 6077, loss 0.089871, acc 0.984375
2017-03-02T17:40:09.753485: step 6078, loss 0.159689, acc 0.96875
2017-03-02T17:40:09.822315: step 6079, loss 0.25584, acc 0.90625
2017-03-02T17:40:09.899214: step 6080, loss 0.146236, acc 0.96875
2017-03-02T17:40:09.965308: step 6081, loss 0.188731, acc 0.921875
2017-03-02T17:40:10.049159: step 6082, loss 0.11584, acc 0.96875
2017-03-02T17:40:10.122576: step 6083, loss 0.0906916, acc 0.96875
2017-03-02T17:40:10.194361: step 6084, loss 0.196199, acc 0.9375
2017-03-02T17:40:10.264812: step 6085, loss 0.190962, acc 0.921875
2017-03-02T17:40:10.337138: step 6086, loss 0.130063, acc 0.9375
2017-03-02T17:40:10.423275: step 6087, loss 0.389307, acc 0.890625
2017-03-02T17:40:10.497544: step 6088, loss 0.151201, acc 0.953125
2017-03-02T17:40:10.568609: step 6089, loss 0.252207, acc 0.90625
2017-03-02T17:40:10.637894: step 6090, loss 0.194984, acc 0.921875
2017-03-02T17:40:10.711503: step 6091, loss 0.123691, acc 0.953125
2017-03-02T17:40:10.783464: step 6092, loss 0.120297, acc 0.953125
2017-03-02T17:40:10.867871: step 6093, loss 0.101392, acc 0.984375
2017-03-02T17:40:10.942208: step 6094, loss 0.178139, acc 0.921875
2017-03-02T17:40:11.054919: step 6095, loss 0.250474, acc 0.90625
2017-03-02T17:40:11.130573: step 6096, loss 0.206109, acc 0.875
2017-03-02T17:40:11.212289: step 6097, loss 0.181478, acc 0.921875
2017-03-02T17:40:11.284451: step 6098, loss 0.228705, acc 0.921875
2017-03-02T17:40:11.350277: step 6099, loss 0.225681, acc 0.890625
2017-03-02T17:40:11.426828: step 6100, loss 0.278146, acc 0.90625

Evaluation:
2017-03-02T17:40:11.464638: step 6100, loss 1.19226, acc 0.675559

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6100

2017-03-02T17:40:11.945179: step 6101, loss 0.156958, acc 0.9375
2017-03-02T17:40:12.017871: step 6102, loss 0.16341, acc 0.953125
2017-03-02T17:40:12.093933: step 6103, loss 0.047015, acc 0.984375
2017-03-02T17:40:12.164968: step 6104, loss 0.194892, acc 0.921875
2017-03-02T17:40:12.233383: step 6105, loss 0.184885, acc 0.90625
2017-03-02T17:40:12.313903: step 6106, loss 0.260916, acc 0.875
2017-03-02T17:40:12.390265: step 6107, loss 0.215594, acc 0.921875
2017-03-02T17:40:12.462060: step 6108, loss 0.103021, acc 0.96875
2017-03-02T17:40:12.535427: step 6109, loss 0.15256, acc 0.96875
2017-03-02T17:40:12.617324: step 6110, loss 0.190913, acc 0.921875
2017-03-02T17:40:12.690808: step 6111, loss 0.286233, acc 0.875
2017-03-02T17:40:12.757468: step 6112, loss 0.110485, acc 0.953125
2017-03-02T17:40:12.843678: step 6113, loss 0.277865, acc 0.84375
2017-03-02T17:40:12.917468: step 6114, loss 0.206334, acc 0.875
2017-03-02T17:40:13.006143: step 6115, loss 0.267016, acc 0.890625
2017-03-02T17:40:13.087305: step 6116, loss 0.194245, acc 0.9375
2017-03-02T17:40:13.170132: step 6117, loss 0.347574, acc 0.890625
2017-03-02T17:40:13.251532: step 6118, loss 0.124895, acc 0.953125
2017-03-02T17:40:13.323727: step 6119, loss 0.153084, acc 0.9375
2017-03-02T17:40:13.395342: step 6120, loss 0.235495, acc 0.875
2017-03-02T17:40:13.466893: step 6121, loss 0.297642, acc 0.890625
2017-03-02T17:40:13.542022: step 6122, loss 0.0960553, acc 0.953125
2017-03-02T17:40:13.615094: step 6123, loss 0.308971, acc 0.90625
2017-03-02T17:40:13.688643: step 6124, loss 0.398858, acc 0.84375
2017-03-02T17:40:13.778137: step 6125, loss 0.189858, acc 0.921875
2017-03-02T17:40:13.857350: step 6126, loss 0.213868, acc 0.9375
2017-03-02T17:40:13.935583: step 6127, loss 0.279888, acc 0.9375
2017-03-02T17:40:14.012325: step 6128, loss 0.264683, acc 0.90625
2017-03-02T17:40:14.079357: step 6129, loss 0.210952, acc 0.921875
2017-03-02T17:40:14.161989: step 6130, loss 0.174804, acc 0.921875
2017-03-02T17:40:14.235730: step 6131, loss 0.300215, acc 0.890625
2017-03-02T17:40:14.310269: step 6132, loss 0.279056, acc 0.890625
2017-03-02T17:40:14.381718: step 6133, loss 0.142633, acc 0.953125
2017-03-02T17:40:14.453763: step 6134, loss 0.234426, acc 0.9375
2017-03-02T17:40:14.525697: step 6135, loss 0.114666, acc 0.953125
2017-03-02T17:40:14.598794: step 6136, loss 0.139225, acc 0.9375
2017-03-02T17:40:14.675294: step 6137, loss 0.371181, acc 0.890625
2017-03-02T17:40:14.751399: step 6138, loss 0.059586, acc 0.984375
2017-03-02T17:40:14.815687: step 6139, loss 0.127886, acc 0.9375
2017-03-02T17:40:14.886126: step 6140, loss 0.163337, acc 0.921875
2017-03-02T17:40:14.959087: step 6141, loss 0.204229, acc 0.90625
2017-03-02T17:40:15.039052: step 6142, loss 0.079043, acc 0.96875
2017-03-02T17:40:15.114685: step 6143, loss 0.186916, acc 0.9375
2017-03-02T17:40:15.186604: step 6144, loss 0.258099, acc 0.90625
2017-03-02T17:40:15.262781: step 6145, loss 0.22661, acc 0.921875
2017-03-02T17:40:15.341471: step 6146, loss 0.30641, acc 0.890625
2017-03-02T17:40:15.414078: step 6147, loss 0.169831, acc 0.921875
2017-03-02T17:40:15.484878: step 6148, loss 0.121192, acc 0.953125
2017-03-02T17:40:15.553533: step 6149, loss 0.236462, acc 0.90625
2017-03-02T17:40:15.628925: step 6150, loss 0.21697, acc 0.921875
2017-03-02T17:40:15.701685: step 6151, loss 0.133448, acc 0.953125
2017-03-02T17:40:15.778139: step 6152, loss 0.229754, acc 0.9375
2017-03-02T17:40:15.844733: step 6153, loss 0.28124, acc 0.90625
2017-03-02T17:40:15.918243: step 6154, loss 0.302713, acc 0.921875
2017-03-02T17:40:15.991320: step 6155, loss 0.141577, acc 0.96875
2017-03-02T17:40:16.058676: step 6156, loss 0.309214, acc 0.890625
2017-03-02T17:40:16.133919: step 6157, loss 0.153117, acc 0.953125
2017-03-02T17:40:16.199199: step 6158, loss 0.174069, acc 0.921875
2017-03-02T17:40:16.286620: step 6159, loss 0.150814, acc 0.9375
2017-03-02T17:40:16.359708: step 6160, loss 0.188537, acc 0.9375
2017-03-02T17:40:16.430884: step 6161, loss 0.132824, acc 0.96875
2017-03-02T17:40:16.505798: step 6162, loss 0.14006, acc 0.9375
2017-03-02T17:40:16.577106: step 6163, loss 0.145533, acc 0.96875
2017-03-02T17:40:16.648871: step 6164, loss 0.100521, acc 0.953125
2017-03-02T17:40:16.721910: step 6165, loss 0.277828, acc 0.890625
2017-03-02T17:40:16.785809: step 6166, loss 0.147129, acc 0.921875
2017-03-02T17:40:16.856486: step 6167, loss 0.218929, acc 0.90625
2017-03-02T17:40:16.924097: step 6168, loss 0.162582, acc 0.953125
2017-03-02T17:40:16.999207: step 6169, loss 0.238448, acc 0.90625
2017-03-02T17:40:17.071452: step 6170, loss 0.282472, acc 0.90625
2017-03-02T17:40:17.139979: step 6171, loss 0.203378, acc 0.921875
2017-03-02T17:40:17.218480: step 6172, loss 0.173882, acc 0.921875
2017-03-02T17:40:17.300155: step 6173, loss 0.114438, acc 0.984375
2017-03-02T17:40:17.371604: step 6174, loss 0.259861, acc 0.890625
2017-03-02T17:40:17.444178: step 6175, loss 0.167643, acc 0.953125
2017-03-02T17:40:17.517007: step 6176, loss 0.248639, acc 0.875
2017-03-02T17:40:17.581874: step 6177, loss 0.188035, acc 0.90625
2017-03-02T17:40:17.654706: step 6178, loss 0.193315, acc 0.90625
2017-03-02T17:40:17.734823: step 6179, loss 0.102427, acc 0.984375
2017-03-02T17:40:17.821203: step 6180, loss 0.200334, acc 0.9375
2017-03-02T17:40:17.895910: step 6181, loss 0.301018, acc 0.921875
2017-03-02T17:40:17.977196: step 6182, loss 0.197639, acc 0.921875
2017-03-02T17:40:18.052812: step 6183, loss 0.21646, acc 0.90625
2017-03-02T17:40:18.127710: step 6184, loss 0.146404, acc 0.953125
2017-03-02T17:40:18.198558: step 6185, loss 0.0849296, acc 0.96875
2017-03-02T17:40:18.264469: step 6186, loss 0.340455, acc 0.890625
2017-03-02T17:40:18.348092: step 6187, loss 0.139212, acc 0.953125
2017-03-02T17:40:18.424418: step 6188, loss 0.253022, acc 0.921875
2017-03-02T17:40:18.499407: step 6189, loss 0.115726, acc 0.953125
2017-03-02T17:40:18.587116: step 6190, loss 0.2607, acc 0.921875
2017-03-02T17:40:18.657813: step 6191, loss 0.104134, acc 0.9375
2017-03-02T17:40:18.728408: step 6192, loss 0.216558, acc 0.90625
2017-03-02T17:40:18.800317: step 6193, loss 0.178837, acc 0.9375
2017-03-02T17:40:18.861919: step 6194, loss 0.0813723, acc 0.953125
2017-03-02T17:40:18.932910: step 6195, loss 0.147321, acc 0.953125
2017-03-02T17:40:19.007616: step 6196, loss 0.186618, acc 0.953125
2017-03-02T17:40:19.085910: step 6197, loss 0.365816, acc 0.890625
2017-03-02T17:40:19.157758: step 6198, loss 0.0794181, acc 0.984375
2017-03-02T17:40:19.236762: step 6199, loss 0.125935, acc 0.96875
2017-03-02T17:40:19.312971: step 6200, loss 0.214369, acc 0.890625

Evaluation:
2017-03-02T17:40:19.347802: step 6200, loss 1.18279, acc 0.68421

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6200

2017-03-02T17:40:19.795262: step 6201, loss 0.118701, acc 0.953125
2017-03-02T17:40:19.871744: step 6202, loss 0.357361, acc 0.890625
2017-03-02T17:40:19.948469: step 6203, loss 0.148812, acc 0.921875
2017-03-02T17:40:20.020337: step 6204, loss 0.206367, acc 0.921875
2017-03-02T17:40:20.093811: step 6205, loss 0.261612, acc 0.90625
2017-03-02T17:40:20.173517: step 6206, loss 0.0967331, acc 0.96875
2017-03-02T17:40:20.243107: step 6207, loss 0.286345, acc 0.875
2017-03-02T17:40:20.308533: step 6208, loss 0.480981, acc 0.84375
2017-03-02T17:40:20.386355: step 6209, loss 0.18076, acc 0.921875
2017-03-02T17:40:20.463504: step 6210, loss 0.2283, acc 0.921875
2017-03-02T17:40:20.534906: step 6211, loss 0.189738, acc 0.921875
2017-03-02T17:40:20.607505: step 6212, loss 0.14052, acc 0.953125
2017-03-02T17:40:20.672712: step 6213, loss 0.173929, acc 0.921875
2017-03-02T17:40:20.742172: step 6214, loss 0.310982, acc 0.90625
2017-03-02T17:40:20.813810: step 6215, loss 0.195091, acc 0.921875
2017-03-02T17:40:20.889339: step 6216, loss 0.108711, acc 0.96875
2017-03-02T17:40:20.957090: step 6217, loss 0.281909, acc 0.875
2017-03-02T17:40:21.023257: step 6218, loss 0.148748, acc 0.9375
2017-03-02T17:40:21.099229: step 6219, loss 0.0995341, acc 0.96875
2017-03-02T17:40:21.178651: step 6220, loss 0.194943, acc 0.9375
2017-03-02T17:40:21.258914: step 6221, loss 0.162532, acc 0.953125
2017-03-02T17:40:21.340643: step 6222, loss 0.16548, acc 0.9375
2017-03-02T17:40:21.410457: step 6223, loss 0.128423, acc 0.9375
2017-03-02T17:40:21.484162: step 6224, loss 0.230118, acc 0.921875
2017-03-02T17:40:21.566383: step 6225, loss 0.274734, acc 0.890625
2017-03-02T17:40:21.637899: step 6226, loss 0.303241, acc 0.90625
2017-03-02T17:40:21.705544: step 6227, loss 0.199062, acc 0.921875
2017-03-02T17:40:21.776448: step 6228, loss 0.134059, acc 0.9375
2017-03-02T17:40:21.848572: step 6229, loss 0.197077, acc 0.921875
2017-03-02T17:40:21.928478: step 6230, loss 0.0954225, acc 0.953125
2017-03-02T17:40:21.999157: step 6231, loss 0.187887, acc 0.90625
2017-03-02T17:40:22.076947: step 6232, loss 0.210192, acc 0.9375
2017-03-02T17:40:22.156609: step 6233, loss 0.328022, acc 0.84375
2017-03-02T17:40:22.234167: step 6234, loss 0.173423, acc 0.9375
2017-03-02T17:40:22.307282: step 6235, loss 0.264751, acc 0.890625
2017-03-02T17:40:22.380401: step 6236, loss 0.203215, acc 0.890625
2017-03-02T17:40:22.453615: step 6237, loss 0.232998, acc 0.890625
2017-03-02T17:40:22.521480: step 6238, loss 0.265029, acc 0.890625
2017-03-02T17:40:22.593601: step 6239, loss 0.145374, acc 0.921875
2017-03-02T17:40:22.663463: step 6240, loss 0.178016, acc 0.9375
2017-03-02T17:40:22.739011: step 6241, loss 0.245129, acc 0.921875
2017-03-02T17:40:22.810626: step 6242, loss 0.290607, acc 0.890625
2017-03-02T17:40:22.879899: step 6243, loss 0.27243, acc 0.859375
2017-03-02T17:40:22.956683: step 6244, loss 0.116891, acc 0.953125
2017-03-02T17:40:23.026185: step 6245, loss 0.219281, acc 0.890625
2017-03-02T17:40:23.110110: step 6246, loss 0.252693, acc 0.921875
2017-03-02T17:40:23.186982: step 6247, loss 0.240385, acc 0.921875
2017-03-02T17:40:23.258276: step 6248, loss 0.171212, acc 0.953125
2017-03-02T17:40:23.331294: step 6249, loss 0.104862, acc 0.953125
2017-03-02T17:40:23.397134: step 6250, loss 0.199214, acc 0.921875
2017-03-02T17:40:23.474134: step 6251, loss 0.346634, acc 0.921875
2017-03-02T17:40:23.548392: step 6252, loss 0.318947, acc 0.875
2017-03-02T17:40:23.623112: step 6253, loss 0.123709, acc 0.96875
2017-03-02T17:40:23.693598: step 6254, loss 0.266321, acc 0.890625
2017-03-02T17:40:23.762235: step 6255, loss 0.169705, acc 0.921875
2017-03-02T17:40:23.835323: step 6256, loss 0.104479, acc 0.9375
2017-03-02T17:40:23.907743: step 6257, loss 0.136637, acc 0.953125
2017-03-02T17:40:23.976473: step 6258, loss 0.26703, acc 0.875
2017-03-02T17:40:24.057727: step 6259, loss 0.309156, acc 0.921875
2017-03-02T17:40:24.133954: step 6260, loss 0.253619, acc 0.9375
2017-03-02T17:40:24.214196: step 6261, loss 0.184325, acc 0.921875
2017-03-02T17:40:24.298870: step 6262, loss 0.209632, acc 0.90625
2017-03-02T17:40:24.371023: step 6263, loss 0.24596, acc 0.890625
2017-03-02T17:40:24.441504: step 6264, loss 0.237254, acc 0.890625
2017-03-02T17:40:24.517060: step 6265, loss 0.165522, acc 0.921875
2017-03-02T17:40:24.595520: step 6266, loss 0.29203, acc 0.90625
2017-03-02T17:40:24.666986: step 6267, loss 0.28828, acc 0.84375
2017-03-02T17:40:24.753611: step 6268, loss 0.243261, acc 0.921875
2017-03-02T17:40:24.824631: step 6269, loss 0.202842, acc 0.921875
2017-03-02T17:40:24.893348: step 6270, loss 0.228475, acc 0.90625
2017-03-02T17:40:24.966036: step 6271, loss 0.222438, acc 0.9375
2017-03-02T17:40:25.035863: step 6272, loss 1.4007, acc 0.75
2017-03-02T17:40:25.107976: step 6273, loss 0.251311, acc 0.875
2017-03-02T17:40:25.192532: step 6274, loss 0.238601, acc 0.890625
2017-03-02T17:40:25.306982: step 6275, loss 0.0803573, acc 1
2017-03-02T17:40:25.376083: step 6276, loss 0.34015, acc 0.890625
2017-03-02T17:40:25.452594: step 6277, loss 0.165974, acc 0.953125
2017-03-02T17:40:25.528377: step 6278, loss 0.254283, acc 0.875
2017-03-02T17:40:25.602630: step 6279, loss 0.113315, acc 0.984375
2017-03-02T17:40:25.677297: step 6280, loss 0.159938, acc 0.953125
2017-03-02T17:40:25.752193: step 6281, loss 0.396098, acc 0.890625
2017-03-02T17:40:25.818753: step 6282, loss 0.182163, acc 0.953125
2017-03-02T17:40:25.887159: step 6283, loss 0.128612, acc 0.953125
2017-03-02T17:40:25.960454: step 6284, loss 0.281783, acc 0.90625
2017-03-02T17:40:26.032657: step 6285, loss 0.131019, acc 0.9375
2017-03-02T17:40:26.113932: step 6286, loss 0.194371, acc 0.890625
2017-03-02T17:40:26.188127: step 6287, loss 0.426453, acc 0.859375
2017-03-02T17:40:26.258534: step 6288, loss 0.174413, acc 0.921875
2017-03-02T17:40:26.333833: step 6289, loss 0.210148, acc 0.9375
2017-03-02T17:40:26.403903: step 6290, loss 0.193984, acc 0.921875
2017-03-02T17:40:26.474647: step 6291, loss 0.0673991, acc 0.984375
2017-03-02T17:40:26.546958: step 6292, loss 0.206097, acc 0.921875
2017-03-02T17:40:26.618449: step 6293, loss 0.124277, acc 0.953125
2017-03-02T17:40:26.691108: step 6294, loss 0.195566, acc 0.875
2017-03-02T17:40:26.774015: step 6295, loss 0.194569, acc 0.953125
2017-03-02T17:40:26.844061: step 6296, loss 0.162226, acc 0.921875
2017-03-02T17:40:26.920759: step 6297, loss 0.161729, acc 0.90625
2017-03-02T17:40:26.992160: step 6298, loss 0.170281, acc 0.953125
2017-03-02T17:40:27.066611: step 6299, loss 0.187887, acc 0.9375
2017-03-02T17:40:27.143137: step 6300, loss 0.266474, acc 0.90625

Evaluation:
2017-03-02T17:40:27.178767: step 6300, loss 1.18903, acc 0.677722

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6300

2017-03-02T17:40:27.649508: step 6301, loss 0.108771, acc 0.96875
2017-03-02T17:40:27.727090: step 6302, loss 0.174817, acc 0.90625
2017-03-02T17:40:27.802556: step 6303, loss 0.147067, acc 0.96875
2017-03-02T17:40:27.875598: step 6304, loss 0.301899, acc 0.859375
2017-03-02T17:40:27.957429: step 6305, loss 0.684763, acc 0.828125
2017-03-02T17:40:28.038677: step 6306, loss 0.105746, acc 0.96875
2017-03-02T17:40:28.125617: step 6307, loss 0.335094, acc 0.875
2017-03-02T17:40:28.196017: step 6308, loss 0.265792, acc 0.9375
2017-03-02T17:40:28.269665: step 6309, loss 0.134567, acc 0.921875
2017-03-02T17:40:28.344504: step 6310, loss 0.134021, acc 0.953125
2017-03-02T17:40:28.417452: step 6311, loss 0.261487, acc 0.921875
2017-03-02T17:40:28.484728: step 6312, loss 0.365231, acc 0.90625
2017-03-02T17:40:28.552952: step 6313, loss 0.227148, acc 0.875
2017-03-02T17:40:28.623041: step 6314, loss 0.22544, acc 0.890625
2017-03-02T17:40:28.694760: step 6315, loss 0.067328, acc 0.984375
2017-03-02T17:40:28.769295: step 6316, loss 0.24458, acc 0.875
2017-03-02T17:40:28.841987: step 6317, loss 0.207487, acc 0.96875
2017-03-02T17:40:28.913369: step 6318, loss 0.149334, acc 0.96875
2017-03-02T17:40:28.987232: step 6319, loss 0.117235, acc 0.953125
2017-03-02T17:40:29.065114: step 6320, loss 0.186642, acc 0.9375
2017-03-02T17:40:29.146229: step 6321, loss 0.0867331, acc 0.953125
2017-03-02T17:40:29.215445: step 6322, loss 0.159736, acc 0.953125
2017-03-02T17:40:29.294245: step 6323, loss 0.225952, acc 0.921875
2017-03-02T17:40:29.369152: step 6324, loss 0.366219, acc 0.875
2017-03-02T17:40:29.440926: step 6325, loss 0.169713, acc 0.9375
2017-03-02T17:40:29.512009: step 6326, loss 0.133767, acc 0.96875
2017-03-02T17:40:29.587418: step 6327, loss 0.177853, acc 0.953125
2017-03-02T17:40:29.670341: step 6328, loss 0.244531, acc 0.875
2017-03-02T17:40:29.751456: step 6329, loss 0.231649, acc 0.921875
2017-03-02T17:40:29.824333: step 6330, loss 0.213471, acc 0.875
2017-03-02T17:40:29.893460: step 6331, loss 0.109056, acc 0.9375
2017-03-02T17:40:29.970671: step 6332, loss 0.0828385, acc 0.953125
2017-03-02T17:40:30.043721: step 6333, loss 0.224899, acc 0.90625
2017-03-02T17:40:30.119540: step 6334, loss 0.180201, acc 0.90625
2017-03-02T17:40:30.198276: step 6335, loss 0.165906, acc 0.953125
2017-03-02T17:40:30.276942: step 6336, loss 0.163805, acc 0.9375
2017-03-02T17:40:30.350025: step 6337, loss 0.241332, acc 0.890625
2017-03-02T17:40:30.427903: step 6338, loss 0.157822, acc 0.9375
2017-03-02T17:40:30.502974: step 6339, loss 0.12852, acc 0.90625
2017-03-02T17:40:30.574298: step 6340, loss 0.295906, acc 0.890625
2017-03-02T17:40:30.644675: step 6341, loss 0.396458, acc 0.84375
2017-03-02T17:40:30.715465: step 6342, loss 0.141506, acc 0.953125
2017-03-02T17:40:30.787503: step 6343, loss 0.212702, acc 0.921875
2017-03-02T17:40:30.858533: step 6344, loss 0.399986, acc 0.859375
2017-03-02T17:40:30.930250: step 6345, loss 0.277765, acc 0.890625
2017-03-02T17:40:31.004243: step 6346, loss 0.259705, acc 0.921875
2017-03-02T17:40:31.075945: step 6347, loss 0.20166, acc 0.921875
2017-03-02T17:40:31.156113: step 6348, loss 0.330303, acc 0.859375
2017-03-02T17:40:31.237663: step 6349, loss 0.362343, acc 0.875
2017-03-02T17:40:31.313825: step 6350, loss 0.258044, acc 0.90625
2017-03-02T17:40:31.389046: step 6351, loss 0.209802, acc 0.90625
2017-03-02T17:40:31.463700: step 6352, loss 0.268794, acc 0.890625
2017-03-02T17:40:31.552785: step 6353, loss 0.177261, acc 0.953125
2017-03-02T17:40:31.624539: step 6354, loss 0.167726, acc 0.921875
2017-03-02T17:40:31.706364: step 6355, loss 0.195903, acc 0.921875
2017-03-02T17:40:31.779736: step 6356, loss 0.177844, acc 0.90625
2017-03-02T17:40:31.853499: step 6357, loss 0.192187, acc 0.953125
2017-03-02T17:40:31.916023: step 6358, loss 0.102203, acc 0.921875
2017-03-02T17:40:31.983565: step 6359, loss 0.175893, acc 0.9375
2017-03-02T17:40:32.053230: step 6360, loss 0.348454, acc 0.875
2017-03-02T17:40:32.125017: step 6361, loss 0.133798, acc 0.953125
2017-03-02T17:40:32.223675: step 6362, loss 0.330241, acc 0.921875
2017-03-02T17:40:32.301282: step 6363, loss 0.40755, acc 0.859375
2017-03-02T17:40:32.374157: step 6364, loss 0.190714, acc 0.9375
2017-03-02T17:40:32.450238: step 6365, loss 0.177254, acc 0.9375
2017-03-02T17:40:32.522983: step 6366, loss 0.0423971, acc 1
2017-03-02T17:40:32.604035: step 6367, loss 0.204294, acc 0.9375
2017-03-02T17:40:32.673694: step 6368, loss 0.419354, acc 0.84375
2017-03-02T17:40:32.749927: step 6369, loss 0.162347, acc 0.9375
2017-03-02T17:40:32.834817: step 6370, loss 0.307131, acc 0.84375
2017-03-02T17:40:32.907122: step 6371, loss 0.156162, acc 0.96875
2017-03-02T17:40:32.972654: step 6372, loss 0.167446, acc 0.921875
2017-03-02T17:40:33.056250: step 6373, loss 0.124693, acc 0.953125
2017-03-02T17:40:33.126188: step 6374, loss 0.121343, acc 0.953125
2017-03-02T17:40:33.200351: step 6375, loss 0.241035, acc 0.921875
2017-03-02T17:40:33.273113: step 6376, loss 0.270351, acc 0.90625
2017-03-02T17:40:33.342338: step 6377, loss 0.164953, acc 0.953125
2017-03-02T17:40:33.412428: step 6378, loss 0.156454, acc 0.953125
2017-03-02T17:40:33.488433: step 6379, loss 0.202347, acc 0.921875
2017-03-02T17:40:33.559891: step 6380, loss 0.164652, acc 0.9375
2017-03-02T17:40:33.632532: step 6381, loss 0.20585, acc 0.90625
2017-03-02T17:40:33.712690: step 6382, loss 0.220218, acc 0.921875
2017-03-02T17:40:33.788208: step 6383, loss 0.130209, acc 0.953125
2017-03-02T17:40:33.859722: step 6384, loss 0.242, acc 0.90625
2017-03-02T17:40:33.944092: step 6385, loss 0.288894, acc 0.90625
2017-03-02T17:40:34.011667: step 6386, loss 0.19703, acc 0.921875
2017-03-02T17:40:34.079081: step 6387, loss 0.235657, acc 0.90625
2017-03-02T17:40:34.148832: step 6388, loss 0.119009, acc 0.96875
2017-03-02T17:40:34.219956: step 6389, loss 0.286586, acc 0.921875
2017-03-02T17:40:34.293610: step 6390, loss 0.173685, acc 0.90625
2017-03-02T17:40:34.362735: step 6391, loss 0.107241, acc 0.96875
2017-03-02T17:40:34.437859: step 6392, loss 0.191387, acc 0.921875
2017-03-02T17:40:34.506316: step 6393, loss 0.231089, acc 0.90625
2017-03-02T17:40:34.579722: step 6394, loss 0.15008, acc 0.921875
2017-03-02T17:40:34.661636: step 6395, loss 0.231669, acc 0.875
2017-03-02T17:40:34.726642: step 6396, loss 0.203564, acc 0.9375
2017-03-02T17:40:34.793842: step 6397, loss 0.171072, acc 0.921875
2017-03-02T17:40:34.874860: step 6398, loss 0.224834, acc 0.875
2017-03-02T17:40:34.940401: step 6399, loss 0.271848, acc 0.90625
2017-03-02T17:40:35.015308: step 6400, loss 0.123416, acc 0.96875

Evaluation:
2017-03-02T17:40:35.050932: step 6400, loss 1.22401, acc 0.675559

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6400

2017-03-02T17:40:35.492547: step 6401, loss 0.105508, acc 0.96875
2017-03-02T17:40:35.565788: step 6402, loss 0.341948, acc 0.90625
2017-03-02T17:40:35.642754: step 6403, loss 0.246784, acc 0.90625
2017-03-02T17:40:35.712999: step 6404, loss 0.145012, acc 0.9375
2017-03-02T17:40:35.784110: step 6405, loss 0.337111, acc 0.859375
2017-03-02T17:40:35.857936: step 6406, loss 0.109657, acc 0.9375
2017-03-02T17:40:35.931331: step 6407, loss 0.158727, acc 0.953125
2017-03-02T17:40:36.007977: step 6408, loss 0.149776, acc 0.921875
2017-03-02T17:40:36.078096: step 6409, loss 0.140958, acc 0.921875
2017-03-02T17:40:36.142227: step 6410, loss 0.263383, acc 0.875
2017-03-02T17:40:36.212988: step 6411, loss 0.220771, acc 0.90625
2017-03-02T17:40:36.284344: step 6412, loss 0.227821, acc 0.90625
2017-03-02T17:40:36.360054: step 6413, loss 0.264521, acc 0.90625
2017-03-02T17:40:36.437341: step 6414, loss 0.150785, acc 0.953125
2017-03-02T17:40:36.512973: step 6415, loss 0.21181, acc 0.953125
2017-03-02T17:40:36.588451: step 6416, loss 0.241864, acc 0.890625
2017-03-02T17:40:36.660620: step 6417, loss 0.0903882, acc 0.96875
2017-03-02T17:40:36.734302: step 6418, loss 0.306581, acc 0.90625
2017-03-02T17:40:36.805778: step 6419, loss 0.208771, acc 0.90625
2017-03-02T17:40:36.874071: step 6420, loss 0.412591, acc 0.828125
2017-03-02T17:40:36.948306: step 6421, loss 0.106788, acc 0.953125
2017-03-02T17:40:37.016314: step 6422, loss 0.244313, acc 0.9375
2017-03-02T17:40:37.100984: step 6423, loss 0.137041, acc 0.96875
2017-03-02T17:40:37.178814: step 6424, loss 0.305843, acc 0.90625
2017-03-02T17:40:37.247018: step 6425, loss 0.279809, acc 0.859375
2017-03-02T17:40:37.324037: step 6426, loss 0.231072, acc 0.9375
2017-03-02T17:40:37.398308: step 6427, loss 0.296721, acc 0.828125
2017-03-02T17:40:37.470634: step 6428, loss 0.0987384, acc 0.953125
2017-03-02T17:40:37.542391: step 6429, loss 0.183407, acc 0.953125
2017-03-02T17:40:37.621350: step 6430, loss 0.148271, acc 0.953125
2017-03-02T17:40:37.697133: step 6431, loss 0.254266, acc 0.890625
2017-03-02T17:40:37.774241: step 6432, loss 0.143287, acc 0.953125
2017-03-02T17:40:37.851835: step 6433, loss 0.346428, acc 0.890625
2017-03-02T17:40:37.926055: step 6434, loss 0.185969, acc 0.9375
2017-03-02T17:40:37.997846: step 6435, loss 0.188479, acc 0.90625
2017-03-02T17:40:38.074000: step 6436, loss 0.12208, acc 0.96875
2017-03-02T17:40:38.142965: step 6437, loss 0.329061, acc 0.90625
2017-03-02T17:40:38.210825: step 6438, loss 0.18468, acc 0.9375
2017-03-02T17:40:38.287741: step 6439, loss 0.253979, acc 0.90625
2017-03-02T17:40:38.375711: step 6440, loss 0.10001, acc 0.953125
2017-03-02T17:40:38.456213: step 6441, loss 0.360674, acc 0.859375
2017-03-02T17:40:38.536237: step 6442, loss 0.260418, acc 0.9375
2017-03-02T17:40:38.611539: step 6443, loss 0.277446, acc 0.921875
2017-03-02T17:40:38.685215: step 6444, loss 0.179356, acc 0.9375
2017-03-02T17:40:38.761681: step 6445, loss 0.317569, acc 0.875
2017-03-02T17:40:38.829607: step 6446, loss 0.379687, acc 0.921875
2017-03-02T17:40:38.896711: step 6447, loss 0.176219, acc 0.9375
2017-03-02T17:40:38.972835: step 6448, loss 0.260349, acc 0.875
2017-03-02T17:40:39.043435: step 6449, loss 0.0763122, acc 0.984375
2017-03-02T17:40:39.126006: step 6450, loss 0.141123, acc 0.9375
2017-03-02T17:40:39.196733: step 6451, loss 0.144061, acc 0.921875
2017-03-02T17:40:39.270413: step 6452, loss 0.22996, acc 0.921875
2017-03-02T17:40:39.346806: step 6453, loss 0.189135, acc 0.921875
2017-03-02T17:40:39.421857: step 6454, loss 0.131222, acc 0.96875
2017-03-02T17:40:39.493374: step 6455, loss 0.155601, acc 0.921875
2017-03-02T17:40:39.562540: step 6456, loss 0.214665, acc 0.890625
2017-03-02T17:40:39.629695: step 6457, loss 0.305997, acc 0.84375
2017-03-02T17:40:39.700163: step 6458, loss 0.0566213, acc 0.984375
2017-03-02T17:40:39.772906: step 6459, loss 0.381542, acc 0.84375
2017-03-02T17:40:39.844741: step 6460, loss 0.124429, acc 0.953125
2017-03-02T17:40:39.922610: step 6461, loss 0.232229, acc 0.875
2017-03-02T17:40:40.001993: step 6462, loss 0.2498, acc 0.890625
2017-03-02T17:40:40.077750: step 6463, loss 0.198893, acc 0.921875
2017-03-02T17:40:40.155456: step 6464, loss 0.490683, acc 0.828125
2017-03-02T17:40:40.224094: step 6465, loss 0.164045, acc 0.890625
2017-03-02T17:40:40.292442: step 6466, loss 0.214962, acc 0.96875
2017-03-02T17:40:40.366284: step 6467, loss 0.145379, acc 0.90625
2017-03-02T17:40:40.435053: step 6468, loss 0.00355247, acc 1
2017-03-02T17:40:40.512525: step 6469, loss 0.200308, acc 0.953125
2017-03-02T17:40:40.588083: step 6470, loss 0.203814, acc 0.953125
2017-03-02T17:40:40.662719: step 6471, loss 0.186646, acc 0.90625
2017-03-02T17:40:40.736225: step 6472, loss 0.108751, acc 0.9375
2017-03-02T17:40:40.808349: step 6473, loss 0.146194, acc 0.953125
2017-03-02T17:40:40.886877: step 6474, loss 0.292201, acc 0.921875
2017-03-02T17:40:40.955340: step 6475, loss 0.153932, acc 0.953125
2017-03-02T17:40:41.027133: step 6476, loss 0.0815466, acc 0.984375
2017-03-02T17:40:41.112132: step 6477, loss 0.205116, acc 0.953125
2017-03-02T17:40:41.191758: step 6478, loss 0.248572, acc 0.921875
2017-03-02T17:40:41.264084: step 6479, loss 0.185042, acc 0.9375
2017-03-02T17:40:41.337721: step 6480, loss 0.264252, acc 0.859375
2017-03-02T17:40:41.414932: step 6481, loss 0.190957, acc 0.890625
2017-03-02T17:40:41.496837: step 6482, loss 0.215412, acc 0.859375
2017-03-02T17:40:41.574700: step 6483, loss 0.1682, acc 0.953125
2017-03-02T17:40:41.644472: step 6484, loss 0.17233, acc 0.9375
2017-03-02T17:40:41.713752: step 6485, loss 0.17358, acc 0.90625
2017-03-02T17:40:41.786612: step 6486, loss 0.196687, acc 0.90625
2017-03-02T17:40:41.852728: step 6487, loss 0.0867299, acc 0.96875
2017-03-02T17:40:41.922637: step 6488, loss 0.133669, acc 0.9375
2017-03-02T17:40:41.996917: step 6489, loss 0.131696, acc 0.953125
2017-03-02T17:40:42.075034: step 6490, loss 0.10636, acc 0.9375
2017-03-02T17:40:42.149533: step 6491, loss 0.280963, acc 0.859375
2017-03-02T17:40:42.224177: step 6492, loss 0.328098, acc 0.875
2017-03-02T17:40:42.298727: step 6493, loss 0.287893, acc 0.90625
2017-03-02T17:40:42.365730: step 6494, loss 0.257204, acc 0.90625
2017-03-02T17:40:42.446290: step 6495, loss 0.226392, acc 0.9375
2017-03-02T17:40:42.518337: step 6496, loss 0.185429, acc 0.9375
2017-03-02T17:40:42.591899: step 6497, loss 0.150423, acc 0.9375
2017-03-02T17:40:42.667451: step 6498, loss 0.144006, acc 0.9375
2017-03-02T17:40:42.746898: step 6499, loss 0.169768, acc 0.953125
2017-03-02T17:40:42.826290: step 6500, loss 0.174752, acc 0.921875

Evaluation:
2017-03-02T17:40:42.859792: step 6500, loss 1.2361, acc 0.667628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6500

2017-03-02T17:40:43.325684: step 6501, loss 0.145524, acc 0.953125
2017-03-02T17:40:43.397871: step 6502, loss 0.298493, acc 0.875
2017-03-02T17:40:43.469585: step 6503, loss 0.224834, acc 0.90625
2017-03-02T17:40:43.545522: step 6504, loss 0.153684, acc 0.953125
2017-03-02T17:40:43.624690: step 6505, loss 0.266912, acc 0.90625
2017-03-02T17:40:43.693482: step 6506, loss 0.25043, acc 0.890625
2017-03-02T17:40:43.767013: step 6507, loss 0.168386, acc 0.9375
2017-03-02T17:40:43.841352: step 6508, loss 0.19017, acc 0.890625
2017-03-02T17:40:43.922145: step 6509, loss 0.0796126, acc 0.96875
2017-03-02T17:40:43.995980: step 6510, loss 0.258537, acc 0.875
2017-03-02T17:40:44.072364: step 6511, loss 0.164567, acc 0.953125
2017-03-02T17:40:44.144473: step 6512, loss 0.0784868, acc 0.96875
2017-03-02T17:40:44.216452: step 6513, loss 0.22169, acc 0.90625
2017-03-02T17:40:44.287482: step 6514, loss 0.241067, acc 0.875
2017-03-02T17:40:44.357138: step 6515, loss 0.19343, acc 0.921875
2017-03-02T17:40:44.419251: step 6516, loss 0.14416, acc 0.984375
2017-03-02T17:40:44.482227: step 6517, loss 0.390412, acc 0.90625
2017-03-02T17:40:44.558763: step 6518, loss 0.141404, acc 0.9375
2017-03-02T17:40:44.627072: step 6519, loss 0.177231, acc 0.921875
2017-03-02T17:40:44.700530: step 6520, loss 0.181732, acc 0.96875
2017-03-02T17:40:44.778086: step 6521, loss 0.191844, acc 0.9375
2017-03-02T17:40:44.850456: step 6522, loss 0.251383, acc 0.921875
2017-03-02T17:40:44.919759: step 6523, loss 0.164517, acc 0.9375
2017-03-02T17:40:44.995550: step 6524, loss 0.17774, acc 0.90625
2017-03-02T17:40:45.070693: step 6525, loss 0.168681, acc 0.953125
2017-03-02T17:40:45.142687: step 6526, loss 0.185778, acc 0.921875
2017-03-02T17:40:45.208966: step 6527, loss 0.125533, acc 0.921875
2017-03-02T17:40:45.277661: step 6528, loss 0.113531, acc 0.9375
2017-03-02T17:40:45.351204: step 6529, loss 0.253835, acc 0.921875
2017-03-02T17:40:45.433280: step 6530, loss 0.179449, acc 0.96875
2017-03-02T17:40:45.520975: step 6531, loss 0.265647, acc 0.90625
2017-03-02T17:40:45.597257: step 6532, loss 0.390714, acc 0.875
2017-03-02T17:40:45.674686: step 6533, loss 0.122131, acc 0.96875
2017-03-02T17:40:45.743552: step 6534, loss 0.229162, acc 0.90625
2017-03-02T17:40:45.809653: step 6535, loss 0.213658, acc 0.921875
2017-03-02T17:40:45.882275: step 6536, loss 0.137606, acc 0.9375
2017-03-02T17:40:45.960667: step 6537, loss 0.140572, acc 0.9375
2017-03-02T17:40:46.030039: step 6538, loss 0.123309, acc 0.9375
2017-03-02T17:40:46.106556: step 6539, loss 0.0894595, acc 0.96875
2017-03-02T17:40:46.174135: step 6540, loss 0.252828, acc 0.90625
2017-03-02T17:40:46.252348: step 6541, loss 0.158489, acc 0.90625
2017-03-02T17:40:46.324396: step 6542, loss 0.163852, acc 0.9375
2017-03-02T17:40:46.391845: step 6543, loss 0.204533, acc 0.90625
2017-03-02T17:40:46.458783: step 6544, loss 0.150051, acc 0.90625
2017-03-02T17:40:46.527795: step 6545, loss 0.330785, acc 0.890625
2017-03-02T17:40:46.601458: step 6546, loss 0.174294, acc 0.921875
2017-03-02T17:40:46.675702: step 6547, loss 0.163139, acc 0.9375
2017-03-02T17:40:46.753538: step 6548, loss 0.247579, acc 0.90625
2017-03-02T17:40:46.828673: step 6549, loss 0.2502, acc 0.90625
2017-03-02T17:40:46.909976: step 6550, loss 0.231695, acc 0.921875
2017-03-02T17:40:46.984594: step 6551, loss 0.166195, acc 0.953125
2017-03-02T17:40:47.059431: step 6552, loss 0.232356, acc 0.90625
2017-03-02T17:40:47.131109: step 6553, loss 0.356691, acc 0.859375
2017-03-02T17:40:47.208382: step 6554, loss 0.29806, acc 0.90625
2017-03-02T17:40:47.283421: step 6555, loss 0.10775, acc 0.96875
2017-03-02T17:40:47.355954: step 6556, loss 0.174837, acc 0.90625
2017-03-02T17:40:47.429358: step 6557, loss 0.44307, acc 0.8125
2017-03-02T17:40:47.502736: step 6558, loss 0.214161, acc 0.921875
2017-03-02T17:40:47.575150: step 6559, loss 0.0719739, acc 0.984375
2017-03-02T17:40:47.658935: step 6560, loss 0.0752371, acc 0.984375
2017-03-02T17:40:47.728363: step 6561, loss 0.259917, acc 0.921875
2017-03-02T17:40:47.796088: step 6562, loss 0.251814, acc 0.90625
2017-03-02T17:40:47.871133: step 6563, loss 0.124436, acc 0.921875
2017-03-02T17:40:47.942201: step 6564, loss 0.217649, acc 0.921875
2017-03-02T17:40:48.014091: step 6565, loss 0.251007, acc 0.9375
2017-03-02T17:40:48.088605: step 6566, loss 0.145154, acc 0.921875
2017-03-02T17:40:48.154986: step 6567, loss 0.308837, acc 0.828125
2017-03-02T17:40:48.229775: step 6568, loss 0.179976, acc 0.953125
2017-03-02T17:40:48.306128: step 6569, loss 0.121845, acc 0.953125
2017-03-02T17:40:48.379687: step 6570, loss 0.156322, acc 0.921875
2017-03-02T17:40:48.454111: step 6571, loss 0.120494, acc 0.9375
2017-03-02T17:40:48.522188: step 6572, loss 0.122977, acc 0.9375
2017-03-02T17:40:48.592437: step 6573, loss 0.172561, acc 0.9375
2017-03-02T17:40:48.664847: step 6574, loss 0.484827, acc 0.84375
2017-03-02T17:40:48.740881: step 6575, loss 0.17596, acc 0.9375
2017-03-02T17:40:48.834404: step 6576, loss 0.138608, acc 0.96875
2017-03-02T17:40:48.911266: step 6577, loss 0.296198, acc 0.921875
2017-03-02T17:40:48.978138: step 6578, loss 0.240401, acc 0.890625
2017-03-02T17:40:49.050339: step 6579, loss 0.46336, acc 0.84375
2017-03-02T17:40:49.120673: step 6580, loss 0.284824, acc 0.859375
2017-03-02T17:40:49.194972: step 6581, loss 0.252506, acc 0.890625
2017-03-02T17:40:49.265125: step 6582, loss 0.292423, acc 0.875
2017-03-02T17:40:49.337394: step 6583, loss 0.175017, acc 0.9375
2017-03-02T17:40:49.410311: step 6584, loss 0.224598, acc 0.875
2017-03-02T17:40:49.484203: step 6585, loss 0.295711, acc 0.890625
2017-03-02T17:40:49.554527: step 6586, loss 0.194868, acc 0.9375
2017-03-02T17:40:49.631171: step 6587, loss 0.182882, acc 0.921875
2017-03-02T17:40:49.700785: step 6588, loss 0.129414, acc 0.953125
2017-03-02T17:40:49.791231: step 6589, loss 0.266586, acc 0.90625
2017-03-02T17:40:49.865176: step 6590, loss 0.172673, acc 0.921875
2017-03-02T17:40:49.937081: step 6591, loss 0.183778, acc 0.953125
2017-03-02T17:40:50.009899: step 6592, loss 0.148139, acc 0.9375
2017-03-02T17:40:50.083587: step 6593, loss 0.221012, acc 0.875
2017-03-02T17:40:50.156483: step 6594, loss 0.268974, acc 0.875
2017-03-02T17:40:50.231690: step 6595, loss 0.25788, acc 0.875
2017-03-02T17:40:50.306699: step 6596, loss 0.172968, acc 0.953125
2017-03-02T17:40:50.381163: step 6597, loss 0.280822, acc 0.90625
2017-03-02T17:40:50.453617: step 6598, loss 0.135894, acc 0.953125
2017-03-02T17:40:50.527754: step 6599, loss 0.261627, acc 0.90625
2017-03-02T17:40:50.596406: step 6600, loss 0.141658, acc 0.953125

Evaluation:
2017-03-02T17:40:50.635653: step 6600, loss 1.21992, acc 0.68421

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6600

2017-03-02T17:40:51.080307: step 6601, loss 0.100116, acc 0.9375
2017-03-02T17:40:51.150274: step 6602, loss 0.214288, acc 0.921875
2017-03-02T17:40:51.222538: step 6603, loss 0.385333, acc 0.890625
2017-03-02T17:40:51.293357: step 6604, loss 0.213297, acc 0.890625
2017-03-02T17:40:51.381189: step 6605, loss 0.126234, acc 0.96875
2017-03-02T17:40:51.449205: step 6606, loss 0.229691, acc 0.890625
2017-03-02T17:40:51.529958: step 6607, loss 0.232433, acc 0.875
2017-03-02T17:40:51.603002: step 6608, loss 0.208675, acc 0.890625
2017-03-02T17:40:51.675204: step 6609, loss 0.374667, acc 0.859375
2017-03-02T17:40:51.748651: step 6610, loss 0.0912315, acc 0.984375
2017-03-02T17:40:51.826415: step 6611, loss 0.274343, acc 0.859375
2017-03-02T17:40:51.898938: step 6612, loss 0.320291, acc 0.890625
2017-03-02T17:40:51.968054: step 6613, loss 0.229281, acc 0.890625
2017-03-02T17:40:52.038619: step 6614, loss 0.331467, acc 0.890625
2017-03-02T17:40:52.110697: step 6615, loss 0.0882743, acc 0.953125
2017-03-02T17:40:52.181827: step 6616, loss 0.101078, acc 0.953125
2017-03-02T17:40:52.254465: step 6617, loss 0.240847, acc 0.921875
2017-03-02T17:40:52.329345: step 6618, loss 0.111937, acc 0.984375
2017-03-02T17:40:52.399114: step 6619, loss 0.0949679, acc 0.96875
2017-03-02T17:40:52.471686: step 6620, loss 0.159231, acc 0.9375
2017-03-02T17:40:52.552815: step 6621, loss 0.21335, acc 0.921875
2017-03-02T17:40:52.622974: step 6622, loss 0.251532, acc 0.859375
2017-03-02T17:40:52.693814: step 6623, loss 0.196443, acc 0.9375
2017-03-02T17:40:52.782446: step 6624, loss 0.162564, acc 0.921875
2017-03-02T17:40:52.851778: step 6625, loss 0.167039, acc 0.9375
2017-03-02T17:40:52.935110: step 6626, loss 0.108175, acc 0.96875
2017-03-02T17:40:53.011780: step 6627, loss 0.216405, acc 0.890625
2017-03-02T17:40:53.083090: step 6628, loss 0.139499, acc 0.953125
2017-03-02T17:40:53.154515: step 6629, loss 0.26245, acc 0.9375
2017-03-02T17:40:53.230923: step 6630, loss 0.186379, acc 0.890625
2017-03-02T17:40:53.307532: step 6631, loss 0.296395, acc 0.859375
2017-03-02T17:40:53.379026: step 6632, loss 0.143259, acc 0.9375
2017-03-02T17:40:53.451878: step 6633, loss 0.160868, acc 0.953125
2017-03-02T17:40:53.525309: step 6634, loss 0.132302, acc 0.96875
2017-03-02T17:40:53.598379: step 6635, loss 0.0863485, acc 0.984375
2017-03-02T17:40:53.665266: step 6636, loss 0.126825, acc 0.9375
2017-03-02T17:40:53.740405: step 6637, loss 0.166341, acc 0.921875
2017-03-02T17:40:53.813122: step 6638, loss 0.263536, acc 0.90625
2017-03-02T17:40:53.886146: step 6639, loss 0.178986, acc 0.90625
2017-03-02T17:40:53.970519: step 6640, loss 0.148891, acc 0.953125
2017-03-02T17:40:54.035310: step 6641, loss 0.136051, acc 0.921875
2017-03-02T17:40:54.113108: step 6642, loss 0.180321, acc 0.890625
2017-03-02T17:40:54.185666: step 6643, loss 0.232531, acc 0.90625
2017-03-02T17:40:54.258051: step 6644, loss 0.170869, acc 0.953125
2017-03-02T17:40:54.333823: step 6645, loss 0.185344, acc 0.921875
2017-03-02T17:40:54.414986: step 6646, loss 0.135671, acc 0.953125
2017-03-02T17:40:54.484615: step 6647, loss 0.0614205, acc 0.984375
2017-03-02T17:40:54.556203: step 6648, loss 0.200541, acc 0.890625
2017-03-02T17:40:54.631094: step 6649, loss 0.146475, acc 0.9375
2017-03-02T17:40:54.700023: step 6650, loss 0.103354, acc 0.953125
2017-03-02T17:40:54.772051: step 6651, loss 0.0891259, acc 0.96875
2017-03-02T17:40:54.846373: step 6652, loss 0.207076, acc 0.90625
2017-03-02T17:40:54.918186: step 6653, loss 0.192145, acc 0.90625
2017-03-02T17:40:55.000706: step 6654, loss 0.182009, acc 0.921875
2017-03-02T17:40:55.072878: step 6655, loss 0.399431, acc 0.859375
2017-03-02T17:40:55.146071: step 6656, loss 0.121423, acc 0.921875
2017-03-02T17:40:55.217809: step 6657, loss 0.113804, acc 0.953125
2017-03-02T17:40:55.292534: step 6658, loss 0.160857, acc 0.921875
2017-03-02T17:40:55.366941: step 6659, loss 0.143545, acc 0.9375
2017-03-02T17:40:55.442134: step 6660, loss 0.240505, acc 0.921875
2017-03-02T17:40:55.512550: step 6661, loss 0.150419, acc 0.9375
2017-03-02T17:40:55.586713: step 6662, loss 0.146968, acc 0.953125
2017-03-02T17:40:55.660251: step 6663, loss 0.250496, acc 0.890625
2017-03-02T17:40:55.732074: step 6664, loss 0.169673, acc 1
2017-03-02T17:40:55.807456: step 6665, loss 0.133785, acc 0.953125
2017-03-02T17:40:55.885208: step 6666, loss 0.186763, acc 0.921875
2017-03-02T17:40:55.961038: step 6667, loss 0.148399, acc 0.921875
2017-03-02T17:40:56.036826: step 6668, loss 0.185345, acc 0.921875
2017-03-02T17:40:56.106691: step 6669, loss 0.185431, acc 0.921875
2017-03-02T17:40:56.179937: step 6670, loss 0.0766607, acc 0.96875
2017-03-02T17:40:56.256401: step 6671, loss 0.149391, acc 0.9375
2017-03-02T17:40:56.325145: step 6672, loss 0.212552, acc 0.890625
2017-03-02T17:40:56.397205: step 6673, loss 0.186928, acc 0.890625
2017-03-02T17:40:56.469076: step 6674, loss 0.214993, acc 0.90625
2017-03-02T17:40:56.539146: step 6675, loss 0.136198, acc 0.953125
2017-03-02T17:40:56.617296: step 6676, loss 0.274867, acc 0.890625
2017-03-02T17:40:56.685066: step 6677, loss 0.125678, acc 0.953125
2017-03-02T17:40:56.753704: step 6678, loss 0.323267, acc 0.921875
2017-03-02T17:40:56.826450: step 6679, loss 0.170538, acc 0.921875
2017-03-02T17:40:56.899992: step 6680, loss 0.242074, acc 0.90625
2017-03-02T17:40:56.970911: step 6681, loss 0.126951, acc 0.953125
2017-03-02T17:40:57.037471: step 6682, loss 0.275392, acc 0.90625
2017-03-02T17:40:57.125851: step 6683, loss 0.156208, acc 0.953125
2017-03-02T17:40:57.196462: step 6684, loss 0.11364, acc 0.953125
2017-03-02T17:40:57.267527: step 6685, loss 0.14197, acc 0.96875
2017-03-02T17:40:57.341539: step 6686, loss 0.129175, acc 0.921875
2017-03-02T17:40:57.414004: step 6687, loss 0.189661, acc 0.921875
2017-03-02T17:40:57.489726: step 6688, loss 0.125568, acc 0.9375
2017-03-02T17:40:57.567243: step 6689, loss 0.14694, acc 0.921875
2017-03-02T17:40:57.643121: step 6690, loss 0.132369, acc 0.984375
2017-03-02T17:40:57.714295: step 6691, loss 0.176872, acc 0.9375
2017-03-02T17:40:57.795397: step 6692, loss 0.156696, acc 0.953125
2017-03-02T17:40:57.870795: step 6693, loss 0.0581914, acc 0.96875
2017-03-02T17:40:57.946022: step 6694, loss 0.185467, acc 0.921875
2017-03-02T17:40:58.016664: step 6695, loss 0.302787, acc 0.890625
2017-03-02T17:40:58.087392: step 6696, loss 0.179862, acc 0.921875
2017-03-02T17:40:58.156991: step 6697, loss 0.312738, acc 0.890625
2017-03-02T17:40:58.229047: step 6698, loss 0.196985, acc 0.953125
2017-03-02T17:40:58.304198: step 6699, loss 0.144162, acc 0.953125
2017-03-02T17:40:58.374744: step 6700, loss 0.194092, acc 0.953125

Evaluation:
2017-03-02T17:40:58.416581: step 6700, loss 1.25525, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6700

2017-03-02T17:40:59.023665: step 6701, loss 0.273916, acc 0.890625
2017-03-02T17:40:59.095940: step 6702, loss 0.17052, acc 0.9375
2017-03-02T17:40:59.165579: step 6703, loss 0.101365, acc 0.96875
2017-03-02T17:40:59.238698: step 6704, loss 0.382228, acc 0.828125
2017-03-02T17:40:59.308552: step 6705, loss 0.276152, acc 0.90625
2017-03-02T17:40:59.382555: step 6706, loss 0.133496, acc 0.9375
2017-03-02T17:40:59.459660: step 6707, loss 0.209648, acc 0.90625
2017-03-02T17:40:59.528053: step 6708, loss 0.227265, acc 0.9375
2017-03-02T17:40:59.598680: step 6709, loss 0.129494, acc 0.921875
2017-03-02T17:40:59.666111: step 6710, loss 0.155239, acc 0.953125
2017-03-02T17:40:59.738587: step 6711, loss 0.237723, acc 0.90625
2017-03-02T17:40:59.812964: step 6712, loss 0.0625679, acc 0.96875
2017-03-02T17:40:59.884692: step 6713, loss 0.231004, acc 0.90625
2017-03-02T17:40:59.990537: step 6714, loss 0.11076, acc 0.953125
2017-03-02T17:41:00.059707: step 6715, loss 0.0841454, acc 0.984375
2017-03-02T17:41:00.124695: step 6716, loss 0.163606, acc 0.921875
2017-03-02T17:41:00.186117: step 6717, loss 0.111169, acc 0.9375
2017-03-02T17:41:00.257874: step 6718, loss 0.252803, acc 0.921875
2017-03-02T17:41:00.324301: step 6719, loss 0.12132, acc 0.953125
2017-03-02T17:41:00.390728: step 6720, loss 0.411362, acc 0.84375
2017-03-02T17:41:00.456772: step 6721, loss 0.232257, acc 0.890625
2017-03-02T17:41:00.531275: step 6722, loss 0.187938, acc 0.921875
2017-03-02T17:41:00.604104: step 6723, loss 0.259591, acc 0.90625
2017-03-02T17:41:00.675788: step 6724, loss 0.143312, acc 0.96875
2017-03-02T17:41:00.755353: step 6725, loss 0.0881658, acc 0.96875
2017-03-02T17:41:00.827569: step 6726, loss 0.234864, acc 0.90625
2017-03-02T17:41:00.897083: step 6727, loss 0.150193, acc 0.953125
2017-03-02T17:41:00.976767: step 6728, loss 0.235888, acc 0.9375
2017-03-02T17:41:01.049037: step 6729, loss 0.158272, acc 0.953125
2017-03-02T17:41:01.129924: step 6730, loss 0.12273, acc 0.96875
2017-03-02T17:41:01.204217: step 6731, loss 0.146338, acc 0.9375
2017-03-02T17:41:01.278074: step 6732, loss 0.193159, acc 0.9375
2017-03-02T17:41:01.350247: step 6733, loss 0.326203, acc 0.90625
2017-03-02T17:41:01.421231: step 6734, loss 0.345353, acc 0.84375
2017-03-02T17:41:01.497143: step 6735, loss 0.171382, acc 0.953125
2017-03-02T17:41:01.565768: step 6736, loss 0.166231, acc 0.9375
2017-03-02T17:41:01.642843: step 6737, loss 0.168109, acc 0.921875
2017-03-02T17:41:01.719379: step 6738, loss 0.120488, acc 0.96875
2017-03-02T17:41:01.793929: step 6739, loss 0.197483, acc 0.921875
2017-03-02T17:41:01.872598: step 6740, loss 0.203573, acc 0.90625
2017-03-02T17:41:01.941328: step 6741, loss 0.172548, acc 0.9375
2017-03-02T17:41:02.013765: step 6742, loss 0.374317, acc 0.875
2017-03-02T17:41:02.088211: step 6743, loss 0.221377, acc 0.921875
2017-03-02T17:41:02.154729: step 6744, loss 0.331422, acc 0.921875
2017-03-02T17:41:02.226261: step 6745, loss 0.165881, acc 0.953125
2017-03-02T17:41:02.299912: step 6746, loss 0.274964, acc 0.90625
2017-03-02T17:41:02.370103: step 6747, loss 0.102742, acc 0.953125
2017-03-02T17:41:02.451982: step 6748, loss 0.12345, acc 0.96875
2017-03-02T17:41:02.529950: step 6749, loss 0.10557, acc 0.953125
2017-03-02T17:41:02.601227: step 6750, loss 0.217963, acc 0.921875
2017-03-02T17:41:02.671265: step 6751, loss 0.162169, acc 0.9375
2017-03-02T17:41:02.742715: step 6752, loss 0.156507, acc 0.9375
2017-03-02T17:41:02.814095: step 6753, loss 0.148472, acc 0.921875
2017-03-02T17:41:02.882882: step 6754, loss 0.200477, acc 0.921875
2017-03-02T17:41:02.958932: step 6755, loss 0.351734, acc 0.875
2017-03-02T17:41:03.031309: step 6756, loss 0.336743, acc 0.859375
2017-03-02T17:41:03.106460: step 6757, loss 0.290929, acc 0.859375
2017-03-02T17:41:03.166819: step 6758, loss 0.293481, acc 0.828125
2017-03-02T17:41:03.244074: step 6759, loss 0.145885, acc 0.9375
2017-03-02T17:41:03.314854: step 6760, loss 0.251352, acc 0.890625
2017-03-02T17:41:03.387761: step 6761, loss 0.258552, acc 0.9375
2017-03-02T17:41:03.460610: step 6762, loss 0.270558, acc 0.875
2017-03-02T17:41:03.528402: step 6763, loss 0.186507, acc 0.9375
2017-03-02T17:41:03.601134: step 6764, loss 0.322294, acc 0.875
2017-03-02T17:41:03.673745: step 6765, loss 0.154483, acc 0.953125
2017-03-02T17:41:03.749837: step 6766, loss 0.189994, acc 0.953125
2017-03-02T17:41:03.822254: step 6767, loss 0.191834, acc 0.953125
2017-03-02T17:41:03.904845: step 6768, loss 0.20962, acc 0.9375
2017-03-02T17:41:03.992744: step 6769, loss 0.421409, acc 0.875
2017-03-02T17:41:04.068603: step 6770, loss 0.175773, acc 0.921875
2017-03-02T17:41:04.141727: step 6771, loss 0.521991, acc 0.828125
2017-03-02T17:41:04.210477: step 6772, loss 0.388219, acc 0.828125
2017-03-02T17:41:04.279889: step 6773, loss 0.144132, acc 0.9375
2017-03-02T17:41:04.352141: step 6774, loss 0.2705, acc 0.90625
2017-03-02T17:41:04.424176: step 6775, loss 0.24373, acc 0.921875
2017-03-02T17:41:04.503449: step 6776, loss 0.255676, acc 0.90625
2017-03-02T17:41:04.605888: step 6777, loss 0.123177, acc 0.921875
2017-03-02T17:41:04.687518: step 6778, loss 0.133212, acc 0.953125
2017-03-02T17:41:04.757484: step 6779, loss 0.0982906, acc 0.953125
2017-03-02T17:41:04.833317: step 6780, loss 0.167248, acc 0.9375
2017-03-02T17:41:04.900853: step 6781, loss 0.274243, acc 0.875
2017-03-02T17:41:04.973404: step 6782, loss 0.115894, acc 0.9375
2017-03-02T17:41:05.047897: step 6783, loss 0.145273, acc 0.953125
2017-03-02T17:41:05.125328: step 6784, loss 0.198253, acc 0.90625
2017-03-02T17:41:05.200667: step 6785, loss 0.199371, acc 0.9375
2017-03-02T17:41:05.275619: step 6786, loss 0.349606, acc 0.84375
2017-03-02T17:41:05.347967: step 6787, loss 0.11374, acc 0.953125
2017-03-02T17:41:05.421962: step 6788, loss 0.219526, acc 0.9375
2017-03-02T17:41:05.501394: step 6789, loss 0.122627, acc 0.953125
2017-03-02T17:41:05.572956: step 6790, loss 0.313336, acc 0.890625
2017-03-02T17:41:05.643651: step 6791, loss 0.0990391, acc 0.96875
2017-03-02T17:41:05.714648: step 6792, loss 0.3186, acc 0.90625
2017-03-02T17:41:05.784982: step 6793, loss 0.251807, acc 0.890625
2017-03-02T17:41:05.856892: step 6794, loss 0.213844, acc 0.953125
2017-03-02T17:41:05.930432: step 6795, loss 0.159822, acc 0.953125
2017-03-02T17:41:06.002087: step 6796, loss 0.221908, acc 0.890625
2017-03-02T17:41:06.083796: step 6797, loss 0.266922, acc 0.875
2017-03-02T17:41:06.159064: step 6798, loss 0.268002, acc 0.9375
2017-03-02T17:41:06.233508: step 6799, loss 0.112545, acc 0.953125
2017-03-02T17:41:06.301733: step 6800, loss 0.258223, acc 0.921875

Evaluation:
2017-03-02T17:41:06.337464: step 6800, loss 1.22715, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6800

2017-03-02T17:41:06.789381: step 6801, loss 0.193009, acc 0.921875
2017-03-02T17:41:06.859494: step 6802, loss 0.107435, acc 0.953125
2017-03-02T17:41:06.935337: step 6803, loss 0.188714, acc 0.9375
2017-03-02T17:41:07.006326: step 6804, loss 0.178482, acc 0.921875
2017-03-02T17:41:07.078986: step 6805, loss 0.245293, acc 0.890625
2017-03-02T17:41:07.154908: step 6806, loss 0.170579, acc 0.953125
2017-03-02T17:41:07.249798: step 6807, loss 0.196887, acc 0.921875
2017-03-02T17:41:07.326434: step 6808, loss 0.212785, acc 0.90625
2017-03-02T17:41:07.398748: step 6809, loss 0.208392, acc 0.921875
2017-03-02T17:41:07.474293: step 6810, loss 0.113614, acc 0.953125
2017-03-02T17:41:07.549673: step 6811, loss 0.228035, acc 0.9375
2017-03-02T17:41:07.628917: step 6812, loss 0.243185, acc 0.921875
2017-03-02T17:41:07.696420: step 6813, loss 0.0734636, acc 0.96875
2017-03-02T17:41:07.766266: step 6814, loss 0.157999, acc 0.9375
2017-03-02T17:41:07.842070: step 6815, loss 0.235685, acc 0.90625
2017-03-02T17:41:07.917675: step 6816, loss 0.0830686, acc 0.984375
2017-03-02T17:41:07.996820: step 6817, loss 0.142972, acc 0.921875
2017-03-02T17:41:08.073101: step 6818, loss 0.201404, acc 0.890625
2017-03-02T17:41:08.150231: step 6819, loss 0.214713, acc 0.90625
2017-03-02T17:41:08.225353: step 6820, loss 0.243171, acc 0.890625
2017-03-02T17:41:08.295890: step 6821, loss 0.210608, acc 0.890625
2017-03-02T17:41:08.367601: step 6822, loss 0.106863, acc 0.953125
2017-03-02T17:41:08.439142: step 6823, loss 0.19655, acc 0.9375
2017-03-02T17:41:08.512586: step 6824, loss 0.211069, acc 0.9375
2017-03-02T17:41:08.577827: step 6825, loss 0.258866, acc 0.859375
2017-03-02T17:41:08.649723: step 6826, loss 0.230232, acc 0.90625
2017-03-02T17:41:08.722003: step 6827, loss 0.279138, acc 0.90625
2017-03-02T17:41:08.789001: step 6828, loss 0.157469, acc 0.9375
2017-03-02T17:41:08.861005: step 6829, loss 0.320825, acc 0.859375
2017-03-02T17:41:08.940443: step 6830, loss 0.111646, acc 0.96875
2017-03-02T17:41:09.013154: step 6831, loss 0.127779, acc 0.9375
2017-03-02T17:41:09.075898: step 6832, loss 0.219456, acc 0.90625
2017-03-02T17:41:09.147625: step 6833, loss 0.19892, acc 0.9375
2017-03-02T17:41:09.220167: step 6834, loss 0.16534, acc 0.953125
2017-03-02T17:41:09.294370: step 6835, loss 0.19498, acc 0.953125
2017-03-02T17:41:09.364822: step 6836, loss 0.170098, acc 0.953125
2017-03-02T17:41:09.439059: step 6837, loss 0.248695, acc 0.90625
2017-03-02T17:41:09.508593: step 6838, loss 0.331792, acc 0.859375
2017-03-02T17:41:09.578727: step 6839, loss 0.131577, acc 0.953125
2017-03-02T17:41:09.649547: step 6840, loss 0.230712, acc 0.921875
2017-03-02T17:41:09.724832: step 6841, loss 0.17007, acc 0.9375
2017-03-02T17:41:09.796189: step 6842, loss 0.149368, acc 0.953125
2017-03-02T17:41:09.872516: step 6843, loss 0.111999, acc 0.953125
2017-03-02T17:41:09.953533: step 6844, loss 0.286092, acc 0.890625
2017-03-02T17:41:10.033055: step 6845, loss 0.176893, acc 0.921875
2017-03-02T17:41:10.107482: step 6846, loss 0.34398, acc 0.90625
2017-03-02T17:41:10.197709: step 6847, loss 0.201043, acc 0.921875
2017-03-02T17:41:10.268121: step 6848, loss 0.181617, acc 0.9375
2017-03-02T17:41:10.337003: step 6849, loss 0.117024, acc 0.953125
2017-03-02T17:41:10.407778: step 6850, loss 0.409894, acc 0.90625
2017-03-02T17:41:10.477060: step 6851, loss 0.299996, acc 0.859375
2017-03-02T17:41:10.546029: step 6852, loss 0.12783, acc 0.96875
2017-03-02T17:41:10.633484: step 6853, loss 0.165832, acc 0.921875
2017-03-02T17:41:10.704964: step 6854, loss 0.296882, acc 0.890625
2017-03-02T17:41:10.779517: step 6855, loss 0.0804256, acc 0.96875
2017-03-02T17:41:10.852799: step 6856, loss 0.272936, acc 0.90625
2017-03-02T17:41:10.923469: step 6857, loss 0.313236, acc 0.90625
2017-03-02T17:41:10.997274: step 6858, loss 0.0984547, acc 0.96875
2017-03-02T17:41:11.075784: step 6859, loss 0.161713, acc 0.953125
2017-03-02T17:41:11.139905: step 6860, loss 0.110239, acc 1
2017-03-02T17:41:11.217014: step 6861, loss 0.164834, acc 0.9375
2017-03-02T17:41:11.283844: step 6862, loss 0.0898335, acc 0.984375
2017-03-02T17:41:11.359428: step 6863, loss 0.241655, acc 0.921875
2017-03-02T17:41:11.430843: step 6864, loss 0.0763174, acc 0.984375
2017-03-02T17:41:11.501857: step 6865, loss 0.189732, acc 0.921875
2017-03-02T17:41:11.592342: step 6866, loss 0.1085, acc 0.96875
2017-03-02T17:41:11.660226: step 6867, loss 0.208942, acc 0.921875
2017-03-02T17:41:11.729741: step 6868, loss 0.14248, acc 0.953125
2017-03-02T17:41:11.807863: step 6869, loss 0.145458, acc 0.9375
2017-03-02T17:41:11.881721: step 6870, loss 0.11856, acc 0.984375
2017-03-02T17:41:11.965384: step 6871, loss 0.278031, acc 0.90625
2017-03-02T17:41:12.038587: step 6872, loss 0.167689, acc 0.90625
2017-03-02T17:41:12.114685: step 6873, loss 0.112572, acc 0.953125
2017-03-02T17:41:12.184309: step 6874, loss 0.117092, acc 0.9375
2017-03-02T17:41:12.268634: step 6875, loss 0.135188, acc 0.9375
2017-03-02T17:41:12.351973: step 6876, loss 0.122662, acc 0.9375
2017-03-02T17:41:12.425546: step 6877, loss 0.182765, acc 0.921875
2017-03-02T17:41:12.501566: step 6878, loss 0.0952918, acc 0.96875
2017-03-02T17:41:12.573932: step 6879, loss 0.186219, acc 0.96875
2017-03-02T17:41:12.639567: step 6880, loss 0.142991, acc 0.921875
2017-03-02T17:41:12.722250: step 6881, loss 0.0512258, acc 0.984375
2017-03-02T17:41:12.801773: step 6882, loss 0.111438, acc 0.96875
2017-03-02T17:41:12.873915: step 6883, loss 0.13865, acc 0.96875
2017-03-02T17:41:12.944070: step 6884, loss 0.15171, acc 0.953125
2017-03-02T17:41:13.016967: step 6885, loss 0.188216, acc 0.9375
2017-03-02T17:41:13.078322: step 6886, loss 0.153748, acc 0.921875
2017-03-02T17:41:13.154340: step 6887, loss 0.34414, acc 0.90625
2017-03-02T17:41:13.227056: step 6888, loss 0.0914149, acc 0.96875
2017-03-02T17:41:13.298031: step 6889, loss 0.0966603, acc 0.96875
2017-03-02T17:41:13.372245: step 6890, loss 0.122818, acc 0.953125
2017-03-02T17:41:13.449417: step 6891, loss 0.229904, acc 0.90625
2017-03-02T17:41:13.520719: step 6892, loss 0.191052, acc 0.9375
2017-03-02T17:41:13.590532: step 6893, loss 0.267004, acc 0.90625
2017-03-02T17:41:13.671429: step 6894, loss 0.118162, acc 0.9375
2017-03-02T17:41:13.741936: step 6895, loss 0.269498, acc 0.921875
2017-03-02T17:41:13.816604: step 6896, loss 0.093059, acc 0.96875
2017-03-02T17:41:13.888855: step 6897, loss 0.159812, acc 0.90625
2017-03-02T17:41:13.966476: step 6898, loss 0.145427, acc 0.921875
2017-03-02T17:41:14.038161: step 6899, loss 0.188985, acc 0.921875
2017-03-02T17:41:14.111208: step 6900, loss 0.157893, acc 0.921875

Evaluation:
2017-03-02T17:41:14.150171: step 6900, loss 1.26554, acc 0.665465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-6900

2017-03-02T17:41:14.622430: step 6901, loss 0.201602, acc 0.90625
2017-03-02T17:41:14.694190: step 6902, loss 0.247245, acc 0.890625
2017-03-02T17:41:14.775596: step 6903, loss 0.097382, acc 0.96875
2017-03-02T17:41:14.848159: step 6904, loss 0.0733407, acc 0.96875
2017-03-02T17:41:14.922505: step 6905, loss 0.177948, acc 0.96875
2017-03-02T17:41:14.995893: step 6906, loss 0.186245, acc 0.953125
2017-03-02T17:41:15.067862: step 6907, loss 0.190935, acc 0.90625
2017-03-02T17:41:15.148979: step 6908, loss 0.203187, acc 0.90625
2017-03-02T17:41:15.228938: step 6909, loss 0.11646, acc 0.96875
2017-03-02T17:41:15.295770: step 6910, loss 0.217615, acc 0.921875
2017-03-02T17:41:15.360382: step 6911, loss 0.181758, acc 0.90625
2017-03-02T17:41:15.434202: step 6912, loss 0.303228, acc 0.90625
2017-03-02T17:41:15.517701: step 6913, loss 0.118718, acc 0.9375
2017-03-02T17:41:15.593193: step 6914, loss 0.156072, acc 0.9375
2017-03-02T17:41:15.673824: step 6915, loss 0.113319, acc 0.96875
2017-03-02T17:41:15.747003: step 6916, loss 0.146987, acc 0.9375
2017-03-02T17:41:15.814018: step 6917, loss 0.167424, acc 0.9375
2017-03-02T17:41:15.885852: step 6918, loss 0.302029, acc 0.859375
2017-03-02T17:41:15.953716: step 6919, loss 0.0800451, acc 0.96875
2017-03-02T17:41:16.023513: step 6920, loss 0.130746, acc 0.953125
2017-03-02T17:41:16.098220: step 6921, loss 0.276769, acc 0.859375
2017-03-02T17:41:16.177595: step 6922, loss 0.131952, acc 0.9375
2017-03-02T17:41:16.249549: step 6923, loss 0.127158, acc 0.953125
2017-03-02T17:41:16.335432: step 6924, loss 0.179203, acc 0.9375
2017-03-02T17:41:16.407988: step 6925, loss 0.167003, acc 0.953125
2017-03-02T17:41:16.484249: step 6926, loss 0.163844, acc 0.9375
2017-03-02T17:41:16.554727: step 6927, loss 0.177891, acc 0.9375
2017-03-02T17:41:16.627407: step 6928, loss 0.318495, acc 0.90625
2017-03-02T17:41:16.697337: step 6929, loss 0.165267, acc 0.921875
2017-03-02T17:41:16.769015: step 6930, loss 0.225488, acc 0.890625
2017-03-02T17:41:16.848716: step 6931, loss 0.147273, acc 0.9375
2017-03-02T17:41:16.922665: step 6932, loss 0.14381, acc 0.953125
2017-03-02T17:41:16.997306: step 6933, loss 0.107558, acc 0.953125
2017-03-02T17:41:17.074974: step 6934, loss 0.238581, acc 0.890625
2017-03-02T17:41:17.152287: step 6935, loss 0.171501, acc 0.953125
2017-03-02T17:41:17.225746: step 6936, loss 0.220269, acc 0.90625
2017-03-02T17:41:17.299557: step 6937, loss 0.158533, acc 0.9375
2017-03-02T17:41:17.364134: step 6938, loss 0.199024, acc 0.890625
2017-03-02T17:41:17.431003: step 6939, loss 0.134117, acc 0.9375
2017-03-02T17:41:17.506738: step 6940, loss 0.192482, acc 0.921875
2017-03-02T17:41:17.575070: step 6941, loss 0.186146, acc 0.90625
2017-03-02T17:41:17.647541: step 6942, loss 0.179764, acc 0.9375
2017-03-02T17:41:17.717800: step 6943, loss 0.164852, acc 0.9375
2017-03-02T17:41:17.792452: step 6944, loss 0.0834504, acc 0.96875
2017-03-02T17:41:17.864396: step 6945, loss 0.166938, acc 0.9375
2017-03-02T17:41:17.934251: step 6946, loss 0.253868, acc 0.90625
2017-03-02T17:41:18.006179: step 6947, loss 0.242177, acc 0.890625
2017-03-02T17:41:18.079323: step 6948, loss 0.172116, acc 0.96875
2017-03-02T17:41:18.147434: step 6949, loss 0.125097, acc 0.953125
2017-03-02T17:41:18.233768: step 6950, loss 0.159087, acc 0.9375
2017-03-02T17:41:18.304033: step 6951, loss 0.0841867, acc 0.96875
2017-03-02T17:41:18.379090: step 6952, loss 0.281781, acc 0.921875
2017-03-02T17:41:18.469779: step 6953, loss 0.272513, acc 0.9375
2017-03-02T17:41:18.543245: step 6954, loss 0.211669, acc 0.90625
2017-03-02T17:41:18.619318: step 6955, loss 0.155215, acc 0.90625
2017-03-02T17:41:18.691403: step 6956, loss 0.133853, acc 0.9375
2017-03-02T17:41:18.761024: step 6957, loss 0.253614, acc 0.90625
2017-03-02T17:41:18.832279: step 6958, loss 0.266891, acc 0.875
2017-03-02T17:41:18.904716: step 6959, loss 0.0285828, acc 1
2017-03-02T17:41:18.976404: step 6960, loss 0.0830399, acc 0.96875
2017-03-02T17:41:19.047070: step 6961, loss 0.273321, acc 0.859375
2017-03-02T17:41:19.120598: step 6962, loss 0.168778, acc 0.921875
2017-03-02T17:41:19.206744: step 6963, loss 0.140074, acc 0.9375
2017-03-02T17:41:19.278018: step 6964, loss 0.22954, acc 0.921875
2017-03-02T17:41:19.351449: step 6965, loss 0.15469, acc 0.90625
2017-03-02T17:41:19.420468: step 6966, loss 0.256172, acc 0.890625
2017-03-02T17:41:19.492406: step 6967, loss 0.270772, acc 0.875
2017-03-02T17:41:19.566329: step 6968, loss 0.24681, acc 0.90625
2017-03-02T17:41:19.634534: step 6969, loss 0.131656, acc 0.953125
2017-03-02T17:41:19.697877: step 6970, loss 0.420213, acc 0.8125
2017-03-02T17:41:19.783647: step 6971, loss 0.167302, acc 0.9375
2017-03-02T17:41:19.855198: step 6972, loss 0.343605, acc 0.90625
2017-03-02T17:41:19.922294: step 6973, loss 0.250428, acc 0.921875
2017-03-02T17:41:19.999568: step 6974, loss 0.23647, acc 0.90625
2017-03-02T17:41:20.072618: step 6975, loss 0.243051, acc 0.875
2017-03-02T17:41:20.141556: step 6976, loss 0.140091, acc 0.921875
2017-03-02T17:41:20.215599: step 6977, loss 0.159214, acc 0.90625
2017-03-02T17:41:20.286489: step 6978, loss 0.19597, acc 0.890625
2017-03-02T17:41:20.358596: step 6979, loss 0.172277, acc 0.890625
2017-03-02T17:41:20.429256: step 6980, loss 0.293018, acc 0.859375
2017-03-02T17:41:20.495731: step 6981, loss 0.221651, acc 0.890625
2017-03-02T17:41:20.571224: step 6982, loss 0.105753, acc 0.96875
2017-03-02T17:41:20.646556: step 6983, loss 0.343493, acc 0.90625
2017-03-02T17:41:20.720253: step 6984, loss 0.21528, acc 0.90625
2017-03-02T17:41:20.794670: step 6985, loss 0.128075, acc 0.9375
2017-03-02T17:41:20.863164: step 6986, loss 0.333442, acc 0.875
2017-03-02T17:41:20.938236: step 6987, loss 0.23789, acc 0.890625
2017-03-02T17:41:21.009521: step 6988, loss 0.193658, acc 0.90625
2017-03-02T17:41:21.081057: step 6989, loss 0.267571, acc 0.890625
2017-03-02T17:41:21.160836: step 6990, loss 0.329003, acc 0.875
2017-03-02T17:41:21.237213: step 6991, loss 0.336642, acc 0.875
2017-03-02T17:41:21.308145: step 6992, loss 0.149546, acc 0.953125
2017-03-02T17:41:21.380405: step 6993, loss 0.167062, acc 0.9375
2017-03-02T17:41:21.456813: step 6994, loss 0.233344, acc 0.90625
2017-03-02T17:41:21.525351: step 6995, loss 0.150758, acc 0.9375
2017-03-02T17:41:21.592709: step 6996, loss 0.475542, acc 0.828125
2017-03-02T17:41:21.663981: step 6997, loss 0.203628, acc 0.921875
2017-03-02T17:41:21.745767: step 6998, loss 0.109047, acc 0.953125
2017-03-02T17:41:21.817717: step 6999, loss 0.132382, acc 0.953125
2017-03-02T17:41:21.887578: step 7000, loss 0.294951, acc 0.890625

Evaluation:
2017-03-02T17:41:21.920939: step 7000, loss 1.25914, acc 0.667628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7000

2017-03-02T17:41:22.456087: step 7001, loss 0.0882981, acc 0.96875
2017-03-02T17:41:22.529086: step 7002, loss 0.187034, acc 0.90625
2017-03-02T17:41:22.601176: step 7003, loss 0.12594, acc 0.953125
2017-03-02T17:41:22.676076: step 7004, loss 0.106578, acc 0.953125
2017-03-02T17:41:22.737266: step 7005, loss 0.270302, acc 0.890625
2017-03-02T17:41:22.807914: step 7006, loss 0.177212, acc 0.921875
2017-03-02T17:41:22.876854: step 7007, loss 0.194907, acc 0.90625
2017-03-02T17:41:22.950483: step 7008, loss 0.224354, acc 0.890625
2017-03-02T17:41:23.021617: step 7009, loss 0.309598, acc 0.875
2017-03-02T17:41:23.092775: step 7010, loss 0.226758, acc 0.90625
2017-03-02T17:41:23.165283: step 7011, loss 0.169879, acc 0.9375
2017-03-02T17:41:23.235491: step 7012, loss 0.12186, acc 0.9375
2017-03-02T17:41:23.308663: step 7013, loss 0.184346, acc 0.921875
2017-03-02T17:41:23.380429: step 7014, loss 0.278583, acc 0.90625
2017-03-02T17:41:23.450825: step 7015, loss 0.206798, acc 0.921875
2017-03-02T17:41:23.521066: step 7016, loss 0.315163, acc 0.921875
2017-03-02T17:41:23.589460: step 7017, loss 0.318802, acc 0.90625
2017-03-02T17:41:23.660442: step 7018, loss 0.178346, acc 0.90625
2017-03-02T17:41:23.733115: step 7019, loss 0.316189, acc 0.859375
2017-03-02T17:41:23.809515: step 7020, loss 0.0942063, acc 0.984375
2017-03-02T17:41:23.884870: step 7021, loss 0.172493, acc 0.921875
2017-03-02T17:41:23.983649: step 7022, loss 0.209315, acc 0.9375
2017-03-02T17:41:24.059310: step 7023, loss 0.155523, acc 0.9375
2017-03-02T17:41:24.133475: step 7024, loss 0.132642, acc 0.96875
2017-03-02T17:41:24.207210: step 7025, loss 0.240101, acc 0.953125
2017-03-02T17:41:24.277063: step 7026, loss 0.376339, acc 0.84375
2017-03-02T17:41:24.350930: step 7027, loss 0.10014, acc 0.953125
2017-03-02T17:41:24.425612: step 7028, loss 0.185296, acc 0.9375
2017-03-02T17:41:24.499198: step 7029, loss 0.27588, acc 0.921875
2017-03-02T17:41:24.572425: step 7030, loss 0.169095, acc 0.921875
2017-03-02T17:41:24.644441: step 7031, loss 0.138658, acc 0.953125
2017-03-02T17:41:24.717940: step 7032, loss 0.15267, acc 0.9375
2017-03-02T17:41:24.790185: step 7033, loss 0.233787, acc 0.921875
2017-03-02T17:41:24.866951: step 7034, loss 0.149365, acc 0.9375
2017-03-02T17:41:24.949252: step 7035, loss 0.239444, acc 0.890625
2017-03-02T17:41:25.024465: step 7036, loss 0.252009, acc 0.921875
2017-03-02T17:41:25.097140: step 7037, loss 0.206998, acc 0.9375
2017-03-02T17:41:25.171257: step 7038, loss 0.073978, acc 0.96875
2017-03-02T17:41:25.242854: step 7039, loss 0.233522, acc 0.90625
2017-03-02T17:41:25.315487: step 7040, loss 0.417071, acc 0.859375
2017-03-02T17:41:25.407525: step 7041, loss 0.169832, acc 0.953125
2017-03-02T17:41:25.489898: step 7042, loss 0.392879, acc 0.84375
2017-03-02T17:41:25.563245: step 7043, loss 0.181891, acc 0.921875
2017-03-02T17:41:25.631773: step 7044, loss 0.0937411, acc 0.953125
2017-03-02T17:41:25.698470: step 7045, loss 0.243561, acc 0.890625
2017-03-02T17:41:25.770938: step 7046, loss 0.198739, acc 0.921875
2017-03-02T17:41:25.837155: step 7047, loss 0.199039, acc 0.921875
2017-03-02T17:41:25.912747: step 7048, loss 0.234434, acc 0.921875
2017-03-02T17:41:26.014692: step 7049, loss 0.213132, acc 0.921875
2017-03-02T17:41:26.083892: step 7050, loss 0.28421, acc 0.875
2017-03-02T17:41:26.169197: step 7051, loss 0.323394, acc 0.875
2017-03-02T17:41:26.250078: step 7052, loss 0.176792, acc 0.9375
2017-03-02T17:41:26.327790: step 7053, loss 0.344297, acc 0.90625
2017-03-02T17:41:26.401818: step 7054, loss 0.139743, acc 0.921875
2017-03-02T17:41:26.476666: step 7055, loss 0.246876, acc 0.875
2017-03-02T17:41:26.544154: step 7056, loss 0.0906092, acc 1
2017-03-02T17:41:26.627705: step 7057, loss 0.175669, acc 0.9375
2017-03-02T17:41:26.694488: step 7058, loss 0.22007, acc 0.875
2017-03-02T17:41:26.766942: step 7059, loss 0.217786, acc 0.921875
2017-03-02T17:41:26.836654: step 7060, loss 0.0956586, acc 0.984375
2017-03-02T17:41:26.909793: step 7061, loss 0.19552, acc 0.90625
2017-03-02T17:41:26.980268: step 7062, loss 0.122637, acc 0.9375
2017-03-02T17:41:27.057791: step 7063, loss 0.159018, acc 0.96875
2017-03-02T17:41:27.133620: step 7064, loss 0.0875966, acc 0.984375
2017-03-02T17:41:27.207214: step 7065, loss 0.122792, acc 0.9375
2017-03-02T17:41:27.291924: step 7066, loss 0.207803, acc 0.9375
2017-03-02T17:41:27.365630: step 7067, loss 0.154046, acc 0.953125
2017-03-02T17:41:27.445745: step 7068, loss 0.321615, acc 0.90625
2017-03-02T17:41:27.521671: step 7069, loss 0.182557, acc 0.90625
2017-03-02T17:41:27.604704: step 7070, loss 0.258528, acc 0.890625
2017-03-02T17:41:27.675320: step 7071, loss 0.0956026, acc 0.96875
2017-03-02T17:41:27.750227: step 7072, loss 0.217055, acc 0.921875
2017-03-02T17:41:27.824525: step 7073, loss 0.134466, acc 0.953125
2017-03-02T17:41:27.895906: step 7074, loss 0.105187, acc 0.953125
2017-03-02T17:41:27.965510: step 7075, loss 0.116613, acc 0.9375
2017-03-02T17:41:28.036411: step 7076, loss 0.0993389, acc 0.953125
2017-03-02T17:41:28.110698: step 7077, loss 0.196597, acc 0.90625
2017-03-02T17:41:28.180010: step 7078, loss 0.101927, acc 0.96875
2017-03-02T17:41:28.253860: step 7079, loss 0.29972, acc 0.90625
2017-03-02T17:41:28.325663: step 7080, loss 0.118605, acc 0.953125
2017-03-02T17:41:28.391026: step 7081, loss 0.109597, acc 0.953125
2017-03-02T17:41:28.457646: step 7082, loss 0.0655434, acc 0.96875
2017-03-02T17:41:28.530142: step 7083, loss 0.130973, acc 0.953125
2017-03-02T17:41:28.600442: step 7084, loss 0.243822, acc 0.921875
2017-03-02T17:41:28.669190: step 7085, loss 0.0939735, acc 0.953125
2017-03-02T17:41:28.743976: step 7086, loss 0.191173, acc 0.953125
2017-03-02T17:41:28.818334: step 7087, loss 0.0987575, acc 0.96875
2017-03-02T17:41:28.895005: step 7088, loss 0.154102, acc 0.90625
2017-03-02T17:41:28.974977: step 7089, loss 0.121762, acc 0.984375
2017-03-02T17:41:29.042416: step 7090, loss 0.258693, acc 0.90625
2017-03-02T17:41:29.113691: step 7091, loss 0.188183, acc 0.90625
2017-03-02T17:41:29.188448: step 7092, loss 0.242506, acc 0.890625
2017-03-02T17:41:29.260815: step 7093, loss 0.166945, acc 0.953125
2017-03-02T17:41:29.338972: step 7094, loss 0.102, acc 0.953125
2017-03-02T17:41:29.414361: step 7095, loss 0.293501, acc 0.875
2017-03-02T17:41:29.489341: step 7096, loss 0.154976, acc 0.90625
2017-03-02T17:41:29.561178: step 7097, loss 0.185516, acc 0.9375
2017-03-02T17:41:29.634103: step 7098, loss 0.0426069, acc 0.984375
2017-03-02T17:41:29.710546: step 7099, loss 0.230522, acc 0.90625
2017-03-02T17:41:29.780020: step 7100, loss 0.0843133, acc 0.96875

Evaluation:
2017-03-02T17:41:29.815046: step 7100, loss 1.27346, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7100

2017-03-02T17:41:30.292523: step 7101, loss 0.21301, acc 0.90625
2017-03-02T17:41:30.374444: step 7102, loss 0.297011, acc 0.859375
2017-03-02T17:41:30.441970: step 7103, loss 0.284257, acc 0.859375
2017-03-02T17:41:30.513646: step 7104, loss 0.365163, acc 0.875
2017-03-02T17:41:30.583864: step 7105, loss 0.363853, acc 0.828125
2017-03-02T17:41:30.660652: step 7106, loss 0.128408, acc 0.953125
2017-03-02T17:41:30.738415: step 7107, loss 0.224239, acc 0.890625
2017-03-02T17:41:30.810602: step 7108, loss 0.0641888, acc 0.984375
2017-03-02T17:41:30.891017: step 7109, loss 0.272839, acc 0.859375
2017-03-02T17:41:30.970625: step 7110, loss 0.14382, acc 0.9375
2017-03-02T17:41:31.060631: step 7111, loss 0.215659, acc 0.890625
2017-03-02T17:41:31.130027: step 7112, loss 0.461554, acc 0.828125
2017-03-02T17:41:31.202163: step 7113, loss 0.098084, acc 0.9375
2017-03-02T17:41:31.284162: step 7114, loss 0.271544, acc 0.875
2017-03-02T17:41:31.355783: step 7115, loss 0.13764, acc 0.9375
2017-03-02T17:41:31.433037: step 7116, loss 0.259885, acc 0.859375
2017-03-02T17:41:31.521155: step 7117, loss 0.0747264, acc 0.984375
2017-03-02T17:41:31.591380: step 7118, loss 0.0885117, acc 0.953125
2017-03-02T17:41:31.669811: step 7119, loss 0.178822, acc 0.921875
2017-03-02T17:41:31.740622: step 7120, loss 0.162415, acc 0.9375
2017-03-02T17:41:31.808543: step 7121, loss 0.126018, acc 0.953125
2017-03-02T17:41:31.883576: step 7122, loss 0.20055, acc 0.921875
2017-03-02T17:41:31.954031: step 7123, loss 0.066332, acc 1
2017-03-02T17:41:32.023584: step 7124, loss 0.101738, acc 0.96875
2017-03-02T17:41:32.109677: step 7125, loss 0.131488, acc 0.984375
2017-03-02T17:41:32.180517: step 7126, loss 0.171398, acc 0.953125
2017-03-02T17:41:32.250470: step 7127, loss 0.17831, acc 0.921875
2017-03-02T17:41:32.363533: step 7128, loss 0.104793, acc 0.96875
2017-03-02T17:41:32.442889: step 7129, loss 0.101138, acc 0.96875
2017-03-02T17:41:32.524665: step 7130, loss 0.438127, acc 0.84375
2017-03-02T17:41:32.597984: step 7131, loss 0.176031, acc 0.921875
2017-03-02T17:41:32.676694: step 7132, loss 0.228182, acc 0.921875
2017-03-02T17:41:32.751779: step 7133, loss 0.156537, acc 0.9375
2017-03-02T17:41:32.824685: step 7134, loss 0.235937, acc 0.921875
2017-03-02T17:41:32.893410: step 7135, loss 0.260138, acc 0.875
2017-03-02T17:41:32.966163: step 7136, loss 0.393598, acc 0.859375
2017-03-02T17:41:33.040603: step 7137, loss 0.187396, acc 0.90625
2017-03-02T17:41:33.114079: step 7138, loss 0.112088, acc 0.9375
2017-03-02T17:41:33.178901: step 7139, loss 0.137579, acc 0.9375
2017-03-02T17:41:33.249711: step 7140, loss 0.0979682, acc 0.96875
2017-03-02T17:41:33.337050: step 7141, loss 0.15509, acc 0.953125
2017-03-02T17:41:33.411311: step 7142, loss 0.218996, acc 0.921875
2017-03-02T17:41:33.491086: step 7143, loss 0.15498, acc 0.9375
2017-03-02T17:41:33.566553: step 7144, loss 0.287666, acc 0.90625
2017-03-02T17:41:33.639591: step 7145, loss 0.306943, acc 0.90625
2017-03-02T17:41:33.712390: step 7146, loss 0.260122, acc 0.921875
2017-03-02T17:41:33.785102: step 7147, loss 0.265089, acc 0.90625
2017-03-02T17:41:33.856282: step 7148, loss 0.149509, acc 0.953125
2017-03-02T17:41:33.925484: step 7149, loss 0.0994739, acc 0.96875
2017-03-02T17:41:33.994961: step 7150, loss 0.205862, acc 0.921875
2017-03-02T17:41:34.070456: step 7151, loss 0.081019, acc 0.953125
2017-03-02T17:41:34.145813: step 7152, loss 0.206436, acc 0.890625
2017-03-02T17:41:34.218098: step 7153, loss 0.201638, acc 0.9375
2017-03-02T17:41:34.288362: step 7154, loss 0.070346, acc 0.984375
2017-03-02T17:41:34.367772: step 7155, loss 0.282515, acc 0.875
2017-03-02T17:41:34.442746: step 7156, loss 0.0958663, acc 0.96875
2017-03-02T17:41:34.508441: step 7157, loss 0.112593, acc 0.96875
2017-03-02T17:41:34.576545: step 7158, loss 0.162903, acc 0.9375
2017-03-02T17:41:34.656417: step 7159, loss 0.462286, acc 0.890625
2017-03-02T17:41:34.727865: step 7160, loss 0.0573861, acc 1
2017-03-02T17:41:34.803287: step 7161, loss 0.322464, acc 0.921875
2017-03-02T17:41:34.874572: step 7162, loss 0.249714, acc 0.90625
2017-03-02T17:41:34.952042: step 7163, loss 0.153259, acc 0.953125
2017-03-02T17:41:35.022366: step 7164, loss 0.21781, acc 0.875
2017-03-02T17:41:35.093702: step 7165, loss 0.0379339, acc 1
2017-03-02T17:41:35.158434: step 7166, loss 0.303333, acc 0.859375
2017-03-02T17:41:35.230592: step 7167, loss 0.191332, acc 0.9375
2017-03-02T17:41:35.298128: step 7168, loss 0.0921146, acc 0.984375
2017-03-02T17:41:35.363330: step 7169, loss 0.136669, acc 0.953125
2017-03-02T17:41:35.437456: step 7170, loss 0.329605, acc 0.84375
2017-03-02T17:41:35.510523: step 7171, loss 0.106123, acc 0.953125
2017-03-02T17:41:35.580290: step 7172, loss 0.274835, acc 0.921875
2017-03-02T17:41:35.651064: step 7173, loss 0.169436, acc 0.921875
2017-03-02T17:41:35.718098: step 7174, loss 0.18244, acc 0.9375
2017-03-02T17:41:35.788033: step 7175, loss 0.186109, acc 0.921875
2017-03-02T17:41:35.872935: step 7176, loss 0.176997, acc 0.9375
2017-03-02T17:41:35.941663: step 7177, loss 0.0715188, acc 0.984375
2017-03-02T17:41:36.014314: step 7178, loss 0.24311, acc 0.90625
2017-03-02T17:41:36.080208: step 7179, loss 0.329934, acc 0.859375
2017-03-02T17:41:36.149793: step 7180, loss 0.232224, acc 0.875
2017-03-02T17:41:36.222432: step 7181, loss 0.133991, acc 0.96875
2017-03-02T17:41:36.296198: step 7182, loss 0.234238, acc 0.9375
2017-03-02T17:41:36.369384: step 7183, loss 0.190771, acc 0.9375
2017-03-02T17:41:36.436394: step 7184, loss 0.140966, acc 0.9375
2017-03-02T17:41:36.515053: step 7185, loss 0.312702, acc 0.875
2017-03-02T17:41:36.583446: step 7186, loss 0.172443, acc 0.9375
2017-03-02T17:41:36.652373: step 7187, loss 0.101186, acc 0.953125
2017-03-02T17:41:36.718507: step 7188, loss 0.171317, acc 0.953125
2017-03-02T17:41:36.791442: step 7189, loss 0.194119, acc 0.921875
2017-03-02T17:41:36.867934: step 7190, loss 0.204352, acc 0.953125
2017-03-02T17:41:36.942035: step 7191, loss 0.293332, acc 0.890625
2017-03-02T17:41:37.047805: step 7192, loss 0.112249, acc 0.96875
2017-03-02T17:41:37.120631: step 7193, loss 0.210833, acc 0.9375
2017-03-02T17:41:37.195302: step 7194, loss 0.0867757, acc 0.953125
2017-03-02T17:41:37.267993: step 7195, loss 0.151094, acc 0.921875
2017-03-02T17:41:37.336490: step 7196, loss 0.166845, acc 0.921875
2017-03-02T17:41:37.408466: step 7197, loss 0.205761, acc 0.9375
2017-03-02T17:41:37.481515: step 7198, loss 0.252376, acc 0.875
2017-03-02T17:41:37.554212: step 7199, loss 0.159799, acc 0.9375
2017-03-02T17:41:37.624001: step 7200, loss 0.275325, acc 0.90625

Evaluation:
2017-03-02T17:41:37.659409: step 7200, loss 1.23731, acc 0.663302

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7200

2017-03-02T17:41:38.103118: step 7201, loss 0.16009, acc 0.921875
2017-03-02T17:41:38.177559: step 7202, loss 0.105272, acc 0.96875
2017-03-02T17:41:38.262474: step 7203, loss 0.176345, acc 0.9375
2017-03-02T17:41:38.336784: step 7204, loss 0.282912, acc 0.90625
2017-03-02T17:41:38.413565: step 7205, loss 0.192812, acc 0.921875
2017-03-02T17:41:38.486361: step 7206, loss 0.111855, acc 0.9375
2017-03-02T17:41:38.567927: step 7207, loss 0.348652, acc 0.84375
2017-03-02T17:41:38.642140: step 7208, loss 0.206201, acc 0.9375
2017-03-02T17:41:38.712335: step 7209, loss 0.248302, acc 0.875
2017-03-02T17:41:38.781118: step 7210, loss 0.371647, acc 0.828125
2017-03-02T17:41:38.855851: step 7211, loss 0.0952287, acc 0.953125
2017-03-02T17:41:38.930867: step 7212, loss 0.10638, acc 0.953125
2017-03-02T17:41:39.005767: step 7213, loss 0.146353, acc 0.9375
2017-03-02T17:41:39.077218: step 7214, loss 0.161943, acc 0.953125
2017-03-02T17:41:39.139739: step 7215, loss 0.198283, acc 0.90625
2017-03-02T17:41:39.215681: step 7216, loss 0.0746213, acc 0.96875
2017-03-02T17:41:39.296174: step 7217, loss 0.179554, acc 0.953125
2017-03-02T17:41:39.370606: step 7218, loss 0.451602, acc 0.859375
2017-03-02T17:41:39.438016: step 7219, loss 0.163111, acc 0.921875
2017-03-02T17:41:39.533400: step 7220, loss 0.12671, acc 0.953125
2017-03-02T17:41:39.606329: step 7221, loss 0.263457, acc 0.90625
2017-03-02T17:41:39.686495: step 7222, loss 0.256298, acc 0.890625
2017-03-02T17:41:39.764791: step 7223, loss 0.15575, acc 0.953125
2017-03-02T17:41:39.835079: step 7224, loss 0.12758, acc 0.953125
2017-03-02T17:41:39.912342: step 7225, loss 0.192177, acc 0.90625
2017-03-02T17:41:39.986472: step 7226, loss 0.187948, acc 0.9375
2017-03-02T17:41:40.058286: step 7227, loss 0.207569, acc 0.921875
2017-03-02T17:41:40.130274: step 7228, loss 0.391255, acc 0.859375
2017-03-02T17:41:40.202568: step 7229, loss 0.190525, acc 0.921875
2017-03-02T17:41:40.277123: step 7230, loss 0.174704, acc 0.9375
2017-03-02T17:41:40.361419: step 7231, loss 0.172277, acc 0.921875
2017-03-02T17:41:40.442357: step 7232, loss 0.253623, acc 0.921875
2017-03-02T17:41:40.515340: step 7233, loss 0.202113, acc 0.921875
2017-03-02T17:41:40.589001: step 7234, loss 0.342829, acc 0.875
2017-03-02T17:41:40.662094: step 7235, loss 0.200173, acc 0.921875
2017-03-02T17:41:40.726397: step 7236, loss 0.0934575, acc 0.96875
2017-03-02T17:41:40.792518: step 7237, loss 0.178384, acc 0.96875
2017-03-02T17:41:40.869271: step 7238, loss 0.249768, acc 0.921875
2017-03-02T17:41:40.944558: step 7239, loss 0.121424, acc 0.953125
2017-03-02T17:41:41.019286: step 7240, loss 0.166162, acc 0.953125
2017-03-02T17:41:41.099675: step 7241, loss 0.554091, acc 0.8125
2017-03-02T17:41:41.182072: step 7242, loss 0.232085, acc 0.9375
2017-03-02T17:41:41.263654: step 7243, loss 0.256251, acc 0.890625
2017-03-02T17:41:41.336918: step 7244, loss 0.218665, acc 0.9375
2017-03-02T17:41:41.411613: step 7245, loss 0.272161, acc 0.875
2017-03-02T17:41:41.479446: step 7246, loss 0.284506, acc 0.921875
2017-03-02T17:41:41.549352: step 7247, loss 0.296805, acc 0.875
2017-03-02T17:41:41.624074: step 7248, loss 0.115183, acc 0.984375
2017-03-02T17:41:41.708237: step 7249, loss 0.151861, acc 0.921875
2017-03-02T17:41:41.779272: step 7250, loss 0.216197, acc 0.90625
2017-03-02T17:41:41.851136: step 7251, loss 0.248754, acc 0.875
2017-03-02T17:41:41.920759: step 7252, loss 0.00587883, acc 1
2017-03-02T17:41:41.993982: step 7253, loss 0.128593, acc 0.921875
2017-03-02T17:41:42.069997: step 7254, loss 0.249598, acc 0.9375
2017-03-02T17:41:42.142424: step 7255, loss 0.122727, acc 0.953125
2017-03-02T17:41:42.209603: step 7256, loss 0.14253, acc 0.953125
2017-03-02T17:41:42.279545: step 7257, loss 0.20378, acc 0.890625
2017-03-02T17:41:42.357020: step 7258, loss 0.137532, acc 0.9375
2017-03-02T17:41:42.437215: step 7259, loss 0.127243, acc 0.953125
2017-03-02T17:41:42.509477: step 7260, loss 0.20483, acc 0.90625
2017-03-02T17:41:42.581479: step 7261, loss 0.249777, acc 0.859375
2017-03-02T17:41:42.652651: step 7262, loss 0.117941, acc 0.953125
2017-03-02T17:41:42.719585: step 7263, loss 0.215604, acc 0.90625
2017-03-02T17:41:42.793268: step 7264, loss 0.0881263, acc 0.96875
2017-03-02T17:41:42.860591: step 7265, loss 0.223878, acc 0.890625
2017-03-02T17:41:42.929443: step 7266, loss 0.235779, acc 0.890625
2017-03-02T17:41:43.001300: step 7267, loss 0.0762679, acc 0.984375
2017-03-02T17:41:43.083173: step 7268, loss 0.123764, acc 0.953125
2017-03-02T17:41:43.159296: step 7269, loss 0.235421, acc 0.890625
2017-03-02T17:41:43.231686: step 7270, loss 0.144713, acc 0.9375
2017-03-02T17:41:43.309625: step 7271, loss 0.156696, acc 0.9375
2017-03-02T17:41:43.383246: step 7272, loss 0.0638679, acc 0.984375
2017-03-02T17:41:43.459560: step 7273, loss 0.194041, acc 0.9375
2017-03-02T17:41:43.527077: step 7274, loss 0.170402, acc 0.9375
2017-03-02T17:41:43.597359: step 7275, loss 0.210057, acc 0.921875
2017-03-02T17:41:43.672782: step 7276, loss 0.210863, acc 0.921875
2017-03-02T17:41:43.754757: step 7277, loss 0.191055, acc 0.90625
2017-03-02T17:41:43.841292: step 7278, loss 0.0850651, acc 0.96875
2017-03-02T17:41:43.913777: step 7279, loss 0.226901, acc 0.9375
2017-03-02T17:41:43.986529: step 7280, loss 0.100784, acc 0.96875
2017-03-02T17:41:44.056832: step 7281, loss 0.161714, acc 0.9375
2017-03-02T17:41:44.129242: step 7282, loss 0.042762, acc 1
2017-03-02T17:41:44.202823: step 7283, loss 0.0496184, acc 0.984375
2017-03-02T17:41:44.265792: step 7284, loss 0.14827, acc 0.921875
2017-03-02T17:41:44.333660: step 7285, loss 0.249745, acc 0.890625
2017-03-02T17:41:44.409102: step 7286, loss 0.0234742, acc 1
2017-03-02T17:41:44.484731: step 7287, loss 0.25692, acc 0.890625
2017-03-02T17:41:44.559580: step 7288, loss 0.120206, acc 0.953125
2017-03-02T17:41:44.632329: step 7289, loss 0.121741, acc 0.9375
2017-03-02T17:41:44.706826: step 7290, loss 0.0951196, acc 0.96875
2017-03-02T17:41:44.782651: step 7291, loss 0.15824, acc 0.921875
2017-03-02T17:41:44.848971: step 7292, loss 0.230177, acc 0.90625
2017-03-02T17:41:44.922359: step 7293, loss 0.237659, acc 0.875
2017-03-02T17:41:44.994095: step 7294, loss 0.142785, acc 0.9375
2017-03-02T17:41:45.064546: step 7295, loss 0.139847, acc 0.9375
2017-03-02T17:41:45.136237: step 7296, loss 0.193396, acc 0.953125
2017-03-02T17:41:45.207043: step 7297, loss 0.125791, acc 0.953125
2017-03-02T17:41:45.280182: step 7298, loss 0.154133, acc 0.953125
2017-03-02T17:41:45.368653: step 7299, loss 0.198557, acc 0.953125
2017-03-02T17:41:45.445959: step 7300, loss 0.194374, acc 0.921875

Evaluation:
2017-03-02T17:41:45.473068: step 7300, loss 1.26363, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7300

2017-03-02T17:41:45.906337: step 7301, loss 0.126876, acc 0.921875
2017-03-02T17:41:45.979714: step 7302, loss 0.314194, acc 0.875
2017-03-02T17:41:46.053668: step 7303, loss 0.271608, acc 0.921875
2017-03-02T17:41:46.134077: step 7304, loss 0.15176, acc 0.9375
2017-03-02T17:41:46.207775: step 7305, loss 0.247794, acc 0.875
2017-03-02T17:41:46.277238: step 7306, loss 0.0846996, acc 0.96875
2017-03-02T17:41:46.348937: step 7307, loss 0.198809, acc 0.90625
2017-03-02T17:41:46.428329: step 7308, loss 0.177485, acc 0.90625
2017-03-02T17:41:46.506280: step 7309, loss 0.214702, acc 0.90625
2017-03-02T17:41:46.577103: step 7310, loss 0.175165, acc 0.9375
2017-03-02T17:41:46.650828: step 7311, loss 0.247548, acc 0.859375
2017-03-02T17:41:46.715222: step 7312, loss 0.274509, acc 0.890625
2017-03-02T17:41:46.791178: step 7313, loss 0.101473, acc 0.953125
2017-03-02T17:41:46.867226: step 7314, loss 0.132058, acc 0.9375
2017-03-02T17:41:46.953986: step 7315, loss 0.322426, acc 0.90625
2017-03-02T17:41:47.022170: step 7316, loss 0.318037, acc 0.859375
2017-03-02T17:41:47.096807: step 7317, loss 0.118477, acc 0.921875
2017-03-02T17:41:47.176311: step 7318, loss 0.30306, acc 0.90625
2017-03-02T17:41:47.251526: step 7319, loss 0.210202, acc 0.921875
2017-03-02T17:41:47.322838: step 7320, loss 0.15656, acc 0.921875
2017-03-02T17:41:47.395156: step 7321, loss 0.301052, acc 0.90625
2017-03-02T17:41:47.464017: step 7322, loss 0.158174, acc 0.9375
2017-03-02T17:41:47.540662: step 7323, loss 0.204259, acc 0.890625
2017-03-02T17:41:47.613267: step 7324, loss 0.209429, acc 0.921875
2017-03-02T17:41:47.677690: step 7325, loss 0.159649, acc 0.9375
2017-03-02T17:41:47.756227: step 7326, loss 0.25408, acc 0.890625
2017-03-02T17:41:47.829658: step 7327, loss 0.29102, acc 0.875
2017-03-02T17:41:47.899193: step 7328, loss 0.164773, acc 0.9375
2017-03-02T17:41:47.971930: step 7329, loss 0.405257, acc 0.890625
2017-03-02T17:41:48.046487: step 7330, loss 0.188995, acc 0.953125
2017-03-02T17:41:48.135244: step 7331, loss 0.0802937, acc 0.96875
2017-03-02T17:41:48.211767: step 7332, loss 0.206027, acc 0.921875
2017-03-02T17:41:48.276349: step 7333, loss 0.0886846, acc 0.96875
2017-03-02T17:41:48.337922: step 7334, loss 0.0972219, acc 0.953125
2017-03-02T17:41:48.425094: step 7335, loss 0.0844772, acc 0.984375
2017-03-02T17:41:48.503069: step 7336, loss 0.221393, acc 0.890625
2017-03-02T17:41:48.587465: step 7337, loss 0.132301, acc 0.953125
2017-03-02T17:41:48.663345: step 7338, loss 0.190775, acc 0.9375
2017-03-02T17:41:48.737534: step 7339, loss 0.204468, acc 0.953125
2017-03-02T17:41:48.810683: step 7340, loss 0.245815, acc 0.90625
2017-03-02T17:41:48.883255: step 7341, loss 0.160556, acc 0.9375
2017-03-02T17:41:48.955471: step 7342, loss 0.128466, acc 0.953125
2017-03-02T17:41:49.025526: step 7343, loss 0.229473, acc 0.890625
2017-03-02T17:41:49.094329: step 7344, loss 0.184554, acc 0.90625
2017-03-02T17:41:49.168150: step 7345, loss 0.209774, acc 0.921875
2017-03-02T17:41:49.241758: step 7346, loss 0.209505, acc 0.90625
2017-03-02T17:41:49.318623: step 7347, loss 0.190057, acc 0.9375
2017-03-02T17:41:49.390729: step 7348, loss 0.173904, acc 0.953125
2017-03-02T17:41:49.459869: step 7349, loss 0.264985, acc 0.90625
2017-03-02T17:41:49.530034: step 7350, loss 0.208956, acc 0.921875
2017-03-02T17:41:49.591465: step 7351, loss 0.186378, acc 0.9375
2017-03-02T17:41:49.678665: step 7352, loss 0.112863, acc 0.96875
2017-03-02T17:41:49.742780: step 7353, loss 0.139355, acc 0.96875
2017-03-02T17:41:49.807781: step 7354, loss 0.155425, acc 0.921875
2017-03-02T17:41:49.875524: step 7355, loss 0.226211, acc 0.890625
2017-03-02T17:41:49.950791: step 7356, loss 0.173979, acc 0.921875
2017-03-02T17:41:50.025800: step 7357, loss 0.343991, acc 0.875
2017-03-02T17:41:50.098077: step 7358, loss 0.122136, acc 0.953125
2017-03-02T17:41:50.175746: step 7359, loss 0.176476, acc 0.921875
2017-03-02T17:41:50.248681: step 7360, loss 0.100638, acc 0.96875
2017-03-02T17:41:50.321008: step 7361, loss 0.119608, acc 0.953125
2017-03-02T17:41:50.392011: step 7362, loss 0.181077, acc 0.9375
2017-03-02T17:41:50.463309: step 7363, loss 0.166171, acc 0.9375
2017-03-02T17:41:50.534924: step 7364, loss 0.249551, acc 0.90625
2017-03-02T17:41:50.608307: step 7365, loss 0.188744, acc 0.9375
2017-03-02T17:41:50.688055: step 7366, loss 0.315867, acc 0.921875
2017-03-02T17:41:50.765629: step 7367, loss 0.160333, acc 0.9375
2017-03-02T17:41:50.841010: step 7368, loss 0.212037, acc 0.9375
2017-03-02T17:41:50.910921: step 7369, loss 0.291437, acc 0.890625
2017-03-02T17:41:50.983583: step 7370, loss 0.212246, acc 0.9375
2017-03-02T17:41:51.056605: step 7371, loss 0.115818, acc 0.953125
2017-03-02T17:41:51.124787: step 7372, loss 0.100682, acc 0.984375
2017-03-02T17:41:51.209884: step 7373, loss 0.152317, acc 0.890625
2017-03-02T17:41:51.300037: step 7374, loss 0.134018, acc 0.9375
2017-03-02T17:41:51.378400: step 7375, loss 0.111298, acc 0.96875
2017-03-02T17:41:51.451118: step 7376, loss 0.250421, acc 0.90625
2017-03-02T17:41:51.521510: step 7377, loss 0.192433, acc 0.90625
2017-03-02T17:41:51.595144: step 7378, loss 0.330967, acc 0.875
2017-03-02T17:41:51.665500: step 7379, loss 0.374462, acc 0.859375
2017-03-02T17:41:51.734820: step 7380, loss 0.24483, acc 0.90625
2017-03-02T17:41:51.815337: step 7381, loss 0.290506, acc 0.890625
2017-03-02T17:41:51.882877: step 7382, loss 0.283358, acc 0.875
2017-03-02T17:41:51.952803: step 7383, loss 0.308233, acc 0.859375
2017-03-02T17:41:52.024302: step 7384, loss 0.19609, acc 0.90625
2017-03-02T17:41:52.097900: step 7385, loss 0.113293, acc 0.953125
2017-03-02T17:41:52.178329: step 7386, loss 0.201733, acc 0.921875
2017-03-02T17:41:52.252285: step 7387, loss 0.182138, acc 0.875
2017-03-02T17:41:52.325010: step 7388, loss 0.141145, acc 0.953125
2017-03-02T17:41:52.403660: step 7389, loss 0.102272, acc 0.984375
2017-03-02T17:41:52.479350: step 7390, loss 0.155377, acc 0.9375
2017-03-02T17:41:52.552415: step 7391, loss 0.122841, acc 0.953125
2017-03-02T17:41:52.640987: step 7392, loss 0.231288, acc 0.921875
2017-03-02T17:41:52.713992: step 7393, loss 0.212932, acc 0.875
2017-03-02T17:41:52.809521: step 7394, loss 0.264509, acc 0.90625
2017-03-02T17:41:52.883293: step 7395, loss 0.160263, acc 0.90625
2017-03-02T17:41:52.961475: step 7396, loss 0.229417, acc 0.90625
2017-03-02T17:41:53.036044: step 7397, loss 0.171239, acc 0.9375
2017-03-02T17:41:53.107435: step 7398, loss 0.266664, acc 0.890625
2017-03-02T17:41:53.183508: step 7399, loss 0.113868, acc 0.96875
2017-03-02T17:41:53.251187: step 7400, loss 0.190654, acc 0.90625

Evaluation:
2017-03-02T17:41:53.287336: step 7400, loss 1.26768, acc 0.66907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7400

2017-03-02T17:41:53.738960: step 7401, loss 0.310891, acc 0.890625
2017-03-02T17:41:53.818074: step 7402, loss 0.157181, acc 0.921875
2017-03-02T17:41:53.879984: step 7403, loss 0.210877, acc 0.90625
2017-03-02T17:41:53.948325: step 7404, loss 0.182507, acc 0.921875
2017-03-02T17:41:54.013320: step 7405, loss 0.153929, acc 0.953125
2017-03-02T17:41:54.095702: step 7406, loss 0.125993, acc 0.9375
2017-03-02T17:41:54.176529: step 7407, loss 0.317806, acc 0.9375
2017-03-02T17:41:54.247634: step 7408, loss 0.197763, acc 0.9375
2017-03-02T17:41:54.321874: step 7409, loss 0.257374, acc 0.875
2017-03-02T17:41:54.392209: step 7410, loss 0.230434, acc 0.90625
2017-03-02T17:41:54.466834: step 7411, loss 0.158022, acc 0.953125
2017-03-02T17:41:54.539058: step 7412, loss 0.158171, acc 0.96875
2017-03-02T17:41:54.603792: step 7413, loss 0.128332, acc 0.96875
2017-03-02T17:41:54.672425: step 7414, loss 0.269611, acc 0.890625
2017-03-02T17:41:54.741048: step 7415, loss 0.190119, acc 0.953125
2017-03-02T17:41:54.827786: step 7416, loss 0.151237, acc 0.953125
2017-03-02T17:41:54.902240: step 7417, loss 0.101231, acc 0.953125
2017-03-02T17:41:54.982499: step 7418, loss 0.0935, acc 0.96875
2017-03-02T17:41:55.052425: step 7419, loss 0.136812, acc 0.9375
2017-03-02T17:41:55.124791: step 7420, loss 0.151124, acc 0.921875
2017-03-02T17:41:55.193171: step 7421, loss 0.16444, acc 0.953125
2017-03-02T17:41:55.274356: step 7422, loss 0.51804, acc 0.859375
2017-03-02T17:41:55.348573: step 7423, loss 0.110322, acc 0.953125
2017-03-02T17:41:55.442535: step 7424, loss 0.295261, acc 0.90625
2017-03-02T17:41:55.516679: step 7425, loss 0.14142, acc 0.9375
2017-03-02T17:41:55.590724: step 7426, loss 0.235842, acc 0.90625
2017-03-02T17:41:55.661715: step 7427, loss 0.220437, acc 0.90625
2017-03-02T17:41:55.738054: step 7428, loss 0.0416007, acc 0.984375
2017-03-02T17:41:55.813048: step 7429, loss 0.263165, acc 0.890625
2017-03-02T17:41:55.894360: step 7430, loss 0.118547, acc 0.96875
2017-03-02T17:41:55.962833: step 7431, loss 0.142111, acc 0.953125
2017-03-02T17:41:56.027458: step 7432, loss 0.128439, acc 0.953125
2017-03-02T17:41:56.100896: step 7433, loss 0.348576, acc 0.890625
2017-03-02T17:41:56.174046: step 7434, loss 0.112021, acc 0.984375
2017-03-02T17:41:56.247088: step 7435, loss 0.214016, acc 0.90625
2017-03-02T17:41:56.321823: step 7436, loss 0.37852, acc 0.890625
2017-03-02T17:41:56.395434: step 7437, loss 0.255861, acc 0.90625
2017-03-02T17:41:56.468763: step 7438, loss 0.263202, acc 0.859375
2017-03-02T17:41:56.554538: step 7439, loss 0.25011, acc 0.90625
2017-03-02T17:41:56.627188: step 7440, loss 0.175731, acc 0.90625
2017-03-02T17:41:56.696360: step 7441, loss 0.212903, acc 0.921875
2017-03-02T17:41:56.771012: step 7442, loss 0.139438, acc 0.953125
2017-03-02T17:41:56.847810: step 7443, loss 0.132002, acc 0.953125
2017-03-02T17:41:56.924300: step 7444, loss 0.31664, acc 0.84375
2017-03-02T17:41:56.994688: step 7445, loss 0.136713, acc 0.921875
2017-03-02T17:41:57.076378: step 7446, loss 0.149891, acc 0.953125
2017-03-02T17:41:57.150532: step 7447, loss 0.210967, acc 0.9375
2017-03-02T17:41:57.220801: step 7448, loss 0.00487315, acc 1
2017-03-02T17:41:57.301254: step 7449, loss 0.216229, acc 0.90625
2017-03-02T17:41:57.371916: step 7450, loss 0.0639536, acc 0.984375
2017-03-02T17:41:57.437274: step 7451, loss 0.311599, acc 0.890625
2017-03-02T17:41:57.508212: step 7452, loss 0.128467, acc 0.9375
2017-03-02T17:41:57.580547: step 7453, loss 0.0827734, acc 0.984375
2017-03-02T17:41:57.652214: step 7454, loss 0.190711, acc 0.90625
2017-03-02T17:41:57.728168: step 7455, loss 0.132, acc 0.9375
2017-03-02T17:41:57.799621: step 7456, loss 0.303869, acc 0.890625
2017-03-02T17:41:57.873126: step 7457, loss 0.13731, acc 0.921875
2017-03-02T17:41:57.947573: step 7458, loss 0.157035, acc 0.953125
2017-03-02T17:41:58.016991: step 7459, loss 0.20624, acc 0.890625
2017-03-02T17:41:58.086004: step 7460, loss 0.134452, acc 0.921875
2017-03-02T17:41:58.163265: step 7461, loss 0.0793806, acc 0.984375
2017-03-02T17:41:58.242650: step 7462, loss 0.162479, acc 0.890625
2017-03-02T17:41:58.316139: step 7463, loss 0.104455, acc 0.96875
2017-03-02T17:41:58.389343: step 7464, loss 0.0736795, acc 0.96875
2017-03-02T17:41:58.470926: step 7465, loss 0.153295, acc 0.9375
2017-03-02T17:41:58.548696: step 7466, loss 0.19341, acc 0.921875
2017-03-02T17:41:58.623201: step 7467, loss 0.134104, acc 0.9375
2017-03-02T17:41:58.689095: step 7468, loss 0.124647, acc 0.953125
2017-03-02T17:41:58.759939: step 7469, loss 0.2015, acc 0.9375
2017-03-02T17:41:58.826253: step 7470, loss 0.312987, acc 0.890625
2017-03-02T17:41:58.900085: step 7471, loss 0.200125, acc 0.90625
2017-03-02T17:41:58.967499: step 7472, loss 0.125372, acc 0.96875
2017-03-02T17:41:59.042287: step 7473, loss 0.186464, acc 0.921875
2017-03-02T17:41:59.121956: step 7474, loss 0.0754508, acc 0.953125
2017-03-02T17:41:59.195633: step 7475, loss 0.130296, acc 0.953125
2017-03-02T17:41:59.269141: step 7476, loss 0.12395, acc 0.953125
2017-03-02T17:41:59.337015: step 7477, loss 0.179894, acc 0.953125
2017-03-02T17:41:59.399627: step 7478, loss 0.171843, acc 0.9375
2017-03-02T17:41:59.468253: step 7479, loss 0.185424, acc 0.9375
2017-03-02T17:41:59.542656: step 7480, loss 0.102074, acc 0.953125
2017-03-02T17:41:59.633752: step 7481, loss 0.122053, acc 0.953125
2017-03-02T17:41:59.717569: step 7482, loss 0.16375, acc 0.953125
2017-03-02T17:41:59.799658: step 7483, loss 0.231309, acc 0.921875
2017-03-02T17:41:59.872835: step 7484, loss 0.286472, acc 0.890625
2017-03-02T17:41:59.944636: step 7485, loss 0.166116, acc 0.90625
2017-03-02T17:42:00.016176: step 7486, loss 0.234037, acc 0.90625
2017-03-02T17:42:00.083551: step 7487, loss 0.127715, acc 0.9375
2017-03-02T17:42:00.150215: step 7488, loss 0.14463, acc 0.9375
2017-03-02T17:42:00.218373: step 7489, loss 0.195223, acc 0.90625
2017-03-02T17:42:00.293552: step 7490, loss 0.111965, acc 0.921875
2017-03-02T17:42:00.364203: step 7491, loss 0.254096, acc 0.875
2017-03-02T17:42:00.435804: step 7492, loss 0.184827, acc 0.890625
2017-03-02T17:42:00.509446: step 7493, loss 0.165737, acc 0.953125
2017-03-02T17:42:00.581861: step 7494, loss 0.160209, acc 0.921875
2017-03-02T17:42:00.659760: step 7495, loss 0.182831, acc 0.9375
2017-03-02T17:42:00.727680: step 7496, loss 0.154164, acc 0.921875
2017-03-02T17:42:00.798766: step 7497, loss 0.242683, acc 0.921875
2017-03-02T17:42:00.862242: step 7498, loss 0.227608, acc 0.921875
2017-03-02T17:42:00.928144: step 7499, loss 0.160231, acc 0.90625
2017-03-02T17:42:01.002744: step 7500, loss 0.196507, acc 0.90625

Evaluation:
2017-03-02T17:42:01.034173: step 7500, loss 1.27947, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7500

2017-03-02T17:42:01.488376: step 7501, loss 0.0912766, acc 0.953125
2017-03-02T17:42:01.561580: step 7502, loss 0.288227, acc 0.90625
2017-03-02T17:42:01.635746: step 7503, loss 0.154914, acc 0.953125
2017-03-02T17:42:01.705120: step 7504, loss 0.115687, acc 0.984375
2017-03-02T17:42:01.804723: step 7505, loss 0.371173, acc 0.890625
2017-03-02T17:42:01.887678: step 7506, loss 0.185408, acc 0.90625
2017-03-02T17:42:01.963333: step 7507, loss 0.137746, acc 0.921875
2017-03-02T17:42:02.033060: step 7508, loss 0.16597, acc 0.9375
2017-03-02T17:42:02.105218: step 7509, loss 0.16049, acc 0.90625
2017-03-02T17:42:02.179831: step 7510, loss 0.118455, acc 0.953125
2017-03-02T17:42:02.254350: step 7511, loss 0.116251, acc 0.953125
2017-03-02T17:42:02.331808: step 7512, loss 0.0994384, acc 0.96875
2017-03-02T17:42:02.408026: step 7513, loss 0.13802, acc 0.953125
2017-03-02T17:42:02.478303: step 7514, loss 0.157733, acc 0.9375
2017-03-02T17:42:02.550145: step 7515, loss 0.179333, acc 0.9375
2017-03-02T17:42:02.616651: step 7516, loss 0.216729, acc 0.90625
2017-03-02T17:42:02.686942: step 7517, loss 0.12083, acc 0.984375
2017-03-02T17:42:02.758343: step 7518, loss 0.145329, acc 0.953125
2017-03-02T17:42:02.829755: step 7519, loss 0.143151, acc 0.9375
2017-03-02T17:42:02.897999: step 7520, loss 0.192937, acc 0.921875
2017-03-02T17:42:02.987502: step 7521, loss 0.222845, acc 0.9375
2017-03-02T17:42:03.077318: step 7522, loss 0.166398, acc 0.9375
2017-03-02T17:42:03.150406: step 7523, loss 0.287259, acc 0.890625
2017-03-02T17:42:03.218575: step 7524, loss 0.215903, acc 0.890625
2017-03-02T17:42:03.294971: step 7525, loss 0.194164, acc 0.9375
2017-03-02T17:42:03.364797: step 7526, loss 0.166746, acc 0.9375
2017-03-02T17:42:03.438894: step 7527, loss 0.0993809, acc 0.953125
2017-03-02T17:42:03.522034: step 7528, loss 0.198687, acc 0.921875
2017-03-02T17:42:03.600942: step 7529, loss 0.256689, acc 0.9375
2017-03-02T17:42:03.665723: step 7530, loss 0.115259, acc 0.984375
2017-03-02T17:42:03.739562: step 7531, loss 0.147566, acc 0.953125
2017-03-02T17:42:03.825514: step 7532, loss 0.176698, acc 0.921875
2017-03-02T17:42:03.897146: step 7533, loss 0.0637132, acc 0.984375
2017-03-02T17:42:03.970605: step 7534, loss 0.290615, acc 0.921875
2017-03-02T17:42:04.040111: step 7535, loss 0.176855, acc 0.921875
2017-03-02T17:42:04.111000: step 7536, loss 0.0860002, acc 0.984375
2017-03-02T17:42:04.182892: step 7537, loss 0.225278, acc 0.90625
2017-03-02T17:42:04.252240: step 7538, loss 0.168779, acc 0.953125
2017-03-02T17:42:04.323147: step 7539, loss 0.110484, acc 0.953125
2017-03-02T17:42:04.391550: step 7540, loss 0.196215, acc 0.921875
2017-03-02T17:42:04.467650: step 7541, loss 0.213363, acc 0.890625
2017-03-02T17:42:04.542238: step 7542, loss 0.0852456, acc 0.984375
2017-03-02T17:42:04.615668: step 7543, loss 0.15878, acc 0.953125
2017-03-02T17:42:04.687054: step 7544, loss 0.203416, acc 0.953125
2017-03-02T17:42:04.758700: step 7545, loss 0.124936, acc 0.9375
2017-03-02T17:42:04.833250: step 7546, loss 0.276938, acc 0.875
2017-03-02T17:42:04.908548: step 7547, loss 0.144243, acc 0.953125
2017-03-02T17:42:04.981391: step 7548, loss 0.224687, acc 0.90625
2017-03-02T17:42:05.048874: step 7549, loss 0.23354, acc 0.921875
2017-03-02T17:42:05.119270: step 7550, loss 0.167197, acc 0.953125
2017-03-02T17:42:05.202253: step 7551, loss 0.186722, acc 0.9375
2017-03-02T17:42:05.272556: step 7552, loss 0.135755, acc 0.953125
2017-03-02T17:42:05.333989: step 7553, loss 0.422233, acc 0.84375
2017-03-02T17:42:05.404325: step 7554, loss 0.260568, acc 0.90625
2017-03-02T17:42:05.475393: step 7555, loss 0.113864, acc 0.953125
2017-03-02T17:42:05.554159: step 7556, loss 0.281638, acc 0.875
2017-03-02T17:42:05.622962: step 7557, loss 0.281799, acc 0.90625
2017-03-02T17:42:05.690684: step 7558, loss 0.134195, acc 0.9375
2017-03-02T17:42:05.756888: step 7559, loss 0.173203, acc 0.9375
2017-03-02T17:42:05.834523: step 7560, loss 0.0871948, acc 0.96875
2017-03-02T17:42:05.906652: step 7561, loss 0.155261, acc 0.9375
2017-03-02T17:42:05.989763: step 7562, loss 0.312467, acc 0.875
2017-03-02T17:42:06.065268: step 7563, loss 0.141325, acc 0.953125
2017-03-02T17:42:06.136735: step 7564, loss 0.163482, acc 0.9375
2017-03-02T17:42:06.209767: step 7565, loss 0.176623, acc 0.9375
2017-03-02T17:42:06.281668: step 7566, loss 0.261546, acc 0.90625
2017-03-02T17:42:06.349550: step 7567, loss 0.169334, acc 0.921875
2017-03-02T17:42:06.418351: step 7568, loss 0.152118, acc 0.9375
2017-03-02T17:42:06.485064: step 7569, loss 0.19071, acc 0.9375
2017-03-02T17:42:06.559088: step 7570, loss 0.170296, acc 0.9375
2017-03-02T17:42:06.633214: step 7571, loss 0.237534, acc 0.890625
2017-03-02T17:42:06.705118: step 7572, loss 0.176342, acc 0.96875
2017-03-02T17:42:06.775238: step 7573, loss 0.160679, acc 0.9375
2017-03-02T17:42:06.845561: step 7574, loss 0.233244, acc 0.9375
2017-03-02T17:42:06.918172: step 7575, loss 0.364085, acc 0.90625
2017-03-02T17:42:07.003978: step 7576, loss 0.229047, acc 0.890625
2017-03-02T17:42:07.069154: step 7577, loss 0.164572, acc 0.9375
2017-03-02T17:42:07.136102: step 7578, loss 0.1949, acc 0.9375
2017-03-02T17:42:07.211837: step 7579, loss 0.267935, acc 0.890625
2017-03-02T17:42:07.285691: step 7580, loss 0.300894, acc 0.859375
2017-03-02T17:42:07.358306: step 7581, loss 0.326412, acc 0.859375
2017-03-02T17:42:07.437106: step 7582, loss 0.27675, acc 0.90625
2017-03-02T17:42:07.508426: step 7583, loss 0.212142, acc 0.90625
2017-03-02T17:42:07.585550: step 7584, loss 0.129608, acc 0.953125
2017-03-02T17:42:07.660914: step 7585, loss 0.166116, acc 0.9375
2017-03-02T17:42:07.728456: step 7586, loss 0.101573, acc 0.96875
2017-03-02T17:42:07.796242: step 7587, loss 0.182327, acc 0.90625
2017-03-02T17:42:07.872446: step 7588, loss 0.27384, acc 0.90625
2017-03-02T17:42:07.948381: step 7589, loss 0.0675885, acc 0.984375
2017-03-02T17:42:08.029963: step 7590, loss 0.279139, acc 0.90625
2017-03-02T17:42:08.105664: step 7591, loss 0.161736, acc 0.9375
2017-03-02T17:42:08.178403: step 7592, loss 0.0807971, acc 0.984375
2017-03-02T17:42:08.253757: step 7593, loss 0.122719, acc 0.9375
2017-03-02T17:42:08.324383: step 7594, loss 0.231589, acc 0.921875
2017-03-02T17:42:08.401504: step 7595, loss 0.183525, acc 0.90625
2017-03-02T17:42:08.480675: step 7596, loss 0.274219, acc 0.84375
2017-03-02T17:42:08.551716: step 7597, loss 0.233749, acc 0.890625
2017-03-02T17:42:08.616341: step 7598, loss 0.274424, acc 0.890625
2017-03-02T17:42:08.690279: step 7599, loss 0.119351, acc 0.96875
2017-03-02T17:42:08.782796: step 7600, loss 0.244121, acc 0.875

Evaluation:
2017-03-02T17:42:08.813951: step 7600, loss 1.26971, acc 0.66186

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7600

2017-03-02T17:42:09.270597: step 7601, loss 0.188306, acc 0.921875
2017-03-02T17:42:09.336324: step 7602, loss 0.254322, acc 0.875
2017-03-02T17:42:09.412549: step 7603, loss 0.0971976, acc 0.953125
2017-03-02T17:42:09.485669: step 7604, loss 0.265677, acc 0.890625
2017-03-02T17:42:09.558828: step 7605, loss 0.29126, acc 0.90625
2017-03-02T17:42:09.631388: step 7606, loss 0.218943, acc 0.921875
2017-03-02T17:42:09.710460: step 7607, loss 0.234283, acc 0.890625
2017-03-02T17:42:09.777695: step 7608, loss 0.170935, acc 0.890625
2017-03-02T17:42:09.849014: step 7609, loss 0.160186, acc 0.953125
2017-03-02T17:42:09.920074: step 7610, loss 0.208971, acc 0.921875
2017-03-02T17:42:10.000699: step 7611, loss 0.11506, acc 0.9375
2017-03-02T17:42:10.077216: step 7612, loss 0.17471, acc 0.9375
2017-03-02T17:42:10.149770: step 7613, loss 0.179111, acc 0.90625
2017-03-02T17:42:10.227532: step 7614, loss 0.130406, acc 0.953125
2017-03-02T17:42:10.296180: step 7615, loss 0.159085, acc 0.96875
2017-03-02T17:42:10.372968: step 7616, loss 0.122007, acc 0.9375
2017-03-02T17:42:10.444822: step 7617, loss 0.141976, acc 0.953125
2017-03-02T17:42:10.519355: step 7618, loss 0.138477, acc 0.9375
2017-03-02T17:42:10.585358: step 7619, loss 0.19464, acc 0.9375
2017-03-02T17:42:10.662659: step 7620, loss 0.257552, acc 0.921875
2017-03-02T17:42:10.735898: step 7621, loss 0.204718, acc 0.90625
2017-03-02T17:42:10.813507: step 7622, loss 0.174276, acc 0.9375
2017-03-02T17:42:10.895377: step 7623, loss 0.343965, acc 0.828125
2017-03-02T17:42:10.969049: step 7624, loss 0.145934, acc 0.953125
2017-03-02T17:42:11.043459: step 7625, loss 0.176874, acc 0.921875
2017-03-02T17:42:11.124478: step 7626, loss 0.154425, acc 0.953125
2017-03-02T17:42:11.207911: step 7627, loss 0.25281, acc 0.921875
2017-03-02T17:42:11.278565: step 7628, loss 0.154115, acc 0.96875
2017-03-02T17:42:11.354440: step 7629, loss 0.160601, acc 0.921875
2017-03-02T17:42:11.432028: step 7630, loss 0.125224, acc 0.9375
2017-03-02T17:42:11.505135: step 7631, loss 0.162117, acc 0.96875
2017-03-02T17:42:11.580368: step 7632, loss 0.159183, acc 0.9375
2017-03-02T17:42:11.661920: step 7633, loss 0.181804, acc 0.921875
2017-03-02T17:42:11.738600: step 7634, loss 0.157793, acc 0.921875
2017-03-02T17:42:11.818050: step 7635, loss 0.27595, acc 0.890625
2017-03-02T17:42:11.880020: step 7636, loss 0.315516, acc 0.90625
2017-03-02T17:42:11.943995: step 7637, loss 0.209375, acc 0.90625
2017-03-02T17:42:12.014898: step 7638, loss 0.185487, acc 0.921875
2017-03-02T17:42:12.090351: step 7639, loss 0.24422, acc 0.875
2017-03-02T17:42:12.175384: step 7640, loss 0.186418, acc 0.953125
2017-03-02T17:42:12.238021: step 7641, loss 0.28437, acc 0.875
2017-03-02T17:42:12.312919: step 7642, loss 0.146796, acc 0.953125
2017-03-02T17:42:12.386242: step 7643, loss 0.130235, acc 0.921875
2017-03-02T17:42:12.455323: step 7644, loss 0.0210639, acc 1
2017-03-02T17:42:12.533881: step 7645, loss 0.17428, acc 0.921875
2017-03-02T17:42:12.610282: step 7646, loss 0.230262, acc 0.9375
2017-03-02T17:42:12.681191: step 7647, loss 0.264094, acc 0.875
2017-03-02T17:42:12.751644: step 7648, loss 0.174257, acc 0.921875
2017-03-02T17:42:12.821051: step 7649, loss 0.123261, acc 0.921875
2017-03-02T17:42:12.893744: step 7650, loss 0.112791, acc 0.953125
2017-03-02T17:42:12.967737: step 7651, loss 0.16529, acc 0.9375
2017-03-02T17:42:13.051658: step 7652, loss 0.0811405, acc 0.984375
2017-03-02T17:42:13.133473: step 7653, loss 0.149196, acc 0.9375
2017-03-02T17:42:13.213343: step 7654, loss 0.0823496, acc 0.984375
2017-03-02T17:42:13.280346: step 7655, loss 0.0774354, acc 0.953125
2017-03-02T17:42:13.350209: step 7656, loss 0.146239, acc 0.953125
2017-03-02T17:42:13.420039: step 7657, loss 0.146364, acc 0.96875
2017-03-02T17:42:13.499745: step 7658, loss 0.0984722, acc 0.96875
2017-03-02T17:42:13.569267: step 7659, loss 0.25669, acc 0.890625
2017-03-02T17:42:13.642313: step 7660, loss 0.188247, acc 0.921875
2017-03-02T17:42:13.710390: step 7661, loss 0.232902, acc 0.890625
2017-03-02T17:42:13.780802: step 7662, loss 0.183177, acc 0.9375
2017-03-02T17:42:13.858769: step 7663, loss 0.183218, acc 0.921875
2017-03-02T17:42:13.935304: step 7664, loss 0.182874, acc 0.921875
2017-03-02T17:42:14.005732: step 7665, loss 0.318075, acc 0.90625
2017-03-02T17:42:14.073740: step 7666, loss 0.28322, acc 0.859375
2017-03-02T17:42:14.150510: step 7667, loss 0.183255, acc 0.9375
2017-03-02T17:42:14.225297: step 7668, loss 0.228103, acc 0.90625
2017-03-02T17:42:14.300902: step 7669, loss 0.118854, acc 0.96875
2017-03-02T17:42:14.409759: step 7670, loss 0.127223, acc 0.96875
2017-03-02T17:42:14.491148: step 7671, loss 0.184355, acc 0.921875
2017-03-02T17:42:14.571469: step 7672, loss 0.211476, acc 0.953125
2017-03-02T17:42:14.657252: step 7673, loss 0.226951, acc 0.921875
2017-03-02T17:42:14.728251: step 7674, loss 0.203367, acc 0.875
2017-03-02T17:42:14.810577: step 7675, loss 0.117314, acc 0.953125
2017-03-02T17:42:14.882438: step 7676, loss 0.142997, acc 0.9375
2017-03-02T17:42:14.957705: step 7677, loss 0.178435, acc 0.921875
2017-03-02T17:42:15.034452: step 7678, loss 0.0564315, acc 1
2017-03-02T17:42:15.110695: step 7679, loss 0.109007, acc 0.9375
2017-03-02T17:42:15.184651: step 7680, loss 0.198843, acc 0.921875
2017-03-02T17:42:15.261736: step 7681, loss 0.135977, acc 0.953125
2017-03-02T17:42:15.325392: step 7682, loss 0.0711633, acc 0.984375
2017-03-02T17:42:15.404418: step 7683, loss 0.139732, acc 0.9375
2017-03-02T17:42:15.472819: step 7684, loss 0.156493, acc 0.953125
2017-03-02T17:42:15.540286: step 7685, loss 0.19823, acc 0.921875
2017-03-02T17:42:15.609364: step 7686, loss 0.201554, acc 0.921875
2017-03-02T17:42:15.687300: step 7687, loss 0.1046, acc 0.96875
2017-03-02T17:42:15.757783: step 7688, loss 0.129437, acc 0.921875
2017-03-02T17:42:15.831875: step 7689, loss 0.148027, acc 0.921875
2017-03-02T17:42:15.909695: step 7690, loss 0.157228, acc 0.9375
2017-03-02T17:42:15.980223: step 7691, loss 0.197559, acc 0.9375
2017-03-02T17:42:16.048903: step 7692, loss 0.147091, acc 0.953125
2017-03-02T17:42:16.118515: step 7693, loss 0.180777, acc 0.921875
2017-03-02T17:42:16.182380: step 7694, loss 0.0931405, acc 0.96875
2017-03-02T17:42:16.249544: step 7695, loss 0.232583, acc 0.90625
2017-03-02T17:42:16.330760: step 7696, loss 0.270127, acc 0.859375
2017-03-02T17:42:16.402125: step 7697, loss 0.0999251, acc 0.96875
2017-03-02T17:42:16.473787: step 7698, loss 0.147586, acc 0.9375
2017-03-02T17:42:16.548299: step 7699, loss 0.157733, acc 0.9375
2017-03-02T17:42:16.618621: step 7700, loss 0.0946014, acc 0.953125

Evaluation:
2017-03-02T17:42:16.647636: step 7700, loss 1.31165, acc 0.675559

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7700

2017-03-02T17:42:17.105164: step 7701, loss 0.129719, acc 0.96875
2017-03-02T17:42:17.177894: step 7702, loss 0.246852, acc 0.890625
2017-03-02T17:42:17.253069: step 7703, loss 0.275969, acc 0.921875
2017-03-02T17:42:17.333046: step 7704, loss 0.254858, acc 0.921875
2017-03-02T17:42:17.415893: step 7705, loss 0.15277, acc 0.9375
2017-03-02T17:42:17.485490: step 7706, loss 0.221356, acc 0.890625
2017-03-02T17:42:17.568620: step 7707, loss 0.0941027, acc 0.96875
2017-03-02T17:42:17.648880: step 7708, loss 0.232348, acc 0.890625
2017-03-02T17:42:17.726340: step 7709, loss 0.177496, acc 0.921875
2017-03-02T17:42:17.801348: step 7710, loss 0.155192, acc 0.921875
2017-03-02T17:42:17.874878: step 7711, loss 0.169287, acc 0.9375
2017-03-02T17:42:17.950484: step 7712, loss 0.181592, acc 0.921875
2017-03-02T17:42:18.030467: step 7713, loss 0.257739, acc 0.890625
2017-03-02T17:42:18.099963: step 7714, loss 0.241975, acc 0.921875
2017-03-02T17:42:18.172291: step 7715, loss 0.12355, acc 0.953125
2017-03-02T17:42:18.245277: step 7716, loss 0.216612, acc 0.890625
2017-03-02T17:42:18.319264: step 7717, loss 0.126644, acc 0.9375
2017-03-02T17:42:18.394392: step 7718, loss 0.201996, acc 0.921875
2017-03-02T17:42:18.470569: step 7719, loss 0.182367, acc 0.953125
2017-03-02T17:42:18.543666: step 7720, loss 0.219158, acc 0.90625
2017-03-02T17:42:18.616304: step 7721, loss 0.0857392, acc 0.984375
2017-03-02T17:42:18.698111: step 7722, loss 0.211718, acc 0.90625
2017-03-02T17:42:18.766371: step 7723, loss 0.204846, acc 0.90625
2017-03-02T17:42:18.832972: step 7724, loss 0.210487, acc 0.90625
2017-03-02T17:42:18.906999: step 7725, loss 0.322226, acc 0.859375
2017-03-02T17:42:18.977045: step 7726, loss 0.226005, acc 0.90625
2017-03-02T17:42:19.052635: step 7727, loss 0.148716, acc 0.96875
2017-03-02T17:42:19.123350: step 7728, loss 0.164459, acc 0.890625
2017-03-02T17:42:19.192835: step 7729, loss 0.0604153, acc 0.984375
2017-03-02T17:42:19.263651: step 7730, loss 0.125733, acc 0.9375
2017-03-02T17:42:19.335704: step 7731, loss 0.174747, acc 0.9375
2017-03-02T17:42:19.415091: step 7732, loss 0.211859, acc 0.90625
2017-03-02T17:42:19.488002: step 7733, loss 0.125977, acc 0.9375
2017-03-02T17:42:19.557676: step 7734, loss 0.0842464, acc 0.984375
2017-03-02T17:42:19.632573: step 7735, loss 0.0514066, acc 0.984375
2017-03-02T17:42:19.712751: step 7736, loss 0.125436, acc 0.921875
2017-03-02T17:42:19.786063: step 7737, loss 0.0772367, acc 0.984375
2017-03-02T17:42:19.856577: step 7738, loss 0.19231, acc 0.90625
2017-03-02T17:42:19.926931: step 7739, loss 0.222944, acc 0.875
2017-03-02T17:42:19.999258: step 7740, loss 0.261258, acc 0.859375
2017-03-02T17:42:20.078532: step 7741, loss 0.173167, acc 0.890625
2017-03-02T17:42:20.152901: step 7742, loss 0.122305, acc 0.953125
2017-03-02T17:42:20.224647: step 7743, loss 0.234859, acc 0.921875
2017-03-02T17:42:20.298494: step 7744, loss 0.249627, acc 0.90625
2017-03-02T17:42:20.372826: step 7745, loss 0.189358, acc 0.921875
2017-03-02T17:42:20.455210: step 7746, loss 0.128028, acc 0.984375
2017-03-02T17:42:20.530291: step 7747, loss 0.169983, acc 0.921875
2017-03-02T17:42:20.609153: step 7748, loss 0.1894, acc 0.921875
2017-03-02T17:42:20.684041: step 7749, loss 0.26578, acc 0.875
2017-03-02T17:42:20.756833: step 7750, loss 0.385393, acc 0.890625
2017-03-02T17:42:20.830807: step 7751, loss 0.190745, acc 0.90625
2017-03-02T17:42:20.900184: step 7752, loss 0.0444309, acc 0.984375
2017-03-02T17:42:20.977157: step 7753, loss 0.200417, acc 0.921875
2017-03-02T17:42:21.056471: step 7754, loss 0.40474, acc 0.859375
2017-03-02T17:42:21.129717: step 7755, loss 0.180608, acc 0.9375
2017-03-02T17:42:21.203697: step 7756, loss 0.158374, acc 0.9375
2017-03-02T17:42:21.270197: step 7757, loss 0.176987, acc 0.953125
2017-03-02T17:42:21.345426: step 7758, loss 0.28152, acc 0.890625
2017-03-02T17:42:21.423500: step 7759, loss 0.211878, acc 0.921875
2017-03-02T17:42:21.493772: step 7760, loss 0.263374, acc 0.9375
2017-03-02T17:42:21.561468: step 7761, loss 0.16851, acc 0.953125
2017-03-02T17:42:21.631041: step 7762, loss 0.141193, acc 0.96875
2017-03-02T17:42:21.700602: step 7763, loss 0.128758, acc 0.96875
2017-03-02T17:42:21.777558: step 7764, loss 0.322975, acc 0.90625
2017-03-02T17:42:21.856906: step 7765, loss 0.094103, acc 0.953125
2017-03-02T17:42:21.929412: step 7766, loss 0.106022, acc 0.96875
2017-03-02T17:42:22.006931: step 7767, loss 0.0673377, acc 0.984375
2017-03-02T17:42:22.084054: step 7768, loss 0.230753, acc 0.953125
2017-03-02T17:42:22.161873: step 7769, loss 0.225907, acc 0.875
2017-03-02T17:42:22.230936: step 7770, loss 0.21056, acc 0.90625
2017-03-02T17:42:22.315908: step 7771, loss 0.147031, acc 0.9375
2017-03-02T17:42:22.386685: step 7772, loss 0.108564, acc 0.953125
2017-03-02T17:42:22.465291: step 7773, loss 0.1863, acc 0.953125
2017-03-02T17:42:22.541158: step 7774, loss 0.312264, acc 0.8125
2017-03-02T17:42:22.613701: step 7775, loss 0.0560035, acc 0.984375
2017-03-02T17:42:22.684902: step 7776, loss 0.180959, acc 0.9375
2017-03-02T17:42:22.766337: step 7777, loss 0.256202, acc 0.875
2017-03-02T17:42:22.840904: step 7778, loss 0.134617, acc 0.953125
2017-03-02T17:42:22.910815: step 7779, loss 0.341982, acc 0.859375
2017-03-02T17:42:22.974907: step 7780, loss 0.053023, acc 0.984375
2017-03-02T17:42:23.048607: step 7781, loss 0.116056, acc 0.96875
2017-03-02T17:42:23.126566: step 7782, loss 0.185733, acc 0.921875
2017-03-02T17:42:23.201103: step 7783, loss 0.245615, acc 0.96875
2017-03-02T17:42:23.273131: step 7784, loss 0.130561, acc 0.96875
2017-03-02T17:42:23.363224: step 7785, loss 0.216758, acc 0.921875
2017-03-02T17:42:23.432440: step 7786, loss 0.158116, acc 0.9375
2017-03-02T17:42:23.506164: step 7787, loss 0.179741, acc 0.90625
2017-03-02T17:42:23.575235: step 7788, loss 0.429358, acc 0.84375
2017-03-02T17:42:23.646552: step 7789, loss 0.246341, acc 0.890625
2017-03-02T17:42:23.717412: step 7790, loss 0.212531, acc 0.859375
2017-03-02T17:42:23.790992: step 7791, loss 0.134104, acc 0.921875
2017-03-02T17:42:23.866885: step 7792, loss 0.143263, acc 0.953125
2017-03-02T17:42:23.951350: step 7793, loss 0.133906, acc 0.921875
2017-03-02T17:42:24.024677: step 7794, loss 0.250937, acc 0.890625
2017-03-02T17:42:24.113748: step 7795, loss 0.127088, acc 0.984375
2017-03-02T17:42:24.202164: step 7796, loss 0.206545, acc 0.921875
2017-03-02T17:42:24.275103: step 7797, loss 0.330552, acc 0.875
2017-03-02T17:42:24.347903: step 7798, loss 0.438447, acc 0.859375
2017-03-02T17:42:24.419244: step 7799, loss 0.103481, acc 0.96875
2017-03-02T17:42:24.493870: step 7800, loss 0.240497, acc 0.859375

Evaluation:
2017-03-02T17:42:24.526112: step 7800, loss 1.29011, acc 0.664023

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7800

2017-03-02T17:42:24.956553: step 7801, loss 0.345727, acc 0.84375
2017-03-02T17:42:25.026657: step 7802, loss 0.213849, acc 0.890625
2017-03-02T17:42:25.098792: step 7803, loss 0.20032, acc 0.921875
2017-03-02T17:42:25.170166: step 7804, loss 0.130004, acc 0.953125
2017-03-02T17:42:25.241840: step 7805, loss 0.294936, acc 0.90625
2017-03-02T17:42:25.324833: step 7806, loss 0.141162, acc 0.953125
2017-03-02T17:42:25.400005: step 7807, loss 0.274587, acc 0.90625
2017-03-02T17:42:25.476732: step 7808, loss 0.196474, acc 0.921875
2017-03-02T17:42:25.556603: step 7809, loss 0.136021, acc 0.953125
2017-03-02T17:42:25.642311: step 7810, loss 0.175148, acc 0.9375
2017-03-02T17:42:25.709921: step 7811, loss 0.300309, acc 0.875
2017-03-02T17:42:25.779433: step 7812, loss 0.335725, acc 0.828125
2017-03-02T17:42:25.851738: step 7813, loss 0.204031, acc 0.9375
2017-03-02T17:42:25.932314: step 7814, loss 0.1318, acc 0.953125
2017-03-02T17:42:26.007813: step 7815, loss 0.421764, acc 0.875
2017-03-02T17:42:26.086027: step 7816, loss 0.255793, acc 0.875
2017-03-02T17:42:26.163950: step 7817, loss 0.0934428, acc 0.96875
2017-03-02T17:42:26.234847: step 7818, loss 0.0968342, acc 0.96875
2017-03-02T17:42:26.307731: step 7819, loss 0.214281, acc 0.90625
2017-03-02T17:42:26.387898: step 7820, loss 0.272085, acc 0.90625
2017-03-02T17:42:26.457985: step 7821, loss 0.253592, acc 0.875
2017-03-02T17:42:26.523145: step 7822, loss 0.19066, acc 0.953125
2017-03-02T17:42:26.596381: step 7823, loss 0.136806, acc 0.953125
2017-03-02T17:42:26.669183: step 7824, loss 0.286185, acc 0.890625
2017-03-02T17:42:26.752065: step 7825, loss 0.238863, acc 0.890625
2017-03-02T17:42:26.827811: step 7826, loss 0.153758, acc 0.953125
2017-03-02T17:42:26.908296: step 7827, loss 0.194114, acc 0.9375
2017-03-02T17:42:26.992750: step 7828, loss 0.142156, acc 0.9375
2017-03-02T17:42:27.058435: step 7829, loss 0.124409, acc 0.96875
2017-03-02T17:42:27.131421: step 7830, loss 0.246423, acc 0.921875
2017-03-02T17:42:27.208930: step 7831, loss 0.219684, acc 0.921875
2017-03-02T17:42:27.284838: step 7832, loss 0.0951476, acc 0.984375
2017-03-02T17:42:27.370948: step 7833, loss 0.244238, acc 0.890625
2017-03-02T17:42:27.446653: step 7834, loss 0.264722, acc 0.875
2017-03-02T17:42:27.522535: step 7835, loss 0.211257, acc 0.921875
2017-03-02T17:42:27.594962: step 7836, loss 0.282222, acc 0.890625
2017-03-02T17:42:27.672584: step 7837, loss 0.0830665, acc 0.96875
2017-03-02T17:42:27.739818: step 7838, loss 0.176406, acc 0.953125
2017-03-02T17:42:27.811456: step 7839, loss 0.145247, acc 0.9375
2017-03-02T17:42:27.886995: step 7840, loss 0.00663013, acc 1
2017-03-02T17:42:27.964770: step 7841, loss 0.184222, acc 0.890625
2017-03-02T17:42:28.042158: step 7842, loss 0.0845975, acc 0.96875
2017-03-02T17:42:28.111981: step 7843, loss 0.115237, acc 0.953125
2017-03-02T17:42:28.179010: step 7844, loss 0.214235, acc 0.90625
2017-03-02T17:42:28.243722: step 7845, loss 0.126618, acc 0.9375
2017-03-02T17:42:28.314673: step 7846, loss 0.193089, acc 0.90625
2017-03-02T17:42:28.386067: step 7847, loss 0.186677, acc 0.890625
2017-03-02T17:42:28.456310: step 7848, loss 0.155917, acc 0.921875
2017-03-02T17:42:28.528746: step 7849, loss 0.168006, acc 0.921875
2017-03-02T17:42:28.620391: step 7850, loss 0.0418548, acc 1
2017-03-02T17:42:28.700151: step 7851, loss 0.309032, acc 0.859375
2017-03-02T17:42:28.774543: step 7852, loss 0.160466, acc 0.9375
2017-03-02T17:42:28.850715: step 7853, loss 0.104027, acc 0.953125
2017-03-02T17:42:28.924748: step 7854, loss 0.110357, acc 0.9375
2017-03-02T17:42:28.999125: step 7855, loss 0.242371, acc 0.875
2017-03-02T17:42:29.073784: step 7856, loss 0.131294, acc 0.953125
2017-03-02T17:42:29.142203: step 7857, loss 0.151724, acc 0.90625
2017-03-02T17:42:29.212320: step 7858, loss 0.128137, acc 0.921875
2017-03-02T17:42:29.288523: step 7859, loss 0.19393, acc 0.890625
2017-03-02T17:42:29.357029: step 7860, loss 0.159584, acc 0.921875
2017-03-02T17:42:29.432157: step 7861, loss 0.155893, acc 0.90625
2017-03-02T17:42:29.504800: step 7862, loss 0.125327, acc 0.921875
2017-03-02T17:42:29.581507: step 7863, loss 0.123897, acc 0.953125
2017-03-02T17:42:29.659069: step 7864, loss 0.0398695, acc 1
2017-03-02T17:42:29.730267: step 7865, loss 0.135313, acc 0.9375
2017-03-02T17:42:29.799140: step 7866, loss 0.193966, acc 0.953125
2017-03-02T17:42:29.867950: step 7867, loss 0.42383, acc 0.875
2017-03-02T17:42:29.940527: step 7868, loss 0.152541, acc 0.9375
2017-03-02T17:42:30.017115: step 7869, loss 0.189872, acc 0.9375
2017-03-02T17:42:30.091521: step 7870, loss 0.223903, acc 0.90625
2017-03-02T17:42:30.166929: step 7871, loss 0.190938, acc 0.90625
2017-03-02T17:42:30.244525: step 7872, loss 0.16006, acc 0.921875
2017-03-02T17:42:30.315936: step 7873, loss 0.216454, acc 0.921875
2017-03-02T17:42:30.392715: step 7874, loss 0.186063, acc 0.9375
2017-03-02T17:42:30.463621: step 7875, loss 0.229126, acc 0.90625
2017-03-02T17:42:30.535876: step 7876, loss 0.262889, acc 0.90625
2017-03-02T17:42:30.605900: step 7877, loss 0.234544, acc 0.90625
2017-03-02T17:42:30.678648: step 7878, loss 0.222021, acc 0.90625
2017-03-02T17:42:30.758053: step 7879, loss 0.111434, acc 0.96875
2017-03-02T17:42:30.833916: step 7880, loss 0.231263, acc 0.921875
2017-03-02T17:42:30.907081: step 7881, loss 0.137955, acc 0.953125
2017-03-02T17:42:30.985045: step 7882, loss 0.131416, acc 0.953125
2017-03-02T17:42:31.060673: step 7883, loss 0.109322, acc 0.953125
2017-03-02T17:42:31.143066: step 7884, loss 0.107329, acc 0.953125
2017-03-02T17:42:31.212695: step 7885, loss 0.18384, acc 0.953125
2017-03-02T17:42:31.284855: step 7886, loss 0.190693, acc 0.9375
2017-03-02T17:42:31.357996: step 7887, loss 0.119421, acc 0.953125
2017-03-02T17:42:31.433397: step 7888, loss 0.274635, acc 0.921875
2017-03-02T17:42:31.498744: step 7889, loss 0.23354, acc 0.90625
2017-03-02T17:42:31.574223: step 7890, loss 0.223591, acc 0.921875
2017-03-02T17:42:31.642748: step 7891, loss 0.12719, acc 0.953125
2017-03-02T17:42:31.710303: step 7892, loss 0.182934, acc 0.953125
2017-03-02T17:42:31.781114: step 7893, loss 0.167781, acc 0.921875
2017-03-02T17:42:31.853513: step 7894, loss 0.107775, acc 0.96875
2017-03-02T17:42:31.926510: step 7895, loss 0.207925, acc 0.921875
2017-03-02T17:42:31.999383: step 7896, loss 0.0641465, acc 1
2017-03-02T17:42:32.078618: step 7897, loss 0.163761, acc 0.9375
2017-03-02T17:42:32.158271: step 7898, loss 0.120669, acc 0.96875
2017-03-02T17:42:32.233736: step 7899, loss 0.203996, acc 0.921875
2017-03-02T17:42:32.308320: step 7900, loss 0.144964, acc 0.953125

Evaluation:
2017-03-02T17:42:32.341288: step 7900, loss 1.33136, acc 0.677001

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-7900

2017-03-02T17:42:32.788440: step 7901, loss 0.0994219, acc 0.96875
2017-03-02T17:42:32.855322: step 7902, loss 0.171742, acc 0.921875
2017-03-02T17:42:32.932806: step 7903, loss 0.221381, acc 0.890625
2017-03-02T17:42:33.012135: step 7904, loss 0.122855, acc 0.96875
2017-03-02T17:42:33.080560: step 7905, loss 0.162193, acc 0.9375
2017-03-02T17:42:33.153900: step 7906, loss 0.200193, acc 0.921875
2017-03-02T17:42:33.228207: step 7907, loss 0.239142, acc 0.90625
2017-03-02T17:42:33.295397: step 7908, loss 0.266553, acc 0.9375
2017-03-02T17:42:33.361917: step 7909, loss 0.181034, acc 0.953125
2017-03-02T17:42:33.438698: step 7910, loss 0.149446, acc 0.921875
2017-03-02T17:42:33.515728: step 7911, loss 0.207154, acc 0.921875
2017-03-02T17:42:33.586817: step 7912, loss 0.268675, acc 0.890625
2017-03-02T17:42:33.659628: step 7913, loss 0.13878, acc 0.953125
2017-03-02T17:42:33.735384: step 7914, loss 0.182859, acc 0.90625
2017-03-02T17:42:33.803968: step 7915, loss 0.184307, acc 0.921875
2017-03-02T17:42:33.877336: step 7916, loss 0.071794, acc 0.96875
2017-03-02T17:42:33.944371: step 7917, loss 0.211519, acc 0.9375
2017-03-02T17:42:34.015200: step 7918, loss 0.163529, acc 0.9375
2017-03-02T17:42:34.082638: step 7919, loss 0.182971, acc 0.9375
2017-03-02T17:42:34.159894: step 7920, loss 0.189156, acc 0.9375
2017-03-02T17:42:34.239571: step 7921, loss 0.208949, acc 0.890625
2017-03-02T17:42:34.314955: step 7922, loss 0.219935, acc 0.890625
2017-03-02T17:42:34.395306: step 7923, loss 0.256478, acc 0.921875
2017-03-02T17:42:34.471281: step 7924, loss 0.225272, acc 0.921875
2017-03-02T17:42:34.554129: step 7925, loss 0.186659, acc 0.90625
2017-03-02T17:42:34.630443: step 7926, loss 0.315349, acc 0.875
2017-03-02T17:42:34.698309: step 7927, loss 0.166304, acc 0.921875
2017-03-02T17:42:34.766526: step 7928, loss 0.0994607, acc 0.953125
2017-03-02T17:42:34.841578: step 7929, loss 0.241159, acc 0.9375
2017-03-02T17:42:34.915649: step 7930, loss 0.0648208, acc 1
2017-03-02T17:42:34.987804: step 7931, loss 0.0449359, acc 0.984375
2017-03-02T17:42:35.060449: step 7932, loss 0.130164, acc 0.953125
2017-03-02T17:42:35.132614: step 7933, loss 0.137695, acc 0.953125
2017-03-02T17:42:35.204599: step 7934, loss 0.125079, acc 0.9375
2017-03-02T17:42:35.277578: step 7935, loss 0.108561, acc 0.96875
2017-03-02T17:42:35.340087: step 7936, loss 0.241321, acc 0.9375
2017-03-02T17:42:35.407020: step 7937, loss 0.172786, acc 0.90625
2017-03-02T17:42:35.480421: step 7938, loss 0.078795, acc 0.96875
2017-03-02T17:42:35.573336: step 7939, loss 0.313409, acc 0.875
2017-03-02T17:42:35.646689: step 7940, loss 0.25269, acc 0.90625
2017-03-02T17:42:35.722370: step 7941, loss 0.164958, acc 0.9375
2017-03-02T17:42:35.801763: step 7942, loss 0.171307, acc 0.890625
2017-03-02T17:42:35.871991: step 7943, loss 0.195461, acc 0.921875
2017-03-02T17:42:35.977297: step 7944, loss 0.247859, acc 0.890625
2017-03-02T17:42:36.050029: step 7945, loss 0.0883327, acc 0.953125
2017-03-02T17:42:36.126609: step 7946, loss 0.239527, acc 0.890625
2017-03-02T17:42:36.208860: step 7947, loss 0.104082, acc 0.96875
2017-03-02T17:42:36.280359: step 7948, loss 0.224833, acc 0.875
2017-03-02T17:42:36.353856: step 7949, loss 0.229746, acc 0.921875
2017-03-02T17:42:36.422034: step 7950, loss 0.1466, acc 0.953125
2017-03-02T17:42:36.495582: step 7951, loss 0.107747, acc 0.9375
2017-03-02T17:42:36.568857: step 7952, loss 0.202952, acc 0.9375
2017-03-02T17:42:36.641128: step 7953, loss 0.279012, acc 0.84375
2017-03-02T17:42:36.709316: step 7954, loss 0.161201, acc 0.921875
2017-03-02T17:42:36.779060: step 7955, loss 0.220601, acc 0.890625
2017-03-02T17:42:36.843852: step 7956, loss 0.224762, acc 0.953125
2017-03-02T17:42:36.916131: step 7957, loss 0.318561, acc 0.859375
2017-03-02T17:42:36.989649: step 7958, loss 0.185389, acc 0.921875
2017-03-02T17:42:37.060507: step 7959, loss 0.112549, acc 0.953125
2017-03-02T17:42:37.136793: step 7960, loss 0.147216, acc 0.9375
2017-03-02T17:42:37.212429: step 7961, loss 0.221146, acc 0.90625
2017-03-02T17:42:37.282473: step 7962, loss 0.230116, acc 0.9375
2017-03-02T17:42:37.359649: step 7963, loss 0.363741, acc 0.859375
2017-03-02T17:42:37.432853: step 7964, loss 0.133107, acc 0.9375
2017-03-02T17:42:37.504621: step 7965, loss 0.251502, acc 0.90625
2017-03-02T17:42:37.576976: step 7966, loss 0.173401, acc 0.953125
2017-03-02T17:42:37.652330: step 7967, loss 0.163588, acc 0.921875
2017-03-02T17:42:37.722830: step 7968, loss 0.124578, acc 0.953125
2017-03-02T17:42:37.795258: step 7969, loss 0.254324, acc 0.90625
2017-03-02T17:42:37.867288: step 7970, loss 0.137619, acc 0.9375
2017-03-02T17:42:37.938337: step 7971, loss 0.097967, acc 0.9375
2017-03-02T17:42:38.015613: step 7972, loss 0.082024, acc 0.96875
2017-03-02T17:42:38.083910: step 7973, loss 0.150762, acc 0.9375
2017-03-02T17:42:38.153516: step 7974, loss 0.200428, acc 0.890625
2017-03-02T17:42:38.222636: step 7975, loss 0.256479, acc 0.90625
2017-03-02T17:42:38.294219: step 7976, loss 0.143457, acc 0.921875
2017-03-02T17:42:38.367148: step 7977, loss 0.190581, acc 0.90625
2017-03-02T17:42:38.440214: step 7978, loss 0.116197, acc 0.953125
2017-03-02T17:42:38.521311: step 7979, loss 0.217893, acc 0.90625
2017-03-02T17:42:38.587330: step 7980, loss 0.130606, acc 0.953125
2017-03-02T17:42:38.661467: step 7981, loss 0.249061, acc 0.90625
2017-03-02T17:42:38.737518: step 7982, loss 0.238005, acc 0.875
2017-03-02T17:42:38.806320: step 7983, loss 0.279741, acc 0.921875
2017-03-02T17:42:38.874346: step 7984, loss 0.297939, acc 0.9375
2017-03-02T17:42:38.948492: step 7985, loss 0.120007, acc 0.96875
2017-03-02T17:42:39.047121: step 7986, loss 0.0646689, acc 0.984375
2017-03-02T17:42:39.128857: step 7987, loss 0.114382, acc 0.953125
2017-03-02T17:42:39.194639: step 7988, loss 0.201486, acc 0.921875
2017-03-02T17:42:39.261172: step 7989, loss 0.3962, acc 0.84375
2017-03-02T17:42:39.336635: step 7990, loss 0.291885, acc 0.90625
2017-03-02T17:42:39.416149: step 7991, loss 0.208676, acc 0.9375
2017-03-02T17:42:39.486794: step 7992, loss 0.248073, acc 0.90625
2017-03-02T17:42:39.563144: step 7993, loss 0.19106, acc 0.9375
2017-03-02T17:42:39.631490: step 7994, loss 0.144693, acc 0.9375
2017-03-02T17:42:39.713478: step 7995, loss 0.146311, acc 0.921875
2017-03-02T17:42:39.785240: step 7996, loss 0.124191, acc 0.953125
2017-03-02T17:42:39.853796: step 7997, loss 0.290481, acc 0.84375
2017-03-02T17:42:39.928671: step 7998, loss 0.226521, acc 0.921875
2017-03-02T17:42:40.029460: step 7999, loss 0.297235, acc 0.921875
2017-03-02T17:42:40.113277: step 8000, loss 0.293421, acc 0.84375

Evaluation:
2017-03-02T17:42:40.144136: step 8000, loss 1.27409, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8000

2017-03-02T17:42:40.639565: step 8001, loss 0.0814686, acc 1
2017-03-02T17:42:40.744824: step 8002, loss 0.0998527, acc 0.96875
2017-03-02T17:42:40.817308: step 8003, loss 0.255666, acc 0.890625
2017-03-02T17:42:40.881370: step 8004, loss 0.240948, acc 0.90625
2017-03-02T17:42:40.950099: step 8005, loss 0.15589, acc 0.9375
2017-03-02T17:42:41.036578: step 8006, loss 0.193225, acc 0.90625
2017-03-02T17:42:41.105047: step 8007, loss 0.222303, acc 0.90625
2017-03-02T17:42:41.178543: step 8008, loss 0.0904437, acc 0.96875
2017-03-02T17:42:41.278206: step 8009, loss 0.117631, acc 0.96875
2017-03-02T17:42:41.351832: step 8010, loss 0.111185, acc 0.96875
2017-03-02T17:42:41.423496: step 8011, loss 0.128408, acc 0.9375
2017-03-02T17:42:41.494862: step 8012, loss 0.17181, acc 0.890625
2017-03-02T17:42:41.565302: step 8013, loss 0.0654791, acc 0.984375
2017-03-02T17:42:41.633829: step 8014, loss 0.103723, acc 0.953125
2017-03-02T17:42:41.705661: step 8015, loss 0.189292, acc 0.921875
2017-03-02T17:42:41.784835: step 8016, loss 0.234427, acc 0.84375
2017-03-02T17:42:41.853794: step 8017, loss 0.11803, acc 0.953125
2017-03-02T17:42:41.929269: step 8018, loss 0.116975, acc 0.953125
2017-03-02T17:42:42.012370: step 8019, loss 0.177712, acc 0.921875
2017-03-02T17:42:42.090498: step 8020, loss 0.0740727, acc 0.984375
2017-03-02T17:42:42.161409: step 8021, loss 0.0484464, acc 1
2017-03-02T17:42:42.229388: step 8022, loss 0.364135, acc 0.84375
2017-03-02T17:42:42.310396: step 8023, loss 0.0991671, acc 0.96875
2017-03-02T17:42:42.380938: step 8024, loss 0.181447, acc 0.953125
2017-03-02T17:42:42.462396: step 8025, loss 0.329947, acc 0.875
2017-03-02T17:42:42.535694: step 8026, loss 0.254973, acc 0.90625
2017-03-02T17:42:42.607054: step 8027, loss 0.213557, acc 0.890625
2017-03-02T17:42:42.675064: step 8028, loss 0.185866, acc 0.890625
2017-03-02T17:42:42.746897: step 8029, loss 0.175643, acc 0.90625
2017-03-02T17:42:42.827582: step 8030, loss 0.258934, acc 0.90625
2017-03-02T17:42:42.896532: step 8031, loss 0.182155, acc 0.9375
2017-03-02T17:42:42.962060: step 8032, loss 0.19767, acc 0.9375
2017-03-02T17:42:43.035271: step 8033, loss 0.170658, acc 0.953125
2017-03-02T17:42:43.109113: step 8034, loss 0.339316, acc 0.875
2017-03-02T17:42:43.183743: step 8035, loss 0.267967, acc 0.890625
2017-03-02T17:42:43.255790: step 8036, loss 0.0033252, acc 1
2017-03-02T17:42:43.330666: step 8037, loss 0.176914, acc 0.90625
2017-03-02T17:42:43.413263: step 8038, loss 0.17421, acc 0.90625
2017-03-02T17:42:43.491328: step 8039, loss 0.103929, acc 0.953125
2017-03-02T17:42:43.576418: step 8040, loss 0.137811, acc 0.9375
2017-03-02T17:42:43.645395: step 8041, loss 0.179284, acc 0.96875
2017-03-02T17:42:43.720801: step 8042, loss 0.296971, acc 0.859375
2017-03-02T17:42:43.800432: step 8043, loss 0.15497, acc 0.9375
2017-03-02T17:42:43.880738: step 8044, loss 0.24782, acc 0.890625
2017-03-02T17:42:43.950360: step 8045, loss 0.19519, acc 0.90625
2017-03-02T17:42:44.024139: step 8046, loss 0.12863, acc 0.9375
2017-03-02T17:42:44.104402: step 8047, loss 0.0738787, acc 0.96875
2017-03-02T17:42:44.176510: step 8048, loss 0.0275493, acc 1
2017-03-02T17:42:44.246178: step 8049, loss 0.175297, acc 0.953125
2017-03-02T17:42:44.319779: step 8050, loss 0.175331, acc 0.90625
2017-03-02T17:42:44.390153: step 8051, loss 0.240584, acc 0.890625
2017-03-02T17:42:44.463453: step 8052, loss 0.180685, acc 0.9375
2017-03-02T17:42:44.537480: step 8053, loss 0.182401, acc 0.9375
2017-03-02T17:42:44.624689: step 8054, loss 0.120767, acc 0.953125
2017-03-02T17:42:44.697238: step 8055, loss 0.150474, acc 0.9375
2017-03-02T17:42:44.775962: step 8056, loss 0.188464, acc 0.921875
2017-03-02T17:42:44.848973: step 8057, loss 0.215601, acc 0.9375
2017-03-02T17:42:44.917039: step 8058, loss 0.341495, acc 0.890625
2017-03-02T17:42:44.991094: step 8059, loss 0.186054, acc 0.9375
2017-03-02T17:42:45.061947: step 8060, loss 0.134913, acc 0.9375
2017-03-02T17:42:45.141552: step 8061, loss 0.189836, acc 0.9375
2017-03-02T17:42:45.208627: step 8062, loss 0.0976663, acc 0.953125
2017-03-02T17:42:45.280039: step 8063, loss 0.207392, acc 0.921875
2017-03-02T17:42:45.350515: step 8064, loss 0.236477, acc 0.890625
2017-03-02T17:42:45.424338: step 8065, loss 0.177292, acc 0.9375
2017-03-02T17:42:45.500135: step 8066, loss 0.0983924, acc 0.96875
2017-03-02T17:42:45.572710: step 8067, loss 0.136431, acc 0.890625
2017-03-02T17:42:45.639334: step 8068, loss 0.156573, acc 0.953125
2017-03-02T17:42:45.710150: step 8069, loss 0.206237, acc 0.859375
2017-03-02T17:42:45.804403: step 8070, loss 0.24604, acc 0.921875
2017-03-02T17:42:45.869749: step 8071, loss 0.104319, acc 0.96875
2017-03-02T17:42:45.940568: step 8072, loss 0.34473, acc 0.90625
2017-03-02T17:42:46.039790: step 8073, loss 0.202509, acc 0.921875
2017-03-02T17:42:46.110075: step 8074, loss 0.159907, acc 0.96875
2017-03-02T17:42:46.195101: step 8075, loss 0.334721, acc 0.875
2017-03-02T17:42:46.266118: step 8076, loss 0.201882, acc 0.890625
2017-03-02T17:42:46.342669: step 8077, loss 0.167842, acc 0.921875
2017-03-02T17:42:46.406904: step 8078, loss 0.15376, acc 0.9375
2017-03-02T17:42:46.480425: step 8079, loss 0.215515, acc 0.90625
2017-03-02T17:42:46.558328: step 8080, loss 0.202248, acc 0.9375
2017-03-02T17:42:46.635216: step 8081, loss 0.222783, acc 0.890625
2017-03-02T17:42:46.709083: step 8082, loss 0.239211, acc 0.921875
2017-03-02T17:42:46.780899: step 8083, loss 0.135185, acc 0.9375
2017-03-02T17:42:46.852999: step 8084, loss 0.142006, acc 0.96875
2017-03-02T17:42:46.933184: step 8085, loss 0.126771, acc 0.9375
2017-03-02T17:42:46.998877: step 8086, loss 0.179073, acc 0.90625
2017-03-02T17:42:47.067498: step 8087, loss 0.142555, acc 0.921875
2017-03-02T17:42:47.141651: step 8088, loss 0.222298, acc 0.9375
2017-03-02T17:42:47.216015: step 8089, loss 0.24576, acc 0.859375
2017-03-02T17:42:47.296276: step 8090, loss 0.147174, acc 0.9375
2017-03-02T17:42:47.370534: step 8091, loss 0.208142, acc 0.890625
2017-03-02T17:42:47.443172: step 8092, loss 0.0827654, acc 0.953125
2017-03-02T17:42:47.513414: step 8093, loss 0.0712432, acc 0.984375
2017-03-02T17:42:47.583053: step 8094, loss 0.0967775, acc 0.9375
2017-03-02T17:42:47.649129: step 8095, loss 0.111391, acc 0.96875
2017-03-02T17:42:47.722253: step 8096, loss 0.11854, acc 0.953125
2017-03-02T17:42:47.791364: step 8097, loss 0.126398, acc 0.9375
2017-03-02T17:42:47.875334: step 8098, loss 0.202989, acc 0.9375
2017-03-02T17:42:47.948558: step 8099, loss 0.0757617, acc 0.953125
2017-03-02T17:42:48.016924: step 8100, loss 0.0945413, acc 0.96875

Evaluation:
2017-03-02T17:42:48.049842: step 8100, loss 1.34326, acc 0.680606

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8100

2017-03-02T17:42:48.504194: step 8101, loss 0.126452, acc 0.921875
2017-03-02T17:42:48.583662: step 8102, loss 0.236417, acc 0.921875
2017-03-02T17:42:48.649902: step 8103, loss 0.132433, acc 0.96875
2017-03-02T17:42:48.731196: step 8104, loss 0.194848, acc 0.921875
2017-03-02T17:42:48.800464: step 8105, loss 0.197567, acc 0.953125
2017-03-02T17:42:48.881529: step 8106, loss 0.0891587, acc 0.953125
2017-03-02T17:42:48.956563: step 8107, loss 0.168237, acc 0.921875
2017-03-02T17:42:49.029074: step 8108, loss 0.216573, acc 0.921875
2017-03-02T17:42:49.087842: step 8109, loss 0.22163, acc 0.921875
2017-03-02T17:42:49.157650: step 8110, loss 0.232654, acc 0.90625
2017-03-02T17:42:49.226907: step 8111, loss 0.171524, acc 0.90625
2017-03-02T17:42:49.308607: step 8112, loss 0.0556828, acc 1
2017-03-02T17:42:49.384752: step 8113, loss 0.101614, acc 0.953125
2017-03-02T17:42:49.458663: step 8114, loss 0.0975399, acc 0.984375
2017-03-02T17:42:49.531287: step 8115, loss 0.148646, acc 0.96875
2017-03-02T17:42:49.598325: step 8116, loss 0.179531, acc 0.90625
2017-03-02T17:42:49.677918: step 8117, loss 0.113613, acc 0.96875
2017-03-02T17:42:49.755707: step 8118, loss 0.178494, acc 0.890625
2017-03-02T17:42:49.828003: step 8119, loss 0.172195, acc 0.9375
2017-03-02T17:42:49.904123: step 8120, loss 0.131466, acc 0.953125
2017-03-02T17:42:49.975748: step 8121, loss 0.0748121, acc 0.96875
2017-03-02T17:42:50.050188: step 8122, loss 0.138487, acc 0.953125
2017-03-02T17:42:50.129584: step 8123, loss 0.243086, acc 0.921875
2017-03-02T17:42:50.207471: step 8124, loss 0.160928, acc 0.953125
2017-03-02T17:42:50.275592: step 8125, loss 0.143754, acc 0.9375
2017-03-02T17:42:50.348821: step 8126, loss 0.162341, acc 0.890625
2017-03-02T17:42:50.430217: step 8127, loss 0.0911921, acc 0.96875
2017-03-02T17:42:50.501326: step 8128, loss 0.142186, acc 0.953125
2017-03-02T17:42:50.581466: step 8129, loss 0.364732, acc 0.890625
2017-03-02T17:42:50.655419: step 8130, loss 0.255916, acc 0.890625
2017-03-02T17:42:50.730503: step 8131, loss 0.261015, acc 0.921875
2017-03-02T17:42:50.801532: step 8132, loss 0.317982, acc 0.875
2017-03-02T17:42:50.874337: step 8133, loss 0.113056, acc 0.9375
2017-03-02T17:42:50.949808: step 8134, loss 0.153437, acc 0.90625
2017-03-02T17:42:51.020115: step 8135, loss 0.0619195, acc 0.984375
2017-03-02T17:42:51.092839: step 8136, loss 0.105509, acc 0.984375
2017-03-02T17:42:51.158296: step 8137, loss 0.26117, acc 0.84375
2017-03-02T17:42:51.231360: step 8138, loss 0.146587, acc 0.9375
2017-03-02T17:42:51.305189: step 8139, loss 0.101357, acc 0.953125
2017-03-02T17:42:51.385225: step 8140, loss 0.268725, acc 0.890625
2017-03-02T17:42:51.462096: step 8141, loss 0.151337, acc 0.953125
2017-03-02T17:42:51.534535: step 8142, loss 0.144893, acc 0.953125
2017-03-02T17:42:51.605952: step 8143, loss 0.246655, acc 0.921875
2017-03-02T17:42:51.684214: step 8144, loss 0.17643, acc 0.9375
2017-03-02T17:42:51.751101: step 8145, loss 0.313606, acc 0.890625
2017-03-02T17:42:51.817574: step 8146, loss 0.162393, acc 0.90625
2017-03-02T17:42:51.889085: step 8147, loss 0.174288, acc 0.921875
2017-03-02T17:42:51.962807: step 8148, loss 0.173298, acc 0.9375
2017-03-02T17:42:52.037973: step 8149, loss 0.150496, acc 0.9375
2017-03-02T17:42:52.110063: step 8150, loss 0.159025, acc 0.953125
2017-03-02T17:42:52.182809: step 8151, loss 0.13847, acc 0.9375
2017-03-02T17:42:52.253465: step 8152, loss 0.157315, acc 0.9375
2017-03-02T17:42:52.332115: step 8153, loss 0.14406, acc 0.90625
2017-03-02T17:42:52.397440: step 8154, loss 0.253649, acc 0.875
2017-03-02T17:42:52.470149: step 8155, loss 0.261598, acc 0.921875
2017-03-02T17:42:52.545962: step 8156, loss 0.168049, acc 0.921875
2017-03-02T17:42:52.615835: step 8157, loss 0.168694, acc 0.9375
2017-03-02T17:42:52.687469: step 8158, loss 0.211025, acc 0.890625
2017-03-02T17:42:52.759680: step 8159, loss 0.0828623, acc 0.921875
2017-03-02T17:42:52.832437: step 8160, loss 0.200184, acc 0.90625
2017-03-02T17:42:52.905071: step 8161, loss 0.123155, acc 0.96875
2017-03-02T17:42:52.986882: step 8162, loss 0.0878842, acc 0.96875
2017-03-02T17:42:53.065510: step 8163, loss 0.18577, acc 0.9375
2017-03-02T17:42:53.143318: step 8164, loss 0.143217, acc 0.953125
2017-03-02T17:42:53.217897: step 8165, loss 0.127252, acc 0.9375
2017-03-02T17:42:53.290612: step 8166, loss 0.2643, acc 0.859375
2017-03-02T17:42:53.369833: step 8167, loss 0.317021, acc 0.921875
2017-03-02T17:42:53.442243: step 8168, loss 0.317062, acc 0.859375
2017-03-02T17:42:53.516043: step 8169, loss 0.134144, acc 0.921875
2017-03-02T17:42:53.586137: step 8170, loss 0.0913879, acc 0.96875
2017-03-02T17:42:53.659206: step 8171, loss 0.304691, acc 0.921875
2017-03-02T17:42:53.731585: step 8172, loss 0.214273, acc 0.9375
2017-03-02T17:42:53.806478: step 8173, loss 0.246803, acc 0.890625
2017-03-02T17:42:53.875832: step 8174, loss 0.207312, acc 0.875
2017-03-02T17:42:53.946554: step 8175, loss 0.199072, acc 0.9375
2017-03-02T17:42:54.025580: step 8176, loss 0.230591, acc 0.90625
2017-03-02T17:42:54.101854: step 8177, loss 0.224425, acc 0.90625
2017-03-02T17:42:54.187745: step 8178, loss 0.178199, acc 0.921875
2017-03-02T17:42:54.260472: step 8179, loss 0.155221, acc 0.921875
2017-03-02T17:42:54.331670: step 8180, loss 0.258273, acc 0.90625
2017-03-02T17:42:54.399818: step 8181, loss 0.130964, acc 0.9375
2017-03-02T17:42:54.464048: step 8182, loss 0.320478, acc 0.921875
2017-03-02T17:42:54.547212: step 8183, loss 0.266218, acc 0.90625
2017-03-02T17:42:54.609431: step 8184, loss 0.174277, acc 0.9375
2017-03-02T17:42:54.679268: step 8185, loss 0.195023, acc 0.921875
2017-03-02T17:42:54.755700: step 8186, loss 0.137092, acc 0.953125
2017-03-02T17:42:54.839172: step 8187, loss 0.203848, acc 0.90625
2017-03-02T17:42:54.909707: step 8188, loss 0.196169, acc 0.90625
2017-03-02T17:42:54.987860: step 8189, loss 0.1427, acc 0.953125
2017-03-02T17:42:55.059798: step 8190, loss 0.206071, acc 0.921875
2017-03-02T17:42:55.133345: step 8191, loss 0.241194, acc 0.90625
2017-03-02T17:42:55.211887: step 8192, loss 0.290588, acc 0.890625
2017-03-02T17:42:55.279369: step 8193, loss 0.0975516, acc 0.953125
2017-03-02T17:42:55.342649: step 8194, loss 0.256038, acc 0.921875
2017-03-02T17:42:55.419308: step 8195, loss 0.151963, acc 0.90625
2017-03-02T17:42:55.491991: step 8196, loss 0.144063, acc 0.96875
2017-03-02T17:42:55.568633: step 8197, loss 0.0914106, acc 0.953125
2017-03-02T17:42:55.646872: step 8198, loss 0.201818, acc 0.921875
2017-03-02T17:42:55.719292: step 8199, loss 0.17185, acc 0.921875
2017-03-02T17:42:55.802933: step 8200, loss 0.20539, acc 0.9375

Evaluation:
2017-03-02T17:42:55.842384: step 8200, loss 1.31594, acc 0.66186

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8200

2017-03-02T17:42:56.300541: step 8201, loss 0.170801, acc 0.9375
2017-03-02T17:42:56.373543: step 8202, loss 0.224704, acc 0.90625
2017-03-02T17:42:56.444658: step 8203, loss 0.305957, acc 0.90625
2017-03-02T17:42:56.517826: step 8204, loss 0.197437, acc 0.90625
2017-03-02T17:42:56.594359: step 8205, loss 0.300498, acc 0.890625
2017-03-02T17:42:56.666113: step 8206, loss 0.153028, acc 0.90625
2017-03-02T17:42:56.732106: step 8207, loss 0.18556, acc 0.9375
2017-03-02T17:42:56.809778: step 8208, loss 0.18479, acc 0.921875
2017-03-02T17:42:56.883852: step 8209, loss 0.127834, acc 0.9375
2017-03-02T17:42:56.959612: step 8210, loss 0.237412, acc 0.890625
2017-03-02T17:42:57.033580: step 8211, loss 0.206299, acc 0.875
2017-03-02T17:42:57.110879: step 8212, loss 0.208192, acc 0.890625
2017-03-02T17:42:57.184482: step 8213, loss 0.143465, acc 0.953125
2017-03-02T17:42:57.259500: step 8214, loss 0.293182, acc 0.90625
2017-03-02T17:42:57.339752: step 8215, loss 0.25644, acc 0.90625
2017-03-02T17:42:57.407035: step 8216, loss 0.242708, acc 0.9375
2017-03-02T17:42:57.475937: step 8217, loss 0.171493, acc 0.90625
2017-03-02T17:42:57.548180: step 8218, loss 0.341649, acc 0.859375
2017-03-02T17:42:57.620538: step 8219, loss 0.140152, acc 0.9375
2017-03-02T17:42:57.700005: step 8220, loss 0.195903, acc 0.890625
2017-03-02T17:42:57.775425: step 8221, loss 0.21481, acc 0.875
2017-03-02T17:42:57.837060: step 8222, loss 0.17994, acc 0.921875
2017-03-02T17:42:57.908745: step 8223, loss 0.179291, acc 0.921875
2017-03-02T17:42:57.982228: step 8224, loss 0.11995, acc 0.9375
2017-03-02T17:42:58.055464: step 8225, loss 0.197859, acc 0.9375
2017-03-02T17:42:58.128086: step 8226, loss 0.115511, acc 0.96875
2017-03-02T17:42:58.209500: step 8227, loss 0.108202, acc 0.984375
2017-03-02T17:42:58.280861: step 8228, loss 0.261092, acc 0.90625
2017-03-02T17:42:58.355192: step 8229, loss 0.28925, acc 0.859375
2017-03-02T17:42:58.427407: step 8230, loss 0.227473, acc 0.890625
2017-03-02T17:42:58.495758: step 8231, loss 0.221654, acc 0.9375
2017-03-02T17:42:58.564795: step 8232, loss 0.00448362, acc 1
2017-03-02T17:42:58.648660: step 8233, loss 0.159486, acc 0.90625
2017-03-02T17:42:58.740346: step 8234, loss 0.148741, acc 0.9375
2017-03-02T17:42:58.808403: step 8235, loss 0.119276, acc 0.953125
2017-03-02T17:42:58.877005: step 8236, loss 0.137745, acc 0.96875
2017-03-02T17:42:58.955305: step 8237, loss 0.369223, acc 0.84375
2017-03-02T17:42:59.024226: step 8238, loss 0.151682, acc 0.921875
2017-03-02T17:42:59.093615: step 8239, loss 0.131944, acc 0.96875
2017-03-02T17:42:59.168687: step 8240, loss 0.0854714, acc 0.96875
2017-03-02T17:42:59.242366: step 8241, loss 0.266901, acc 0.90625
2017-03-02T17:42:59.312092: step 8242, loss 0.104708, acc 0.9375
2017-03-02T17:42:59.387540: step 8243, loss 0.124223, acc 0.96875
2017-03-02T17:42:59.454057: step 8244, loss 0.171878, acc 0.9375
2017-03-02T17:42:59.528438: step 8245, loss 0.180794, acc 0.921875
2017-03-02T17:42:59.604824: step 8246, loss 0.0858739, acc 0.96875
2017-03-02T17:42:59.681746: step 8247, loss 0.110099, acc 0.96875
2017-03-02T17:42:59.755817: step 8248, loss 0.0744715, acc 0.96875
2017-03-02T17:42:59.835882: step 8249, loss 0.170875, acc 0.953125
2017-03-02T17:42:59.919809: step 8250, loss 0.116365, acc 0.9375
2017-03-02T17:42:59.993409: step 8251, loss 0.220502, acc 0.890625
2017-03-02T17:43:00.069024: step 8252, loss 0.104993, acc 0.953125
2017-03-02T17:43:00.138902: step 8253, loss 0.298064, acc 0.828125
2017-03-02T17:43:00.209603: step 8254, loss 0.201438, acc 0.921875
2017-03-02T17:43:00.282086: step 8255, loss 0.219233, acc 0.953125
2017-03-02T17:43:00.360190: step 8256, loss 0.158235, acc 0.9375
2017-03-02T17:43:00.440638: step 8257, loss 0.153102, acc 0.9375
2017-03-02T17:43:00.511774: step 8258, loss 0.1565, acc 0.984375
2017-03-02T17:43:00.591550: step 8259, loss 0.0994719, acc 0.9375
2017-03-02T17:43:00.666645: step 8260, loss 0.125609, acc 0.953125
2017-03-02T17:43:00.744597: step 8261, loss 0.229829, acc 0.90625
2017-03-02T17:43:00.819150: step 8262, loss 0.124765, acc 0.9375
2017-03-02T17:43:00.895086: step 8263, loss 0.167315, acc 0.9375
2017-03-02T17:43:00.961660: step 8264, loss 0.0682105, acc 0.984375
2017-03-02T17:43:01.031339: step 8265, loss 0.143788, acc 0.9375
2017-03-02T17:43:01.115201: step 8266, loss 0.113607, acc 0.9375
2017-03-02T17:43:01.189206: step 8267, loss 0.133409, acc 0.9375
2017-03-02T17:43:01.261651: step 8268, loss 0.101059, acc 0.96875
2017-03-02T17:43:01.332011: step 8269, loss 0.221734, acc 0.90625
2017-03-02T17:43:01.409450: step 8270, loss 0.1858, acc 0.9375
2017-03-02T17:43:01.474089: step 8271, loss 0.213493, acc 0.921875
2017-03-02T17:43:01.546782: step 8272, loss 0.168266, acc 0.90625
2017-03-02T17:43:01.623553: step 8273, loss 0.073737, acc 0.96875
2017-03-02T17:43:01.696675: step 8274, loss 0.0710398, acc 0.984375
2017-03-02T17:43:01.772335: step 8275, loss 0.286706, acc 0.890625
2017-03-02T17:43:01.842297: step 8276, loss 0.201872, acc 0.90625
2017-03-02T17:43:01.929728: step 8277, loss 0.216717, acc 0.953125
2017-03-02T17:43:01.999470: step 8278, loss 0.0791018, acc 0.96875
2017-03-02T17:43:02.069072: step 8279, loss 0.129234, acc 0.90625
2017-03-02T17:43:02.143354: step 8280, loss 0.265092, acc 0.90625
2017-03-02T17:43:02.207298: step 8281, loss 0.155892, acc 0.9375
2017-03-02T17:43:02.274008: step 8282, loss 0.157297, acc 0.953125
2017-03-02T17:43:02.348819: step 8283, loss 0.142305, acc 0.96875
2017-03-02T17:43:02.419666: step 8284, loss 0.127211, acc 0.96875
2017-03-02T17:43:02.491477: step 8285, loss 0.218041, acc 0.921875
2017-03-02T17:43:02.562918: step 8286, loss 0.168303, acc 0.921875
2017-03-02T17:43:02.635953: step 8287, loss 0.201911, acc 0.90625
2017-03-02T17:43:02.707366: step 8288, loss 0.240408, acc 0.921875
2017-03-02T17:43:02.780713: step 8289, loss 0.21257, acc 0.921875
2017-03-02T17:43:02.848765: step 8290, loss 0.12379, acc 0.96875
2017-03-02T17:43:02.917319: step 8291, loss 0.114434, acc 0.921875
2017-03-02T17:43:03.006657: step 8292, loss 0.196205, acc 0.90625
2017-03-02T17:43:03.078377: step 8293, loss 0.175546, acc 0.921875
2017-03-02T17:43:03.144385: step 8294, loss 0.168991, acc 0.953125
2017-03-02T17:43:03.219860: step 8295, loss 0.172721, acc 0.921875
2017-03-02T17:43:03.313757: step 8296, loss 0.138339, acc 0.953125
2017-03-02T17:43:03.392165: step 8297, loss 0.149911, acc 0.921875
2017-03-02T17:43:03.463006: step 8298, loss 0.17323, acc 0.921875
2017-03-02T17:43:03.536805: step 8299, loss 0.242057, acc 0.875
2017-03-02T17:43:03.611081: step 8300, loss 0.19451, acc 0.90625

Evaluation:
2017-03-02T17:43:03.643784: step 8300, loss 1.32524, acc 0.663302

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8300

2017-03-02T17:43:04.189919: step 8301, loss 0.195863, acc 0.90625
2017-03-02T17:43:04.254688: step 8302, loss 0.176536, acc 0.921875
2017-03-02T17:43:04.324579: step 8303, loss 0.164187, acc 0.9375
2017-03-02T17:43:04.395958: step 8304, loss 0.163402, acc 0.9375
2017-03-02T17:43:04.468442: step 8305, loss 0.189128, acc 0.921875
2017-03-02T17:43:04.547771: step 8306, loss 0.215113, acc 0.921875
2017-03-02T17:43:04.615707: step 8307, loss 0.130883, acc 0.90625
2017-03-02T17:43:04.686958: step 8308, loss 0.140067, acc 0.921875
2017-03-02T17:43:04.756009: step 8309, loss 0.160507, acc 0.953125
2017-03-02T17:43:04.824062: step 8310, loss 0.158193, acc 0.90625
2017-03-02T17:43:04.904546: step 8311, loss 0.209021, acc 0.890625
2017-03-02T17:43:04.972280: step 8312, loss 0.134292, acc 0.9375
2017-03-02T17:43:05.038564: step 8313, loss 0.242473, acc 0.921875
2017-03-02T17:43:05.107304: step 8314, loss 0.187767, acc 0.953125
2017-03-02T17:43:05.190058: step 8315, loss 0.109265, acc 0.96875
2017-03-02T17:43:05.261355: step 8316, loss 0.158939, acc 0.9375
2017-03-02T17:43:05.334083: step 8317, loss 0.125876, acc 0.953125
2017-03-02T17:43:05.404024: step 8318, loss 0.243909, acc 0.890625
2017-03-02T17:43:05.473963: step 8319, loss 0.28143, acc 0.875
2017-03-02T17:43:05.537313: step 8320, loss 0.110748, acc 0.96875
2017-03-02T17:43:05.609060: step 8321, loss 0.399693, acc 0.875
2017-03-02T17:43:05.677628: step 8322, loss 0.159839, acc 0.90625
2017-03-02T17:43:05.747743: step 8323, loss 0.288198, acc 0.921875
2017-03-02T17:43:05.821453: step 8324, loss 0.225521, acc 0.90625
2017-03-02T17:43:05.898343: step 8325, loss 0.174326, acc 0.921875
2017-03-02T17:43:05.976736: step 8326, loss 0.209136, acc 0.921875
2017-03-02T17:43:06.082253: step 8327, loss 0.0418772, acc 1
2017-03-02T17:43:06.160091: step 8328, loss 0.143227, acc 0.921875
2017-03-02T17:43:06.235058: step 8329, loss 0.0918565, acc 0.9375
2017-03-02T17:43:06.306516: step 8330, loss 0.196162, acc 0.90625
2017-03-02T17:43:06.377879: step 8331, loss 0.151887, acc 0.9375
2017-03-02T17:43:06.447645: step 8332, loss 0.264773, acc 0.9375
2017-03-02T17:43:06.522081: step 8333, loss 0.200146, acc 0.90625
2017-03-02T17:43:06.604945: step 8334, loss 0.116976, acc 0.953125
2017-03-02T17:43:06.697020: step 8335, loss 0.0767254, acc 0.96875
2017-03-02T17:43:06.774712: step 8336, loss 0.131254, acc 0.9375
2017-03-02T17:43:06.846432: step 8337, loss 0.119002, acc 0.9375
2017-03-02T17:43:06.918250: step 8338, loss 0.083634, acc 0.984375
2017-03-02T17:43:06.989166: step 8339, loss 0.114598, acc 0.953125
2017-03-02T17:43:07.059213: step 8340, loss 0.245974, acc 0.859375
2017-03-02T17:43:07.133168: step 8341, loss 0.17128, acc 0.9375
2017-03-02T17:43:07.207692: step 8342, loss 0.368498, acc 0.875
2017-03-02T17:43:07.282279: step 8343, loss 0.218486, acc 0.90625
2017-03-02T17:43:07.356059: step 8344, loss 0.237374, acc 0.890625
2017-03-02T17:43:07.426598: step 8345, loss 0.104335, acc 0.96875
2017-03-02T17:43:07.500971: step 8346, loss 0.235006, acc 0.890625
2017-03-02T17:43:07.579106: step 8347, loss 0.0938083, acc 0.953125
2017-03-02T17:43:07.649909: step 8348, loss 0.23348, acc 0.921875
2017-03-02T17:43:07.719604: step 8349, loss 0.0789469, acc 0.96875
2017-03-02T17:43:07.790565: step 8350, loss 0.123402, acc 0.953125
2017-03-02T17:43:07.874029: step 8351, loss 0.0910903, acc 0.953125
2017-03-02T17:43:07.955065: step 8352, loss 0.249536, acc 0.921875
2017-03-02T17:43:08.023657: step 8353, loss 0.176917, acc 0.953125
2017-03-02T17:43:08.099539: step 8354, loss 0.296647, acc 0.84375
2017-03-02T17:43:08.173339: step 8355, loss 0.284789, acc 0.875
2017-03-02T17:43:08.245803: step 8356, loss 0.219449, acc 0.921875
2017-03-02T17:43:08.322247: step 8357, loss 0.180841, acc 0.9375
2017-03-02T17:43:08.400487: step 8358, loss 0.128251, acc 0.9375
2017-03-02T17:43:08.467311: step 8359, loss 0.104792, acc 0.96875
2017-03-02T17:43:08.533653: step 8360, loss 0.229684, acc 0.921875
2017-03-02T17:43:08.601165: step 8361, loss 0.104515, acc 0.96875
2017-03-02T17:43:08.681028: step 8362, loss 0.129759, acc 0.9375
2017-03-02T17:43:08.761788: step 8363, loss 0.156633, acc 0.90625
2017-03-02T17:43:08.827558: step 8364, loss 0.131013, acc 0.9375
2017-03-02T17:43:08.901353: step 8365, loss 0.23878, acc 0.921875
2017-03-02T17:43:08.968471: step 8366, loss 0.161705, acc 0.921875
2017-03-02T17:43:09.034715: step 8367, loss 0.235805, acc 0.875
2017-03-02T17:43:09.105977: step 8368, loss 0.265923, acc 0.875
2017-03-02T17:43:09.178477: step 8369, loss 0.255683, acc 0.921875
2017-03-02T17:43:09.249277: step 8370, loss 0.212935, acc 0.953125
2017-03-02T17:43:09.318362: step 8371, loss 0.169535, acc 0.90625
2017-03-02T17:43:09.398697: step 8372, loss 0.269734, acc 0.921875
2017-03-02T17:43:09.472805: step 8373, loss 0.133169, acc 0.921875
2017-03-02T17:43:09.547160: step 8374, loss 0.24739, acc 0.921875
2017-03-02T17:43:09.617950: step 8375, loss 0.182752, acc 0.9375
2017-03-02T17:43:09.694749: step 8376, loss 0.0985279, acc 0.96875
2017-03-02T17:43:09.778360: step 8377, loss 0.351347, acc 0.859375
2017-03-02T17:43:09.848326: step 8378, loss 0.211669, acc 0.9375
2017-03-02T17:43:09.916849: step 8379, loss 0.247338, acc 0.890625
2017-03-02T17:43:09.989621: step 8380, loss 0.243902, acc 0.890625
2017-03-02T17:43:10.069088: step 8381, loss 0.20859, acc 0.921875
2017-03-02T17:43:10.146983: step 8382, loss 0.0972145, acc 0.96875
2017-03-02T17:43:10.226702: step 8383, loss 0.241355, acc 0.890625
2017-03-02T17:43:10.302523: step 8384, loss 0.131972, acc 0.9375
2017-03-02T17:43:10.376526: step 8385, loss 0.268313, acc 0.890625
2017-03-02T17:43:10.447294: step 8386, loss 0.241741, acc 0.875
2017-03-02T17:43:10.516298: step 8387, loss 0.211179, acc 0.890625
2017-03-02T17:43:10.587758: step 8388, loss 0.1513, acc 0.921875
2017-03-02T17:43:10.662166: step 8389, loss 0.190702, acc 0.96875
2017-03-02T17:43:10.734995: step 8390, loss 0.173623, acc 0.921875
2017-03-02T17:43:10.809834: step 8391, loss 0.176731, acc 0.921875
2017-03-02T17:43:10.887721: step 8392, loss 0.257055, acc 0.890625
2017-03-02T17:43:10.961188: step 8393, loss 0.143272, acc 0.953125
2017-03-02T17:43:11.033375: step 8394, loss 0.190607, acc 0.890625
2017-03-02T17:43:11.104863: step 8395, loss 0.18253, acc 0.921875
2017-03-02T17:43:11.173637: step 8396, loss 0.190781, acc 0.9375
2017-03-02T17:43:11.250068: step 8397, loss 0.1998, acc 0.90625
2017-03-02T17:43:11.316424: step 8398, loss 0.0440067, acc 0.984375
2017-03-02T17:43:11.386171: step 8399, loss 0.166729, acc 0.90625
2017-03-02T17:43:11.465604: step 8400, loss 0.168207, acc 0.953125

Evaluation:
2017-03-02T17:43:11.502525: step 8400, loss 1.35904, acc 0.682769

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8400

2017-03-02T17:43:11.987675: step 8401, loss 0.138188, acc 0.953125
2017-03-02T17:43:12.062780: step 8402, loss 0.225208, acc 0.921875
2017-03-02T17:43:12.133969: step 8403, loss 0.203446, acc 0.921875
2017-03-02T17:43:12.207796: step 8404, loss 0.25002, acc 0.921875
2017-03-02T17:43:12.275756: step 8405, loss 0.353568, acc 0.828125
2017-03-02T17:43:12.350053: step 8406, loss 0.102813, acc 0.984375
2017-03-02T17:43:12.423079: step 8407, loss 0.244337, acc 0.875
2017-03-02T17:43:12.496498: step 8408, loss 0.125109, acc 0.953125
2017-03-02T17:43:12.563196: step 8409, loss 0.208413, acc 0.9375
2017-03-02T17:43:12.629908: step 8410, loss 0.0817554, acc 0.96875
2017-03-02T17:43:12.704306: step 8411, loss 0.166607, acc 0.953125
2017-03-02T17:43:12.775575: step 8412, loss 0.160867, acc 0.953125
2017-03-02T17:43:12.847464: step 8413, loss 0.185024, acc 0.953125
2017-03-02T17:43:12.919710: step 8414, loss 0.354826, acc 0.875
2017-03-02T17:43:12.992177: step 8415, loss 0.19317, acc 0.90625
2017-03-02T17:43:13.063757: step 8416, loss 0.164877, acc 0.953125
2017-03-02T17:43:13.134875: step 8417, loss 0.266599, acc 0.875
2017-03-02T17:43:13.209826: step 8418, loss 0.199304, acc 0.9375
2017-03-02T17:43:13.276594: step 8419, loss 0.216879, acc 0.90625
2017-03-02T17:43:13.350885: step 8420, loss 0.114806, acc 0.953125
2017-03-02T17:43:13.429373: step 8421, loss 0.218798, acc 0.890625
2017-03-02T17:43:13.504810: step 8422, loss 0.184951, acc 0.9375
2017-03-02T17:43:13.581188: step 8423, loss 0.302838, acc 0.875
2017-03-02T17:43:13.662125: step 8424, loss 0.135773, acc 0.96875
2017-03-02T17:43:13.731971: step 8425, loss 0.523029, acc 0.8125
2017-03-02T17:43:13.806044: step 8426, loss 0.161196, acc 0.9375
2017-03-02T17:43:13.878208: step 8427, loss 0.172812, acc 0.921875
2017-03-02T17:43:13.953283: step 8428, loss 0.0171873, acc 1
2017-03-02T17:43:14.023364: step 8429, loss 0.219243, acc 0.921875
2017-03-02T17:43:14.094198: step 8430, loss 0.0807606, acc 0.953125
2017-03-02T17:43:14.168076: step 8431, loss 0.266995, acc 0.890625
2017-03-02T17:43:14.245059: step 8432, loss 0.184119, acc 0.90625
2017-03-02T17:43:14.318831: step 8433, loss 0.125519, acc 0.953125
2017-03-02T17:43:14.393653: step 8434, loss 0.168481, acc 0.9375
2017-03-02T17:43:14.462586: step 8435, loss 0.0483394, acc 0.984375
2017-03-02T17:43:14.536199: step 8436, loss 0.325241, acc 0.875
2017-03-02T17:43:14.605217: step 8437, loss 0.0920389, acc 0.96875
2017-03-02T17:43:14.670743: step 8438, loss 0.167758, acc 0.921875
2017-03-02T17:43:14.740251: step 8439, loss 0.157024, acc 0.9375
2017-03-02T17:43:14.812147: step 8440, loss 0.302479, acc 0.859375
2017-03-02T17:43:14.890473: step 8441, loss 0.145483, acc 0.9375
2017-03-02T17:43:14.958041: step 8442, loss 0.0742203, acc 0.984375
2017-03-02T17:43:15.037115: step 8443, loss 0.252483, acc 0.875
2017-03-02T17:43:15.109622: step 8444, loss 0.152915, acc 0.9375
2017-03-02T17:43:15.180033: step 8445, loss 0.101431, acc 0.96875
2017-03-02T17:43:15.255896: step 8446, loss 0.0968473, acc 0.984375
2017-03-02T17:43:15.323503: step 8447, loss 0.111838, acc 0.953125
2017-03-02T17:43:15.388905: step 8448, loss 0.193737, acc 0.90625
2017-03-02T17:43:15.461326: step 8449, loss 0.117157, acc 0.953125
2017-03-02T17:43:15.534659: step 8450, loss 0.180308, acc 0.9375
2017-03-02T17:43:15.616023: step 8451, loss 0.130702, acc 0.9375
2017-03-02T17:43:15.689025: step 8452, loss 0.0752357, acc 0.984375
2017-03-02T17:43:15.762435: step 8453, loss 0.22494, acc 0.90625
2017-03-02T17:43:15.832620: step 8454, loss 0.107485, acc 0.9375
2017-03-02T17:43:15.904831: step 8455, loss 0.179887, acc 0.953125
2017-03-02T17:43:15.978993: step 8456, loss 0.163785, acc 0.921875
2017-03-02T17:43:16.054258: step 8457, loss 0.235658, acc 0.9375
2017-03-02T17:43:16.129824: step 8458, loss 0.208297, acc 0.890625
2017-03-02T17:43:16.206727: step 8459, loss 0.1521, acc 0.9375
2017-03-02T17:43:16.280599: step 8460, loss 0.0878734, acc 0.953125
2017-03-02T17:43:16.351206: step 8461, loss 0.0835242, acc 0.96875
2017-03-02T17:43:16.423890: step 8462, loss 0.302786, acc 0.875
2017-03-02T17:43:16.495597: step 8463, loss 0.199118, acc 0.90625
2017-03-02T17:43:16.568467: step 8464, loss 0.216042, acc 0.953125
2017-03-02T17:43:16.636162: step 8465, loss 0.312109, acc 0.859375
2017-03-02T17:43:16.711546: step 8466, loss 0.181822, acc 0.921875
2017-03-02T17:43:16.783675: step 8467, loss 0.496472, acc 0.828125
2017-03-02T17:43:16.857796: step 8468, loss 0.208741, acc 0.9375
2017-03-02T17:43:16.938928: step 8469, loss 0.254881, acc 0.875
2017-03-02T17:43:17.016136: step 8470, loss 0.188163, acc 0.921875
2017-03-02T17:43:17.093704: step 8471, loss 0.218808, acc 0.9375
2017-03-02T17:43:17.166325: step 8472, loss 0.293028, acc 0.875
2017-03-02T17:43:17.239866: step 8473, loss 0.298464, acc 0.890625
2017-03-02T17:43:17.303524: step 8474, loss 0.138237, acc 0.90625
2017-03-02T17:43:17.373867: step 8475, loss 0.124938, acc 0.953125
2017-03-02T17:43:17.442061: step 8476, loss 0.203013, acc 0.890625
2017-03-02T17:43:17.515210: step 8477, loss 0.166792, acc 0.921875
2017-03-02T17:43:17.586007: step 8478, loss 0.122587, acc 0.9375
2017-03-02T17:43:17.657555: step 8479, loss 0.325189, acc 0.890625
2017-03-02T17:43:17.732591: step 8480, loss 0.19106, acc 0.90625
2017-03-02T17:43:17.804828: step 8481, loss 0.0976832, acc 0.96875
2017-03-02T17:43:17.873969: step 8482, loss 0.076446, acc 0.984375
2017-03-02T17:43:17.949013: step 8483, loss 0.149459, acc 0.9375
2017-03-02T17:43:18.012815: step 8484, loss 0.141912, acc 0.9375
2017-03-02T17:43:18.080460: step 8485, loss 0.100971, acc 0.96875
2017-03-02T17:43:18.155559: step 8486, loss 0.182118, acc 0.890625
2017-03-02T17:43:18.230784: step 8487, loss 0.216288, acc 0.921875
2017-03-02T17:43:18.306426: step 8488, loss 0.0628998, acc 0.984375
2017-03-02T17:43:18.381356: step 8489, loss 0.335403, acc 0.859375
2017-03-02T17:43:18.456931: step 8490, loss 0.320121, acc 0.84375
2017-03-02T17:43:18.520291: step 8491, loss 0.157326, acc 0.90625
2017-03-02T17:43:18.593716: step 8492, loss 0.201512, acc 0.859375
2017-03-02T17:43:18.666719: step 8493, loss 0.211139, acc 0.90625
2017-03-02T17:43:18.739308: step 8494, loss 0.0827477, acc 0.953125
2017-03-02T17:43:18.807160: step 8495, loss 0.250288, acc 0.875
2017-03-02T17:43:18.877575: step 8496, loss 0.194139, acc 0.90625
2017-03-02T17:43:18.947395: step 8497, loss 0.13769, acc 0.921875
2017-03-02T17:43:19.015852: step 8498, loss 0.320033, acc 0.875
2017-03-02T17:43:19.100103: step 8499, loss 0.236284, acc 0.921875
2017-03-02T17:43:19.169961: step 8500, loss 0.250812, acc 0.859375

Evaluation:
2017-03-02T17:43:19.205755: step 8500, loss 1.30594, acc 0.662581

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8500

2017-03-02T17:43:19.663104: step 8501, loss 0.0551676, acc 0.96875
2017-03-02T17:43:19.731509: step 8502, loss 0.114764, acc 0.953125
2017-03-02T17:43:19.807250: step 8503, loss 0.247748, acc 0.921875
2017-03-02T17:43:19.886308: step 8504, loss 0.116687, acc 0.9375
2017-03-02T17:43:19.958590: step 8505, loss 0.110522, acc 0.984375
2017-03-02T17:43:20.034251: step 8506, loss 0.171055, acc 0.90625
2017-03-02T17:43:20.103809: step 8507, loss 0.169345, acc 0.90625
2017-03-02T17:43:20.172926: step 8508, loss 0.213859, acc 0.90625
2017-03-02T17:43:20.245998: step 8509, loss 0.114039, acc 0.9375
2017-03-02T17:43:20.321345: step 8510, loss 0.173279, acc 0.921875
2017-03-02T17:43:20.394916: step 8511, loss 0.250858, acc 0.875
2017-03-02T17:43:20.467240: step 8512, loss 0.204133, acc 0.9375
2017-03-02T17:43:20.541048: step 8513, loss 0.105071, acc 0.9375
2017-03-02T17:43:20.617446: step 8514, loss 0.106236, acc 0.96875
2017-03-02T17:43:20.691282: step 8515, loss 0.227293, acc 0.890625
2017-03-02T17:43:20.766333: step 8516, loss 0.0980359, acc 0.96875
2017-03-02T17:43:20.834222: step 8517, loss 0.141626, acc 0.953125
2017-03-02T17:43:20.943211: step 8518, loss 0.146363, acc 0.953125
2017-03-02T17:43:21.022850: step 8519, loss 0.0907819, acc 0.984375
2017-03-02T17:43:21.108554: step 8520, loss 0.124164, acc 0.953125
2017-03-02T17:43:21.186804: step 8521, loss 0.201206, acc 0.921875
2017-03-02T17:43:21.263733: step 8522, loss 0.102573, acc 0.96875
2017-03-02T17:43:21.340712: step 8523, loss 0.35065, acc 0.859375
2017-03-02T17:43:21.417673: step 8524, loss 0.174264, acc 0.9375
2017-03-02T17:43:21.496163: step 8525, loss 0.330443, acc 0.890625
2017-03-02T17:43:21.565662: step 8526, loss 0.208819, acc 0.890625
2017-03-02T17:43:21.645102: step 8527, loss 0.214299, acc 0.875
2017-03-02T17:43:21.724760: step 8528, loss 0.052659, acc 0.984375
2017-03-02T17:43:21.799571: step 8529, loss 0.184083, acc 0.90625
2017-03-02T17:43:21.878951: step 8530, loss 0.24823, acc 0.921875
2017-03-02T17:43:21.954749: step 8531, loss 0.288909, acc 0.921875
2017-03-02T17:43:22.041100: step 8532, loss 0.19622, acc 0.890625
2017-03-02T17:43:22.113366: step 8533, loss 0.152581, acc 0.953125
2017-03-02T17:43:22.181226: step 8534, loss 0.129734, acc 0.9375
2017-03-02T17:43:22.254477: step 8535, loss 0.349956, acc 0.859375
2017-03-02T17:43:22.337678: step 8536, loss 0.226763, acc 0.90625
2017-03-02T17:43:22.417715: step 8537, loss 0.203819, acc 0.890625
2017-03-02T17:43:22.491886: step 8538, loss 0.229489, acc 0.890625
2017-03-02T17:43:22.566326: step 8539, loss 0.230937, acc 0.921875
2017-03-02T17:43:22.639619: step 8540, loss 0.270641, acc 0.90625
2017-03-02T17:43:22.709458: step 8541, loss 0.162863, acc 0.953125
2017-03-02T17:43:22.782358: step 8542, loss 0.257046, acc 0.90625
2017-03-02T17:43:22.851993: step 8543, loss 0.195863, acc 0.921875
2017-03-02T17:43:22.921669: step 8544, loss 0.234758, acc 0.90625
2017-03-02T17:43:22.994784: step 8545, loss 0.371188, acc 0.875
2017-03-02T17:43:23.069675: step 8546, loss 0.0933726, acc 0.953125
2017-03-02T17:43:23.154682: step 8547, loss 0.199973, acc 0.921875
2017-03-02T17:43:23.226485: step 8548, loss 0.18477, acc 0.953125
2017-03-02T17:43:23.307651: step 8549, loss 0.200429, acc 0.921875
2017-03-02T17:43:23.386836: step 8550, loss 0.159237, acc 0.9375
2017-03-02T17:43:23.464315: step 8551, loss 0.166607, acc 0.9375
2017-03-02T17:43:23.536223: step 8552, loss 0.16074, acc 0.953125
2017-03-02T17:43:23.608740: step 8553, loss 0.199724, acc 0.90625
2017-03-02T17:43:23.682900: step 8554, loss 0.188081, acc 0.9375
2017-03-02T17:43:23.768387: step 8555, loss 0.236369, acc 0.859375
2017-03-02T17:43:23.846919: step 8556, loss 0.104503, acc 0.96875
2017-03-02T17:43:23.925704: step 8557, loss 0.188323, acc 0.9375
2017-03-02T17:43:23.995833: step 8558, loss 0.169359, acc 0.9375
2017-03-02T17:43:24.073298: step 8559, loss 0.0952826, acc 0.953125
2017-03-02T17:43:24.148905: step 8560, loss 0.146738, acc 0.9375
2017-03-02T17:43:24.229781: step 8561, loss 0.348641, acc 0.875
2017-03-02T17:43:24.301024: step 8562, loss 0.161344, acc 0.921875
2017-03-02T17:43:24.371785: step 8563, loss 0.16293, acc 0.953125
2017-03-02T17:43:24.441972: step 8564, loss 0.161709, acc 0.96875
2017-03-02T17:43:24.516609: step 8565, loss 0.16104, acc 0.953125
2017-03-02T17:43:24.584156: step 8566, loss 0.151959, acc 0.953125
2017-03-02T17:43:24.654459: step 8567, loss 0.188647, acc 0.953125
2017-03-02T17:43:24.724115: step 8568, loss 0.102782, acc 0.953125
2017-03-02T17:43:24.796806: step 8569, loss 0.153117, acc 0.921875
2017-03-02T17:43:24.868162: step 8570, loss 0.090005, acc 0.953125
2017-03-02T17:43:24.938867: step 8571, loss 0.167054, acc 0.9375
2017-03-02T17:43:25.009529: step 8572, loss 0.065821, acc 1
2017-03-02T17:43:25.079969: step 8573, loss 0.198661, acc 0.9375
2017-03-02T17:43:25.166997: step 8574, loss 0.143363, acc 0.9375
2017-03-02T17:43:25.242749: step 8575, loss 0.164916, acc 0.953125
2017-03-02T17:43:25.316346: step 8576, loss 0.125353, acc 0.9375
2017-03-02T17:43:25.393599: step 8577, loss 0.200103, acc 0.90625
2017-03-02T17:43:25.474033: step 8578, loss 0.124442, acc 0.9375
2017-03-02T17:43:25.546803: step 8579, loss 0.207798, acc 0.90625
2017-03-02T17:43:25.613643: step 8580, loss 0.0976368, acc 0.953125
2017-03-02T17:43:25.688730: step 8581, loss 0.0870777, acc 0.96875
2017-03-02T17:43:25.756353: step 8582, loss 0.328591, acc 0.875
2017-03-02T17:43:25.830607: step 8583, loss 0.225551, acc 0.90625
2017-03-02T17:43:25.908558: step 8584, loss 0.165852, acc 0.96875
2017-03-02T17:43:25.986802: step 8585, loss 0.397265, acc 0.8125
2017-03-02T17:43:26.057911: step 8586, loss 0.138173, acc 0.9375
2017-03-02T17:43:26.128088: step 8587, loss 0.082399, acc 0.953125
2017-03-02T17:43:26.202126: step 8588, loss 0.0439884, acc 0.984375
2017-03-02T17:43:26.275017: step 8589, loss 0.118188, acc 0.9375
2017-03-02T17:43:26.346117: step 8590, loss 0.125003, acc 0.96875
2017-03-02T17:43:26.422318: step 8591, loss 0.147789, acc 0.9375
2017-03-02T17:43:26.494108: step 8592, loss 0.109487, acc 0.96875
2017-03-02T17:43:26.573860: step 8593, loss 0.127584, acc 0.9375
2017-03-02T17:43:26.647870: step 8594, loss 0.330328, acc 0.875
2017-03-02T17:43:26.718458: step 8595, loss 0.229398, acc 0.890625
2017-03-02T17:43:26.790857: step 8596, loss 0.257292, acc 0.921875
2017-03-02T17:43:26.877923: step 8597, loss 0.177732, acc 0.9375
2017-03-02T17:43:26.947212: step 8598, loss 0.163688, acc 0.921875
2017-03-02T17:43:27.020343: step 8599, loss 0.240305, acc 0.921875
2017-03-02T17:43:27.089957: step 8600, loss 0.147052, acc 0.921875

Evaluation:
2017-03-02T17:43:27.124870: step 8600, loss 1.32661, acc 0.665465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8600

2017-03-02T17:43:27.588811: step 8601, loss 0.232591, acc 0.90625
2017-03-02T17:43:27.656648: step 8602, loss 0.191859, acc 0.90625
2017-03-02T17:43:27.724182: step 8603, loss 0.16027, acc 0.9375
2017-03-02T17:43:27.789044: step 8604, loss 0.193926, acc 0.90625
2017-03-02T17:43:27.860427: step 8605, loss 0.213968, acc 0.90625
2017-03-02T17:43:27.931682: step 8606, loss 0.115975, acc 0.953125
2017-03-02T17:43:28.001398: step 8607, loss 0.122605, acc 0.921875
2017-03-02T17:43:28.072065: step 8608, loss 0.254556, acc 0.890625
2017-03-02T17:43:28.144208: step 8609, loss 0.18176, acc 0.921875
2017-03-02T17:43:28.221437: step 8610, loss 0.172896, acc 0.921875
2017-03-02T17:43:28.299465: step 8611, loss 0.148051, acc 0.9375
2017-03-02T17:43:28.371472: step 8612, loss 0.164002, acc 0.90625
2017-03-02T17:43:28.447987: step 8613, loss 0.147617, acc 0.9375
2017-03-02T17:43:28.522545: step 8614, loss 0.193141, acc 0.9375
2017-03-02T17:43:28.600711: step 8615, loss 0.349008, acc 0.875
2017-03-02T17:43:28.671445: step 8616, loss 0.168492, acc 0.9375
2017-03-02T17:43:28.745693: step 8617, loss 0.157332, acc 0.9375
2017-03-02T17:43:28.816902: step 8618, loss 0.133067, acc 0.9375
2017-03-02T17:43:28.889956: step 8619, loss 0.177595, acc 0.9375
2017-03-02T17:43:28.971511: step 8620, loss 0.13645, acc 0.9375
2017-03-02T17:43:29.043369: step 8621, loss 0.152199, acc 0.921875
2017-03-02T17:43:29.106424: step 8622, loss 0.200703, acc 0.953125
2017-03-02T17:43:29.176601: step 8623, loss 0.103859, acc 0.96875
2017-03-02T17:43:29.250424: step 8624, loss 0.37454, acc 0.75
2017-03-02T17:43:29.324511: step 8625, loss 0.16381, acc 0.9375
2017-03-02T17:43:29.400512: step 8626, loss 0.214925, acc 0.921875
2017-03-02T17:43:29.492050: step 8627, loss 0.0830543, acc 0.984375
2017-03-02T17:43:29.564788: step 8628, loss 0.135143, acc 0.96875
2017-03-02T17:43:29.637121: step 8629, loss 0.166856, acc 0.90625
2017-03-02T17:43:29.708899: step 8630, loss 0.128277, acc 0.953125
2017-03-02T17:43:29.778301: step 8631, loss 0.123307, acc 0.9375
2017-03-02T17:43:29.843233: step 8632, loss 0.250479, acc 0.890625
2017-03-02T17:43:29.917456: step 8633, loss 0.25528, acc 0.90625
2017-03-02T17:43:29.995240: step 8634, loss 0.106822, acc 0.96875
2017-03-02T17:43:30.066350: step 8635, loss 0.245173, acc 0.921875
2017-03-02T17:43:30.138733: step 8636, loss 0.177259, acc 0.921875
2017-03-02T17:43:30.210643: step 8637, loss 0.15586, acc 0.921875
2017-03-02T17:43:30.282909: step 8638, loss 0.258085, acc 0.890625
2017-03-02T17:43:30.354718: step 8639, loss 0.175115, acc 0.9375
2017-03-02T17:43:30.426621: step 8640, loss 0.27264, acc 0.90625
2017-03-02T17:43:30.498604: step 8641, loss 0.141506, acc 0.921875
2017-03-02T17:43:30.577285: step 8642, loss 0.347417, acc 0.875
2017-03-02T17:43:30.662645: step 8643, loss 0.154692, acc 0.921875
2017-03-02T17:43:30.735164: step 8644, loss 0.251715, acc 0.921875
2017-03-02T17:43:30.809830: step 8645, loss 0.187631, acc 0.9375
2017-03-02T17:43:30.881565: step 8646, loss 0.138649, acc 0.953125
2017-03-02T17:43:30.945277: step 8647, loss 0.129768, acc 0.9375
2017-03-02T17:43:31.004131: step 8648, loss 0.230259, acc 0.953125
2017-03-02T17:43:31.074481: step 8649, loss 0.169215, acc 0.921875
2017-03-02T17:43:31.156989: step 8650, loss 0.132073, acc 0.953125
2017-03-02T17:43:31.225013: step 8651, loss 0.411691, acc 0.859375
2017-03-02T17:43:31.297773: step 8652, loss 0.216536, acc 0.90625
2017-03-02T17:43:31.371589: step 8653, loss 0.164593, acc 0.9375
2017-03-02T17:43:31.446326: step 8654, loss 0.0570737, acc 0.984375
2017-03-02T17:43:31.516765: step 8655, loss 0.180243, acc 0.90625
2017-03-02T17:43:31.605071: step 8656, loss 0.148269, acc 0.953125
2017-03-02T17:43:31.673582: step 8657, loss 0.0733311, acc 0.984375
2017-03-02T17:43:31.752436: step 8658, loss 0.225733, acc 0.90625
2017-03-02T17:43:31.820700: step 8659, loss 0.218707, acc 0.890625
2017-03-02T17:43:31.886631: step 8660, loss 0.139012, acc 0.953125
2017-03-02T17:43:31.961358: step 8661, loss 0.197009, acc 0.90625
2017-03-02T17:43:32.036457: step 8662, loss 0.113543, acc 0.984375
2017-03-02T17:43:32.111825: step 8663, loss 0.135485, acc 0.9375
2017-03-02T17:43:32.187268: step 8664, loss 0.111314, acc 0.953125
2017-03-02T17:43:32.261765: step 8665, loss 0.248039, acc 0.90625
2017-03-02T17:43:32.338844: step 8666, loss 0.254616, acc 0.890625
2017-03-02T17:43:32.413163: step 8667, loss 0.220417, acc 0.90625
2017-03-02T17:43:32.485415: step 8668, loss 0.221704, acc 0.921875
2017-03-02T17:43:32.566618: step 8669, loss 0.185267, acc 0.9375
2017-03-02T17:43:32.635213: step 8670, loss 0.127324, acc 0.9375
2017-03-02T17:43:32.709444: step 8671, loss 0.300518, acc 0.859375
2017-03-02T17:43:32.783577: step 8672, loss 0.144461, acc 0.90625
2017-03-02T17:43:32.872604: step 8673, loss 0.238157, acc 0.921875
2017-03-02T17:43:32.946033: step 8674, loss 0.160753, acc 0.90625
2017-03-02T17:43:33.016986: step 8675, loss 0.12991, acc 0.921875
2017-03-02T17:43:33.091129: step 8676, loss 0.11383, acc 0.9375
2017-03-02T17:43:33.163585: step 8677, loss 0.160329, acc 0.9375
2017-03-02T17:43:33.231033: step 8678, loss 0.153498, acc 0.9375
2017-03-02T17:43:33.304980: step 8679, loss 0.239757, acc 0.9375
2017-03-02T17:43:33.377360: step 8680, loss 0.17707, acc 0.953125
2017-03-02T17:43:33.445437: step 8681, loss 0.141637, acc 0.9375
2017-03-02T17:43:33.519085: step 8682, loss 0.288392, acc 0.90625
2017-03-02T17:43:33.596036: step 8683, loss 0.0700793, acc 0.953125
2017-03-02T17:43:33.668581: step 8684, loss 0.316035, acc 0.859375
2017-03-02T17:43:33.738343: step 8685, loss 0.142966, acc 0.921875
2017-03-02T17:43:33.811175: step 8686, loss 0.186391, acc 0.9375
2017-03-02T17:43:33.875766: step 8687, loss 0.171916, acc 0.9375
2017-03-02T17:43:33.946660: step 8688, loss 0.311852, acc 0.859375
2017-03-02T17:43:34.018226: step 8689, loss 0.125415, acc 0.9375
2017-03-02T17:43:34.092106: step 8690, loss 0.127806, acc 0.9375
2017-03-02T17:43:34.166834: step 8691, loss 0.251967, acc 0.90625
2017-03-02T17:43:34.243668: step 8692, loss 0.187829, acc 0.9375
2017-03-02T17:43:34.328353: step 8693, loss 0.164217, acc 0.921875
2017-03-02T17:43:34.401557: step 8694, loss 0.12649, acc 0.953125
2017-03-02T17:43:34.470651: step 8695, loss 0.086768, acc 0.953125
2017-03-02T17:43:34.542042: step 8696, loss 0.256623, acc 0.890625
2017-03-02T17:43:34.621031: step 8697, loss 0.168251, acc 0.9375
2017-03-02T17:43:34.702116: step 8698, loss 0.0903969, acc 0.984375
2017-03-02T17:43:34.782469: step 8699, loss 0.295836, acc 0.875
2017-03-02T17:43:34.853776: step 8700, loss 0.183978, acc 0.921875

Evaluation:
2017-03-02T17:43:34.894395: step 8700, loss 1.33504, acc 0.678443

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8700

2017-03-02T17:43:35.334802: step 8701, loss 0.106871, acc 0.96875
2017-03-02T17:43:35.408812: step 8702, loss 0.171726, acc 0.9375
2017-03-02T17:43:35.505479: step 8703, loss 0.168974, acc 0.921875
2017-03-02T17:43:35.579806: step 8704, loss 0.115814, acc 0.953125
2017-03-02T17:43:35.654316: step 8705, loss 0.0766243, acc 0.96875
2017-03-02T17:43:35.739221: step 8706, loss 0.0877846, acc 0.984375
2017-03-02T17:43:35.811645: step 8707, loss 0.105835, acc 0.96875
2017-03-02T17:43:35.884667: step 8708, loss 0.14519, acc 0.9375
2017-03-02T17:43:35.961643: step 8709, loss 0.150036, acc 0.9375
2017-03-02T17:43:36.028044: step 8710, loss 0.225191, acc 0.921875
2017-03-02T17:43:36.105006: step 8711, loss 0.17897, acc 0.90625
2017-03-02T17:43:36.173887: step 8712, loss 0.262546, acc 0.890625
2017-03-02T17:43:36.248706: step 8713, loss 0.165072, acc 0.9375
2017-03-02T17:43:36.320733: step 8714, loss 0.15525, acc 0.953125
2017-03-02T17:43:36.391150: step 8715, loss 0.221544, acc 0.921875
2017-03-02T17:43:36.473925: step 8716, loss 0.232515, acc 0.90625
2017-03-02T17:43:36.543979: step 8717, loss 0.222407, acc 0.890625
2017-03-02T17:43:36.613692: step 8718, loss 0.172074, acc 0.953125
2017-03-02T17:43:36.686115: step 8719, loss 0.15428, acc 0.9375
2017-03-02T17:43:36.753623: step 8720, loss 0.181901, acc 0.90625
2017-03-02T17:43:36.822563: step 8721, loss 0.171691, acc 0.921875
2017-03-02T17:43:36.896721: step 8722, loss 0.134936, acc 0.9375
2017-03-02T17:43:36.969681: step 8723, loss 0.176845, acc 0.9375
2017-03-02T17:43:37.049427: step 8724, loss 0.149126, acc 0.953125
2017-03-02T17:43:37.123337: step 8725, loss 0.194274, acc 0.9375
2017-03-02T17:43:37.195353: step 8726, loss 0.0921629, acc 0.96875
2017-03-02T17:43:37.286984: step 8727, loss 0.199441, acc 0.9375
2017-03-02T17:43:37.366072: step 8728, loss 0.0867974, acc 0.953125
2017-03-02T17:43:37.434602: step 8729, loss 0.15346, acc 0.921875
2017-03-02T17:43:37.508471: step 8730, loss 0.158267, acc 0.90625
2017-03-02T17:43:37.582326: step 8731, loss 0.175651, acc 0.90625
2017-03-02T17:43:37.644175: step 8732, loss 0.28306, acc 0.890625
2017-03-02T17:43:37.724052: step 8733, loss 0.168999, acc 0.9375
2017-03-02T17:43:37.793519: step 8734, loss 0.0990234, acc 0.953125
2017-03-02T17:43:37.865552: step 8735, loss 0.109261, acc 0.96875
2017-03-02T17:43:37.942276: step 8736, loss 0.234593, acc 0.84375
2017-03-02T17:43:38.006296: step 8737, loss 0.241109, acc 0.875
2017-03-02T17:43:38.073272: step 8738, loss 0.350653, acc 0.90625
2017-03-02T17:43:38.147338: step 8739, loss 0.15819, acc 0.921875
2017-03-02T17:43:38.231105: step 8740, loss 0.147241, acc 0.953125
2017-03-02T17:43:38.306699: step 8741, loss 0.139056, acc 0.9375
2017-03-02T17:43:38.379527: step 8742, loss 0.171537, acc 0.921875
2017-03-02T17:43:38.451291: step 8743, loss 0.140857, acc 0.9375
2017-03-02T17:43:38.522665: step 8744, loss 0.251885, acc 0.890625
2017-03-02T17:43:38.595510: step 8745, loss 0.106027, acc 0.953125
2017-03-02T17:43:38.673182: step 8746, loss 0.146615, acc 0.9375
2017-03-02T17:43:38.741003: step 8747, loss 0.070486, acc 0.96875
2017-03-02T17:43:38.807235: step 8748, loss 0.0973484, acc 0.96875
2017-03-02T17:43:38.884414: step 8749, loss 0.390225, acc 0.921875
2017-03-02T17:43:38.966042: step 8750, loss 0.0849337, acc 0.96875
2017-03-02T17:43:39.034736: step 8751, loss 0.11178, acc 0.9375
2017-03-02T17:43:39.112632: step 8752, loss 0.09874, acc 0.953125
2017-03-02T17:43:39.187798: step 8753, loss 0.205062, acc 0.90625
2017-03-02T17:43:39.254698: step 8754, loss 0.171642, acc 0.890625
2017-03-02T17:43:39.328041: step 8755, loss 0.129307, acc 0.9375
2017-03-02T17:43:39.398697: step 8756, loss 0.323048, acc 0.859375
2017-03-02T17:43:39.470311: step 8757, loss 0.253529, acc 0.875
2017-03-02T17:43:39.552577: step 8758, loss 0.0819837, acc 0.96875
2017-03-02T17:43:39.634930: step 8759, loss 0.172716, acc 0.96875
2017-03-02T17:43:39.706835: step 8760, loss 0.243955, acc 0.921875
2017-03-02T17:43:39.783667: step 8761, loss 0.162415, acc 0.921875
2017-03-02T17:43:39.861308: step 8762, loss 0.152614, acc 0.9375
2017-03-02T17:43:39.936854: step 8763, loss 0.0859707, acc 0.96875
2017-03-02T17:43:40.014828: step 8764, loss 0.17708, acc 0.921875
2017-03-02T17:43:40.089580: step 8765, loss 0.0974326, acc 0.96875
2017-03-02T17:43:40.154885: step 8766, loss 0.23718, acc 0.890625
2017-03-02T17:43:40.223516: step 8767, loss 0.104431, acc 0.953125
2017-03-02T17:43:40.304756: step 8768, loss 0.237894, acc 0.890625
2017-03-02T17:43:40.384205: step 8769, loss 0.30374, acc 0.890625
2017-03-02T17:43:40.469834: step 8770, loss 0.0935329, acc 0.9375
2017-03-02T17:43:40.541752: step 8771, loss 0.10561, acc 0.9375
2017-03-02T17:43:40.612800: step 8772, loss 0.178943, acc 0.90625
2017-03-02T17:43:40.690057: step 8773, loss 0.0971393, acc 0.96875
2017-03-02T17:43:40.767154: step 8774, loss 0.198347, acc 0.921875
2017-03-02T17:43:40.833394: step 8775, loss 0.154107, acc 0.96875
2017-03-02T17:43:40.901805: step 8776, loss 0.189185, acc 0.90625
2017-03-02T17:43:40.975458: step 8777, loss 0.314514, acc 0.875
2017-03-02T17:43:41.049501: step 8778, loss 0.283381, acc 0.921875
2017-03-02T17:43:41.128099: step 8779, loss 0.206754, acc 0.921875
2017-03-02T17:43:41.196454: step 8780, loss 0.188819, acc 0.890625
2017-03-02T17:43:41.275195: step 8781, loss 0.142593, acc 0.921875
2017-03-02T17:43:41.346175: step 8782, loss 0.167824, acc 0.875
2017-03-02T17:43:41.417254: step 8783, loss 0.186297, acc 0.90625
2017-03-02T17:43:41.490776: step 8784, loss 0.212714, acc 0.9375
2017-03-02T17:43:41.564328: step 8785, loss 0.234245, acc 0.921875
2017-03-02T17:43:41.635183: step 8786, loss 0.0923771, acc 0.953125
2017-03-02T17:43:41.711758: step 8787, loss 0.145221, acc 0.90625
2017-03-02T17:43:41.785085: step 8788, loss 0.25622, acc 0.90625
2017-03-02T17:43:41.893299: step 8789, loss 0.138159, acc 0.9375
2017-03-02T17:43:41.968128: step 8790, loss 0.220247, acc 0.890625
2017-03-02T17:43:42.043057: step 8791, loss 0.176833, acc 0.9375
2017-03-02T17:43:42.115632: step 8792, loss 0.191712, acc 0.9375
2017-03-02T17:43:42.184785: step 8793, loss 0.182447, acc 0.921875
2017-03-02T17:43:42.260352: step 8794, loss 0.289208, acc 0.9375
2017-03-02T17:43:42.329387: step 8795, loss 0.158313, acc 0.921875
2017-03-02T17:43:42.404951: step 8796, loss 0.2686, acc 0.921875
2017-03-02T17:43:42.477274: step 8797, loss 0.248346, acc 0.890625
2017-03-02T17:43:42.546426: step 8798, loss 0.209037, acc 0.875
2017-03-02T17:43:42.624757: step 8799, loss 0.136676, acc 0.9375
2017-03-02T17:43:42.714360: step 8800, loss 0.163901, acc 0.921875

Evaluation:
2017-03-02T17:43:42.748999: step 8800, loss 1.33464, acc 0.671954

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8800

2017-03-02T17:43:43.244217: step 8801, loss 0.18609, acc 0.921875
2017-03-02T17:43:43.321074: step 8802, loss 0.0987111, acc 0.96875
2017-03-02T17:43:43.395347: step 8803, loss 0.158211, acc 0.890625
2017-03-02T17:43:43.458847: step 8804, loss 0.142343, acc 0.921875
2017-03-02T17:43:43.530780: step 8805, loss 0.14172, acc 0.953125
2017-03-02T17:43:43.597167: step 8806, loss 0.156695, acc 0.96875
2017-03-02T17:43:43.665759: step 8807, loss 0.140375, acc 0.9375
2017-03-02T17:43:43.742008: step 8808, loss 0.210974, acc 0.921875
2017-03-02T17:43:43.814252: step 8809, loss 0.457372, acc 0.828125
2017-03-02T17:43:43.879005: step 8810, loss 0.303701, acc 0.890625
2017-03-02T17:43:43.950603: step 8811, loss 0.104975, acc 0.953125
2017-03-02T17:43:44.020108: step 8812, loss 0.0775913, acc 0.96875
2017-03-02T17:43:44.089366: step 8813, loss 0.172429, acc 0.9375
2017-03-02T17:43:44.173849: step 8814, loss 0.305597, acc 0.890625
2017-03-02T17:43:44.245372: step 8815, loss 0.144589, acc 0.921875
2017-03-02T17:43:44.310814: step 8816, loss 0.151243, acc 0.921875
2017-03-02T17:43:44.377313: step 8817, loss 0.232811, acc 0.875
2017-03-02T17:43:44.449984: step 8818, loss 0.156039, acc 0.9375
2017-03-02T17:43:44.523615: step 8819, loss 0.279298, acc 0.859375
2017-03-02T17:43:44.591646: step 8820, loss 0.00586198, acc 1
2017-03-02T17:43:44.669617: step 8821, loss 0.159378, acc 0.921875
2017-03-02T17:43:44.741204: step 8822, loss 0.150384, acc 0.9375
2017-03-02T17:43:44.812504: step 8823, loss 0.154587, acc 0.953125
2017-03-02T17:43:44.886894: step 8824, loss 0.213045, acc 0.890625
2017-03-02T17:43:44.954945: step 8825, loss 0.125822, acc 0.9375
2017-03-02T17:43:45.021182: step 8826, loss 0.208241, acc 0.90625
2017-03-02T17:43:45.091379: step 8827, loss 0.150795, acc 0.90625
2017-03-02T17:43:45.165037: step 8828, loss 0.140197, acc 0.9375
2017-03-02T17:43:45.234680: step 8829, loss 0.164304, acc 0.953125
2017-03-02T17:43:45.311568: step 8830, loss 0.183001, acc 0.90625
2017-03-02T17:43:45.385785: step 8831, loss 0.162289, acc 0.953125
2017-03-02T17:43:45.459432: step 8832, loss 0.320273, acc 0.875
2017-03-02T17:43:45.533015: step 8833, loss 0.153805, acc 0.90625
2017-03-02T17:43:45.606000: step 8834, loss 0.209566, acc 0.890625
2017-03-02T17:43:45.671520: step 8835, loss 0.216278, acc 0.890625
2017-03-02T17:43:45.738159: step 8836, loss 0.13544, acc 0.953125
2017-03-02T17:43:45.809436: step 8837, loss 0.247353, acc 0.90625
2017-03-02T17:43:45.884533: step 8838, loss 0.135576, acc 0.921875
2017-03-02T17:43:45.954782: step 8839, loss 0.106741, acc 0.96875
2017-03-02T17:43:46.028107: step 8840, loss 0.164587, acc 0.953125
2017-03-02T17:43:46.107675: step 8841, loss 0.10111, acc 0.953125
2017-03-02T17:43:46.180285: step 8842, loss 0.120815, acc 0.9375
2017-03-02T17:43:46.254694: step 8843, loss 0.102123, acc 0.953125
2017-03-02T17:43:46.324421: step 8844, loss 0.151895, acc 0.921875
2017-03-02T17:43:46.397946: step 8845, loss 0.141186, acc 0.90625
2017-03-02T17:43:46.463873: step 8846, loss 0.238687, acc 0.921875
2017-03-02T17:43:46.538565: step 8847, loss 0.188956, acc 0.921875
2017-03-02T17:43:46.611916: step 8848, loss 0.14085, acc 0.9375
2017-03-02T17:43:46.685110: step 8849, loss 0.127703, acc 0.96875
2017-03-02T17:43:46.777672: step 8850, loss 0.0716422, acc 0.984375
2017-03-02T17:43:46.849873: step 8851, loss 0.175836, acc 0.9375
2017-03-02T17:43:46.921206: step 8852, loss 0.134964, acc 0.953125
2017-03-02T17:43:46.994951: step 8853, loss 0.124104, acc 0.9375
2017-03-02T17:43:47.060872: step 8854, loss 0.0436607, acc 0.984375
2017-03-02T17:43:47.132246: step 8855, loss 0.184397, acc 0.953125
2017-03-02T17:43:47.204891: step 8856, loss 0.105984, acc 0.96875
2017-03-02T17:43:47.279732: step 8857, loss 0.0887997, acc 0.96875
2017-03-02T17:43:47.354554: step 8858, loss 0.165869, acc 0.953125
2017-03-02T17:43:47.432614: step 8859, loss 0.18312, acc 0.921875
2017-03-02T17:43:47.498871: step 8860, loss 0.259141, acc 0.890625
2017-03-02T17:43:47.574581: step 8861, loss 0.260711, acc 0.921875
2017-03-02T17:43:47.649170: step 8862, loss 0.139313, acc 0.921875
2017-03-02T17:43:47.721115: step 8863, loss 0.136804, acc 0.96875
2017-03-02T17:43:47.789531: step 8864, loss 0.167188, acc 0.9375
2017-03-02T17:43:47.858728: step 8865, loss 0.0799901, acc 0.96875
2017-03-02T17:43:47.931548: step 8866, loss 0.169242, acc 0.953125
2017-03-02T17:43:48.004869: step 8867, loss 0.257822, acc 0.875
2017-03-02T17:43:48.094162: step 8868, loss 0.169666, acc 0.9375
2017-03-02T17:43:48.167531: step 8869, loss 0.112095, acc 0.953125
2017-03-02T17:43:48.236049: step 8870, loss 0.268676, acc 0.90625
2017-03-02T17:43:48.306846: step 8871, loss 0.191711, acc 0.890625
2017-03-02T17:43:48.377907: step 8872, loss 0.208861, acc 0.859375
2017-03-02T17:43:48.464760: step 8873, loss 0.286138, acc 0.875
2017-03-02T17:43:48.532454: step 8874, loss 0.0977665, acc 0.953125
2017-03-02T17:43:48.606695: step 8875, loss 0.179301, acc 0.921875
2017-03-02T17:43:48.690935: step 8876, loss 0.125037, acc 0.9375
2017-03-02T17:43:48.763504: step 8877, loss 0.150263, acc 0.90625
2017-03-02T17:43:48.825445: step 8878, loss 0.368356, acc 0.875
2017-03-02T17:43:48.885871: step 8879, loss 0.201294, acc 0.921875
2017-03-02T17:43:48.955615: step 8880, loss 0.134627, acc 0.9375
2017-03-02T17:43:49.025712: step 8881, loss 0.177028, acc 0.90625
2017-03-02T17:43:49.101807: step 8882, loss 0.113874, acc 0.9375
2017-03-02T17:43:49.171647: step 8883, loss 0.110696, acc 0.9375
2017-03-02T17:43:49.243491: step 8884, loss 0.192726, acc 0.953125
2017-03-02T17:43:49.304917: step 8885, loss 0.117412, acc 0.9375
2017-03-02T17:43:49.379492: step 8886, loss 0.163208, acc 0.921875
2017-03-02T17:43:49.454716: step 8887, loss 0.158431, acc 0.953125
2017-03-02T17:43:49.528896: step 8888, loss 0.150498, acc 0.921875
2017-03-02T17:43:49.600683: step 8889, loss 0.18455, acc 0.9375
2017-03-02T17:43:49.674447: step 8890, loss 0.167478, acc 0.90625
2017-03-02T17:43:49.744864: step 8891, loss 0.367328, acc 0.859375
2017-03-02T17:43:49.815288: step 8892, loss 0.195603, acc 0.9375
2017-03-02T17:43:49.888214: step 8893, loss 0.120107, acc 0.9375
2017-03-02T17:43:49.963079: step 8894, loss 0.0833232, acc 0.96875
2017-03-02T17:43:50.035482: step 8895, loss 0.234168, acc 0.90625
2017-03-02T17:43:50.105533: step 8896, loss 0.239274, acc 0.921875
2017-03-02T17:43:50.178452: step 8897, loss 0.200237, acc 0.90625
2017-03-02T17:43:50.245818: step 8898, loss 0.246851, acc 0.875
2017-03-02T17:43:50.314957: step 8899, loss 0.189761, acc 0.921875
2017-03-02T17:43:50.383793: step 8900, loss 0.103843, acc 0.953125

Evaluation:
2017-03-02T17:43:50.416596: step 8900, loss 1.3462, acc 0.662581

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-8900

2017-03-02T17:43:50.878365: step 8901, loss 0.234982, acc 0.921875
2017-03-02T17:43:50.955828: step 8902, loss 0.211778, acc 0.890625
2017-03-02T17:43:51.027603: step 8903, loss 0.354859, acc 0.875
2017-03-02T17:43:51.100278: step 8904, loss 0.202274, acc 0.890625
2017-03-02T17:43:51.173472: step 8905, loss 0.269561, acc 0.9375
2017-03-02T17:43:51.245153: step 8906, loss 0.252221, acc 0.875
2017-03-02T17:43:51.311428: step 8907, loss 0.139236, acc 0.96875
2017-03-02T17:43:51.382188: step 8908, loss 0.216048, acc 0.90625
2017-03-02T17:43:51.449401: step 8909, loss 0.16703, acc 0.9375
2017-03-02T17:43:51.522705: step 8910, loss 0.261291, acc 0.921875
2017-03-02T17:43:51.595866: step 8911, loss 0.192953, acc 0.921875
2017-03-02T17:43:51.668668: step 8912, loss 0.115304, acc 0.96875
2017-03-02T17:43:51.740244: step 8913, loss 0.182166, acc 0.953125
2017-03-02T17:43:51.811644: step 8914, loss 0.0824616, acc 0.96875
2017-03-02T17:43:51.894924: step 8915, loss 0.0802591, acc 0.984375
2017-03-02T17:43:51.963967: step 8916, loss 0.122736, acc 0.953125
2017-03-02T17:43:52.033270: step 8917, loss 0.15139, acc 0.890625
2017-03-02T17:43:52.102285: step 8918, loss 0.196989, acc 0.9375
2017-03-02T17:43:52.179754: step 8919, loss 0.178059, acc 0.921875
2017-03-02T17:43:52.260103: step 8920, loss 0.190105, acc 0.921875
2017-03-02T17:43:52.334859: step 8921, loss 0.13308, acc 0.96875
2017-03-02T17:43:52.409824: step 8922, loss 0.146659, acc 0.953125
2017-03-02T17:43:52.483116: step 8923, loss 0.218783, acc 0.921875
2017-03-02T17:43:52.559451: step 8924, loss 0.220781, acc 0.953125
2017-03-02T17:43:52.626101: step 8925, loss 0.221309, acc 0.890625
2017-03-02T17:43:52.696737: step 8926, loss 0.295911, acc 0.875
2017-03-02T17:43:52.769134: step 8927, loss 0.0912713, acc 0.96875
2017-03-02T17:43:52.844540: step 8928, loss 0.136322, acc 0.9375
2017-03-02T17:43:52.920471: step 8929, loss 0.0589752, acc 0.96875
2017-03-02T17:43:52.994794: step 8930, loss 0.134349, acc 0.96875
2017-03-02T17:43:53.082755: step 8931, loss 0.170879, acc 0.9375
2017-03-02T17:43:53.156910: step 8932, loss 0.273512, acc 0.890625
2017-03-02T17:43:53.227530: step 8933, loss 0.147508, acc 0.953125
2017-03-02T17:43:53.298219: step 8934, loss 0.166841, acc 0.921875
2017-03-02T17:43:53.366040: step 8935, loss 0.212103, acc 0.890625
2017-03-02T17:43:53.451734: step 8936, loss 0.157047, acc 0.953125
2017-03-02T17:43:53.532748: step 8937, loss 0.245449, acc 0.9375
2017-03-02T17:43:53.609339: step 8938, loss 0.114702, acc 0.9375
2017-03-02T17:43:53.693672: step 8939, loss 0.0703159, acc 0.96875
2017-03-02T17:43:53.764688: step 8940, loss 0.104099, acc 0.953125
2017-03-02T17:43:53.836748: step 8941, loss 0.2227, acc 0.90625
2017-03-02T17:43:53.908933: step 8942, loss 0.196817, acc 0.90625
2017-03-02T17:43:53.979244: step 8943, loss 0.0663746, acc 0.96875
2017-03-02T17:43:54.043612: step 8944, loss 0.299923, acc 0.875
2017-03-02T17:43:54.113837: step 8945, loss 0.26722, acc 0.875
2017-03-02T17:43:54.188705: step 8946, loss 0.220437, acc 0.921875
2017-03-02T17:43:54.256987: step 8947, loss 0.272211, acc 0.9375
2017-03-02T17:43:54.326151: step 8948, loss 0.204211, acc 0.90625
2017-03-02T17:43:54.398435: step 8949, loss 0.199095, acc 0.90625
2017-03-02T17:43:54.471606: step 8950, loss 0.138606, acc 0.921875
2017-03-02T17:43:54.545637: step 8951, loss 0.140883, acc 0.953125
2017-03-02T17:43:54.613970: step 8952, loss 0.19899, acc 0.90625
2017-03-02T17:43:54.685453: step 8953, loss 0.135452, acc 0.953125
2017-03-02T17:43:54.765361: step 8954, loss 0.167893, acc 0.921875
2017-03-02T17:43:54.834728: step 8955, loss 0.185493, acc 0.953125
2017-03-02T17:43:54.905197: step 8956, loss 0.113218, acc 0.953125
2017-03-02T17:43:54.975300: step 8957, loss 0.142052, acc 0.9375
2017-03-02T17:43:55.049490: step 8958, loss 0.268428, acc 0.9375
2017-03-02T17:43:55.121012: step 8959, loss 0.121433, acc 0.953125
2017-03-02T17:43:55.193602: step 8960, loss 0.266572, acc 0.90625
2017-03-02T17:43:55.262708: step 8961, loss 0.0690913, acc 0.984375
2017-03-02T17:43:55.328948: step 8962, loss 0.210461, acc 0.9375
2017-03-02T17:43:55.398048: step 8963, loss 0.367459, acc 0.84375
2017-03-02T17:43:55.468177: step 8964, loss 0.139155, acc 0.921875
2017-03-02T17:43:55.541961: step 8965, loss 0.127789, acc 0.9375
2017-03-02T17:43:55.614164: step 8966, loss 0.21508, acc 0.890625
2017-03-02T17:43:55.695505: step 8967, loss 0.133227, acc 0.9375
2017-03-02T17:43:55.770137: step 8968, loss 0.179587, acc 0.921875
2017-03-02T17:43:55.839212: step 8969, loss 0.090965, acc 0.96875
2017-03-02T17:43:55.920329: step 8970, loss 0.151494, acc 0.9375
2017-03-02T17:43:55.994208: step 8971, loss 0.179552, acc 0.90625
2017-03-02T17:43:56.062006: step 8972, loss 0.084208, acc 0.953125
2017-03-02T17:43:56.138164: step 8973, loss 0.137388, acc 0.921875
2017-03-02T17:43:56.213281: step 8974, loss 0.298826, acc 0.921875
2017-03-02T17:43:56.288330: step 8975, loss 0.138893, acc 0.953125
2017-03-02T17:43:56.361676: step 8976, loss 0.0722551, acc 0.953125
2017-03-02T17:43:56.429433: step 8977, loss 0.177188, acc 0.921875
2017-03-02T17:43:56.497940: step 8978, loss 0.255703, acc 0.875
2017-03-02T17:43:56.582751: step 8979, loss 0.216251, acc 0.90625
2017-03-02T17:43:56.660437: step 8980, loss 0.135706, acc 0.953125
2017-03-02T17:43:56.733111: step 8981, loss 0.141341, acc 0.890625
2017-03-02T17:43:56.792455: step 8982, loss 0.145872, acc 0.953125
2017-03-02T17:43:56.856835: step 8983, loss 0.143273, acc 0.9375
2017-03-02T17:43:56.934166: step 8984, loss 0.148876, acc 0.96875
2017-03-02T17:43:57.013098: step 8985, loss 0.231467, acc 0.9375
2017-03-02T17:43:57.084387: step 8986, loss 0.324058, acc 0.828125
2017-03-02T17:43:57.159244: step 8987, loss 0.261016, acc 0.90625
2017-03-02T17:43:57.243220: step 8988, loss 0.109712, acc 0.96875
2017-03-02T17:43:57.315073: step 8989, loss 0.217215, acc 0.921875
2017-03-02T17:43:57.389906: step 8990, loss 0.229235, acc 0.890625
2017-03-02T17:43:57.462793: step 8991, loss 0.105794, acc 0.953125
2017-03-02T17:43:57.535463: step 8992, loss 0.15065, acc 0.953125
2017-03-02T17:43:57.610682: step 8993, loss 0.159353, acc 0.953125
2017-03-02T17:43:57.705626: step 8994, loss 0.103579, acc 0.953125
2017-03-02T17:43:57.783031: step 8995, loss 0.163488, acc 0.9375
2017-03-02T17:43:57.860172: step 8996, loss 0.197877, acc 0.890625
2017-03-02T17:43:57.934058: step 8997, loss 0.240123, acc 0.890625
2017-03-02T17:43:58.003971: step 8998, loss 0.250295, acc 0.890625
2017-03-02T17:43:58.068291: step 8999, loss 0.243895, acc 0.90625
2017-03-02T17:43:58.135975: step 9000, loss 0.21884, acc 0.90625

Evaluation:
2017-03-02T17:43:58.166420: step 9000, loss 1.36334, acc 0.672675

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9000

2017-03-02T17:43:58.611199: step 9001, loss 0.146532, acc 0.9375
2017-03-02T17:43:58.682997: step 9002, loss 0.161458, acc 0.953125
2017-03-02T17:43:58.752713: step 9003, loss 0.293381, acc 0.875
2017-03-02T17:43:58.820320: step 9004, loss 0.187946, acc 0.90625
2017-03-02T17:43:58.892615: step 9005, loss 0.427678, acc 0.875
2017-03-02T17:43:58.964897: step 9006, loss 0.177516, acc 0.890625
2017-03-02T17:43:59.043385: step 9007, loss 0.204617, acc 0.9375
2017-03-02T17:43:59.122742: step 9008, loss 0.200213, acc 0.921875
2017-03-02T17:43:59.205161: step 9009, loss 0.264645, acc 0.875
2017-03-02T17:43:59.275046: step 9010, loss 0.241404, acc 0.921875
2017-03-02T17:43:59.349777: step 9011, loss 0.249943, acc 0.90625
2017-03-02T17:43:59.430609: step 9012, loss 0.128885, acc 0.921875
2017-03-02T17:43:59.503167: step 9013, loss 0.238977, acc 0.890625
2017-03-02T17:43:59.574201: step 9014, loss 0.150017, acc 0.921875
2017-03-02T17:43:59.647904: step 9015, loss 0.272921, acc 0.890625
2017-03-02T17:43:59.720783: step 9016, loss 0.127743, acc 1
2017-03-02T17:43:59.799988: step 9017, loss 0.155357, acc 0.9375
2017-03-02T17:43:59.887979: step 9018, loss 0.35963, acc 0.90625
2017-03-02T17:43:59.959388: step 9019, loss 0.127007, acc 0.9375
2017-03-02T17:44:00.030317: step 9020, loss 0.112332, acc 0.984375
2017-03-02T17:44:00.106236: step 9021, loss 0.144082, acc 0.953125
2017-03-02T17:44:00.177199: step 9022, loss 0.171027, acc 0.9375
2017-03-02T17:44:00.245096: step 9023, loss 0.201412, acc 0.890625
2017-03-02T17:44:00.320142: step 9024, loss 0.199318, acc 0.921875
2017-03-02T17:44:00.395504: step 9025, loss 0.0926013, acc 0.9375
2017-03-02T17:44:00.470899: step 9026, loss 0.205307, acc 0.90625
2017-03-02T17:44:00.552973: step 9027, loss 0.185909, acc 0.9375
2017-03-02T17:44:00.627310: step 9028, loss 0.208955, acc 0.921875
2017-03-02T17:44:00.702085: step 9029, loss 0.191191, acc 0.90625
2017-03-02T17:44:00.775396: step 9030, loss 0.241308, acc 0.921875
2017-03-02T17:44:00.844411: step 9031, loss 0.315445, acc 0.90625
2017-03-02T17:44:00.915891: step 9032, loss 0.166897, acc 0.921875
2017-03-02T17:44:00.988665: step 9033, loss 0.125725, acc 0.9375
2017-03-02T17:44:01.065989: step 9034, loss 0.214222, acc 0.921875
2017-03-02T17:44:01.135186: step 9035, loss 0.265017, acc 0.859375
2017-03-02T17:44:01.204307: step 9036, loss 0.222105, acc 0.875
2017-03-02T17:44:01.277591: step 9037, loss 0.13917, acc 0.9375
2017-03-02T17:44:01.350543: step 9038, loss 0.116181, acc 0.953125
2017-03-02T17:44:01.422450: step 9039, loss 0.15144, acc 0.921875
2017-03-02T17:44:01.491489: step 9040, loss 0.189848, acc 0.96875
2017-03-02T17:44:01.558043: step 9041, loss 0.160994, acc 0.90625
2017-03-02T17:44:01.632638: step 9042, loss 0.221638, acc 0.921875
2017-03-02T17:44:01.706592: step 9043, loss 0.205323, acc 0.953125
2017-03-02T17:44:01.780339: step 9044, loss 0.18652, acc 0.890625
2017-03-02T17:44:01.851376: step 9045, loss 0.0916762, acc 0.96875
2017-03-02T17:44:01.928920: step 9046, loss 0.14659, acc 0.953125
2017-03-02T17:44:02.001735: step 9047, loss 0.16331, acc 0.921875
2017-03-02T17:44:02.075570: step 9048, loss 0.0748427, acc 0.96875
2017-03-02T17:44:02.151865: step 9049, loss 0.114234, acc 0.9375
2017-03-02T17:44:02.223216: step 9050, loss 0.0655307, acc 0.984375
2017-03-02T17:44:02.287499: step 9051, loss 0.144832, acc 0.96875
2017-03-02T17:44:02.369979: step 9052, loss 0.412482, acc 0.828125
2017-03-02T17:44:02.449622: step 9053, loss 0.190913, acc 0.921875
2017-03-02T17:44:02.519975: step 9054, loss 0.181641, acc 0.953125
2017-03-02T17:44:02.594650: step 9055, loss 0.139405, acc 0.953125
2017-03-02T17:44:02.669117: step 9056, loss 0.147557, acc 0.921875
2017-03-02T17:44:02.750595: step 9057, loss 0.190596, acc 0.96875
2017-03-02T17:44:02.835609: step 9058, loss 0.230136, acc 0.890625
2017-03-02T17:44:02.904421: step 9059, loss 0.162533, acc 0.921875
2017-03-02T17:44:02.974249: step 9060, loss 0.27755, acc 0.890625
2017-03-02T17:44:03.042347: step 9061, loss 0.148396, acc 0.921875
2017-03-02T17:44:03.115714: step 9062, loss 0.0965596, acc 0.953125
2017-03-02T17:44:03.187653: step 9063, loss 0.237456, acc 0.90625
2017-03-02T17:44:03.269053: step 9064, loss 0.327848, acc 0.859375
2017-03-02T17:44:03.346262: step 9065, loss 0.12443, acc 0.953125
2017-03-02T17:44:03.416583: step 9066, loss 0.105625, acc 0.96875
2017-03-02T17:44:03.492777: step 9067, loss 0.0957244, acc 0.984375
2017-03-02T17:44:03.562880: step 9068, loss 0.081459, acc 1
2017-03-02T17:44:03.629077: step 9069, loss 0.212639, acc 0.921875
2017-03-02T17:44:03.703659: step 9070, loss 0.17811, acc 0.9375
2017-03-02T17:44:03.786879: step 9071, loss 0.164213, acc 0.9375
2017-03-02T17:44:03.858463: step 9072, loss 0.167925, acc 0.921875
2017-03-02T17:44:03.933470: step 9073, loss 0.138958, acc 0.9375
2017-03-02T17:44:04.003365: step 9074, loss 0.154738, acc 0.921875
2017-03-02T17:44:04.076651: step 9075, loss 0.127092, acc 0.953125
2017-03-02T17:44:04.150375: step 9076, loss 0.172779, acc 0.9375
2017-03-02T17:44:04.225993: step 9077, loss 0.212712, acc 0.90625
2017-03-02T17:44:04.293356: step 9078, loss 0.121249, acc 0.921875
2017-03-02T17:44:04.362827: step 9079, loss 0.165555, acc 0.96875
2017-03-02T17:44:04.435025: step 9080, loss 0.143985, acc 0.9375
2017-03-02T17:44:04.507490: step 9081, loss 0.0884514, acc 0.96875
2017-03-02T17:44:04.582959: step 9082, loss 0.425007, acc 0.84375
2017-03-02T17:44:04.675165: step 9083, loss 0.10926, acc 0.953125
2017-03-02T17:44:04.754752: step 9084, loss 0.108869, acc 0.953125
2017-03-02T17:44:04.826665: step 9085, loss 0.162083, acc 0.9375
2017-03-02T17:44:04.900731: step 9086, loss 0.12937, acc 0.921875
2017-03-02T17:44:04.979679: step 9087, loss 0.259934, acc 0.921875
2017-03-02T17:44:05.061313: step 9088, loss 0.0599211, acc 0.96875
2017-03-02T17:44:05.137279: step 9089, loss 0.0981003, acc 0.96875
2017-03-02T17:44:05.212212: step 9090, loss 0.279533, acc 0.875
2017-03-02T17:44:05.284549: step 9091, loss 0.236253, acc 0.90625
2017-03-02T17:44:05.357582: step 9092, loss 0.104915, acc 0.96875
2017-03-02T17:44:05.436739: step 9093, loss 0.136987, acc 0.921875
2017-03-02T17:44:05.512649: step 9094, loss 0.172728, acc 0.9375
2017-03-02T17:44:05.591433: step 9095, loss 0.108722, acc 0.953125
2017-03-02T17:44:05.660328: step 9096, loss 0.0933986, acc 0.984375
2017-03-02T17:44:05.729976: step 9097, loss 0.139046, acc 0.953125
2017-03-02T17:44:05.806304: step 9098, loss 0.162639, acc 0.90625
2017-03-02T17:44:05.879856: step 9099, loss 0.0996436, acc 0.953125
2017-03-02T17:44:05.956108: step 9100, loss 0.102949, acc 0.96875

Evaluation:
2017-03-02T17:44:05.993757: step 9100, loss 1.42204, acc 0.675559

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9100

2017-03-02T17:44:06.453393: step 9101, loss 0.263665, acc 0.890625
2017-03-02T17:44:06.529857: step 9102, loss 0.121891, acc 0.953125
2017-03-02T17:44:06.605694: step 9103, loss 0.10616, acc 0.9375
2017-03-02T17:44:06.682749: step 9104, loss 0.251639, acc 0.875
2017-03-02T17:44:06.765750: step 9105, loss 0.0671555, acc 0.96875
2017-03-02T17:44:06.836612: step 9106, loss 0.120288, acc 0.9375
2017-03-02T17:44:06.914911: step 9107, loss 0.315107, acc 0.859375
2017-03-02T17:44:06.984724: step 9108, loss 0.287651, acc 0.84375
2017-03-02T17:44:07.051512: step 9109, loss 0.212537, acc 0.875
2017-03-02T17:44:07.124734: step 9110, loss 0.123391, acc 0.953125
2017-03-02T17:44:07.199669: step 9111, loss 0.165653, acc 0.90625
2017-03-02T17:44:07.278141: step 9112, loss 0.163459, acc 0.953125
2017-03-02T17:44:07.352288: step 9113, loss 0.179629, acc 0.921875
2017-03-02T17:44:07.427265: step 9114, loss 0.0671149, acc 0.984375
2017-03-02T17:44:07.506074: step 9115, loss 0.142524, acc 0.9375
2017-03-02T17:44:07.575707: step 9116, loss 0.270726, acc 0.890625
2017-03-02T17:44:07.649606: step 9117, loss 0.0596376, acc 0.984375
2017-03-02T17:44:07.713722: step 9118, loss 0.292274, acc 0.890625
2017-03-02T17:44:07.778953: step 9119, loss 0.343698, acc 0.890625
2017-03-02T17:44:07.851738: step 9120, loss 0.105176, acc 0.9375
2017-03-02T17:44:07.923832: step 9121, loss 0.190899, acc 0.890625
2017-03-02T17:44:07.989017: step 9122, loss 0.192088, acc 0.890625
2017-03-02T17:44:08.075672: step 9123, loss 0.196105, acc 0.921875
2017-03-02T17:44:08.146983: step 9124, loss 0.164681, acc 0.90625
2017-03-02T17:44:08.218394: step 9125, loss 0.192164, acc 0.890625
2017-03-02T17:44:08.294768: step 9126, loss 0.250027, acc 0.90625
2017-03-02T17:44:08.366305: step 9127, loss 0.189763, acc 0.921875
2017-03-02T17:44:08.435192: step 9128, loss 0.0704769, acc 0.96875
2017-03-02T17:44:08.507693: step 9129, loss 0.215778, acc 0.90625
2017-03-02T17:44:08.577200: step 9130, loss 0.113404, acc 0.96875
2017-03-02T17:44:08.655793: step 9131, loss 0.177689, acc 0.9375
2017-03-02T17:44:08.730108: step 9132, loss 0.149002, acc 0.921875
2017-03-02T17:44:08.819067: step 9133, loss 0.116725, acc 0.9375
2017-03-02T17:44:08.894763: step 9134, loss 0.185789, acc 0.890625
2017-03-02T17:44:08.968819: step 9135, loss 0.178749, acc 0.921875
2017-03-02T17:44:09.045524: step 9136, loss 0.301785, acc 0.921875
2017-03-02T17:44:09.113570: step 9137, loss 0.121075, acc 0.953125
2017-03-02T17:44:09.183855: step 9138, loss 0.416264, acc 0.875
2017-03-02T17:44:09.264943: step 9139, loss 0.178656, acc 0.953125
2017-03-02T17:44:09.341637: step 9140, loss 0.118291, acc 0.953125
2017-03-02T17:44:09.415288: step 9141, loss 0.109559, acc 0.953125
2017-03-02T17:44:09.487325: step 9142, loss 0.225352, acc 0.90625
2017-03-02T17:44:09.565479: step 9143, loss 0.153293, acc 0.921875
2017-03-02T17:44:09.637786: step 9144, loss 0.240422, acc 0.90625
2017-03-02T17:44:09.713800: step 9145, loss 0.135997, acc 0.953125
2017-03-02T17:44:09.776777: step 9146, loss 0.309895, acc 0.875
2017-03-02T17:44:09.855424: step 9147, loss 0.191109, acc 0.921875
2017-03-02T17:44:09.925746: step 9148, loss 0.207553, acc 0.921875
2017-03-02T17:44:10.001053: step 9149, loss 0.090396, acc 0.984375
2017-03-02T17:44:10.072796: step 9150, loss 0.0623645, acc 0.96875
2017-03-02T17:44:10.147587: step 9151, loss 0.169081, acc 0.9375
2017-03-02T17:44:10.222804: step 9152, loss 0.259841, acc 0.921875
2017-03-02T17:44:10.307087: step 9153, loss 0.209567, acc 0.921875
2017-03-02T17:44:10.379278: step 9154, loss 0.263013, acc 0.90625
2017-03-02T17:44:10.442700: step 9155, loss 0.116765, acc 0.953125
2017-03-02T17:44:10.523120: step 9156, loss 0.270366, acc 0.875
2017-03-02T17:44:10.594785: step 9157, loss 0.242147, acc 0.890625
2017-03-02T17:44:10.661787: step 9158, loss 0.0943698, acc 0.96875
2017-03-02T17:44:10.741760: step 9159, loss 0.165858, acc 0.953125
2017-03-02T17:44:10.815379: step 9160, loss 0.291749, acc 0.859375
2017-03-02T17:44:10.887944: step 9161, loss 0.309148, acc 0.84375
2017-03-02T17:44:10.962722: step 9162, loss 0.0759585, acc 0.96875
2017-03-02T17:44:11.034214: step 9163, loss 0.186848, acc 0.921875
2017-03-02T17:44:11.101539: step 9164, loss 0.317082, acc 0.890625
2017-03-02T17:44:11.170086: step 9165, loss 0.150858, acc 0.953125
2017-03-02T17:44:11.237530: step 9166, loss 0.207539, acc 0.921875
2017-03-02T17:44:11.304596: step 9167, loss 0.117502, acc 0.953125
2017-03-02T17:44:11.384610: step 9168, loss 0.242521, acc 0.90625
2017-03-02T17:44:11.450182: step 9169, loss 0.241268, acc 0.859375
2017-03-02T17:44:11.523603: step 9170, loss 0.157791, acc 0.9375
2017-03-02T17:44:11.605504: step 9171, loss 0.182646, acc 0.9375
2017-03-02T17:44:11.684752: step 9172, loss 0.198009, acc 0.90625
2017-03-02T17:44:11.763701: step 9173, loss 0.270378, acc 0.859375
2017-03-02T17:44:11.834756: step 9174, loss 0.170731, acc 0.90625
2017-03-02T17:44:11.904667: step 9175, loss 0.156939, acc 0.9375
2017-03-02T17:44:11.986484: step 9176, loss 0.112252, acc 0.953125
2017-03-02T17:44:12.063739: step 9177, loss 0.140007, acc 0.9375
2017-03-02T17:44:12.142298: step 9178, loss 0.214525, acc 0.9375
2017-03-02T17:44:12.217669: step 9179, loss 0.205936, acc 0.921875
2017-03-02T17:44:12.292594: step 9180, loss 0.224721, acc 0.921875
2017-03-02T17:44:12.367041: step 9181, loss 0.226276, acc 0.921875
2017-03-02T17:44:12.448910: step 9182, loss 0.0868888, acc 0.9375
2017-03-02T17:44:12.520174: step 9183, loss 0.175784, acc 0.921875
2017-03-02T17:44:12.591657: step 9184, loss 0.196855, acc 0.921875
2017-03-02T17:44:12.683230: step 9185, loss 0.143958, acc 0.9375
2017-03-02T17:44:12.763262: step 9186, loss 0.110234, acc 0.96875
2017-03-02T17:44:12.836810: step 9187, loss 0.22403, acc 0.921875
2017-03-02T17:44:12.908007: step 9188, loss 0.140779, acc 0.9375
2017-03-02T17:44:12.981433: step 9189, loss 0.252384, acc 0.921875
2017-03-02T17:44:13.044403: step 9190, loss 0.151777, acc 0.9375
2017-03-02T17:44:13.119466: step 9191, loss 0.166283, acc 0.9375
2017-03-02T17:44:13.189158: step 9192, loss 0.185549, acc 0.90625
2017-03-02T17:44:13.258026: step 9193, loss 0.188479, acc 0.921875
2017-03-02T17:44:13.320049: step 9194, loss 0.142148, acc 0.9375
2017-03-02T17:44:13.397914: step 9195, loss 0.205729, acc 0.90625
2017-03-02T17:44:13.473206: step 9196, loss 0.167243, acc 0.890625
2017-03-02T17:44:13.545601: step 9197, loss 0.20594, acc 0.9375
2017-03-02T17:44:13.622213: step 9198, loss 0.254065, acc 0.890625
2017-03-02T17:44:13.698848: step 9199, loss 0.299554, acc 0.859375
2017-03-02T17:44:13.770883: step 9200, loss 0.203247, acc 0.96875

Evaluation:
2017-03-02T17:44:13.809057: step 9200, loss 1.35187, acc 0.660418

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9200

2017-03-02T17:44:14.267109: step 9201, loss 0.281183, acc 0.828125
2017-03-02T17:44:14.340620: step 9202, loss 0.120743, acc 0.921875
2017-03-02T17:44:14.413945: step 9203, loss 0.16299, acc 0.90625
2017-03-02T17:44:14.489779: step 9204, loss 0.198771, acc 0.921875
2017-03-02T17:44:14.564694: step 9205, loss 0.17796, acc 0.890625
2017-03-02T17:44:14.633997: step 9206, loss 0.155346, acc 0.90625
2017-03-02T17:44:14.704018: step 9207, loss 0.223787, acc 0.921875
2017-03-02T17:44:14.778536: step 9208, loss 0.16567, acc 0.921875
2017-03-02T17:44:14.865806: step 9209, loss 0.170996, acc 0.90625
2017-03-02T17:44:14.934162: step 9210, loss 0.150012, acc 0.953125
2017-03-02T17:44:15.010179: step 9211, loss 0.188987, acc 0.9375
2017-03-02T17:44:15.086077: step 9212, loss 0.0130531, acc 1
2017-03-02T17:44:15.162922: step 9213, loss 0.133169, acc 0.953125
2017-03-02T17:44:15.245424: step 9214, loss 0.114308, acc 0.953125
2017-03-02T17:44:15.316143: step 9215, loss 0.182818, acc 0.921875
2017-03-02T17:44:15.389999: step 9216, loss 0.0605514, acc 0.984375
2017-03-02T17:44:15.466877: step 9217, loss 0.162823, acc 0.96875
2017-03-02T17:44:15.541390: step 9218, loss 0.178438, acc 0.921875
2017-03-02T17:44:15.615050: step 9219, loss 0.12025, acc 0.9375
2017-03-02T17:44:15.693642: step 9220, loss 0.117573, acc 0.953125
2017-03-02T17:44:15.768308: step 9221, loss 0.151226, acc 0.921875
2017-03-02T17:44:15.834822: step 9222, loss 0.0825937, acc 0.984375
2017-03-02T17:44:15.910784: step 9223, loss 0.0912783, acc 0.96875
2017-03-02T17:44:15.981522: step 9224, loss 0.0897984, acc 0.953125
2017-03-02T17:44:16.048485: step 9225, loss 0.192963, acc 0.90625
2017-03-02T17:44:16.120464: step 9226, loss 0.276319, acc 0.859375
2017-03-02T17:44:16.200148: step 9227, loss 0.194349, acc 0.90625
2017-03-02T17:44:16.276766: step 9228, loss 0.126947, acc 0.953125
2017-03-02T17:44:16.349392: step 9229, loss 0.0840088, acc 0.96875
2017-03-02T17:44:16.421047: step 9230, loss 0.145783, acc 0.9375
2017-03-02T17:44:16.493934: step 9231, loss 0.230668, acc 0.890625
2017-03-02T17:44:16.567260: step 9232, loss 0.186369, acc 0.890625
2017-03-02T17:44:16.641751: step 9233, loss 0.215548, acc 0.859375
2017-03-02T17:44:16.717386: step 9234, loss 0.124041, acc 0.953125
2017-03-02T17:44:16.792672: step 9235, loss 0.142108, acc 0.953125
2017-03-02T17:44:16.868475: step 9236, loss 0.297729, acc 0.890625
2017-03-02T17:44:16.943678: step 9237, loss 0.223425, acc 0.875
2017-03-02T17:44:17.022411: step 9238, loss 0.152753, acc 0.953125
2017-03-02T17:44:17.092942: step 9239, loss 0.415356, acc 0.875
2017-03-02T17:44:17.170742: step 9240, loss 0.198436, acc 0.921875
2017-03-02T17:44:17.247169: step 9241, loss 0.192484, acc 0.9375
2017-03-02T17:44:17.323669: step 9242, loss 0.145378, acc 0.90625
2017-03-02T17:44:17.390501: step 9243, loss 0.350921, acc 0.875
2017-03-02T17:44:17.456053: step 9244, loss 0.189228, acc 0.921875
2017-03-02T17:44:17.526657: step 9245, loss 0.062206, acc 0.984375
2017-03-02T17:44:17.598173: step 9246, loss 0.170223, acc 0.921875
2017-03-02T17:44:17.689173: step 9247, loss 0.143999, acc 0.953125
2017-03-02T17:44:17.768309: step 9248, loss 0.166341, acc 0.953125
2017-03-02T17:44:17.837588: step 9249, loss 0.162138, acc 0.890625
2017-03-02T17:44:17.906818: step 9250, loss 0.108603, acc 0.96875
2017-03-02T17:44:17.979595: step 9251, loss 0.138733, acc 0.953125
2017-03-02T17:44:18.049815: step 9252, loss 0.105774, acc 0.984375
2017-03-02T17:44:18.120044: step 9253, loss 0.162874, acc 0.921875
2017-03-02T17:44:18.187886: step 9254, loss 0.184025, acc 0.953125
2017-03-02T17:44:18.277945: step 9255, loss 0.165055, acc 0.921875
2017-03-02T17:44:18.351265: step 9256, loss 0.22106, acc 0.921875
2017-03-02T17:44:18.430730: step 9257, loss 0.172374, acc 0.90625
2017-03-02T17:44:18.534492: step 9258, loss 0.0504861, acc 0.984375
2017-03-02T17:44:18.618517: step 9259, loss 0.133128, acc 0.921875
2017-03-02T17:44:18.692018: step 9260, loss 0.116784, acc 0.9375
2017-03-02T17:44:18.761128: step 9261, loss 0.148682, acc 0.9375
2017-03-02T17:44:18.824579: step 9262, loss 0.234454, acc 0.9375
2017-03-02T17:44:18.913877: step 9263, loss 0.148612, acc 0.90625
2017-03-02T17:44:19.009713: step 9264, loss 0.161595, acc 0.9375
2017-03-02T17:44:19.083192: step 9265, loss 0.155303, acc 0.953125
2017-03-02T17:44:19.162478: step 9266, loss 0.0864874, acc 0.953125
2017-03-02T17:44:19.236355: step 9267, loss 0.11192, acc 0.96875
2017-03-02T17:44:19.310616: step 9268, loss 0.102634, acc 0.953125
2017-03-02T17:44:19.384913: step 9269, loss 0.0999222, acc 0.9375
2017-03-02T17:44:19.450564: step 9270, loss 0.115996, acc 0.9375
2017-03-02T17:44:19.525342: step 9271, loss 0.231522, acc 0.90625
2017-03-02T17:44:19.599226: step 9272, loss 0.170329, acc 0.953125
2017-03-02T17:44:19.671653: step 9273, loss 0.238173, acc 0.890625
2017-03-02T17:44:19.742864: step 9274, loss 0.102092, acc 0.953125
2017-03-02T17:44:19.821498: step 9275, loss 0.194496, acc 0.890625
2017-03-02T17:44:19.895749: step 9276, loss 0.231087, acc 0.9375
2017-03-02T17:44:19.964311: step 9277, loss 0.177948, acc 0.90625
2017-03-02T17:44:20.035194: step 9278, loss 0.125627, acc 0.96875
2017-03-02T17:44:20.108742: step 9279, loss 0.105277, acc 0.953125
2017-03-02T17:44:20.177517: step 9280, loss 0.193737, acc 0.890625
2017-03-02T17:44:20.245681: step 9281, loss 0.132402, acc 0.953125
2017-03-02T17:44:20.320210: step 9282, loss 0.147532, acc 0.984375
2017-03-02T17:44:20.386807: step 9283, loss 0.212638, acc 0.875
2017-03-02T17:44:20.460851: step 9284, loss 0.381162, acc 0.890625
2017-03-02T17:44:20.536611: step 9285, loss 0.204046, acc 0.90625
2017-03-02T17:44:20.603738: step 9286, loss 0.0989944, acc 0.9375
2017-03-02T17:44:20.672645: step 9287, loss 0.147665, acc 0.953125
2017-03-02T17:44:20.736046: step 9288, loss 0.120657, acc 0.96875
2017-03-02T17:44:20.806752: step 9289, loss 0.162727, acc 0.9375
2017-03-02T17:44:20.885784: step 9290, loss 0.30966, acc 0.90625
2017-03-02T17:44:20.956613: step 9291, loss 0.164696, acc 0.90625
2017-03-02T17:44:21.032220: step 9292, loss 0.238565, acc 0.90625
2017-03-02T17:44:21.102660: step 9293, loss 0.203216, acc 0.921875
2017-03-02T17:44:21.210127: step 9294, loss 0.140009, acc 0.921875
2017-03-02T17:44:21.285406: step 9295, loss 0.272382, acc 0.890625
2017-03-02T17:44:21.354163: step 9296, loss 0.197896, acc 0.90625
2017-03-02T17:44:21.426704: step 9297, loss 0.148541, acc 0.921875
2017-03-02T17:44:21.501692: step 9298, loss 0.112953, acc 0.953125
2017-03-02T17:44:21.569844: step 9299, loss 0.145572, acc 0.96875
2017-03-02T17:44:21.639315: step 9300, loss 0.263666, acc 0.84375

Evaluation:
2017-03-02T17:44:21.674965: step 9300, loss 1.41302, acc 0.677001

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9300

2017-03-02T17:44:22.131927: step 9301, loss 0.16945, acc 0.921875
2017-03-02T17:44:22.200899: step 9302, loss 0.104616, acc 0.96875
2017-03-02T17:44:22.270830: step 9303, loss 0.197201, acc 0.921875
2017-03-02T17:44:22.339413: step 9304, loss 0.224643, acc 0.90625
2017-03-02T17:44:22.411959: step 9305, loss 0.149773, acc 0.9375
2017-03-02T17:44:22.489490: step 9306, loss 0.144055, acc 0.9375
2017-03-02T17:44:22.563334: step 9307, loss 0.272654, acc 0.90625
2017-03-02T17:44:22.642257: step 9308, loss 0.174856, acc 0.9375
2017-03-02T17:44:22.716895: step 9309, loss 0.198166, acc 0.890625
2017-03-02T17:44:22.795491: step 9310, loss 0.332159, acc 0.859375
2017-03-02T17:44:22.866980: step 9311, loss 0.298537, acc 0.875
2017-03-02T17:44:22.944454: step 9312, loss 0.0991434, acc 0.96875
2017-03-02T17:44:23.016112: step 9313, loss 0.143786, acc 0.9375
2017-03-02T17:44:23.091192: step 9314, loss 0.262439, acc 0.90625
2017-03-02T17:44:23.164607: step 9315, loss 0.0661931, acc 0.96875
2017-03-02T17:44:23.242324: step 9316, loss 0.282736, acc 0.90625
2017-03-02T17:44:23.319172: step 9317, loss 0.127367, acc 0.953125
2017-03-02T17:44:23.398431: step 9318, loss 0.228439, acc 0.921875
2017-03-02T17:44:23.472715: step 9319, loss 0.221858, acc 0.953125
2017-03-02T17:44:23.551135: step 9320, loss 0.241389, acc 0.859375
2017-03-02T17:44:23.621575: step 9321, loss 0.127338, acc 0.953125
2017-03-02T17:44:23.701657: step 9322, loss 0.188192, acc 0.953125
2017-03-02T17:44:23.774242: step 9323, loss 0.176337, acc 0.90625
2017-03-02T17:44:23.852880: step 9324, loss 0.218102, acc 0.921875
2017-03-02T17:44:23.930158: step 9325, loss 0.226124, acc 0.90625
2017-03-02T17:44:23.993447: step 9326, loss 0.235909, acc 0.90625
2017-03-02T17:44:24.065079: step 9327, loss 0.0863268, acc 0.96875
2017-03-02T17:44:24.132008: step 9328, loss 0.392767, acc 0.8125
2017-03-02T17:44:24.201601: step 9329, loss 0.0881137, acc 0.953125
2017-03-02T17:44:24.273533: step 9330, loss 0.152782, acc 0.9375
2017-03-02T17:44:24.342715: step 9331, loss 0.343939, acc 0.890625
2017-03-02T17:44:24.418400: step 9332, loss 0.11793, acc 0.9375
2017-03-02T17:44:24.493855: step 9333, loss 0.184335, acc 0.9375
2017-03-02T17:44:24.578363: step 9334, loss 0.132902, acc 0.9375
2017-03-02T17:44:24.648284: step 9335, loss 0.311678, acc 0.890625
2017-03-02T17:44:24.733707: step 9336, loss 0.242733, acc 0.890625
2017-03-02T17:44:24.813862: step 9337, loss 0.195926, acc 0.953125
2017-03-02T17:44:24.882458: step 9338, loss 0.29842, acc 0.875
2017-03-02T17:44:24.952058: step 9339, loss 0.334134, acc 0.890625
2017-03-02T17:44:25.023484: step 9340, loss 0.130671, acc 0.9375
2017-03-02T17:44:25.097251: step 9341, loss 0.172188, acc 0.96875
2017-03-02T17:44:25.165571: step 9342, loss 0.125148, acc 0.9375
2017-03-02T17:44:25.238782: step 9343, loss 0.210413, acc 0.9375
2017-03-02T17:44:25.313222: step 9344, loss 0.230797, acc 0.890625
2017-03-02T17:44:25.393517: step 9345, loss 0.168116, acc 0.953125
2017-03-02T17:44:25.480799: step 9346, loss 0.249436, acc 0.859375
2017-03-02T17:44:25.559160: step 9347, loss 0.212037, acc 0.921875
2017-03-02T17:44:25.627757: step 9348, loss 0.203756, acc 0.9375
2017-03-02T17:44:25.701844: step 9349, loss 0.267834, acc 0.890625
2017-03-02T17:44:25.775314: step 9350, loss 0.176094, acc 0.9375
2017-03-02T17:44:25.853275: step 9351, loss 0.138316, acc 0.9375
2017-03-02T17:44:25.930301: step 9352, loss 0.116215, acc 0.921875
2017-03-02T17:44:26.013376: step 9353, loss 0.117987, acc 0.953125
2017-03-02T17:44:26.083203: step 9354, loss 0.0891054, acc 0.984375
2017-03-02T17:44:26.153411: step 9355, loss 0.147151, acc 0.96875
2017-03-02T17:44:26.215219: step 9356, loss 0.229633, acc 0.9375
2017-03-02T17:44:26.289440: step 9357, loss 0.252501, acc 0.890625
2017-03-02T17:44:26.361423: step 9358, loss 0.17068, acc 0.921875
2017-03-02T17:44:26.430244: step 9359, loss 0.146154, acc 0.9375
2017-03-02T17:44:26.501012: step 9360, loss 0.150949, acc 0.953125
2017-03-02T17:44:26.574928: step 9361, loss 0.147613, acc 0.953125
2017-03-02T17:44:26.646351: step 9362, loss 0.184137, acc 0.9375
2017-03-02T17:44:26.721948: step 9363, loss 0.20756, acc 0.90625
2017-03-02T17:44:26.793744: step 9364, loss 0.114349, acc 0.96875
2017-03-02T17:44:26.877314: step 9365, loss 0.191513, acc 0.921875
2017-03-02T17:44:26.947996: step 9366, loss 0.304884, acc 0.921875
2017-03-02T17:44:27.017112: step 9367, loss 0.160313, acc 0.953125
2017-03-02T17:44:27.083283: step 9368, loss 0.143725, acc 0.953125
2017-03-02T17:44:27.157739: step 9369, loss 0.326127, acc 0.859375
2017-03-02T17:44:27.226901: step 9370, loss 0.175982, acc 0.921875
2017-03-02T17:44:27.300536: step 9371, loss 0.189418, acc 0.90625
2017-03-02T17:44:27.375244: step 9372, loss 0.175046, acc 0.953125
2017-03-02T17:44:27.450428: step 9373, loss 0.157336, acc 0.953125
2017-03-02T17:44:27.522578: step 9374, loss 0.167209, acc 0.953125
2017-03-02T17:44:27.616699: step 9375, loss 0.18067, acc 0.90625
2017-03-02T17:44:27.685876: step 9376, loss 0.33424, acc 0.875
2017-03-02T17:44:27.755977: step 9377, loss 0.139015, acc 0.953125
2017-03-02T17:44:27.829099: step 9378, loss 0.109943, acc 0.953125
2017-03-02T17:44:27.903665: step 9379, loss 0.207552, acc 0.90625
2017-03-02T17:44:27.976716: step 9380, loss 0.154664, acc 0.921875
2017-03-02T17:44:28.048359: step 9381, loss 0.140123, acc 0.953125
2017-03-02T17:44:28.121810: step 9382, loss 0.144898, acc 0.9375
2017-03-02T17:44:28.194016: step 9383, loss 0.380135, acc 0.84375
2017-03-02T17:44:28.267649: step 9384, loss 0.337251, acc 0.875
2017-03-02T17:44:28.345510: step 9385, loss 0.197605, acc 0.921875
2017-03-02T17:44:28.413803: step 9386, loss 0.14882, acc 0.953125
2017-03-02T17:44:28.490403: step 9387, loss 0.158264, acc 0.953125
2017-03-02T17:44:28.564694: step 9388, loss 0.235136, acc 0.921875
2017-03-02T17:44:28.645382: step 9389, loss 0.150259, acc 0.9375
2017-03-02T17:44:28.734135: step 9390, loss 0.169503, acc 0.9375
2017-03-02T17:44:28.805895: step 9391, loss 0.102252, acc 0.953125
2017-03-02T17:44:28.883260: step 9392, loss 0.289688, acc 0.890625
2017-03-02T17:44:28.954652: step 9393, loss 0.154821, acc 0.890625
2017-03-02T17:44:29.031396: step 9394, loss 0.209331, acc 0.9375
2017-03-02T17:44:29.103614: step 9395, loss 0.11935, acc 0.9375
2017-03-02T17:44:29.193528: step 9396, loss 0.244763, acc 0.90625
2017-03-02T17:44:29.266853: step 9397, loss 0.147393, acc 0.9375
2017-03-02T17:44:29.335159: step 9398, loss 0.283791, acc 0.875
2017-03-02T17:44:29.409389: step 9399, loss 0.0963452, acc 0.9375
2017-03-02T17:44:29.478269: step 9400, loss 0.235066, acc 0.90625

Evaluation:
2017-03-02T17:44:29.509587: step 9400, loss 1.3436, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9400

2017-03-02T17:44:29.994765: step 9401, loss 0.155632, acc 0.921875
2017-03-02T17:44:30.089144: step 9402, loss 0.185258, acc 0.921875
2017-03-02T17:44:30.163561: step 9403, loss 0.267273, acc 0.890625
2017-03-02T17:44:30.233892: step 9404, loss 0.242478, acc 0.90625
2017-03-02T17:44:30.306189: step 9405, loss 0.147572, acc 0.953125
2017-03-02T17:44:30.375768: step 9406, loss 0.132595, acc 0.953125
2017-03-02T17:44:30.435341: step 9407, loss 0.150482, acc 0.921875
2017-03-02T17:44:30.505319: step 9408, loss 0.0265943, acc 1
2017-03-02T17:44:30.581865: step 9409, loss 0.150495, acc 0.90625
2017-03-02T17:44:30.657286: step 9410, loss 0.194464, acc 0.921875
2017-03-02T17:44:30.733498: step 9411, loss 0.176885, acc 0.921875
2017-03-02T17:44:30.816107: step 9412, loss 0.110088, acc 0.953125
2017-03-02T17:44:30.893215: step 9413, loss 0.0897728, acc 0.96875
2017-03-02T17:44:30.970104: step 9414, loss 0.0743287, acc 0.984375
2017-03-02T17:44:31.040782: step 9415, loss 0.109428, acc 0.953125
2017-03-02T17:44:31.106483: step 9416, loss 0.113123, acc 0.96875
2017-03-02T17:44:31.176586: step 9417, loss 0.163291, acc 0.953125
2017-03-02T17:44:31.249775: step 9418, loss 0.100312, acc 0.96875
2017-03-02T17:44:31.325824: step 9419, loss 0.145973, acc 0.921875
2017-03-02T17:44:31.407841: step 9420, loss 0.161359, acc 0.953125
2017-03-02T17:44:31.478621: step 9421, loss 0.0971546, acc 0.953125
2017-03-02T17:44:31.552764: step 9422, loss 0.0756934, acc 0.96875
2017-03-02T17:44:31.627625: step 9423, loss 0.113992, acc 0.953125
2017-03-02T17:44:31.699359: step 9424, loss 0.154121, acc 0.9375
2017-03-02T17:44:31.770893: step 9425, loss 0.343304, acc 0.890625
2017-03-02T17:44:31.840345: step 9426, loss 0.13002, acc 0.953125
2017-03-02T17:44:31.916341: step 9427, loss 0.274094, acc 0.90625
2017-03-02T17:44:32.002859: step 9428, loss 0.200207, acc 0.921875
2017-03-02T17:44:32.073310: step 9429, loss 0.228211, acc 0.90625
2017-03-02T17:44:32.161271: step 9430, loss 0.0908214, acc 0.984375
2017-03-02T17:44:32.236685: step 9431, loss 0.134496, acc 0.921875
2017-03-02T17:44:32.313760: step 9432, loss 0.126385, acc 0.9375
2017-03-02T17:44:32.392427: step 9433, loss 0.163544, acc 0.921875
2017-03-02T17:44:32.465713: step 9434, loss 0.160047, acc 0.9375
2017-03-02T17:44:32.535109: step 9435, loss 0.294258, acc 0.921875
2017-03-02T17:44:32.616445: step 9436, loss 0.177584, acc 0.890625
2017-03-02T17:44:32.687787: step 9437, loss 0.177447, acc 0.921875
2017-03-02T17:44:32.749833: step 9438, loss 0.0932743, acc 0.96875
2017-03-02T17:44:32.832236: step 9439, loss 0.143312, acc 0.9375
2017-03-02T17:44:32.909477: step 9440, loss 0.116221, acc 0.953125
2017-03-02T17:44:32.982421: step 9441, loss 0.266162, acc 0.890625
2017-03-02T17:44:33.051636: step 9442, loss 0.154153, acc 0.96875
2017-03-02T17:44:33.126276: step 9443, loss 0.166463, acc 0.921875
2017-03-02T17:44:33.194156: step 9444, loss 0.157007, acc 0.9375
2017-03-02T17:44:33.275916: step 9445, loss 0.100007, acc 0.953125
2017-03-02T17:44:33.347140: step 9446, loss 0.105994, acc 0.953125
2017-03-02T17:44:33.420481: step 9447, loss 0.253367, acc 0.90625
2017-03-02T17:44:33.493957: step 9448, loss 0.231166, acc 0.921875
2017-03-02T17:44:33.575508: step 9449, loss 0.151136, acc 0.953125
2017-03-02T17:44:33.653809: step 9450, loss 0.182181, acc 0.9375
2017-03-02T17:44:33.725498: step 9451, loss 0.0645009, acc 0.984375
2017-03-02T17:44:33.801479: step 9452, loss 0.0947633, acc 0.953125
2017-03-02T17:44:33.879868: step 9453, loss 0.133175, acc 0.953125
2017-03-02T17:44:33.960002: step 9454, loss 0.215233, acc 0.9375
2017-03-02T17:44:34.034087: step 9455, loss 0.157566, acc 0.953125
2017-03-02T17:44:34.108932: step 9456, loss 0.168997, acc 0.9375
2017-03-02T17:44:34.173690: step 9457, loss 0.100406, acc 0.953125
2017-03-02T17:44:34.241954: step 9458, loss 0.265579, acc 0.828125
2017-03-02T17:44:34.313117: step 9459, loss 0.104403, acc 0.9375
2017-03-02T17:44:34.385120: step 9460, loss 0.0627336, acc 0.953125
2017-03-02T17:44:34.464237: step 9461, loss 0.30593, acc 0.890625
2017-03-02T17:44:34.533101: step 9462, loss 0.195574, acc 0.9375
2017-03-02T17:44:34.615386: step 9463, loss 0.137388, acc 0.9375
2017-03-02T17:44:34.687454: step 9464, loss 0.26441, acc 0.890625
2017-03-02T17:44:34.755438: step 9465, loss 0.0386405, acc 1
2017-03-02T17:44:34.822254: step 9466, loss 0.238175, acc 0.875
2017-03-02T17:44:34.920545: step 9467, loss 0.0850547, acc 0.984375
2017-03-02T17:44:34.997409: step 9468, loss 0.182956, acc 0.90625
2017-03-02T17:44:35.070984: step 9469, loss 0.113456, acc 0.953125
2017-03-02T17:44:35.151643: step 9470, loss 0.120441, acc 0.9375
2017-03-02T17:44:35.225115: step 9471, loss 0.179801, acc 0.90625
2017-03-02T17:44:35.301399: step 9472, loss 0.123411, acc 0.953125
2017-03-02T17:44:35.373081: step 9473, loss 0.200479, acc 0.9375
2017-03-02T17:44:35.443779: step 9474, loss 0.230519, acc 0.875
2017-03-02T17:44:35.518744: step 9475, loss 0.315645, acc 0.921875
2017-03-02T17:44:35.598508: step 9476, loss 0.249625, acc 0.890625
2017-03-02T17:44:35.673214: step 9477, loss 0.327129, acc 0.828125
2017-03-02T17:44:35.740858: step 9478, loss 0.294343, acc 0.890625
2017-03-02T17:44:35.808531: step 9479, loss 0.229445, acc 0.90625
2017-03-02T17:44:35.881514: step 9480, loss 0.168506, acc 0.921875
2017-03-02T17:44:35.948318: step 9481, loss 0.0911667, acc 0.984375
2017-03-02T17:44:36.016563: step 9482, loss 0.245163, acc 0.890625
2017-03-02T17:44:36.088035: step 9483, loss 0.212111, acc 0.90625
2017-03-02T17:44:36.160573: step 9484, loss 0.129003, acc 0.9375
2017-03-02T17:44:36.233939: step 9485, loss 0.181223, acc 0.921875
2017-03-02T17:44:36.304536: step 9486, loss 0.165368, acc 0.921875
2017-03-02T17:44:36.378166: step 9487, loss 0.134284, acc 0.9375
2017-03-02T17:44:36.442119: step 9488, loss 0.161221, acc 0.9375
2017-03-02T17:44:36.519578: step 9489, loss 0.105472, acc 0.953125
2017-03-02T17:44:36.600905: step 9490, loss 0.0728133, acc 0.984375
2017-03-02T17:44:36.665851: step 9491, loss 0.101918, acc 0.9375
2017-03-02T17:44:36.736064: step 9492, loss 0.146764, acc 0.96875
2017-03-02T17:44:36.807842: step 9493, loss 0.118624, acc 0.9375
2017-03-02T17:44:36.879093: step 9494, loss 0.119157, acc 0.9375
2017-03-02T17:44:36.954478: step 9495, loss 0.143297, acc 0.953125
2017-03-02T17:44:37.026612: step 9496, loss 0.103174, acc 0.921875
2017-03-02T17:44:37.127861: step 9497, loss 0.136692, acc 0.953125
2017-03-02T17:44:37.193937: step 9498, loss 0.127468, acc 0.953125
2017-03-02T17:44:37.270174: step 9499, loss 0.198675, acc 0.921875
2017-03-02T17:44:37.338495: step 9500, loss 0.168248, acc 0.90625

Evaluation:
2017-03-02T17:44:37.375660: step 9500, loss 1.39582, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9500

2017-03-02T17:44:37.870466: step 9501, loss 0.104218, acc 0.96875
2017-03-02T17:44:37.943044: step 9502, loss 0.150887, acc 0.9375
2017-03-02T17:44:38.017956: step 9503, loss 0.135911, acc 0.953125
2017-03-02T17:44:38.096575: step 9504, loss 0.177138, acc 0.953125
2017-03-02T17:44:38.164399: step 9505, loss 0.254701, acc 0.890625
2017-03-02T17:44:38.241079: step 9506, loss 0.214348, acc 0.9375
2017-03-02T17:44:38.315522: step 9507, loss 0.06522, acc 0.984375
2017-03-02T17:44:38.394776: step 9508, loss 0.274408, acc 0.875
2017-03-02T17:44:38.468180: step 9509, loss 0.12841, acc 0.9375
2017-03-02T17:44:38.538723: step 9510, loss 0.285136, acc 0.875
2017-03-02T17:44:38.614039: step 9511, loss 0.153445, acc 0.90625
2017-03-02T17:44:38.685166: step 9512, loss 0.161146, acc 0.921875
2017-03-02T17:44:38.754640: step 9513, loss 0.236061, acc 0.890625
2017-03-02T17:44:38.826523: step 9514, loss 0.154615, acc 0.953125
2017-03-02T17:44:38.907531: step 9515, loss 0.0799942, acc 0.96875
2017-03-02T17:44:38.977753: step 9516, loss 0.0762602, acc 0.96875
2017-03-02T17:44:39.052004: step 9517, loss 0.124303, acc 0.90625
2017-03-02T17:44:39.119479: step 9518, loss 0.163657, acc 0.9375
2017-03-02T17:44:39.186562: step 9519, loss 0.201496, acc 0.921875
2017-03-02T17:44:39.258350: step 9520, loss 0.248002, acc 0.84375
2017-03-02T17:44:39.329145: step 9521, loss 0.257429, acc 0.90625
2017-03-02T17:44:39.395735: step 9522, loss 0.164685, acc 0.9375
2017-03-02T17:44:39.461904: step 9523, loss 0.102347, acc 0.984375
2017-03-02T17:44:39.534200: step 9524, loss 0.263359, acc 0.890625
2017-03-02T17:44:39.610408: step 9525, loss 0.132617, acc 0.953125
2017-03-02T17:44:39.686176: step 9526, loss 0.150847, acc 0.9375
2017-03-02T17:44:39.762453: step 9527, loss 0.139862, acc 0.9375
2017-03-02T17:44:39.835479: step 9528, loss 0.121066, acc 0.9375
2017-03-02T17:44:39.909454: step 9529, loss 0.14612, acc 0.90625
2017-03-02T17:44:39.984814: step 9530, loss 0.296828, acc 0.890625
2017-03-02T17:44:40.051798: step 9531, loss 0.0993674, acc 0.9375
2017-03-02T17:44:40.120390: step 9532, loss 0.135032, acc 0.9375
2017-03-02T17:44:40.192417: step 9533, loss 0.0966944, acc 0.953125
2017-03-02T17:44:40.261953: step 9534, loss 0.192951, acc 0.96875
2017-03-02T17:44:40.346837: step 9535, loss 0.281811, acc 0.90625
2017-03-02T17:44:40.420795: step 9536, loss 0.109527, acc 0.953125
2017-03-02T17:44:40.495714: step 9537, loss 0.0385861, acc 1
2017-03-02T17:44:40.568281: step 9538, loss 0.268492, acc 0.890625
2017-03-02T17:44:40.640394: step 9539, loss 0.139125, acc 0.953125
2017-03-02T17:44:40.713098: step 9540, loss 0.184589, acc 0.953125
2017-03-02T17:44:40.781288: step 9541, loss 0.278925, acc 0.921875
2017-03-02T17:44:40.854986: step 9542, loss 0.167666, acc 0.890625
2017-03-02T17:44:40.935862: step 9543, loss 0.271277, acc 0.875
2017-03-02T17:44:41.009760: step 9544, loss 0.232488, acc 0.9375
2017-03-02T17:44:41.084650: step 9545, loss 0.232291, acc 0.890625
2017-03-02T17:44:41.159081: step 9546, loss 0.201371, acc 0.9375
2017-03-02T17:44:41.233764: step 9547, loss 0.111348, acc 0.953125
2017-03-02T17:44:41.309020: step 9548, loss 0.236565, acc 0.90625
2017-03-02T17:44:41.377097: step 9549, loss 0.260692, acc 0.890625
2017-03-02T17:44:41.444090: step 9550, loss 0.272022, acc 0.875
2017-03-02T17:44:41.512747: step 9551, loss 0.167913, acc 0.9375
2017-03-02T17:44:41.583527: step 9552, loss 0.0732351, acc 0.96875
2017-03-02T17:44:41.666436: step 9553, loss 0.268335, acc 0.890625
2017-03-02T17:44:41.757621: step 9554, loss 0.275069, acc 0.84375
2017-03-02T17:44:41.827290: step 9555, loss 0.200943, acc 0.890625
2017-03-02T17:44:41.900101: step 9556, loss 0.198147, acc 0.90625
2017-03-02T17:44:41.970948: step 9557, loss 0.0611243, acc 0.96875
2017-03-02T17:44:42.042839: step 9558, loss 0.194899, acc 0.90625
2017-03-02T17:44:42.112695: step 9559, loss 0.0912585, acc 0.984375
2017-03-02T17:44:42.188612: step 9560, loss 0.205887, acc 0.890625
2017-03-02T17:44:42.274777: step 9561, loss 0.109366, acc 0.953125
2017-03-02T17:44:42.347163: step 9562, loss 0.2287, acc 0.921875
2017-03-02T17:44:42.424623: step 9563, loss 0.182345, acc 0.875
2017-03-02T17:44:42.495838: step 9564, loss 0.139612, acc 0.9375
2017-03-02T17:44:42.573468: step 9565, loss 0.202089, acc 0.90625
2017-03-02T17:44:42.642496: step 9566, loss 0.346787, acc 0.875
2017-03-02T17:44:42.713801: step 9567, loss 0.27838, acc 0.921875
2017-03-02T17:44:42.784248: step 9568, loss 0.226031, acc 0.890625
2017-03-02T17:44:42.855312: step 9569, loss 0.228596, acc 0.875
2017-03-02T17:44:42.929051: step 9570, loss 0.0706238, acc 0.984375
2017-03-02T17:44:43.002015: step 9571, loss 0.255438, acc 0.921875
2017-03-02T17:44:43.074469: step 9572, loss 0.181069, acc 0.953125
2017-03-02T17:44:43.154376: step 9573, loss 0.140005, acc 0.921875
2017-03-02T17:44:43.226270: step 9574, loss 0.342942, acc 0.90625
2017-03-02T17:44:43.305406: step 9575, loss 0.257846, acc 0.875
2017-03-02T17:44:43.376002: step 9576, loss 0.206298, acc 0.90625
2017-03-02T17:44:43.438405: step 9577, loss 0.127436, acc 0.953125
2017-03-02T17:44:43.506761: step 9578, loss 0.250186, acc 0.921875
2017-03-02T17:44:43.575013: step 9579, loss 0.112842, acc 0.9375
2017-03-02T17:44:43.649540: step 9580, loss 0.238698, acc 0.90625
2017-03-02T17:44:43.721034: step 9581, loss 0.118644, acc 0.9375
2017-03-02T17:44:43.792811: step 9582, loss 0.128762, acc 0.9375
2017-03-02T17:44:43.862869: step 9583, loss 0.195541, acc 0.890625
2017-03-02T17:44:43.936606: step 9584, loss 0.113856, acc 0.96875
2017-03-02T17:44:44.014707: step 9585, loss 0.153026, acc 0.953125
2017-03-02T17:44:44.101505: step 9586, loss 0.196202, acc 0.890625
2017-03-02T17:44:44.183506: step 9587, loss 0.22307, acc 0.921875
2017-03-02T17:44:44.253313: step 9588, loss 0.155606, acc 0.90625
2017-03-02T17:44:44.330433: step 9589, loss 0.0985186, acc 0.953125
2017-03-02T17:44:44.407232: step 9590, loss 0.0742771, acc 0.96875
2017-03-02T17:44:44.492495: step 9591, loss 0.263249, acc 0.90625
2017-03-02T17:44:44.568348: step 9592, loss 0.132882, acc 0.953125
2017-03-02T17:44:44.643345: step 9593, loss 0.264663, acc 0.890625
2017-03-02T17:44:44.714101: step 9594, loss 0.239305, acc 0.921875
2017-03-02T17:44:44.807717: step 9595, loss 0.102248, acc 0.953125
2017-03-02T17:44:44.880049: step 9596, loss 0.174731, acc 0.90625
2017-03-02T17:44:44.948931: step 9597, loss 0.222463, acc 0.875
2017-03-02T17:44:45.025708: step 9598, loss 0.227348, acc 0.890625
2017-03-02T17:44:45.104085: step 9599, loss 0.301828, acc 0.921875
2017-03-02T17:44:45.178261: step 9600, loss 0.206963, acc 0.90625

Evaluation:
2017-03-02T17:44:45.215246: step 9600, loss 1.38063, acc 0.677722

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9600

2017-03-02T17:44:45.670152: step 9601, loss 0.352444, acc 0.828125
2017-03-02T17:44:45.739795: step 9602, loss 0.29523, acc 0.84375
2017-03-02T17:44:45.804966: step 9603, loss 0.0920384, acc 0.96875
2017-03-02T17:44:45.880142: step 9604, loss 0.207669, acc 1
2017-03-02T17:44:45.966908: step 9605, loss 0.276603, acc 0.859375
2017-03-02T17:44:46.039735: step 9606, loss 0.217085, acc 0.890625
2017-03-02T17:44:46.104471: step 9607, loss 0.148341, acc 0.921875
2017-03-02T17:44:46.177393: step 9608, loss 0.220926, acc 0.90625
2017-03-02T17:44:46.243426: step 9609, loss 0.112623, acc 0.953125
2017-03-02T17:44:46.308446: step 9610, loss 0.0251095, acc 1
2017-03-02T17:44:46.386381: step 9611, loss 0.092265, acc 0.96875
2017-03-02T17:44:46.456867: step 9612, loss 0.238986, acc 0.875
2017-03-02T17:44:46.531980: step 9613, loss 0.171972, acc 0.90625
2017-03-02T17:44:46.609282: step 9614, loss 0.201267, acc 0.921875
2017-03-02T17:44:46.706617: step 9615, loss 0.0949788, acc 0.96875
2017-03-02T17:44:46.775207: step 9616, loss 0.112362, acc 0.96875
2017-03-02T17:44:46.851860: step 9617, loss 0.111266, acc 0.96875
2017-03-02T17:44:46.932852: step 9618, loss 0.122402, acc 0.953125
2017-03-02T17:44:47.007971: step 9619, loss 0.161933, acc 0.9375
2017-03-02T17:44:47.078264: step 9620, loss 0.0784907, acc 0.984375
2017-03-02T17:44:47.152202: step 9621, loss 0.0831885, acc 0.96875
2017-03-02T17:44:47.227984: step 9622, loss 0.0712822, acc 0.96875
2017-03-02T17:44:47.298055: step 9623, loss 0.248513, acc 0.90625
2017-03-02T17:44:47.373730: step 9624, loss 0.194027, acc 0.921875
2017-03-02T17:44:47.465576: step 9625, loss 0.110031, acc 0.96875
2017-03-02T17:44:47.542922: step 9626, loss 0.0820538, acc 0.96875
2017-03-02T17:44:47.613704: step 9627, loss 0.153813, acc 0.921875
2017-03-02T17:44:47.680248: step 9628, loss 0.108144, acc 0.953125
2017-03-02T17:44:47.752295: step 9629, loss 0.0696556, acc 0.984375
2017-03-02T17:44:47.825783: step 9630, loss 0.130233, acc 0.9375
2017-03-02T17:44:47.896763: step 9631, loss 0.185281, acc 0.921875
2017-03-02T17:44:47.970620: step 9632, loss 0.132334, acc 0.953125
2017-03-02T17:44:48.044075: step 9633, loss 0.142543, acc 0.9375
2017-03-02T17:44:48.115949: step 9634, loss 0.355751, acc 0.890625
2017-03-02T17:44:48.185052: step 9635, loss 0.142663, acc 0.9375
2017-03-02T17:44:48.255598: step 9636, loss 0.166378, acc 0.90625
2017-03-02T17:44:48.320402: step 9637, loss 0.303415, acc 0.890625
2017-03-02T17:44:48.390935: step 9638, loss 0.134697, acc 0.9375
2017-03-02T17:44:48.460480: step 9639, loss 0.0874285, acc 0.921875
2017-03-02T17:44:48.538875: step 9640, loss 0.0675822, acc 0.96875
2017-03-02T17:44:48.615665: step 9641, loss 0.165704, acc 0.921875
2017-03-02T17:44:48.691066: step 9642, loss 0.0941855, acc 0.984375
2017-03-02T17:44:48.772880: step 9643, loss 0.161296, acc 0.953125
2017-03-02T17:44:48.852392: step 9644, loss 0.221315, acc 0.875
2017-03-02T17:44:48.929299: step 9645, loss 0.164277, acc 0.9375
2017-03-02T17:44:49.003814: step 9646, loss 0.0735928, acc 0.96875
2017-03-02T17:44:49.080630: step 9647, loss 0.0583846, acc 0.96875
2017-03-02T17:44:49.161854: step 9648, loss 0.249526, acc 0.859375
2017-03-02T17:44:49.252324: step 9649, loss 0.149279, acc 0.96875
2017-03-02T17:44:49.325670: step 9650, loss 0.13604, acc 0.921875
2017-03-02T17:44:49.399250: step 9651, loss 0.167561, acc 0.9375
2017-03-02T17:44:49.484104: step 9652, loss 0.176147, acc 0.96875
2017-03-02T17:44:49.556504: step 9653, loss 0.199272, acc 0.9375
2017-03-02T17:44:49.629285: step 9654, loss 0.158613, acc 0.921875
2017-03-02T17:44:49.703021: step 9655, loss 0.238206, acc 0.890625
2017-03-02T17:44:49.777573: step 9656, loss 0.0501228, acc 0.984375
2017-03-02T17:44:49.850835: step 9657, loss 0.0462272, acc 0.984375
2017-03-02T17:44:49.925960: step 9658, loss 0.189769, acc 0.953125
2017-03-02T17:44:50.001618: step 9659, loss 0.278417, acc 0.890625
2017-03-02T17:44:50.067801: step 9660, loss 0.141231, acc 0.9375
2017-03-02T17:44:50.140030: step 9661, loss 0.156456, acc 0.953125
2017-03-02T17:44:50.213095: step 9662, loss 0.185943, acc 0.90625
2017-03-02T17:44:50.292815: step 9663, loss 0.125741, acc 0.9375
2017-03-02T17:44:50.374798: step 9664, loss 0.169077, acc 0.921875
2017-03-02T17:44:50.451592: step 9665, loss 0.0557287, acc 0.984375
2017-03-02T17:44:50.529316: step 9666, loss 0.132565, acc 0.96875
2017-03-02T17:44:50.596094: step 9667, loss 0.0770334, acc 0.96875
2017-03-02T17:44:50.666591: step 9668, loss 0.242075, acc 0.890625
2017-03-02T17:44:50.747679: step 9669, loss 0.174445, acc 0.921875
2017-03-02T17:44:50.833625: step 9670, loss 0.172708, acc 0.9375
2017-03-02T17:44:50.908222: step 9671, loss 0.118647, acc 0.96875
2017-03-02T17:44:50.983136: step 9672, loss 0.138233, acc 0.953125
2017-03-02T17:44:51.060535: step 9673, loss 0.127522, acc 0.96875
2017-03-02T17:44:51.131580: step 9674, loss 0.171129, acc 0.953125
2017-03-02T17:44:51.199297: step 9675, loss 0.174193, acc 0.9375
2017-03-02T17:44:51.269678: step 9676, loss 0.13953, acc 0.9375
2017-03-02T17:44:51.333568: step 9677, loss 0.188656, acc 0.921875
2017-03-02T17:44:51.395163: step 9678, loss 0.163499, acc 0.9375
2017-03-02T17:44:51.465275: step 9679, loss 0.124081, acc 0.96875
2017-03-02T17:44:51.536704: step 9680, loss 0.210584, acc 0.890625
2017-03-02T17:44:51.616619: step 9681, loss 0.171432, acc 0.9375
2017-03-02T17:44:51.698186: step 9682, loss 0.133331, acc 0.9375
2017-03-02T17:44:51.763477: step 9683, loss 0.141675, acc 0.953125
2017-03-02T17:44:51.836119: step 9684, loss 0.152308, acc 0.953125
2017-03-02T17:44:51.927090: step 9685, loss 0.137727, acc 0.921875
2017-03-02T17:44:52.003823: step 9686, loss 0.0351528, acc 1
2017-03-02T17:44:52.077106: step 9687, loss 0.180759, acc 0.921875
2017-03-02T17:44:52.146846: step 9688, loss 0.217282, acc 0.921875
2017-03-02T17:44:52.219793: step 9689, loss 0.239077, acc 0.9375
2017-03-02T17:44:52.297687: step 9690, loss 0.176277, acc 0.9375
2017-03-02T17:44:52.374187: step 9691, loss 0.270168, acc 0.890625
2017-03-02T17:44:52.440803: step 9692, loss 0.244529, acc 0.9375
2017-03-02T17:44:52.508820: step 9693, loss 0.200884, acc 0.90625
2017-03-02T17:44:52.577200: step 9694, loss 0.235123, acc 0.890625
2017-03-02T17:44:52.654006: step 9695, loss 0.0538764, acc 0.984375
2017-03-02T17:44:52.745369: step 9696, loss 0.112668, acc 0.953125
2017-03-02T17:44:52.820211: step 9697, loss 0.15141, acc 0.921875
2017-03-02T17:44:52.899393: step 9698, loss 0.130145, acc 0.9375
2017-03-02T17:44:52.974308: step 9699, loss 0.1423, acc 0.921875
2017-03-02T17:44:53.047920: step 9700, loss 0.0542643, acc 1

Evaluation:
2017-03-02T17:44:53.072878: step 9700, loss 1.48939, acc 0.682769

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9700

2017-03-02T17:44:53.519472: step 9701, loss 0.212952, acc 0.90625
2017-03-02T17:44:53.590669: step 9702, loss 0.166758, acc 0.921875
2017-03-02T17:44:53.665857: step 9703, loss 0.114995, acc 0.953125
2017-03-02T17:44:53.741904: step 9704, loss 0.200775, acc 0.875
2017-03-02T17:44:53.810480: step 9705, loss 0.179964, acc 0.890625
2017-03-02T17:44:53.879259: step 9706, loss 0.216603, acc 0.90625
2017-03-02T17:44:53.951733: step 9707, loss 0.158295, acc 0.890625
2017-03-02T17:44:54.024567: step 9708, loss 0.156426, acc 0.9375
2017-03-02T17:44:54.105700: step 9709, loss 0.243826, acc 0.90625
2017-03-02T17:44:54.188517: step 9710, loss 0.0855705, acc 0.953125
2017-03-02T17:44:54.257458: step 9711, loss 0.269085, acc 0.921875
2017-03-02T17:44:54.328072: step 9712, loss 0.0294256, acc 1
2017-03-02T17:44:54.402701: step 9713, loss 0.264874, acc 0.90625
2017-03-02T17:44:54.476838: step 9714, loss 0.15466, acc 0.921875
2017-03-02T17:44:54.543582: step 9715, loss 0.31601, acc 0.890625
2017-03-02T17:44:54.614369: step 9716, loss 0.235662, acc 0.90625
2017-03-02T17:44:54.687797: step 9717, loss 0.317725, acc 0.859375
2017-03-02T17:44:54.762664: step 9718, loss 0.0791466, acc 0.953125
2017-03-02T17:44:54.844600: step 9719, loss 0.086735, acc 0.96875
2017-03-02T17:44:54.920146: step 9720, loss 0.220032, acc 0.90625
2017-03-02T17:44:54.987702: step 9721, loss 0.174046, acc 0.90625
2017-03-02T17:44:55.058147: step 9722, loss 0.2177, acc 0.90625
2017-03-02T17:44:55.132311: step 9723, loss 0.296086, acc 0.890625
2017-03-02T17:44:55.207296: step 9724, loss 0.172948, acc 0.953125
2017-03-02T17:44:55.275411: step 9725, loss 0.0925137, acc 0.96875
2017-03-02T17:44:55.351549: step 9726, loss 0.207131, acc 0.921875
2017-03-02T17:44:55.424721: step 9727, loss 0.236155, acc 0.875
2017-03-02T17:44:55.496728: step 9728, loss 0.308846, acc 0.90625
2017-03-02T17:44:55.568537: step 9729, loss 0.128631, acc 0.9375
2017-03-02T17:44:55.642511: step 9730, loss 0.134849, acc 0.953125
2017-03-02T17:44:55.719501: step 9731, loss 0.181843, acc 0.953125
2017-03-02T17:44:55.794082: step 9732, loss 0.112597, acc 0.953125
2017-03-02T17:44:55.870145: step 9733, loss 0.105899, acc 0.96875
2017-03-02T17:44:55.937589: step 9734, loss 0.18318, acc 0.921875
2017-03-02T17:44:56.007706: step 9735, loss 0.0868914, acc 0.96875
2017-03-02T17:44:56.086553: step 9736, loss 0.181311, acc 0.921875
2017-03-02T17:44:56.156243: step 9737, loss 0.172495, acc 0.9375
2017-03-02T17:44:56.229092: step 9738, loss 0.178371, acc 0.875
2017-03-02T17:44:56.312809: step 9739, loss 0.150614, acc 0.9375
2017-03-02T17:44:56.381515: step 9740, loss 0.212693, acc 0.90625
2017-03-02T17:44:56.456791: step 9741, loss 0.174235, acc 0.90625
2017-03-02T17:44:56.525991: step 9742, loss 0.273154, acc 0.9375
2017-03-02T17:44:56.597789: step 9743, loss 0.241455, acc 0.921875
2017-03-02T17:44:56.678800: step 9744, loss 0.344613, acc 0.84375
2017-03-02T17:44:56.752852: step 9745, loss 0.122378, acc 0.921875
2017-03-02T17:44:56.830578: step 9746, loss 0.15108, acc 0.953125
2017-03-02T17:44:56.898866: step 9747, loss 0.103337, acc 0.953125
2017-03-02T17:44:56.983373: step 9748, loss 0.207963, acc 0.921875
2017-03-02T17:44:57.062346: step 9749, loss 0.164787, acc 0.9375
2017-03-02T17:44:57.129356: step 9750, loss 0.209879, acc 0.90625
2017-03-02T17:44:57.207637: step 9751, loss 0.158233, acc 0.921875
2017-03-02T17:44:57.279469: step 9752, loss 0.208257, acc 0.890625
2017-03-02T17:44:57.347436: step 9753, loss 0.129, acc 0.96875
2017-03-02T17:44:57.422324: step 9754, loss 0.226072, acc 0.890625
2017-03-02T17:44:57.497208: step 9755, loss 0.301943, acc 0.890625
2017-03-02T17:44:57.569152: step 9756, loss 0.308387, acc 0.90625
2017-03-02T17:44:57.646979: step 9757, loss 0.138371, acc 0.96875
2017-03-02T17:44:57.719882: step 9758, loss 0.072792, acc 0.984375
2017-03-02T17:44:57.796718: step 9759, loss 0.0318425, acc 1
2017-03-02T17:44:57.885589: step 9760, loss 0.133243, acc 0.96875
2017-03-02T17:44:57.956447: step 9761, loss 0.096047, acc 0.96875
2017-03-02T17:44:58.025229: step 9762, loss 0.0801888, acc 0.96875
2017-03-02T17:44:58.115874: step 9763, loss 0.193306, acc 0.9375
2017-03-02T17:44:58.195190: step 9764, loss 0.261945, acc 0.875
2017-03-02T17:44:58.266198: step 9765, loss 0.219606, acc 0.90625
2017-03-02T17:44:58.361700: step 9766, loss 0.183482, acc 0.96875
2017-03-02T17:44:58.434207: step 9767, loss 0.246846, acc 0.90625
2017-03-02T17:44:58.506350: step 9768, loss 0.235372, acc 0.921875
2017-03-02T17:44:58.581650: step 9769, loss 0.264484, acc 0.84375
2017-03-02T17:44:58.649354: step 9770, loss 0.10564, acc 0.9375
2017-03-02T17:44:58.719567: step 9771, loss 0.32227, acc 0.859375
2017-03-02T17:44:58.785014: step 9772, loss 0.147909, acc 0.921875
2017-03-02T17:44:58.859715: step 9773, loss 0.323679, acc 0.875
2017-03-02T17:44:58.940203: step 9774, loss 0.316844, acc 0.90625
2017-03-02T17:44:59.013485: step 9775, loss 0.141232, acc 0.9375
2017-03-02T17:44:59.090956: step 9776, loss 0.202139, acc 0.9375
2017-03-02T17:44:59.179059: step 9777, loss 0.228694, acc 0.890625
2017-03-02T17:44:59.252404: step 9778, loss 0.150973, acc 0.953125
2017-03-02T17:44:59.316522: step 9779, loss 0.201379, acc 0.875
2017-03-02T17:44:59.382263: step 9780, loss 0.185657, acc 0.9375
2017-03-02T17:44:59.450463: step 9781, loss 0.264445, acc 0.875
2017-03-02T17:44:59.524574: step 9782, loss 0.217869, acc 0.921875
2017-03-02T17:44:59.608918: step 9783, loss 0.338452, acc 0.875
2017-03-02T17:44:59.679463: step 9784, loss 0.25098, acc 0.890625
2017-03-02T17:44:59.763736: step 9785, loss 0.192038, acc 0.90625
2017-03-02T17:44:59.851295: step 9786, loss 0.23789, acc 0.890625
2017-03-02T17:44:59.928060: step 9787, loss 0.265039, acc 0.90625
2017-03-02T17:45:00.004147: step 9788, loss 0.269833, acc 0.890625
2017-03-02T17:45:00.084839: step 9789, loss 0.303514, acc 0.875
2017-03-02T17:45:00.163858: step 9790, loss 0.134882, acc 0.9375
2017-03-02T17:45:00.246023: step 9791, loss 0.175749, acc 0.921875
2017-03-02T17:45:00.325661: step 9792, loss 0.237113, acc 0.890625
2017-03-02T17:45:00.399663: step 9793, loss 0.237262, acc 0.890625
2017-03-02T17:45:00.479631: step 9794, loss 0.307648, acc 0.84375
2017-03-02T17:45:00.547815: step 9795, loss 0.201236, acc 0.921875
2017-03-02T17:45:00.624486: step 9796, loss 0.181061, acc 0.90625
2017-03-02T17:45:00.689574: step 9797, loss 0.117795, acc 0.953125
2017-03-02T17:45:00.757458: step 9798, loss 0.165786, acc 0.921875
2017-03-02T17:45:00.822312: step 9799, loss 0.184011, acc 0.90625
2017-03-02T17:45:00.902141: step 9800, loss 0.0353342, acc 1

Evaluation:
2017-03-02T17:45:00.934784: step 9800, loss 1.41349, acc 0.685652

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9800

2017-03-02T17:45:01.380522: step 9801, loss 0.175537, acc 0.921875
2017-03-02T17:45:01.447291: step 9802, loss 0.073927, acc 0.984375
2017-03-02T17:45:01.525230: step 9803, loss 0.130775, acc 0.921875
2017-03-02T17:45:01.598829: step 9804, loss 0.158982, acc 0.921875
2017-03-02T17:45:01.677818: step 9805, loss 0.102097, acc 0.96875
2017-03-02T17:45:01.754570: step 9806, loss 0.242911, acc 0.9375
2017-03-02T17:45:01.834996: step 9807, loss 0.229449, acc 0.90625
2017-03-02T17:45:01.913793: step 9808, loss 0.105108, acc 0.953125
2017-03-02T17:45:01.994499: step 9809, loss 0.139656, acc 0.9375
2017-03-02T17:45:02.077277: step 9810, loss 0.146075, acc 0.9375
2017-03-02T17:45:02.145159: step 9811, loss 0.183378, acc 0.90625
2017-03-02T17:45:02.214332: step 9812, loss 0.154378, acc 0.90625
2017-03-02T17:45:02.286634: step 9813, loss 0.518311, acc 0.78125
2017-03-02T17:45:02.361718: step 9814, loss 0.199866, acc 0.890625
2017-03-02T17:45:02.437612: step 9815, loss 0.142616, acc 0.9375
2017-03-02T17:45:02.528118: step 9816, loss 0.249483, acc 0.921875
2017-03-02T17:45:02.599283: step 9817, loss 0.135994, acc 0.96875
2017-03-02T17:45:02.676379: step 9818, loss 0.259656, acc 0.890625
2017-03-02T17:45:02.757576: step 9819, loss 0.14132, acc 0.953125
2017-03-02T17:45:02.823445: step 9820, loss 0.134641, acc 0.9375
2017-03-02T17:45:02.893540: step 9821, loss 0.235563, acc 0.921875
2017-03-02T17:45:02.967009: step 9822, loss 0.035538, acc 1
2017-03-02T17:45:03.040537: step 9823, loss 0.234178, acc 0.921875
2017-03-02T17:45:03.116014: step 9824, loss 0.180695, acc 0.90625
2017-03-02T17:45:03.187296: step 9825, loss 0.438494, acc 0.796875
2017-03-02T17:45:03.258159: step 9826, loss 0.225148, acc 0.890625
2017-03-02T17:45:03.330496: step 9827, loss 0.107804, acc 0.921875
2017-03-02T17:45:03.407789: step 9828, loss 0.251183, acc 0.90625
2017-03-02T17:45:03.479655: step 9829, loss 0.181612, acc 0.9375
2017-03-02T17:45:03.546109: step 9830, loss 0.209278, acc 0.890625
2017-03-02T17:45:03.618488: step 9831, loss 0.159037, acc 0.96875
2017-03-02T17:45:03.716514: step 9832, loss 0.25046, acc 0.875
2017-03-02T17:45:03.789859: step 9833, loss 0.0744871, acc 0.96875
2017-03-02T17:45:03.855620: step 9834, loss 0.180166, acc 0.9375
2017-03-02T17:45:03.929820: step 9835, loss 0.174639, acc 0.9375
2017-03-02T17:45:04.006780: step 9836, loss 0.115988, acc 0.96875
2017-03-02T17:45:04.082071: step 9837, loss 0.173437, acc 0.9375
2017-03-02T17:45:04.165533: step 9838, loss 0.108582, acc 0.96875
2017-03-02T17:45:04.233253: step 9839, loss 0.154522, acc 0.90625
2017-03-02T17:45:04.306387: step 9840, loss 0.0867106, acc 0.984375
2017-03-02T17:45:04.384075: step 9841, loss 0.169481, acc 0.90625
2017-03-02T17:45:04.460188: step 9842, loss 0.126738, acc 0.96875
2017-03-02T17:45:04.531399: step 9843, loss 0.140583, acc 0.953125
2017-03-02T17:45:04.608844: step 9844, loss 0.208496, acc 0.890625
2017-03-02T17:45:04.685595: step 9845, loss 0.0534014, acc 0.984375
2017-03-02T17:45:04.757718: step 9846, loss 0.236847, acc 0.890625
2017-03-02T17:45:04.840700: step 9847, loss 0.0869476, acc 0.96875
2017-03-02T17:45:04.909017: step 9848, loss 0.111166, acc 0.9375
2017-03-02T17:45:04.971578: step 9849, loss 0.207915, acc 0.859375
2017-03-02T17:45:05.058891: step 9850, loss 0.11257, acc 0.9375
2017-03-02T17:45:05.140115: step 9851, loss 0.154445, acc 0.921875
2017-03-02T17:45:05.215916: step 9852, loss 0.152719, acc 0.9375
2017-03-02T17:45:05.284791: step 9853, loss 0.133453, acc 0.953125
2017-03-02T17:45:05.349027: step 9854, loss 0.092524, acc 0.96875
2017-03-02T17:45:05.417743: step 9855, loss 0.229627, acc 0.921875
2017-03-02T17:45:05.495507: step 9856, loss 0.291526, acc 0.90625
2017-03-02T17:45:05.562055: step 9857, loss 0.292169, acc 0.90625
2017-03-02T17:45:05.633660: step 9858, loss 0.196248, acc 0.9375
2017-03-02T17:45:05.726008: step 9859, loss 0.204545, acc 0.921875
2017-03-02T17:45:05.798650: step 9860, loss 0.2182, acc 0.890625
2017-03-02T17:45:05.874252: step 9861, loss 0.127218, acc 0.953125
2017-03-02T17:45:05.945644: step 9862, loss 0.222053, acc 0.921875
2017-03-02T17:45:06.031328: step 9863, loss 0.106795, acc 0.9375
2017-03-02T17:45:06.100559: step 9864, loss 0.130484, acc 0.96875
2017-03-02T17:45:06.195252: step 9865, loss 0.218019, acc 0.890625
2017-03-02T17:45:06.285367: step 9866, loss 0.106126, acc 0.96875
2017-03-02T17:45:06.348529: step 9867, loss 0.269413, acc 0.921875
2017-03-02T17:45:06.411558: step 9868, loss 0.259571, acc 0.90625
2017-03-02T17:45:06.486485: step 9869, loss 0.209954, acc 0.96875
2017-03-02T17:45:06.560365: step 9870, loss 0.157792, acc 0.984375
2017-03-02T17:45:06.631037: step 9871, loss 0.253359, acc 0.90625
2017-03-02T17:45:06.703417: step 9872, loss 0.287855, acc 0.953125
2017-03-02T17:45:06.775790: step 9873, loss 0.173992, acc 0.9375
2017-03-02T17:45:06.849597: step 9874, loss 0.104546, acc 0.953125
2017-03-02T17:45:06.919912: step 9875, loss 0.089347, acc 0.953125
2017-03-02T17:45:06.987621: step 9876, loss 0.0560886, acc 1
2017-03-02T17:45:07.058322: step 9877, loss 0.187838, acc 0.890625
2017-03-02T17:45:07.131339: step 9878, loss 0.156835, acc 0.9375
2017-03-02T17:45:07.201429: step 9879, loss 0.167673, acc 0.921875
2017-03-02T17:45:07.279992: step 9880, loss 0.0721234, acc 0.96875
2017-03-02T17:45:07.351506: step 9881, loss 0.162201, acc 0.90625
2017-03-02T17:45:07.423126: step 9882, loss 0.125002, acc 0.9375
2017-03-02T17:45:07.494632: step 9883, loss 0.135277, acc 0.9375
2017-03-02T17:45:07.573012: step 9884, loss 0.187417, acc 0.921875
2017-03-02T17:45:07.638132: step 9885, loss 0.202445, acc 0.9375
2017-03-02T17:45:07.705718: step 9886, loss 0.165794, acc 0.90625
2017-03-02T17:45:07.777416: step 9887, loss 0.154446, acc 0.921875
2017-03-02T17:45:07.856571: step 9888, loss 0.142626, acc 0.953125
2017-03-02T17:45:07.928487: step 9889, loss 0.0727139, acc 0.984375
2017-03-02T17:45:08.006068: step 9890, loss 0.0648223, acc 0.96875
2017-03-02T17:45:08.089924: step 9891, loss 0.206794, acc 0.90625
2017-03-02T17:45:08.161223: step 9892, loss 0.210636, acc 0.90625
2017-03-02T17:45:08.235186: step 9893, loss 0.19419, acc 0.953125
2017-03-02T17:45:08.303621: step 9894, loss 0.160329, acc 0.921875
2017-03-02T17:45:08.375504: step 9895, loss 0.0843821, acc 0.953125
2017-03-02T17:45:08.448586: step 9896, loss 0.308445, acc 0.921875
2017-03-02T17:45:08.523438: step 9897, loss 0.0728092, acc 0.96875
2017-03-02T17:45:08.597070: step 9898, loss 0.153002, acc 0.9375
2017-03-02T17:45:08.667747: step 9899, loss 0.0948925, acc 0.953125
2017-03-02T17:45:08.748667: step 9900, loss 0.206529, acc 0.90625

Evaluation:
2017-03-02T17:45:08.782514: step 9900, loss 1.45797, acc 0.674117

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-9900

2017-03-02T17:45:10.486128: step 9901, loss 0.32559, acc 0.859375
2017-03-02T17:45:10.557767: step 9902, loss 0.162885, acc 0.9375
2017-03-02T17:45:10.634673: step 9903, loss 0.163389, acc 0.9375
2017-03-02T17:45:10.711210: step 9904, loss 0.0786515, acc 0.984375
2017-03-02T17:45:10.786955: step 9905, loss 0.137904, acc 0.9375
2017-03-02T17:45:10.863745: step 9906, loss 0.149386, acc 0.9375
2017-03-02T17:45:10.935076: step 9907, loss 0.173807, acc 0.921875
2017-03-02T17:45:11.007230: step 9908, loss 0.183093, acc 0.890625
2017-03-02T17:45:11.080692: step 9909, loss 0.187517, acc 0.921875
2017-03-02T17:45:11.160381: step 9910, loss 0.204188, acc 0.921875
2017-03-02T17:45:11.240074: step 9911, loss 0.167414, acc 0.921875
2017-03-02T17:45:11.311341: step 9912, loss 0.103944, acc 0.9375
2017-03-02T17:45:11.383168: step 9913, loss 0.230704, acc 0.859375
2017-03-02T17:45:11.458089: step 9914, loss 0.203359, acc 0.9375
2017-03-02T17:45:11.532421: step 9915, loss 0.164145, acc 0.921875
2017-03-02T17:45:11.607767: step 9916, loss 0.164957, acc 0.90625
2017-03-02T17:45:11.679937: step 9917, loss 0.267689, acc 0.875
2017-03-02T17:45:11.757775: step 9918, loss 0.141924, acc 0.921875
2017-03-02T17:45:11.833724: step 9919, loss 0.211396, acc 0.921875
2017-03-02T17:45:11.910967: step 9920, loss 0.353735, acc 0.859375
2017-03-02T17:45:11.981936: step 9921, loss 0.251086, acc 0.90625
2017-03-02T17:45:12.050579: step 9922, loss 0.232184, acc 0.921875
2017-03-02T17:45:12.126120: step 9923, loss 0.0815816, acc 0.96875
2017-03-02T17:45:12.201096: step 9924, loss 0.156786, acc 0.96875
2017-03-02T17:45:12.272935: step 9925, loss 0.178114, acc 0.921875
2017-03-02T17:45:12.340156: step 9926, loss 0.161805, acc 0.96875
2017-03-02T17:45:12.410726: step 9927, loss 0.204446, acc 0.90625
2017-03-02T17:45:12.483971: step 9928, loss 0.120709, acc 0.953125
2017-03-02T17:45:12.565502: step 9929, loss 0.0630308, acc 0.984375
2017-03-02T17:45:12.638088: step 9930, loss 0.238473, acc 0.890625
2017-03-02T17:45:12.714501: step 9931, loss 0.110646, acc 0.953125
2017-03-02T17:45:12.785544: step 9932, loss 0.136503, acc 0.9375
2017-03-02T17:45:12.857344: step 9933, loss 0.112312, acc 0.96875
2017-03-02T17:45:12.929352: step 9934, loss 0.259003, acc 0.875
2017-03-02T17:45:12.999060: step 9935, loss 0.236739, acc 0.90625
2017-03-02T17:45:13.078259: step 9936, loss 0.0819174, acc 0.96875
2017-03-02T17:45:13.156073: step 9937, loss 0.202622, acc 0.890625
2017-03-02T17:45:13.228021: step 9938, loss 0.121556, acc 0.953125
2017-03-02T17:45:13.302467: step 9939, loss 0.12168, acc 0.953125
2017-03-02T17:45:13.373817: step 9940, loss 0.238054, acc 0.921875
2017-03-02T17:45:13.446612: step 9941, loss 0.181575, acc 0.90625
2017-03-02T17:45:13.517831: step 9942, loss 0.0800363, acc 0.96875
2017-03-02T17:45:13.594452: step 9943, loss 0.154707, acc 0.96875
2017-03-02T17:45:13.662784: step 9944, loss 0.0886271, acc 0.96875
2017-03-02T17:45:13.732515: step 9945, loss 0.136632, acc 0.953125
2017-03-02T17:45:13.796930: step 9946, loss 0.0620288, acc 0.96875
2017-03-02T17:45:13.865675: step 9947, loss 0.141475, acc 0.953125
2017-03-02T17:45:13.943129: step 9948, loss 0.0907075, acc 0.984375
2017-03-02T17:45:14.011619: step 9949, loss 0.123865, acc 0.953125
2017-03-02T17:45:14.081715: step 9950, loss 0.170953, acc 0.90625
2017-03-02T17:45:14.152499: step 9951, loss 0.0902737, acc 0.9375
2017-03-02T17:45:14.219134: step 9952, loss 0.142152, acc 0.90625
2017-03-02T17:45:14.286578: step 9953, loss 0.157317, acc 0.921875
2017-03-02T17:45:14.372991: step 9954, loss 0.163706, acc 0.9375
2017-03-02T17:45:14.441398: step 9955, loss 0.338074, acc 0.921875
2017-03-02T17:45:14.528828: step 9956, loss 0.251386, acc 0.90625
2017-03-02T17:45:14.607368: step 9957, loss 0.168025, acc 0.953125
2017-03-02T17:45:14.680013: step 9958, loss 0.119404, acc 0.953125
2017-03-02T17:45:14.757289: step 9959, loss 0.313562, acc 0.90625
2017-03-02T17:45:14.833276: step 9960, loss 0.176924, acc 0.953125
2017-03-02T17:45:14.906546: step 9961, loss 0.142755, acc 0.9375
2017-03-02T17:45:14.980471: step 9962, loss 0.291505, acc 0.890625
2017-03-02T17:45:15.048172: step 9963, loss 0.325286, acc 0.859375
2017-03-02T17:45:15.121693: step 9964, loss 0.146415, acc 0.953125
2017-03-02T17:45:15.194909: step 9965, loss 0.217593, acc 0.9375
2017-03-02T17:45:15.277082: step 9966, loss 0.211926, acc 0.890625
2017-03-02T17:45:15.354123: step 9967, loss 0.169831, acc 0.953125
2017-03-02T17:45:15.427962: step 9968, loss 0.073472, acc 0.96875
2017-03-02T17:45:15.507462: step 9969, loss 0.276052, acc 0.84375
2017-03-02T17:45:15.578441: step 9970, loss 0.188906, acc 0.921875
2017-03-02T17:45:15.661188: step 9971, loss 0.116327, acc 0.953125
2017-03-02T17:45:15.740544: step 9972, loss 0.207331, acc 0.921875
2017-03-02T17:45:15.815084: step 9973, loss 0.132453, acc 0.953125
2017-03-02T17:45:15.883463: step 9974, loss 0.209531, acc 0.921875
2017-03-02T17:45:15.957914: step 9975, loss 0.150678, acc 0.9375
2017-03-02T17:45:16.026956: step 9976, loss 0.158176, acc 0.9375
2017-03-02T17:45:16.098939: step 9977, loss 0.201676, acc 0.9375
2017-03-02T17:45:16.170343: step 9978, loss 0.310184, acc 0.890625
2017-03-02T17:45:16.248364: step 9979, loss 0.176058, acc 0.9375
2017-03-02T17:45:16.324921: step 9980, loss 0.292744, acc 0.921875
2017-03-02T17:45:16.396649: step 9981, loss 0.290174, acc 0.921875
2017-03-02T17:45:16.459293: step 9982, loss 0.121034, acc 0.953125
2017-03-02T17:45:16.527117: step 9983, loss 0.0718527, acc 0.96875
2017-03-02T17:45:16.601553: step 9984, loss 0.20684, acc 0.90625
2017-03-02T17:45:16.674495: step 9985, loss 0.151463, acc 0.9375
2017-03-02T17:45:16.757733: step 9986, loss 0.168567, acc 0.90625
2017-03-02T17:45:16.829966: step 9987, loss 0.346029, acc 0.859375
2017-03-02T17:45:16.900865: step 9988, loss 0.129044, acc 0.953125
2017-03-02T17:45:16.972992: step 9989, loss 0.155832, acc 0.921875
2017-03-02T17:45:17.044867: step 9990, loss 0.196177, acc 0.875
2017-03-02T17:45:17.118669: step 9991, loss 0.109772, acc 0.921875
2017-03-02T17:45:17.183323: step 9992, loss 0.145945, acc 0.953125
2017-03-02T17:45:17.247457: step 9993, loss 0.0989925, acc 0.953125
2017-03-02T17:45:17.326475: step 9994, loss 0.34656, acc 0.875
2017-03-02T17:45:17.409084: step 9995, loss 0.152695, acc 0.9375
2017-03-02T17:45:17.480775: step 9996, loss 0.000135944, acc 1
2017-03-02T17:45:17.572462: step 9997, loss 0.173349, acc 0.90625
2017-03-02T17:45:17.645530: step 9998, loss 0.0910917, acc 0.96875
2017-03-02T17:45:17.729989: step 9999, loss 0.110256, acc 0.921875
2017-03-02T17:45:17.805796: step 10000, loss 0.198136, acc 0.9375

Evaluation:
2017-03-02T17:45:17.829826: step 10000, loss 1.42386, acc 0.660418

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10000

2017-03-02T17:45:18.356765: step 10001, loss 0.0956628, acc 0.953125
2017-03-02T17:45:18.430084: step 10002, loss 0.105547, acc 0.96875
2017-03-02T17:45:18.503439: step 10003, loss 0.0918272, acc 0.96875
2017-03-02T17:45:18.574371: step 10004, loss 0.172792, acc 0.9375
2017-03-02T17:45:18.666287: step 10005, loss 0.108234, acc 0.96875
2017-03-02T17:45:18.737309: step 10006, loss 0.104678, acc 0.984375
2017-03-02T17:45:18.809868: step 10007, loss 0.247872, acc 0.875
2017-03-02T17:45:18.887737: step 10008, loss 0.149769, acc 0.9375
2017-03-02T17:45:18.975872: step 10009, loss 0.162812, acc 0.921875
2017-03-02T17:45:19.047100: step 10010, loss 0.195532, acc 0.921875
2017-03-02T17:45:19.113753: step 10011, loss 0.273452, acc 0.875
2017-03-02T17:45:19.181011: step 10012, loss 0.197217, acc 0.953125
2017-03-02T17:45:19.252935: step 10013, loss 0.15451, acc 0.9375
2017-03-02T17:45:19.323555: step 10014, loss 0.235642, acc 0.921875
2017-03-02T17:45:19.396633: step 10015, loss 0.195469, acc 0.90625
2017-03-02T17:45:19.468167: step 10016, loss 0.177668, acc 0.90625
2017-03-02T17:45:19.534243: step 10017, loss 0.127272, acc 0.96875
2017-03-02T17:45:19.609928: step 10018, loss 0.076209, acc 0.984375
2017-03-02T17:45:19.679694: step 10019, loss 0.125576, acc 0.9375
2017-03-02T17:45:19.753726: step 10020, loss 0.139547, acc 0.9375
2017-03-02T17:45:19.827483: step 10021, loss 0.179114, acc 0.9375
2017-03-02T17:45:19.897515: step 10022, loss 0.148711, acc 0.953125
2017-03-02T17:45:19.969830: step 10023, loss 0.168846, acc 0.9375
2017-03-02T17:45:20.047224: step 10024, loss 0.236322, acc 0.890625
2017-03-02T17:45:20.117429: step 10025, loss 0.0738787, acc 0.984375
2017-03-02T17:45:20.189137: step 10026, loss 0.152443, acc 0.921875
2017-03-02T17:45:20.260394: step 10027, loss 0.202621, acc 0.890625
2017-03-02T17:45:20.339303: step 10028, loss 0.147762, acc 0.96875
2017-03-02T17:45:20.417655: step 10029, loss 0.251414, acc 0.890625
2017-03-02T17:45:20.491600: step 10030, loss 0.217424, acc 0.9375
2017-03-02T17:45:20.562322: step 10031, loss 0.143447, acc 0.9375
2017-03-02T17:45:20.651145: step 10032, loss 0.17978, acc 0.953125
2017-03-02T17:45:20.725391: step 10033, loss 0.0856082, acc 0.96875
2017-03-02T17:45:20.810900: step 10034, loss 0.149268, acc 0.9375
2017-03-02T17:45:20.884924: step 10035, loss 0.138713, acc 0.953125
2017-03-02T17:45:20.957380: step 10036, loss 0.1314, acc 0.921875
2017-03-02T17:45:21.032984: step 10037, loss 0.192038, acc 0.921875
2017-03-02T17:45:21.104466: step 10038, loss 0.142995, acc 0.9375
2017-03-02T17:45:21.185515: step 10039, loss 0.107978, acc 0.96875
2017-03-02T17:45:21.257771: step 10040, loss 0.215705, acc 0.90625
2017-03-02T17:45:21.330113: step 10041, loss 0.200653, acc 0.890625
2017-03-02T17:45:21.403963: step 10042, loss 0.0901822, acc 0.96875
2017-03-02T17:45:21.483963: step 10043, loss 0.390862, acc 0.84375
2017-03-02T17:45:21.555189: step 10044, loss 0.124339, acc 0.953125
2017-03-02T17:45:21.632125: step 10045, loss 0.164639, acc 0.921875
2017-03-02T17:45:21.711970: step 10046, loss 0.0932694, acc 0.953125
2017-03-02T17:45:21.789999: step 10047, loss 0.0419923, acc 0.984375
2017-03-02T17:45:21.862151: step 10048, loss 0.166841, acc 0.9375
2017-03-02T17:45:21.947583: step 10049, loss 0.102637, acc 0.953125
2017-03-02T17:45:22.021068: step 10050, loss 0.213028, acc 0.890625
2017-03-02T17:45:22.089112: step 10051, loss 0.292547, acc 0.890625
2017-03-02T17:45:22.159996: step 10052, loss 0.207231, acc 0.921875
2017-03-02T17:45:22.232606: step 10053, loss 0.0898868, acc 0.96875
2017-03-02T17:45:22.302418: step 10054, loss 0.162747, acc 0.9375
2017-03-02T17:45:22.377558: step 10055, loss 0.221809, acc 0.875
2017-03-02T17:45:22.459252: step 10056, loss 0.154825, acc 0.921875
2017-03-02T17:45:22.533281: step 10057, loss 0.213047, acc 0.921875
2017-03-02T17:45:22.617430: step 10058, loss 0.203587, acc 0.90625
2017-03-02T17:45:22.684178: step 10059, loss 0.132659, acc 0.921875
2017-03-02T17:45:22.754165: step 10060, loss 0.0375404, acc 1
2017-03-02T17:45:22.827897: step 10061, loss 0.196699, acc 0.921875
2017-03-02T17:45:22.907139: step 10062, loss 0.178088, acc 0.921875
2017-03-02T17:45:22.981944: step 10063, loss 0.208967, acc 0.90625
2017-03-02T17:45:23.067804: step 10064, loss 0.171945, acc 0.921875
2017-03-02T17:45:23.142263: step 10065, loss 0.210333, acc 0.9375
2017-03-02T17:45:23.219424: step 10066, loss 0.112512, acc 0.921875
2017-03-02T17:45:23.296924: step 10067, loss 0.0541893, acc 0.984375
2017-03-02T17:45:23.367652: step 10068, loss 0.186492, acc 0.921875
2017-03-02T17:45:23.441410: step 10069, loss 0.0670874, acc 0.953125
2017-03-02T17:45:23.515111: step 10070, loss 0.073529, acc 0.96875
2017-03-02T17:45:23.587372: step 10071, loss 0.120258, acc 0.9375
2017-03-02T17:45:23.658953: step 10072, loss 0.0800451, acc 0.96875
2017-03-02T17:45:23.735974: step 10073, loss 0.137656, acc 0.9375
2017-03-02T17:45:23.821539: step 10074, loss 0.170823, acc 0.921875
2017-03-02T17:45:23.898226: step 10075, loss 0.175438, acc 0.9375
2017-03-02T17:45:23.969238: step 10076, loss 0.109304, acc 0.96875
2017-03-02T17:45:24.039734: step 10077, loss 0.231739, acc 0.890625
2017-03-02T17:45:24.109783: step 10078, loss 0.150647, acc 0.953125
2017-03-02T17:45:24.179719: step 10079, loss 0.227779, acc 0.875
2017-03-02T17:45:24.255473: step 10080, loss 0.203323, acc 0.921875
2017-03-02T17:45:24.328739: step 10081, loss 0.144648, acc 0.9375
2017-03-02T17:45:24.402632: step 10082, loss 0.140069, acc 0.9375
2017-03-02T17:45:24.485144: step 10083, loss 0.121695, acc 0.9375
2017-03-02T17:45:24.560636: step 10084, loss 0.0922422, acc 0.96875
2017-03-02T17:45:24.631439: step 10085, loss 0.155867, acc 0.921875
2017-03-02T17:45:24.705982: step 10086, loss 0.137454, acc 0.953125
2017-03-02T17:45:24.774306: step 10087, loss 0.19287, acc 0.953125
2017-03-02T17:45:24.843794: step 10088, loss 0.165204, acc 0.9375
2017-03-02T17:45:24.919587: step 10089, loss 0.210635, acc 0.90625
2017-03-02T17:45:25.007082: step 10090, loss 0.16561, acc 0.9375
2017-03-02T17:45:25.083383: step 10091, loss 0.156904, acc 0.953125
2017-03-02T17:45:25.155786: step 10092, loss 0.119928, acc 0.9375
2017-03-02T17:45:25.230876: step 10093, loss 0.180579, acc 0.90625
2017-03-02T17:45:25.305049: step 10094, loss 0.152024, acc 0.953125
2017-03-02T17:45:25.379858: step 10095, loss 0.247344, acc 0.90625
2017-03-02T17:45:25.454551: step 10096, loss 0.138591, acc 0.9375
2017-03-02T17:45:25.519933: step 10097, loss 0.107936, acc 0.96875
2017-03-02T17:45:25.590063: step 10098, loss 0.0862227, acc 0.953125
2017-03-02T17:45:25.662324: step 10099, loss 0.136125, acc 0.953125
2017-03-02T17:45:25.735249: step 10100, loss 0.21074, acc 0.9375

Evaluation:
2017-03-02T17:45:25.770675: step 10100, loss 1.4825, acc 0.66907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10100

2017-03-02T17:45:26.244102: step 10101, loss 0.177222, acc 0.9375
2017-03-02T17:45:26.322805: step 10102, loss 0.161078, acc 0.9375
2017-03-02T17:45:26.410638: step 10103, loss 0.167035, acc 0.953125
2017-03-02T17:45:26.485232: step 10104, loss 0.155584, acc 0.90625
2017-03-02T17:45:26.559222: step 10105, loss 0.277448, acc 0.921875
2017-03-02T17:45:26.641722: step 10106, loss 0.210008, acc 0.90625
2017-03-02T17:45:26.715516: step 10107, loss 0.165081, acc 0.921875
2017-03-02T17:45:26.787903: step 10108, loss 0.211471, acc 0.90625
2017-03-02T17:45:26.861410: step 10109, loss 0.219239, acc 0.921875
2017-03-02T17:45:26.931267: step 10110, loss 0.122996, acc 0.9375
2017-03-02T17:45:27.005762: step 10111, loss 0.3035, acc 0.90625
2017-03-02T17:45:27.080155: step 10112, loss 0.112197, acc 0.9375
2017-03-02T17:45:27.151146: step 10113, loss 0.157237, acc 0.921875
2017-03-02T17:45:27.222120: step 10114, loss 0.131877, acc 0.96875
2017-03-02T17:45:27.302571: step 10115, loss 0.124026, acc 0.96875
2017-03-02T17:45:27.383036: step 10116, loss 0.185303, acc 0.9375
2017-03-02T17:45:27.464792: step 10117, loss 0.130001, acc 0.953125
2017-03-02T17:45:27.533334: step 10118, loss 0.337272, acc 0.828125
2017-03-02T17:45:27.602582: step 10119, loss 0.0877086, acc 0.96875
2017-03-02T17:45:27.676000: step 10120, loss 0.136076, acc 0.953125
2017-03-02T17:45:27.751739: step 10121, loss 0.1014, acc 0.953125
2017-03-02T17:45:27.826081: step 10122, loss 0.126883, acc 0.953125
2017-03-02T17:45:27.904361: step 10123, loss 0.245874, acc 0.859375
2017-03-02T17:45:27.977303: step 10124, loss 0.159423, acc 0.921875
2017-03-02T17:45:28.044140: step 10125, loss 0.0943645, acc 0.9375
2017-03-02T17:45:28.121128: step 10126, loss 0.233909, acc 0.921875
2017-03-02T17:45:28.195999: step 10127, loss 0.233465, acc 0.921875
2017-03-02T17:45:28.265067: step 10128, loss 0.114351, acc 0.921875
2017-03-02T17:45:28.337508: step 10129, loss 0.178878, acc 0.921875
2017-03-02T17:45:28.411062: step 10130, loss 0.304614, acc 0.890625
2017-03-02T17:45:28.482471: step 10131, loss 0.278839, acc 0.90625
2017-03-02T17:45:28.553535: step 10132, loss 0.288733, acc 0.875
2017-03-02T17:45:28.619495: step 10133, loss 0.284161, acc 0.890625
2017-03-02T17:45:28.695319: step 10134, loss 0.136597, acc 0.921875
2017-03-02T17:45:28.767847: step 10135, loss 0.0294229, acc 1
2017-03-02T17:45:28.845323: step 10136, loss 0.363584, acc 0.875
2017-03-02T17:45:28.916486: step 10137, loss 0.0564121, acc 0.96875
2017-03-02T17:45:28.978648: step 10138, loss 0.102275, acc 0.9375
2017-03-02T17:45:29.057612: step 10139, loss 0.215309, acc 0.953125
2017-03-02T17:45:29.130225: step 10140, loss 0.0884226, acc 0.984375
2017-03-02T17:45:29.204464: step 10141, loss 0.19074, acc 0.890625
2017-03-02T17:45:29.274974: step 10142, loss 0.130699, acc 0.953125
2017-03-02T17:45:29.345026: step 10143, loss 0.160137, acc 0.953125
2017-03-02T17:45:29.415479: step 10144, loss 0.250713, acc 0.890625
2017-03-02T17:45:29.493631: step 10145, loss 0.197904, acc 0.9375
2017-03-02T17:45:29.558761: step 10146, loss 0.165978, acc 0.921875
2017-03-02T17:45:29.633892: step 10147, loss 0.147231, acc 0.90625
2017-03-02T17:45:29.719218: step 10148, loss 0.318968, acc 0.890625
2017-03-02T17:45:29.791029: step 10149, loss 0.252483, acc 0.859375
2017-03-02T17:45:29.861198: step 10150, loss 0.145901, acc 0.953125
2017-03-02T17:45:29.940973: step 10151, loss 0.13693, acc 0.921875
2017-03-02T17:45:30.015488: step 10152, loss 0.146103, acc 0.921875
2017-03-02T17:45:30.103817: step 10153, loss 0.159265, acc 0.953125
2017-03-02T17:45:30.173614: step 10154, loss 0.209703, acc 0.921875
2017-03-02T17:45:30.244624: step 10155, loss 0.196032, acc 0.9375
2017-03-02T17:45:30.315106: step 10156, loss 0.287468, acc 0.859375
2017-03-02T17:45:30.382534: step 10157, loss 0.095286, acc 0.96875
2017-03-02T17:45:30.455596: step 10158, loss 0.169634, acc 0.890625
2017-03-02T17:45:30.535999: step 10159, loss 0.186034, acc 0.921875
2017-03-02T17:45:30.617585: step 10160, loss 0.143901, acc 0.921875
2017-03-02T17:45:30.694098: step 10161, loss 0.2851, acc 0.875
2017-03-02T17:45:30.778855: step 10162, loss 0.275865, acc 0.90625
2017-03-02T17:45:30.856956: step 10163, loss 0.142898, acc 0.9375
2017-03-02T17:45:30.932324: step 10164, loss 0.132991, acc 0.953125
2017-03-02T17:45:31.001552: step 10165, loss 0.248931, acc 0.90625
2017-03-02T17:45:31.073780: step 10166, loss 0.178236, acc 0.90625
2017-03-02T17:45:31.165706: step 10167, loss 0.146744, acc 0.953125
2017-03-02T17:45:31.243095: step 10168, loss 0.319984, acc 0.84375
2017-03-02T17:45:31.317569: step 10169, loss 0.233073, acc 0.890625
2017-03-02T17:45:31.388251: step 10170, loss 0.0891133, acc 0.9375
2017-03-02T17:45:31.462340: step 10171, loss 0.163175, acc 0.90625
2017-03-02T17:45:31.538537: step 10172, loss 0.231817, acc 0.90625
2017-03-02T17:45:31.614138: step 10173, loss 0.179102, acc 0.921875
2017-03-02T17:45:31.679052: step 10174, loss 0.106436, acc 0.9375
2017-03-02T17:45:31.746779: step 10175, loss 0.125743, acc 0.953125
2017-03-02T17:45:31.822024: step 10176, loss 0.184243, acc 0.90625
2017-03-02T17:45:31.897423: step 10177, loss 0.343331, acc 0.84375
2017-03-02T17:45:31.972959: step 10178, loss 0.257306, acc 0.890625
2017-03-02T17:45:32.046926: step 10179, loss 0.15246, acc 0.953125
2017-03-02T17:45:32.122665: step 10180, loss 0.20806, acc 0.90625
2017-03-02T17:45:32.190406: step 10181, loss 0.0964928, acc 0.984375
2017-03-02T17:45:32.263302: step 10182, loss 0.127584, acc 0.984375
2017-03-02T17:45:32.340045: step 10183, loss 0.29108, acc 0.859375
2017-03-02T17:45:32.416129: step 10184, loss 0.344018, acc 0.890625
2017-03-02T17:45:32.487509: step 10185, loss 0.207211, acc 0.90625
2017-03-02T17:45:32.563027: step 10186, loss 0.408902, acc 0.828125
2017-03-02T17:45:32.637877: step 10187, loss 0.16246, acc 0.921875
2017-03-02T17:45:32.710454: step 10188, loss 0.109672, acc 0.9375
2017-03-02T17:45:32.775579: step 10189, loss 0.213943, acc 0.9375
2017-03-02T17:45:32.849060: step 10190, loss 0.140496, acc 0.9375
2017-03-02T17:45:32.923205: step 10191, loss 0.118257, acc 0.96875
2017-03-02T17:45:32.991108: step 10192, loss 0.0477839, acc 1
2017-03-02T17:45:33.064346: step 10193, loss 0.114198, acc 0.953125
2017-03-02T17:45:33.141964: step 10194, loss 0.1061, acc 0.984375
2017-03-02T17:45:33.209919: step 10195, loss 0.182727, acc 0.921875
2017-03-02T17:45:33.276130: step 10196, loss 0.195141, acc 0.90625
2017-03-02T17:45:33.349439: step 10197, loss 0.0660654, acc 0.96875
2017-03-02T17:45:33.421254: step 10198, loss 0.250463, acc 0.890625
2017-03-02T17:45:33.500135: step 10199, loss 0.138868, acc 0.953125
2017-03-02T17:45:33.570449: step 10200, loss 0.183591, acc 0.953125

Evaluation:
2017-03-02T17:45:33.609832: step 10200, loss 1.41465, acc 0.671954

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10200

2017-03-02T17:45:34.051440: step 10201, loss 0.138929, acc 0.953125
2017-03-02T17:45:34.123190: step 10202, loss 0.0810358, acc 0.953125
2017-03-02T17:45:34.196539: step 10203, loss 0.112157, acc 0.9375
2017-03-02T17:45:34.269952: step 10204, loss 0.126069, acc 0.9375
2017-03-02T17:45:34.345721: step 10205, loss 0.0994819, acc 0.96875
2017-03-02T17:45:34.408829: step 10206, loss 0.201734, acc 0.9375
2017-03-02T17:45:34.478047: step 10207, loss 0.166189, acc 0.9375
2017-03-02T17:45:34.546048: step 10208, loss 0.106835, acc 0.953125
2017-03-02T17:45:34.619760: step 10209, loss 0.159022, acc 0.9375
2017-03-02T17:45:34.697520: step 10210, loss 0.160053, acc 0.921875
2017-03-02T17:45:34.768989: step 10211, loss 0.141025, acc 0.9375
2017-03-02T17:45:34.855664: step 10212, loss 0.198927, acc 0.859375
2017-03-02T17:45:34.931282: step 10213, loss 0.152041, acc 0.9375
2017-03-02T17:45:35.008322: step 10214, loss 0.0785295, acc 0.953125
2017-03-02T17:45:35.082066: step 10215, loss 0.133295, acc 0.9375
2017-03-02T17:45:35.160976: step 10216, loss 0.184146, acc 0.953125
2017-03-02T17:45:35.228634: step 10217, loss 0.111398, acc 0.984375
2017-03-02T17:45:35.301697: step 10218, loss 0.0518647, acc 0.984375
2017-03-02T17:45:35.373063: step 10219, loss 0.197445, acc 0.90625
2017-03-02T17:45:35.448379: step 10220, loss 0.167747, acc 0.9375
2017-03-02T17:45:35.522646: step 10221, loss 0.273788, acc 0.875
2017-03-02T17:45:35.598305: step 10222, loss 0.16057, acc 0.9375
2017-03-02T17:45:35.676246: step 10223, loss 0.212224, acc 0.90625
2017-03-02T17:45:35.756063: step 10224, loss 0.190215, acc 0.890625
2017-03-02T17:45:35.827008: step 10225, loss 0.0702215, acc 1
2017-03-02T17:45:35.897592: step 10226, loss 0.217914, acc 0.921875
2017-03-02T17:45:35.971391: step 10227, loss 0.0974866, acc 0.953125
2017-03-02T17:45:36.043234: step 10228, loss 0.17094, acc 0.921875
2017-03-02T17:45:36.114499: step 10229, loss 0.124819, acc 0.90625
2017-03-02T17:45:36.186010: step 10230, loss 0.204089, acc 0.90625
2017-03-02T17:45:36.256248: step 10231, loss 0.109624, acc 0.953125
2017-03-02T17:45:36.332285: step 10232, loss 0.167933, acc 0.90625
2017-03-02T17:45:36.410831: step 10233, loss 0.192563, acc 0.953125
2017-03-02T17:45:36.482693: step 10234, loss 0.218612, acc 0.90625
2017-03-02T17:45:36.550426: step 10235, loss 0.125882, acc 0.9375
2017-03-02T17:45:36.620425: step 10236, loss 0.129477, acc 0.953125
2017-03-02T17:45:36.689424: step 10237, loss 0.184725, acc 0.90625
2017-03-02T17:45:36.760011: step 10238, loss 0.147762, acc 0.9375
2017-03-02T17:45:36.826424: step 10239, loss 0.126673, acc 0.96875
2017-03-02T17:45:36.900608: step 10240, loss 0.165876, acc 0.9375
2017-03-02T17:45:36.976998: step 10241, loss 0.114688, acc 0.953125
2017-03-02T17:45:37.050277: step 10242, loss 0.10541, acc 0.96875
2017-03-02T17:45:37.121142: step 10243, loss 0.147338, acc 0.9375
2017-03-02T17:45:37.198100: step 10244, loss 0.13572, acc 0.9375
2017-03-02T17:45:37.274275: step 10245, loss 0.135974, acc 0.9375
2017-03-02T17:45:37.344949: step 10246, loss 0.227508, acc 0.90625
2017-03-02T17:45:37.418458: step 10247, loss 0.127503, acc 0.921875
2017-03-02T17:45:37.495986: step 10248, loss 0.163872, acc 0.90625
2017-03-02T17:45:37.570243: step 10249, loss 0.212149, acc 0.921875
2017-03-02T17:45:37.646538: step 10250, loss 0.197739, acc 0.921875
2017-03-02T17:45:37.720038: step 10251, loss 0.248491, acc 0.890625
2017-03-02T17:45:37.793111: step 10252, loss 0.284165, acc 0.921875
2017-03-02T17:45:37.873659: step 10253, loss 0.266163, acc 0.921875
2017-03-02T17:45:37.940566: step 10254, loss 0.0902896, acc 0.953125
2017-03-02T17:45:38.009125: step 10255, loss 0.142768, acc 0.90625
2017-03-02T17:45:38.083022: step 10256, loss 0.168098, acc 0.9375
2017-03-02T17:45:38.156119: step 10257, loss 0.0996611, acc 0.984375
2017-03-02T17:45:38.240143: step 10258, loss 0.192598, acc 0.890625
2017-03-02T17:45:38.314175: step 10259, loss 0.189618, acc 0.9375
2017-03-02T17:45:38.417703: step 10260, loss 0.0594803, acc 1
2017-03-02T17:45:38.486385: step 10261, loss 0.222385, acc 0.921875
2017-03-02T17:45:38.559778: step 10262, loss 0.088144, acc 0.953125
2017-03-02T17:45:38.630415: step 10263, loss 0.161534, acc 0.9375
2017-03-02T17:45:38.696447: step 10264, loss 0.181419, acc 0.921875
2017-03-02T17:45:38.764590: step 10265, loss 0.12973, acc 0.96875
2017-03-02T17:45:38.838193: step 10266, loss 0.143837, acc 0.9375
2017-03-02T17:45:38.912318: step 10267, loss 0.0550084, acc 0.96875
2017-03-02T17:45:38.985256: step 10268, loss 0.323808, acc 0.890625
2017-03-02T17:45:39.058166: step 10269, loss 0.180154, acc 0.921875
2017-03-02T17:45:39.130558: step 10270, loss 0.109461, acc 0.953125
2017-03-02T17:45:39.199722: step 10271, loss 0.0930489, acc 0.96875
2017-03-02T17:45:39.271966: step 10272, loss 0.22841, acc 0.890625
2017-03-02T17:45:39.339124: step 10273, loss 0.175678, acc 0.921875
2017-03-02T17:45:39.409119: step 10274, loss 0.162376, acc 0.921875
2017-03-02T17:45:39.476990: step 10275, loss 0.224669, acc 0.90625
2017-03-02T17:45:39.555159: step 10276, loss 0.376736, acc 0.84375
2017-03-02T17:45:39.627838: step 10277, loss 0.151755, acc 0.921875
2017-03-02T17:45:39.698340: step 10278, loss 0.1442, acc 0.890625
2017-03-02T17:45:39.771588: step 10279, loss 0.194249, acc 0.921875
2017-03-02T17:45:39.839636: step 10280, loss 0.123339, acc 0.96875
2017-03-02T17:45:39.914764: step 10281, loss 0.269063, acc 0.90625
2017-03-02T17:45:39.987858: step 10282, loss 0.144217, acc 0.953125
2017-03-02T17:45:40.057859: step 10283, loss 0.185428, acc 0.921875
2017-03-02T17:45:40.133050: step 10284, loss 0.183406, acc 0.9375
2017-03-02T17:45:40.205035: step 10285, loss 0.265269, acc 0.921875
2017-03-02T17:45:40.277708: step 10286, loss 0.170309, acc 0.921875
2017-03-02T17:45:40.348219: step 10287, loss 0.176194, acc 0.9375
2017-03-02T17:45:40.420740: step 10288, loss 0.147251, acc 0.9375
2017-03-02T17:45:40.509798: step 10289, loss 0.117789, acc 0.953125
2017-03-02T17:45:40.588785: step 10290, loss 0.11393, acc 0.9375
2017-03-02T17:45:40.659861: step 10291, loss 0.256923, acc 0.9375
2017-03-02T17:45:40.730482: step 10292, loss 0.183087, acc 0.90625
2017-03-02T17:45:40.793675: step 10293, loss 0.193494, acc 0.90625
2017-03-02T17:45:40.868035: step 10294, loss 0.185428, acc 0.953125
2017-03-02T17:45:40.940441: step 10295, loss 0.189441, acc 0.90625
2017-03-02T17:45:41.014789: step 10296, loss 0.156099, acc 0.921875
2017-03-02T17:45:41.091038: step 10297, loss 0.151592, acc 0.921875
2017-03-02T17:45:41.168448: step 10298, loss 0.195318, acc 0.921875
2017-03-02T17:45:41.244781: step 10299, loss 0.193604, acc 0.9375
2017-03-02T17:45:41.320627: step 10300, loss 0.199543, acc 0.90625

Evaluation:
2017-03-02T17:45:41.353860: step 10300, loss 1.46161, acc 0.66907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10300

2017-03-02T17:45:41.816831: step 10301, loss 0.159014, acc 0.921875
2017-03-02T17:45:41.888504: step 10302, loss 0.188716, acc 0.9375
2017-03-02T17:45:41.958632: step 10303, loss 0.2302, acc 0.890625
2017-03-02T17:45:42.033504: step 10304, loss 0.242609, acc 0.875
2017-03-02T17:45:42.104031: step 10305, loss 0.508327, acc 0.84375
2017-03-02T17:45:42.176835: step 10306, loss 0.235871, acc 0.90625
2017-03-02T17:45:42.275763: step 10307, loss 0.118772, acc 0.9375
2017-03-02T17:45:42.350322: step 10308, loss 0.220252, acc 0.890625
2017-03-02T17:45:42.428677: step 10309, loss 0.123217, acc 0.9375
2017-03-02T17:45:42.501695: step 10310, loss 0.151594, acc 0.90625
2017-03-02T17:45:42.579769: step 10311, loss 0.27155, acc 0.859375
2017-03-02T17:45:42.650647: step 10312, loss 0.140069, acc 0.953125
2017-03-02T17:45:42.723562: step 10313, loss 0.20347, acc 0.953125
2017-03-02T17:45:42.793000: step 10314, loss 0.07371, acc 0.96875
2017-03-02T17:45:42.859021: step 10315, loss 0.346151, acc 0.890625
2017-03-02T17:45:42.925027: step 10316, loss 0.14984, acc 0.90625
2017-03-02T17:45:43.004827: step 10317, loss 0.139868, acc 0.921875
2017-03-02T17:45:43.081586: step 10318, loss 0.362219, acc 0.90625
2017-03-02T17:45:43.154349: step 10319, loss 0.205459, acc 0.921875
2017-03-02T17:45:43.221514: step 10320, loss 0.222007, acc 0.90625
2017-03-02T17:45:43.285094: step 10321, loss 0.115775, acc 0.9375
2017-03-02T17:45:43.358995: step 10322, loss 0.147396, acc 0.9375
2017-03-02T17:45:43.438094: step 10323, loss 0.134797, acc 0.9375
2017-03-02T17:45:43.505873: step 10324, loss 0.121892, acc 0.953125
2017-03-02T17:45:43.590968: step 10325, loss 0.073759, acc 0.953125
2017-03-02T17:45:43.661539: step 10326, loss 0.170947, acc 0.953125
2017-03-02T17:45:43.746485: step 10327, loss 0.271495, acc 0.859375
2017-03-02T17:45:43.817133: step 10328, loss 0.0335841, acc 0.984375
2017-03-02T17:45:43.892727: step 10329, loss 0.222288, acc 0.875
2017-03-02T17:45:43.974434: step 10330, loss 0.133936, acc 0.9375
2017-03-02T17:45:44.044168: step 10331, loss 0.137179, acc 0.953125
2017-03-02T17:45:44.112730: step 10332, loss 0.0853119, acc 0.953125
2017-03-02T17:45:44.174378: step 10333, loss 0.178552, acc 0.921875
2017-03-02T17:45:44.240597: step 10334, loss 0.127103, acc 0.96875
2017-03-02T17:45:44.313560: step 10335, loss 0.179202, acc 0.9375
2017-03-02T17:45:44.383165: step 10336, loss 0.310165, acc 0.890625
2017-03-02T17:45:44.462910: step 10337, loss 0.277555, acc 0.859375
2017-03-02T17:45:44.539176: step 10338, loss 0.137452, acc 0.9375
2017-03-02T17:45:44.612782: step 10339, loss 0.0901223, acc 0.984375
2017-03-02T17:45:44.683826: step 10340, loss 0.258439, acc 0.859375
2017-03-02T17:45:44.760619: step 10341, loss 0.102361, acc 0.953125
2017-03-02T17:45:44.840219: step 10342, loss 0.0927642, acc 0.953125
2017-03-02T17:45:44.919722: step 10343, loss 0.276577, acc 0.875
2017-03-02T17:45:44.995601: step 10344, loss 0.265375, acc 0.890625
2017-03-02T17:45:45.077462: step 10345, loss 0.238621, acc 0.890625
2017-03-02T17:45:45.149741: step 10346, loss 0.293974, acc 0.875
2017-03-02T17:45:45.239429: step 10347, loss 0.194669, acc 0.921875
2017-03-02T17:45:45.321915: step 10348, loss 0.116314, acc 0.96875
2017-03-02T17:45:45.398127: step 10349, loss 0.179156, acc 0.90625
2017-03-02T17:45:45.481915: step 10350, loss 0.209183, acc 0.921875
2017-03-02T17:45:45.549580: step 10351, loss 0.271917, acc 0.875
2017-03-02T17:45:45.615458: step 10352, loss 0.171139, acc 0.9375
2017-03-02T17:45:45.686583: step 10353, loss 0.16983, acc 0.953125
2017-03-02T17:45:45.759534: step 10354, loss 0.0766544, acc 0.96875
2017-03-02T17:45:45.849814: step 10355, loss 0.0790436, acc 1
2017-03-02T17:45:45.924215: step 10356, loss 0.226167, acc 0.90625
2017-03-02T17:45:45.992531: step 10357, loss 0.0785758, acc 0.96875
2017-03-02T17:45:46.061166: step 10358, loss 0.151346, acc 0.921875
2017-03-02T17:45:46.136604: step 10359, loss 0.151842, acc 0.96875
2017-03-02T17:45:46.211080: step 10360, loss 0.28168, acc 0.875
2017-03-02T17:45:46.279860: step 10361, loss 0.194193, acc 0.9375
2017-03-02T17:45:46.348634: step 10362, loss 0.250923, acc 0.90625
2017-03-02T17:45:46.423806: step 10363, loss 0.148246, acc 0.9375
2017-03-02T17:45:46.497968: step 10364, loss 0.211857, acc 0.921875
2017-03-02T17:45:46.574115: step 10365, loss 0.111012, acc 0.984375
2017-03-02T17:45:46.648909: step 10366, loss 0.118936, acc 0.9375
2017-03-02T17:45:46.729042: step 10367, loss 0.23338, acc 0.875
2017-03-02T17:45:46.794842: step 10368, loss 0.163159, acc 0.890625
2017-03-02T17:45:46.869954: step 10369, loss 0.250856, acc 0.875
2017-03-02T17:45:46.939590: step 10370, loss 0.160592, acc 0.953125
2017-03-02T17:45:47.013206: step 10371, loss 0.116866, acc 0.96875
2017-03-02T17:45:47.103465: step 10372, loss 0.150952, acc 0.890625
2017-03-02T17:45:47.175926: step 10373, loss 0.219681, acc 0.921875
2017-03-02T17:45:47.253299: step 10374, loss 0.2546, acc 0.890625
2017-03-02T17:45:47.340011: step 10375, loss 0.0823196, acc 0.96875
2017-03-02T17:45:47.417539: step 10376, loss 0.240917, acc 0.90625
2017-03-02T17:45:47.491205: step 10377, loss 0.189179, acc 0.921875
2017-03-02T17:45:47.569529: step 10378, loss 0.146182, acc 0.921875
2017-03-02T17:45:47.638714: step 10379, loss 0.196621, acc 0.90625
2017-03-02T17:45:47.708002: step 10380, loss 0.136298, acc 0.9375
2017-03-02T17:45:47.779457: step 10381, loss 0.173612, acc 0.90625
2017-03-02T17:45:47.859900: step 10382, loss 0.153881, acc 0.953125
2017-03-02T17:45:47.935137: step 10383, loss 0.202445, acc 0.921875
2017-03-02T17:45:48.014983: step 10384, loss 0.152874, acc 0.9375
2017-03-02T17:45:48.089692: step 10385, loss 0.145324, acc 0.96875
2017-03-02T17:45:48.164317: step 10386, loss 0.169585, acc 0.953125
2017-03-02T17:45:48.237998: step 10387, loss 0.159681, acc 0.921875
2017-03-02T17:45:48.312125: step 10388, loss 0.300558, acc 0.75
2017-03-02T17:45:48.393674: step 10389, loss 0.217672, acc 0.921875
2017-03-02T17:45:48.461699: step 10390, loss 0.101484, acc 0.953125
2017-03-02T17:45:48.530515: step 10391, loss 0.141775, acc 0.9375
2017-03-02T17:45:48.604638: step 10392, loss 0.127788, acc 0.953125
2017-03-02T17:45:48.684955: step 10393, loss 0.155629, acc 0.90625
2017-03-02T17:45:48.773071: step 10394, loss 0.180125, acc 0.9375
2017-03-02T17:45:48.849712: step 10395, loss 0.162587, acc 0.90625
2017-03-02T17:45:48.920649: step 10396, loss 0.185469, acc 0.921875
2017-03-02T17:45:49.001057: step 10397, loss 0.201573, acc 0.9375
2017-03-02T17:45:49.066897: step 10398, loss 0.130621, acc 0.984375
2017-03-02T17:45:49.126823: step 10399, loss 0.191712, acc 0.890625
2017-03-02T17:45:49.194287: step 10400, loss 0.154811, acc 0.9375

Evaluation:
2017-03-02T17:45:49.231695: step 10400, loss 1.43364, acc 0.662581

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10400

2017-03-02T17:45:49.679075: step 10401, loss 0.100886, acc 0.96875
2017-03-02T17:45:49.745124: step 10402, loss 0.11102, acc 0.9375
2017-03-02T17:45:49.828796: step 10403, loss 0.0992278, acc 0.953125
2017-03-02T17:45:49.901401: step 10404, loss 0.1295, acc 0.9375
2017-03-02T17:45:49.980684: step 10405, loss 0.260632, acc 0.84375
2017-03-02T17:45:50.054018: step 10406, loss 0.281003, acc 0.90625
2017-03-02T17:45:50.127725: step 10407, loss 0.0498274, acc 0.984375
2017-03-02T17:45:50.203992: step 10408, loss 0.215705, acc 0.921875
2017-03-02T17:45:50.281583: step 10409, loss 0.14878, acc 0.9375
2017-03-02T17:45:50.361481: step 10410, loss 0.149274, acc 0.9375
2017-03-02T17:45:50.441468: step 10411, loss 0.119905, acc 0.96875
2017-03-02T17:45:50.513757: step 10412, loss 0.128667, acc 0.9375
2017-03-02T17:45:50.585493: step 10413, loss 0.138857, acc 0.9375
2017-03-02T17:45:50.659851: step 10414, loss 0.172288, acc 0.921875
2017-03-02T17:45:50.733418: step 10415, loss 0.144078, acc 0.921875
2017-03-02T17:45:50.804249: step 10416, loss 0.235177, acc 0.921875
2017-03-02T17:45:50.876991: step 10417, loss 0.1321, acc 0.953125
2017-03-02T17:45:50.947655: step 10418, loss 0.208883, acc 0.921875
2017-03-02T17:45:51.027453: step 10419, loss 0.125458, acc 0.953125
2017-03-02T17:45:51.100741: step 10420, loss 0.204852, acc 0.890625
2017-03-02T17:45:51.172612: step 10421, loss 0.173311, acc 0.921875
2017-03-02T17:45:51.258206: step 10422, loss 0.190325, acc 0.953125
2017-03-02T17:45:51.331604: step 10423, loss 0.161135, acc 0.90625
2017-03-02T17:45:51.421443: step 10424, loss 0.149062, acc 0.9375
2017-03-02T17:45:51.496862: step 10425, loss 0.266243, acc 0.890625
2017-03-02T17:45:51.569707: step 10426, loss 0.210485, acc 0.9375
2017-03-02T17:45:51.641254: step 10427, loss 0.151553, acc 0.953125
2017-03-02T17:45:51.720590: step 10428, loss 0.156132, acc 0.9375
2017-03-02T17:45:51.793133: step 10429, loss 0.178631, acc 0.9375
2017-03-02T17:45:51.860520: step 10430, loss 0.116147, acc 0.9375
2017-03-02T17:45:51.929029: step 10431, loss 0.057125, acc 1
2017-03-02T17:45:52.015532: step 10432, loss 0.0327871, acc 1
2017-03-02T17:45:52.088657: step 10433, loss 0.118131, acc 0.9375
2017-03-02T17:45:52.163278: step 10434, loss 0.112151, acc 0.953125
2017-03-02T17:45:52.232622: step 10435, loss 0.247893, acc 0.84375
2017-03-02T17:45:52.304326: step 10436, loss 0.187067, acc 0.9375
2017-03-02T17:45:52.373074: step 10437, loss 0.259988, acc 0.875
2017-03-02T17:45:52.445495: step 10438, loss 0.202792, acc 0.90625
2017-03-02T17:45:52.508767: step 10439, loss 0.10764, acc 0.953125
2017-03-02T17:45:52.578270: step 10440, loss 0.290622, acc 0.875
2017-03-02T17:45:52.643414: step 10441, loss 0.129614, acc 0.9375
2017-03-02T17:45:52.729023: step 10442, loss 0.244047, acc 0.9375
2017-03-02T17:45:52.803284: step 10443, loss 0.173772, acc 0.921875
2017-03-02T17:45:52.885183: step 10444, loss 0.0987369, acc 0.953125
2017-03-02T17:45:52.962802: step 10445, loss 0.228042, acc 0.90625
2017-03-02T17:45:53.026372: step 10446, loss 0.27642, acc 0.890625
2017-03-02T17:45:53.097230: step 10447, loss 0.123094, acc 0.953125
2017-03-02T17:45:53.170152: step 10448, loss 0.173803, acc 0.9375
2017-03-02T17:45:53.237545: step 10449, loss 0.212478, acc 0.921875
2017-03-02T17:45:53.307722: step 10450, loss 0.103246, acc 0.9375
2017-03-02T17:45:53.380580: step 10451, loss 0.121005, acc 0.96875
2017-03-02T17:45:53.452679: step 10452, loss 0.136225, acc 0.9375
2017-03-02T17:45:53.524957: step 10453, loss 0.22742, acc 0.9375
2017-03-02T17:45:53.596654: step 10454, loss 0.157038, acc 0.921875
2017-03-02T17:45:53.667985: step 10455, loss 0.171687, acc 0.921875
2017-03-02T17:45:53.732383: step 10456, loss 0.206813, acc 0.90625
2017-03-02T17:45:53.811488: step 10457, loss 0.288258, acc 0.859375
2017-03-02T17:45:53.894266: step 10458, loss 0.267468, acc 0.890625
2017-03-02T17:45:53.966777: step 10459, loss 0.203714, acc 0.890625
2017-03-02T17:45:54.042668: step 10460, loss 0.206924, acc 0.921875
2017-03-02T17:45:54.115652: step 10461, loss 0.185728, acc 0.890625
2017-03-02T17:45:54.190142: step 10462, loss 0.0740738, acc 0.984375
2017-03-02T17:45:54.273719: step 10463, loss 0.264163, acc 0.890625
2017-03-02T17:45:54.348370: step 10464, loss 0.174593, acc 0.953125
2017-03-02T17:45:54.425262: step 10465, loss 0.163994, acc 0.9375
2017-03-02T17:45:54.498452: step 10466, loss 0.213576, acc 0.90625
2017-03-02T17:45:54.586925: step 10467, loss 0.161704, acc 0.921875
2017-03-02T17:45:54.658010: step 10468, loss 0.250916, acc 0.890625
2017-03-02T17:45:54.743894: step 10469, loss 0.157305, acc 0.921875
2017-03-02T17:45:54.818531: step 10470, loss 0.0432467, acc 0.984375
2017-03-02T17:45:54.905969: step 10471, loss 0.264586, acc 0.875
2017-03-02T17:45:54.996172: step 10472, loss 0.137781, acc 0.9375
2017-03-02T17:45:55.069359: step 10473, loss 0.169143, acc 0.96875
2017-03-02T17:45:55.138729: step 10474, loss 0.090222, acc 0.96875
2017-03-02T17:45:55.214795: step 10475, loss 0.187615, acc 0.921875
2017-03-02T17:45:55.290571: step 10476, loss 0.152325, acc 0.921875
2017-03-02T17:45:55.361531: step 10477, loss 0.132782, acc 0.921875
2017-03-02T17:45:55.432985: step 10478, loss 0.301877, acc 0.875
2017-03-02T17:45:55.505192: step 10479, loss 0.134427, acc 0.9375
2017-03-02T17:45:55.569997: step 10480, loss 0.126674, acc 0.953125
2017-03-02T17:45:55.642775: step 10481, loss 0.140132, acc 0.953125
2017-03-02T17:45:55.710874: step 10482, loss 0.211381, acc 0.90625
2017-03-02T17:45:55.779295: step 10483, loss 0.183773, acc 0.921875
2017-03-02T17:45:55.853243: step 10484, loss 0.111148, acc 0.953125
2017-03-02T17:45:55.924090: step 10485, loss 0.188188, acc 0.9375
2017-03-02T17:45:55.993672: step 10486, loss 0.210026, acc 0.921875
2017-03-02T17:45:56.067272: step 10487, loss 0.218666, acc 0.90625
2017-03-02T17:45:56.139739: step 10488, loss 0.161293, acc 0.9375
2017-03-02T17:45:56.208007: step 10489, loss 0.094415, acc 0.9375
2017-03-02T17:45:56.282109: step 10490, loss 0.243425, acc 0.9375
2017-03-02T17:45:56.357187: step 10491, loss 0.211109, acc 0.90625
2017-03-02T17:45:56.433185: step 10492, loss 0.0470145, acc 1
2017-03-02T17:45:56.516673: step 10493, loss 0.175425, acc 0.9375
2017-03-02T17:45:56.585581: step 10494, loss 0.16631, acc 0.9375
2017-03-02T17:45:56.658578: step 10495, loss 0.170043, acc 0.921875
2017-03-02T17:45:56.728153: step 10496, loss 0.161927, acc 0.9375
2017-03-02T17:45:56.804893: step 10497, loss 0.16669, acc 0.9375
2017-03-02T17:45:56.882186: step 10498, loss 0.126514, acc 0.921875
2017-03-02T17:45:56.959000: step 10499, loss 0.182577, acc 0.921875
2017-03-02T17:45:57.027266: step 10500, loss 0.174913, acc 0.9375

Evaluation:
2017-03-02T17:45:57.067791: step 10500, loss 1.49154, acc 0.682048

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10500

2017-03-02T17:45:57.539177: step 10501, loss 0.103709, acc 0.953125
2017-03-02T17:45:57.613722: step 10502, loss 0.155068, acc 0.953125
2017-03-02T17:45:57.686687: step 10503, loss 0.251537, acc 0.90625
2017-03-02T17:45:57.761721: step 10504, loss 0.202889, acc 0.890625
2017-03-02T17:45:57.834406: step 10505, loss 0.179389, acc 0.953125
2017-03-02T17:45:57.907205: step 10506, loss 0.169189, acc 0.9375
2017-03-02T17:45:57.977611: step 10507, loss 0.138226, acc 0.96875
2017-03-02T17:45:58.045471: step 10508, loss 0.144485, acc 0.953125
2017-03-02T17:45:58.112036: step 10509, loss 0.236247, acc 0.859375
2017-03-02T17:45:58.204428: step 10510, loss 0.0819405, acc 0.984375
2017-03-02T17:45:58.271146: step 10511, loss 0.129521, acc 0.921875
2017-03-02T17:45:58.344041: step 10512, loss 0.147293, acc 0.953125
2017-03-02T17:45:58.424544: step 10513, loss 0.0285848, acc 1
2017-03-02T17:45:58.495801: step 10514, loss 0.176374, acc 0.90625
2017-03-02T17:45:58.575565: step 10515, loss 0.280639, acc 0.890625
2017-03-02T17:45:58.648647: step 10516, loss 0.188978, acc 0.9375
2017-03-02T17:45:58.716351: step 10517, loss 0.256787, acc 0.890625
2017-03-02T17:45:58.789087: step 10518, loss 0.0895104, acc 0.953125
2017-03-02T17:45:58.865374: step 10519, loss 0.13575, acc 0.96875
2017-03-02T17:45:58.937775: step 10520, loss 0.210046, acc 0.921875
2017-03-02T17:45:59.015242: step 10521, loss 0.230064, acc 0.9375
2017-03-02T17:45:59.089207: step 10522, loss 0.149123, acc 0.9375
2017-03-02T17:45:59.171994: step 10523, loss 0.275205, acc 0.875
2017-03-02T17:45:59.239774: step 10524, loss 0.282853, acc 0.90625
2017-03-02T17:45:59.309624: step 10525, loss 0.223239, acc 0.875
2017-03-02T17:45:59.372905: step 10526, loss 0.284801, acc 0.890625
2017-03-02T17:45:59.441771: step 10527, loss 0.195693, acc 0.90625
2017-03-02T17:45:59.514957: step 10528, loss 0.146602, acc 0.9375
2017-03-02T17:45:59.585354: step 10529, loss 0.234459, acc 0.9375
2017-03-02T17:45:59.662384: step 10530, loss 0.185631, acc 0.9375
2017-03-02T17:45:59.734409: step 10531, loss 0.229014, acc 0.90625
2017-03-02T17:45:59.812678: step 10532, loss 0.0894491, acc 0.953125
2017-03-02T17:45:59.885698: step 10533, loss 0.393262, acc 0.859375
2017-03-02T17:45:59.957840: step 10534, loss 0.136183, acc 0.953125
2017-03-02T17:46:00.027320: step 10535, loss 0.193438, acc 0.953125
2017-03-02T17:46:00.097590: step 10536, loss 0.252713, acc 0.859375
2017-03-02T17:46:00.172440: step 10537, loss 0.169316, acc 0.9375
2017-03-02T17:46:00.247864: step 10538, loss 0.115647, acc 0.9375
2017-03-02T17:46:00.319414: step 10539, loss 0.140722, acc 0.9375
2017-03-02T17:46:00.395025: step 10540, loss 0.0663797, acc 0.96875
2017-03-02T17:46:00.466176: step 10541, loss 0.178228, acc 0.9375
2017-03-02T17:46:00.549602: step 10542, loss 0.181893, acc 0.921875
2017-03-02T17:46:00.622438: step 10543, loss 0.196875, acc 0.9375
2017-03-02T17:46:00.695771: step 10544, loss 0.215087, acc 0.90625
2017-03-02T17:46:00.766938: step 10545, loss 0.145212, acc 0.9375
2017-03-02T17:46:00.835348: step 10546, loss 0.185018, acc 0.90625
2017-03-02T17:46:00.911822: step 10547, loss 0.120389, acc 0.984375
2017-03-02T17:46:00.986730: step 10548, loss 0.158184, acc 0.953125
2017-03-02T17:46:01.060219: step 10549, loss 0.173804, acc 0.9375
2017-03-02T17:46:01.151461: step 10550, loss 0.159752, acc 0.953125
2017-03-02T17:46:01.225288: step 10551, loss 0.185397, acc 0.921875
2017-03-02T17:46:01.301731: step 10552, loss 0.188928, acc 0.890625
2017-03-02T17:46:01.382934: step 10553, loss 0.303473, acc 0.875
2017-03-02T17:46:01.452491: step 10554, loss 0.186178, acc 0.921875
2017-03-02T17:46:01.518607: step 10555, loss 0.169233, acc 0.9375
2017-03-02T17:46:01.592317: step 10556, loss 0.266559, acc 0.890625
2017-03-02T17:46:01.664054: step 10557, loss 0.220671, acc 0.9375
2017-03-02T17:46:01.742032: step 10558, loss 0.106181, acc 0.953125
2017-03-02T17:46:01.814275: step 10559, loss 0.26226, acc 0.890625
2017-03-02T17:46:01.892129: step 10560, loss 0.142188, acc 0.9375
2017-03-02T17:46:01.975304: step 10561, loss 0.236927, acc 0.890625
2017-03-02T17:46:02.048172: step 10562, loss 0.0823201, acc 0.96875
2017-03-02T17:46:02.113707: step 10563, loss 0.124098, acc 0.96875
2017-03-02T17:46:02.180567: step 10564, loss 0.176561, acc 0.90625
2017-03-02T17:46:02.260899: step 10565, loss 0.160173, acc 0.921875
2017-03-02T17:46:02.335410: step 10566, loss 0.247402, acc 0.90625
2017-03-02T17:46:02.414994: step 10567, loss 0.21081, acc 0.890625
2017-03-02T17:46:02.488068: step 10568, loss 0.189869, acc 0.90625
2017-03-02T17:46:02.559465: step 10569, loss 0.190454, acc 0.921875
2017-03-02T17:46:02.632214: step 10570, loss 0.170706, acc 0.9375
2017-03-02T17:46:02.710553: step 10571, loss 0.131276, acc 0.953125
2017-03-02T17:46:02.777251: step 10572, loss 0.157954, acc 0.90625
2017-03-02T17:46:02.851167: step 10573, loss 0.206171, acc 0.890625
2017-03-02T17:46:02.916467: step 10574, loss 0.206379, acc 0.890625
2017-03-02T17:46:02.987130: step 10575, loss 0.250556, acc 0.921875
2017-03-02T17:46:03.062627: step 10576, loss 0.253104, acc 0.890625
2017-03-02T17:46:03.141701: step 10577, loss 0.201353, acc 0.921875
2017-03-02T17:46:03.226747: step 10578, loss 0.145128, acc 0.96875
2017-03-02T17:46:03.299285: step 10579, loss 0.11569, acc 0.96875
2017-03-02T17:46:03.377362: step 10580, loss 0.0975782, acc 0.96875
2017-03-02T17:46:03.450470: step 10581, loss 0.124612, acc 0.9375
2017-03-02T17:46:03.517722: step 10582, loss 0.216688, acc 0.90625
2017-03-02T17:46:03.585557: step 10583, loss 0.107203, acc 0.9375
2017-03-02T17:46:03.656169: step 10584, loss 0.0109788, acc 1
2017-03-02T17:46:03.737414: step 10585, loss 0.214699, acc 0.875
2017-03-02T17:46:03.812134: step 10586, loss 0.114715, acc 0.9375
2017-03-02T17:46:03.884833: step 10587, loss 0.151527, acc 0.921875
2017-03-02T17:46:03.958303: step 10588, loss 0.0865997, acc 0.96875
2017-03-02T17:46:04.030301: step 10589, loss 0.188052, acc 0.90625
2017-03-02T17:46:04.100575: step 10590, loss 0.121263, acc 0.953125
2017-03-02T17:46:04.175036: step 10591, loss 0.177294, acc 0.90625
2017-03-02T17:46:04.253200: step 10592, loss 0.258629, acc 0.921875
2017-03-02T17:46:04.324981: step 10593, loss 0.162666, acc 0.9375
2017-03-02T17:46:04.417098: step 10594, loss 0.175263, acc 0.921875
2017-03-02T17:46:04.493261: step 10595, loss 0.139421, acc 0.96875
2017-03-02T17:46:04.574831: step 10596, loss 0.186008, acc 0.953125
2017-03-02T17:46:04.646357: step 10597, loss 0.164777, acc 0.921875
2017-03-02T17:46:04.719077: step 10598, loss 0.110732, acc 0.96875
2017-03-02T17:46:04.793365: step 10599, loss 0.13377, acc 0.921875
2017-03-02T17:46:04.865918: step 10600, loss 0.129512, acc 0.9375

Evaluation:
2017-03-02T17:46:04.894747: step 10600, loss 1.46783, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10600

2017-03-02T17:46:05.361506: step 10601, loss 0.185961, acc 0.921875
2017-03-02T17:46:05.436572: step 10602, loss 0.198567, acc 0.921875
2017-03-02T17:46:05.516081: step 10603, loss 0.10561, acc 0.953125
2017-03-02T17:46:05.582545: step 10604, loss 0.109158, acc 0.9375
2017-03-02T17:46:05.654910: step 10605, loss 0.163496, acc 0.921875
2017-03-02T17:46:05.737860: step 10606, loss 0.142581, acc 0.9375
2017-03-02T17:46:05.809409: step 10607, loss 0.130622, acc 0.921875
2017-03-02T17:46:05.890894: step 10608, loss 0.143533, acc 0.9375
2017-03-02T17:46:05.970472: step 10609, loss 0.233795, acc 0.859375
2017-03-02T17:46:06.046465: step 10610, loss 0.20028, acc 0.90625
2017-03-02T17:46:06.120711: step 10611, loss 0.169869, acc 0.921875
2017-03-02T17:46:06.196967: step 10612, loss 0.120739, acc 0.953125
2017-03-02T17:46:06.271847: step 10613, loss 0.206315, acc 0.921875
2017-03-02T17:46:06.341770: step 10614, loss 0.19235, acc 0.90625
2017-03-02T17:46:06.416715: step 10615, loss 0.189192, acc 0.90625
2017-03-02T17:46:06.495425: step 10616, loss 0.218336, acc 0.890625
2017-03-02T17:46:06.572552: step 10617, loss 0.156468, acc 0.921875
2017-03-02T17:46:06.646598: step 10618, loss 0.162253, acc 0.890625
2017-03-02T17:46:06.712801: step 10619, loss 0.138562, acc 0.953125
2017-03-02T17:46:06.784609: step 10620, loss 0.220239, acc 0.890625
2017-03-02T17:46:06.859824: step 10621, loss 0.154779, acc 0.9375
2017-03-02T17:46:06.928548: step 10622, loss 0.263315, acc 0.90625
2017-03-02T17:46:06.996732: step 10623, loss 0.122619, acc 0.953125
2017-03-02T17:46:07.061983: step 10624, loss 0.161792, acc 0.953125
2017-03-02T17:46:07.134271: step 10625, loss 0.112726, acc 0.953125
2017-03-02T17:46:07.204570: step 10626, loss 0.15118, acc 0.9375
2017-03-02T17:46:07.285222: step 10627, loss 0.0296232, acc 1
2017-03-02T17:46:07.351126: step 10628, loss 0.184727, acc 0.921875
2017-03-02T17:46:07.422263: step 10629, loss 0.265453, acc 0.875
2017-03-02T17:46:07.493426: step 10630, loss 0.245736, acc 0.890625
2017-03-02T17:46:07.573743: step 10631, loss 0.226365, acc 0.9375
2017-03-02T17:46:07.641276: step 10632, loss 0.153094, acc 0.96875
2017-03-02T17:46:07.715566: step 10633, loss 0.168486, acc 0.9375
2017-03-02T17:46:07.786067: step 10634, loss 0.173086, acc 0.9375
2017-03-02T17:46:07.847976: step 10635, loss 0.101145, acc 0.96875
2017-03-02T17:46:07.923091: step 10636, loss 0.204999, acc 0.875
2017-03-02T17:46:08.006009: step 10637, loss 0.0911956, acc 0.953125
2017-03-02T17:46:08.079807: step 10638, loss 0.13212, acc 0.953125
2017-03-02T17:46:08.152563: step 10639, loss 0.0730823, acc 0.984375
2017-03-02T17:46:08.225445: step 10640, loss 0.0824857, acc 0.953125
2017-03-02T17:46:08.299621: step 10641, loss 0.207695, acc 0.890625
2017-03-02T17:46:08.370742: step 10642, loss 0.145866, acc 0.9375
2017-03-02T17:46:08.436578: step 10643, loss 0.112509, acc 0.984375
2017-03-02T17:46:08.511380: step 10644, loss 0.0924687, acc 0.953125
2017-03-02T17:46:08.586927: step 10645, loss 0.131411, acc 0.953125
2017-03-02T17:46:08.658585: step 10646, loss 0.115006, acc 0.9375
2017-03-02T17:46:08.729957: step 10647, loss 0.191319, acc 0.921875
2017-03-02T17:46:08.801974: step 10648, loss 0.170445, acc 0.921875
2017-03-02T17:46:08.874115: step 10649, loss 0.173216, acc 0.9375
2017-03-02T17:46:08.964615: step 10650, loss 0.150976, acc 0.921875
2017-03-02T17:46:09.026080: step 10651, loss 0.0910395, acc 0.96875
2017-03-02T17:46:09.092704: step 10652, loss 0.12508, acc 0.9375
2017-03-02T17:46:09.159919: step 10653, loss 0.232413, acc 0.921875
2017-03-02T17:46:09.238170: step 10654, loss 0.166823, acc 0.9375
2017-03-02T17:46:09.314873: step 10655, loss 0.125724, acc 0.9375
2017-03-02T17:46:09.393375: step 10656, loss 0.248631, acc 0.890625
2017-03-02T17:46:09.464405: step 10657, loss 0.122067, acc 0.953125
2017-03-02T17:46:09.536610: step 10658, loss 0.10701, acc 0.96875
2017-03-02T17:46:09.608545: step 10659, loss 0.224837, acc 0.90625
2017-03-02T17:46:09.688892: step 10660, loss 0.26246, acc 0.875
2017-03-02T17:46:09.755492: step 10661, loss 0.13835, acc 0.921875
2017-03-02T17:46:09.837936: step 10662, loss 0.0931471, acc 0.96875
2017-03-02T17:46:09.908717: step 10663, loss 0.325981, acc 0.859375
2017-03-02T17:46:09.977429: step 10664, loss 0.0739786, acc 0.96875
2017-03-02T17:46:10.049870: step 10665, loss 0.0877999, acc 0.953125
2017-03-02T17:46:10.119690: step 10666, loss 0.180086, acc 0.921875
2017-03-02T17:46:10.197289: step 10667, loss 0.175707, acc 0.921875
2017-03-02T17:46:10.262825: step 10668, loss 0.163556, acc 0.96875
2017-03-02T17:46:10.332065: step 10669, loss 0.106971, acc 0.953125
2017-03-02T17:46:10.407029: step 10670, loss 0.0861719, acc 0.96875
2017-03-02T17:46:10.476709: step 10671, loss 0.109155, acc 0.96875
2017-03-02T17:46:10.543014: step 10672, loss 0.118531, acc 0.953125
2017-03-02T17:46:10.623550: step 10673, loss 0.136, acc 0.9375
2017-03-02T17:46:10.701977: step 10674, loss 0.173197, acc 0.921875
2017-03-02T17:46:10.776969: step 10675, loss 0.159639, acc 0.90625
2017-03-02T17:46:10.851865: step 10676, loss 0.345518, acc 0.875
2017-03-02T17:46:10.926130: step 10677, loss 0.19179, acc 0.9375
2017-03-02T17:46:10.991699: step 10678, loss 0.172604, acc 0.9375
2017-03-02T17:46:11.063801: step 10679, loss 0.316491, acc 0.875
2017-03-02T17:46:11.126482: step 10680, loss 0.0890313, acc 0.96875
2017-03-02T17:46:11.194111: step 10681, loss 0.149031, acc 0.90625
2017-03-02T17:46:11.260211: step 10682, loss 0.259124, acc 0.890625
2017-03-02T17:46:11.330678: step 10683, loss 0.108771, acc 0.96875
2017-03-02T17:46:11.399999: step 10684, loss 0.170707, acc 0.953125
2017-03-02T17:46:11.474216: step 10685, loss 0.187301, acc 0.90625
2017-03-02T17:46:11.555646: step 10686, loss 0.101377, acc 0.984375
2017-03-02T17:46:11.623363: step 10687, loss 0.191458, acc 0.921875
2017-03-02T17:46:11.699444: step 10688, loss 0.138112, acc 0.9375
2017-03-02T17:46:11.767708: step 10689, loss 0.0816295, acc 0.984375
2017-03-02T17:46:11.835256: step 10690, loss 0.0928717, acc 0.96875
2017-03-02T17:46:11.906142: step 10691, loss 0.258847, acc 0.90625
2017-03-02T17:46:11.981006: step 10692, loss 0.0870902, acc 0.96875
2017-03-02T17:46:12.055973: step 10693, loss 0.141285, acc 0.921875
2017-03-02T17:46:12.131783: step 10694, loss 0.0794937, acc 0.96875
2017-03-02T17:46:12.220385: step 10695, loss 0.192377, acc 0.953125
2017-03-02T17:46:12.295474: step 10696, loss 0.253075, acc 0.890625
2017-03-02T17:46:12.367513: step 10697, loss 0.224704, acc 0.890625
2017-03-02T17:46:12.445454: step 10698, loss 0.0901644, acc 0.953125
2017-03-02T17:46:12.523576: step 10699, loss 0.186326, acc 0.90625
2017-03-02T17:46:12.593218: step 10700, loss 0.0778105, acc 0.984375

Evaluation:
2017-03-02T17:46:12.628581: step 10700, loss 1.49784, acc 0.671233

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10700

2017-03-02T17:46:13.171279: step 10701, loss 0.139508, acc 0.921875
2017-03-02T17:46:13.240763: step 10702, loss 0.150224, acc 0.921875
2017-03-02T17:46:13.309955: step 10703, loss 0.134054, acc 0.9375
2017-03-02T17:46:13.382931: step 10704, loss 0.291517, acc 0.875
2017-03-02T17:46:13.457550: step 10705, loss 0.101272, acc 0.953125
2017-03-02T17:46:13.534538: step 10706, loss 0.157525, acc 0.953125
2017-03-02T17:46:13.607196: step 10707, loss 0.190935, acc 0.953125
2017-03-02T17:46:13.682054: step 10708, loss 0.0973797, acc 0.96875
2017-03-02T17:46:13.758434: step 10709, loss 0.0787126, acc 0.96875
2017-03-02T17:46:13.831967: step 10710, loss 0.336876, acc 0.859375
2017-03-02T17:46:13.908160: step 10711, loss 0.118378, acc 0.9375
2017-03-02T17:46:13.980203: step 10712, loss 0.12181, acc 0.953125
2017-03-02T17:46:14.053265: step 10713, loss 0.188855, acc 0.921875
2017-03-02T17:46:14.128803: step 10714, loss 0.0647199, acc 0.984375
2017-03-02T17:46:14.200821: step 10715, loss 0.10192, acc 0.96875
2017-03-02T17:46:14.288365: step 10716, loss 0.218742, acc 0.9375
2017-03-02T17:46:14.367981: step 10717, loss 0.153653, acc 0.9375
2017-03-02T17:46:14.441246: step 10718, loss 0.30104, acc 0.890625
2017-03-02T17:46:14.512713: step 10719, loss 0.189333, acc 0.9375
2017-03-02T17:46:14.587133: step 10720, loss 0.142783, acc 0.921875
2017-03-02T17:46:14.660018: step 10721, loss 0.24195, acc 0.90625
2017-03-02T17:46:14.728187: step 10722, loss 0.278258, acc 0.875
2017-03-02T17:46:14.803114: step 10723, loss 0.302325, acc 0.859375
2017-03-02T17:46:14.878450: step 10724, loss 0.0677981, acc 0.984375
2017-03-02T17:46:14.950408: step 10725, loss 0.209172, acc 0.9375
2017-03-02T17:46:15.031729: step 10726, loss 0.303336, acc 0.828125
2017-03-02T17:46:15.103493: step 10727, loss 0.147945, acc 0.9375
2017-03-02T17:46:15.181336: step 10728, loss 0.268738, acc 0.859375
2017-03-02T17:46:15.246258: step 10729, loss 0.298852, acc 0.859375
2017-03-02T17:46:15.325333: step 10730, loss 0.131071, acc 0.921875
2017-03-02T17:46:15.393216: step 10731, loss 0.129032, acc 0.9375
2017-03-02T17:46:15.462802: step 10732, loss 0.241089, acc 0.890625
2017-03-02T17:46:15.540300: step 10733, loss 0.155682, acc 0.921875
2017-03-02T17:46:15.612497: step 10734, loss 0.0998824, acc 0.953125
2017-03-02T17:46:15.687686: step 10735, loss 0.14866, acc 0.9375
2017-03-02T17:46:15.766559: step 10736, loss 0.19623, acc 0.90625
2017-03-02T17:46:15.840160: step 10737, loss 0.228724, acc 0.890625
2017-03-02T17:46:15.921623: step 10738, loss 0.0923085, acc 0.96875
2017-03-02T17:46:15.988353: step 10739, loss 0.0934094, acc 0.984375
2017-03-02T17:46:16.058954: step 10740, loss 0.260833, acc 0.890625
2017-03-02T17:46:16.133511: step 10741, loss 0.146594, acc 0.9375
2017-03-02T17:46:16.209874: step 10742, loss 0.0945355, acc 0.984375
2017-03-02T17:46:16.270696: step 10743, loss 0.0724187, acc 0.984375
2017-03-02T17:46:16.343543: step 10744, loss 0.141577, acc 0.9375
2017-03-02T17:46:16.411739: step 10745, loss 0.241104, acc 0.890625
2017-03-02T17:46:16.484123: step 10746, loss 0.383848, acc 0.875
2017-03-02T17:46:16.560648: step 10747, loss 0.149296, acc 0.953125
2017-03-02T17:46:16.637220: step 10748, loss 0.206568, acc 0.890625
2017-03-02T17:46:16.701431: step 10749, loss 0.177473, acc 0.921875
2017-03-02T17:46:16.767218: step 10750, loss 0.268532, acc 0.890625
2017-03-02T17:46:16.844164: step 10751, loss 0.11466, acc 0.953125
2017-03-02T17:46:16.918270: step 10752, loss 0.431972, acc 0.796875
2017-03-02T17:46:16.993411: step 10753, loss 0.114612, acc 0.953125
2017-03-02T17:46:17.067406: step 10754, loss 0.261778, acc 0.875
2017-03-02T17:46:17.138070: step 10755, loss 0.193972, acc 0.90625
2017-03-02T17:46:17.230113: step 10756, loss 0.131896, acc 0.9375
2017-03-02T17:46:17.297715: step 10757, loss 0.297262, acc 0.90625
2017-03-02T17:46:17.364552: step 10758, loss 0.207157, acc 0.859375
2017-03-02T17:46:17.438392: step 10759, loss 0.135063, acc 0.9375
2017-03-02T17:46:17.525977: step 10760, loss 0.159062, acc 0.953125
2017-03-02T17:46:17.596536: step 10761, loss 0.130928, acc 0.90625
2017-03-02T17:46:17.680528: step 10762, loss 0.168579, acc 0.921875
2017-03-02T17:46:17.752131: step 10763, loss 0.119679, acc 0.921875
2017-03-02T17:46:17.824676: step 10764, loss 0.290149, acc 0.890625
2017-03-02T17:46:17.897939: step 10765, loss 0.11359, acc 0.96875
2017-03-02T17:46:17.979325: step 10766, loss 0.161253, acc 0.953125
2017-03-02T17:46:18.054365: step 10767, loss 0.171208, acc 0.953125
2017-03-02T17:46:18.125732: step 10768, loss 0.247349, acc 0.90625
2017-03-02T17:46:18.202624: step 10769, loss 0.113234, acc 0.984375
2017-03-02T17:46:18.272482: step 10770, loss 0.24273, acc 0.890625
2017-03-02T17:46:18.351370: step 10771, loss 0.187745, acc 0.953125
2017-03-02T17:46:18.424224: step 10772, loss 0.18476, acc 0.9375
2017-03-02T17:46:18.500884: step 10773, loss 0.0989127, acc 0.953125
2017-03-02T17:46:18.579954: step 10774, loss 0.0938781, acc 0.953125
2017-03-02T17:46:18.659446: step 10775, loss 0.237602, acc 0.875
2017-03-02T17:46:18.727492: step 10776, loss 0.241003, acc 0.90625
2017-03-02T17:46:18.799499: step 10777, loss 0.171257, acc 0.90625
2017-03-02T17:46:18.870238: step 10778, loss 0.204142, acc 0.90625
2017-03-02T17:46:18.945628: step 10779, loss 0.229835, acc 0.921875
2017-03-02T17:46:19.017329: step 10780, loss 0.0398463, acc 1
2017-03-02T17:46:19.096924: step 10781, loss 0.175472, acc 0.921875
2017-03-02T17:46:19.174715: step 10782, loss 0.258431, acc 0.90625
2017-03-02T17:46:19.244231: step 10783, loss 0.0794925, acc 0.984375
2017-03-02T17:46:19.313612: step 10784, loss 0.349488, acc 0.90625
2017-03-02T17:46:19.384387: step 10785, loss 0.0589141, acc 0.984375
2017-03-02T17:46:19.459980: step 10786, loss 0.223119, acc 0.90625
2017-03-02T17:46:19.529968: step 10787, loss 0.104973, acc 0.9375
2017-03-02T17:46:19.607331: step 10788, loss 0.102231, acc 0.96875
2017-03-02T17:46:19.684925: step 10789, loss 0.17555, acc 0.9375
2017-03-02T17:46:19.757756: step 10790, loss 0.0902614, acc 0.96875
2017-03-02T17:46:19.822447: step 10791, loss 0.13743, acc 0.96875
2017-03-02T17:46:19.891051: step 10792, loss 0.114042, acc 0.96875
2017-03-02T17:46:19.951467: step 10793, loss 0.232704, acc 0.921875
2017-03-02T17:46:20.024068: step 10794, loss 0.254643, acc 0.921875
2017-03-02T17:46:20.098870: step 10795, loss 0.201876, acc 0.90625
2017-03-02T17:46:20.166878: step 10796, loss 0.139374, acc 0.953125
2017-03-02T17:46:20.243821: step 10797, loss 0.221921, acc 0.90625
2017-03-02T17:46:20.334432: step 10798, loss 0.2194, acc 0.875
2017-03-02T17:46:20.406153: step 10799, loss 0.153446, acc 0.953125
2017-03-02T17:46:20.480742: step 10800, loss 0.115131, acc 0.953125

Evaluation:
2017-03-02T17:46:20.515339: step 10800, loss 1.48854, acc 0.668349

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10800

2017-03-02T17:46:20.957728: step 10801, loss 0.173222, acc 0.90625
2017-03-02T17:46:21.021707: step 10802, loss 0.20823, acc 0.90625
2017-03-02T17:46:21.092520: step 10803, loss 0.115479, acc 0.9375
2017-03-02T17:46:21.164734: step 10804, loss 0.154447, acc 0.9375
2017-03-02T17:46:21.238868: step 10805, loss 0.109813, acc 0.953125
2017-03-02T17:46:21.306007: step 10806, loss 0.21394, acc 0.921875
2017-03-02T17:46:21.371416: step 10807, loss 0.133019, acc 0.9375
2017-03-02T17:46:21.441224: step 10808, loss 0.142906, acc 0.921875
2017-03-02T17:46:21.510460: step 10809, loss 0.146358, acc 0.953125
2017-03-02T17:46:21.585151: step 10810, loss 0.207781, acc 0.953125
2017-03-02T17:46:21.654753: step 10811, loss 0.153669, acc 0.90625
2017-03-02T17:46:21.729363: step 10812, loss 0.155984, acc 0.96875
2017-03-02T17:46:21.801021: step 10813, loss 0.154011, acc 0.90625
2017-03-02T17:46:21.868209: step 10814, loss 0.213537, acc 0.90625
2017-03-02T17:46:21.944506: step 10815, loss 0.247232, acc 0.859375
2017-03-02T17:46:22.022377: step 10816, loss 0.144537, acc 0.9375
2017-03-02T17:46:22.094568: step 10817, loss 0.142909, acc 0.9375
2017-03-02T17:46:22.166049: step 10818, loss 0.111445, acc 0.96875
2017-03-02T17:46:22.228309: step 10819, loss 0.123529, acc 0.96875
2017-03-02T17:46:22.293925: step 10820, loss 0.199922, acc 0.90625
2017-03-02T17:46:22.364682: step 10821, loss 0.0953606, acc 0.921875
2017-03-02T17:46:22.434831: step 10822, loss 0.168164, acc 0.90625
2017-03-02T17:46:22.505674: step 10823, loss 0.119229, acc 0.890625
2017-03-02T17:46:22.578016: step 10824, loss 0.168988, acc 0.921875
2017-03-02T17:46:22.655001: step 10825, loss 0.251089, acc 0.90625
2017-03-02T17:46:22.733120: step 10826, loss 0.200295, acc 0.90625
2017-03-02T17:46:22.803566: step 10827, loss 0.273125, acc 0.921875
2017-03-02T17:46:22.878182: step 10828, loss 0.111926, acc 0.9375
2017-03-02T17:46:22.955728: step 10829, loss 0.186267, acc 0.90625
2017-03-02T17:46:23.031479: step 10830, loss 0.0700591, acc 0.984375
2017-03-02T17:46:23.104381: step 10831, loss 0.160612, acc 0.9375
2017-03-02T17:46:23.176096: step 10832, loss 0.230318, acc 0.890625
2017-03-02T17:46:23.246220: step 10833, loss 0.100032, acc 0.9375
2017-03-02T17:46:23.312169: step 10834, loss 0.136794, acc 0.96875
2017-03-02T17:46:23.384885: step 10835, loss 0.1135, acc 0.953125
2017-03-02T17:46:23.462476: step 10836, loss 0.175935, acc 0.9375
2017-03-02T17:46:23.536169: step 10837, loss 0.108055, acc 0.953125
2017-03-02T17:46:23.607674: step 10838, loss 0.0953568, acc 0.953125
2017-03-02T17:46:23.685488: step 10839, loss 0.0995409, acc 0.96875
2017-03-02T17:46:23.761595: step 10840, loss 0.121059, acc 0.953125
2017-03-02T17:46:23.835239: step 10841, loss 0.242744, acc 0.890625
2017-03-02T17:46:23.918432: step 10842, loss 0.204843, acc 0.890625
2017-03-02T17:46:23.996907: step 10843, loss 0.104251, acc 0.984375
2017-03-02T17:46:24.067443: step 10844, loss 0.113913, acc 0.984375
2017-03-02T17:46:24.139131: step 10845, loss 0.0804532, acc 0.953125
2017-03-02T17:46:24.205767: step 10846, loss 0.199698, acc 0.921875
2017-03-02T17:46:24.285054: step 10847, loss 0.430997, acc 0.90625
2017-03-02T17:46:24.356575: step 10848, loss 0.151787, acc 0.9375
2017-03-02T17:46:24.425835: step 10849, loss 0.164491, acc 0.90625
2017-03-02T17:46:24.495895: step 10850, loss 0.0931905, acc 0.96875
2017-03-02T17:46:24.568299: step 10851, loss 0.154912, acc 0.953125
2017-03-02T17:46:24.633958: step 10852, loss 0.144993, acc 0.921875
2017-03-02T17:46:24.708829: step 10853, loss 0.302885, acc 0.875
2017-03-02T17:46:24.782655: step 10854, loss 0.250258, acc 0.921875
2017-03-02T17:46:24.856579: step 10855, loss 0.1611, acc 0.921875
2017-03-02T17:46:24.922888: step 10856, loss 0.0655941, acc 0.953125
2017-03-02T17:46:24.992535: step 10857, loss 0.0835804, acc 0.984375
2017-03-02T17:46:25.066819: step 10858, loss 0.163296, acc 0.9375
2017-03-02T17:46:25.135490: step 10859, loss 0.198106, acc 0.9375
2017-03-02T17:46:25.205647: step 10860, loss 0.231873, acc 0.890625
2017-03-02T17:46:25.279128: step 10861, loss 0.224488, acc 0.875
2017-03-02T17:46:25.342489: step 10862, loss 0.184183, acc 0.9375
2017-03-02T17:46:25.414346: step 10863, loss 0.121368, acc 0.921875
2017-03-02T17:46:25.486972: step 10864, loss 0.17131, acc 0.921875
2017-03-02T17:46:25.558417: step 10865, loss 0.278362, acc 0.890625
2017-03-02T17:46:25.633704: step 10866, loss 0.108802, acc 0.953125
2017-03-02T17:46:25.710375: step 10867, loss 0.271832, acc 0.921875
2017-03-02T17:46:25.784104: step 10868, loss 0.111686, acc 0.9375
2017-03-02T17:46:25.867996: step 10869, loss 0.231566, acc 0.90625
2017-03-02T17:46:25.939128: step 10870, loss 0.196858, acc 0.921875
2017-03-02T17:46:26.011472: step 10871, loss 0.15851, acc 0.921875
2017-03-02T17:46:26.086406: step 10872, loss 0.198157, acc 0.921875
2017-03-02T17:46:26.153152: step 10873, loss 0.208188, acc 0.96875
2017-03-02T17:46:26.227574: step 10874, loss 0.199981, acc 0.890625
2017-03-02T17:46:26.293893: step 10875, loss 0.160626, acc 0.921875
2017-03-02T17:46:26.361354: step 10876, loss 0.230801, acc 0.875
2017-03-02T17:46:26.432871: step 10877, loss 0.260187, acc 0.890625
2017-03-02T17:46:26.505903: step 10878, loss 0.174937, acc 0.9375
2017-03-02T17:46:26.573274: step 10879, loss 0.125526, acc 0.96875
2017-03-02T17:46:26.646818: step 10880, loss 0.0649108, acc 0.984375
2017-03-02T17:46:26.719167: step 10881, loss 0.100954, acc 0.953125
2017-03-02T17:46:26.798554: step 10882, loss 0.15166, acc 0.9375
2017-03-02T17:46:26.876041: step 10883, loss 0.156546, acc 0.921875
2017-03-02T17:46:26.947565: step 10884, loss 0.127199, acc 0.9375
2017-03-02T17:46:27.015116: step 10885, loss 0.25352, acc 0.90625
2017-03-02T17:46:27.088885: step 10886, loss 0.267272, acc 0.921875
2017-03-02T17:46:27.166462: step 10887, loss 0.160018, acc 0.9375
2017-03-02T17:46:27.241451: step 10888, loss 0.0861643, acc 0.96875
2017-03-02T17:46:27.314656: step 10889, loss 0.107423, acc 0.96875
2017-03-02T17:46:27.387613: step 10890, loss 0.176264, acc 0.90625
2017-03-02T17:46:27.458639: step 10891, loss 0.325211, acc 0.9375
2017-03-02T17:46:27.542176: step 10892, loss 0.121932, acc 0.9375
2017-03-02T17:46:27.623379: step 10893, loss 0.303712, acc 0.90625
2017-03-02T17:46:27.697663: step 10894, loss 0.206969, acc 0.90625
2017-03-02T17:46:27.765129: step 10895, loss 0.23767, acc 0.890625
2017-03-02T17:46:27.834910: step 10896, loss 0.24369, acc 0.9375
2017-03-02T17:46:27.918573: step 10897, loss 0.142428, acc 0.9375
2017-03-02T17:46:27.990132: step 10898, loss 0.0747499, acc 0.953125
2017-03-02T17:46:28.060705: step 10899, loss 0.178509, acc 0.921875
2017-03-02T17:46:28.139191: step 10900, loss 0.145817, acc 0.9375

Evaluation:
2017-03-02T17:46:28.170239: step 10900, loss 1.47913, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-10900

2017-03-02T17:46:28.645617: step 10901, loss 0.202612, acc 0.921875
2017-03-02T17:46:28.719982: step 10902, loss 0.156069, acc 0.921875
2017-03-02T17:46:28.788327: step 10903, loss 0.186811, acc 0.921875
2017-03-02T17:46:28.856590: step 10904, loss 0.201523, acc 0.921875
2017-03-02T17:46:28.929470: step 10905, loss 0.235925, acc 0.90625
2017-03-02T17:46:29.002640: step 10906, loss 0.207138, acc 0.921875
2017-03-02T17:46:29.090308: step 10907, loss 0.229943, acc 0.890625
2017-03-02T17:46:29.162418: step 10908, loss 0.142408, acc 0.953125
2017-03-02T17:46:29.265698: step 10909, loss 0.281971, acc 0.90625
2017-03-02T17:46:29.336675: step 10910, loss 0.330601, acc 0.875
2017-03-02T17:46:29.408088: step 10911, loss 0.256691, acc 0.875
2017-03-02T17:46:29.480605: step 10912, loss 0.15461, acc 0.96875
2017-03-02T17:46:29.555698: step 10913, loss 0.198365, acc 0.90625
2017-03-02T17:46:29.625582: step 10914, loss 0.204667, acc 0.890625
2017-03-02T17:46:29.691754: step 10915, loss 0.211281, acc 0.9375
2017-03-02T17:46:29.762382: step 10916, loss 0.0793426, acc 0.96875
2017-03-02T17:46:29.833107: step 10917, loss 0.274056, acc 0.875
2017-03-02T17:46:29.909325: step 10918, loss 0.159526, acc 0.9375
2017-03-02T17:46:29.976520: step 10919, loss 0.107902, acc 0.96875
2017-03-02T17:46:30.054860: step 10920, loss 0.176064, acc 0.890625
2017-03-02T17:46:30.128078: step 10921, loss 0.10835, acc 0.96875
2017-03-02T17:46:30.203980: step 10922, loss 0.111918, acc 0.96875
2017-03-02T17:46:30.271886: step 10923, loss 0.167866, acc 0.921875
2017-03-02T17:46:30.351002: step 10924, loss 0.195484, acc 0.9375
2017-03-02T17:46:30.417382: step 10925, loss 0.193667, acc 0.9375
2017-03-02T17:46:30.484922: step 10926, loss 0.209184, acc 0.90625
2017-03-02T17:46:30.560162: step 10927, loss 0.282739, acc 0.875
2017-03-02T17:46:30.630773: step 10928, loss 0.124425, acc 0.921875
2017-03-02T17:46:30.706530: step 10929, loss 0.122172, acc 0.9375
2017-03-02T17:46:30.775375: step 10930, loss 0.139618, acc 0.9375
2017-03-02T17:46:30.863940: step 10931, loss 0.148638, acc 0.9375
2017-03-02T17:46:30.931653: step 10932, loss 0.156571, acc 0.9375
2017-03-02T17:46:30.998819: step 10933, loss 0.149849, acc 0.9375
2017-03-02T17:46:31.070983: step 10934, loss 0.0706561, acc 0.96875
2017-03-02T17:46:31.134808: step 10935, loss 0.138757, acc 0.9375
2017-03-02T17:46:31.208486: step 10936, loss 0.286054, acc 0.859375
2017-03-02T17:46:31.283837: step 10937, loss 0.137862, acc 0.9375
2017-03-02T17:46:31.367304: step 10938, loss 0.263444, acc 0.921875
2017-03-02T17:46:31.440837: step 10939, loss 0.130187, acc 0.9375
2017-03-02T17:46:31.513555: step 10940, loss 0.0696647, acc 0.984375
2017-03-02T17:46:31.583859: step 10941, loss 0.226686, acc 0.890625
2017-03-02T17:46:31.657798: step 10942, loss 0.132466, acc 0.96875
2017-03-02T17:46:31.732469: step 10943, loss 0.19428, acc 0.921875
2017-03-02T17:46:31.810417: step 10944, loss 0.12712, acc 0.953125
2017-03-02T17:46:31.901021: step 10945, loss 0.125965, acc 0.953125
2017-03-02T17:46:31.972875: step 10946, loss 0.193043, acc 0.875
2017-03-02T17:46:32.050203: step 10947, loss 0.11144, acc 0.953125
2017-03-02T17:46:32.126537: step 10948, loss 0.162874, acc 0.90625
2017-03-02T17:46:32.198260: step 10949, loss 0.191767, acc 0.90625
2017-03-02T17:46:32.270725: step 10950, loss 0.190476, acc 0.921875
2017-03-02T17:46:32.343133: step 10951, loss 0.118422, acc 0.96875
2017-03-02T17:46:32.416102: step 10952, loss 0.132914, acc 0.921875
2017-03-02T17:46:32.485221: step 10953, loss 0.187995, acc 0.90625
2017-03-02T17:46:32.550495: step 10954, loss 0.157634, acc 0.953125
2017-03-02T17:46:32.621771: step 10955, loss 0.18873, acc 0.921875
2017-03-02T17:46:32.694334: step 10956, loss 0.153916, acc 0.921875
2017-03-02T17:46:32.765479: step 10957, loss 0.22473, acc 0.875
2017-03-02T17:46:32.836664: step 10958, loss 0.151595, acc 0.953125
2017-03-02T17:46:32.909116: step 10959, loss 0.0465468, acc 0.96875
2017-03-02T17:46:32.982684: step 10960, loss 0.176047, acc 0.90625
2017-03-02T17:46:33.051693: step 10961, loss 0.105541, acc 0.953125
2017-03-02T17:46:33.123738: step 10962, loss 0.289715, acc 0.875
2017-03-02T17:46:33.198002: step 10963, loss 0.161596, acc 0.9375
2017-03-02T17:46:33.269499: step 10964, loss 0.177815, acc 0.9375
2017-03-02T17:46:33.347047: step 10965, loss 0.168461, acc 0.9375
2017-03-02T17:46:33.425463: step 10966, loss 0.178073, acc 0.90625
2017-03-02T17:46:33.496377: step 10967, loss 0.0429234, acc 0.96875
2017-03-02T17:46:33.567678: step 10968, loss 0.109894, acc 0.953125
2017-03-02T17:46:33.641080: step 10969, loss 0.223892, acc 0.90625
2017-03-02T17:46:33.715947: step 10970, loss 0.134734, acc 0.9375
2017-03-02T17:46:33.791233: step 10971, loss 0.187045, acc 0.890625
2017-03-02T17:46:33.864476: step 10972, loss 0.458503, acc 0.84375
2017-03-02T17:46:33.939989: step 10973, loss 0.132615, acc 0.953125
2017-03-02T17:46:34.011452: step 10974, loss 0.245299, acc 0.875
2017-03-02T17:46:34.087128: step 10975, loss 0.156307, acc 0.921875
2017-03-02T17:46:34.160402: step 10976, loss 0.105259, acc 1
2017-03-02T17:46:34.237648: step 10977, loss 0.100578, acc 0.953125
2017-03-02T17:46:34.315515: step 10978, loss 0.219301, acc 0.90625
2017-03-02T17:46:34.383974: step 10979, loss 0.23947, acc 0.921875
2017-03-02T17:46:34.467976: step 10980, loss 0.219256, acc 0.90625
2017-03-02T17:46:34.538487: step 10981, loss 0.111742, acc 0.953125
2017-03-02T17:46:34.611153: step 10982, loss 0.118188, acc 0.96875
2017-03-02T17:46:34.685713: step 10983, loss 0.0581819, acc 0.96875
2017-03-02T17:46:34.762410: step 10984, loss 0.17259, acc 0.953125
2017-03-02T17:46:34.840012: step 10985, loss 0.167075, acc 0.921875
2017-03-02T17:46:34.929874: step 10986, loss 0.0983253, acc 0.953125
2017-03-02T17:46:35.000966: step 10987, loss 0.12332, acc 0.9375
2017-03-02T17:46:35.061571: step 10988, loss 0.219829, acc 0.890625
2017-03-02T17:46:35.137569: step 10989, loss 0.0984955, acc 0.96875
2017-03-02T17:46:35.217882: step 10990, loss 0.162664, acc 0.9375
2017-03-02T17:46:35.287949: step 10991, loss 0.270708, acc 0.90625
2017-03-02T17:46:35.360286: step 10992, loss 0.15756, acc 0.921875
2017-03-02T17:46:35.428037: step 10993, loss 0.0571952, acc 0.984375
2017-03-02T17:46:35.502647: step 10994, loss 0.191645, acc 0.90625
2017-03-02T17:46:35.572647: step 10995, loss 0.0839496, acc 0.96875
2017-03-02T17:46:35.640581: step 10996, loss 0.179152, acc 0.90625
2017-03-02T17:46:35.714528: step 10997, loss 0.218281, acc 0.921875
2017-03-02T17:46:35.786345: step 10998, loss 0.277618, acc 0.875
2017-03-02T17:46:35.863021: step 10999, loss 0.142907, acc 0.953125
2017-03-02T17:46:35.934060: step 11000, loss 0.197518, acc 0.9375

Evaluation:
2017-03-02T17:46:35.965020: step 11000, loss 1.52555, acc 0.677001

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11000

2017-03-02T17:46:36.445130: step 11001, loss 0.267948, acc 0.890625
2017-03-02T17:46:36.519970: step 11002, loss 0.0807881, acc 0.96875
2017-03-02T17:46:36.591791: step 11003, loss 0.0958698, acc 0.984375
2017-03-02T17:46:36.657157: step 11004, loss 0.0548789, acc 0.984375
2017-03-02T17:46:36.726147: step 11005, loss 0.157899, acc 0.953125
2017-03-02T17:46:36.799185: step 11006, loss 0.161798, acc 0.921875
2017-03-02T17:46:36.874451: step 11007, loss 0.107956, acc 0.984375
2017-03-02T17:46:36.948969: step 11008, loss 0.0444719, acc 0.984375
2017-03-02T17:46:37.023404: step 11009, loss 0.177958, acc 0.921875
2017-03-02T17:46:37.096506: step 11010, loss 0.0906122, acc 0.96875
2017-03-02T17:46:37.171239: step 11011, loss 0.100969, acc 0.96875
2017-03-02T17:46:37.246948: step 11012, loss 0.08941, acc 0.96875
2017-03-02T17:46:37.324638: step 11013, loss 0.110966, acc 0.953125
2017-03-02T17:46:37.398070: step 11014, loss 0.188152, acc 0.875
2017-03-02T17:46:37.473467: step 11015, loss 0.102899, acc 0.96875
2017-03-02T17:46:37.551080: step 11016, loss 0.120442, acc 0.953125
2017-03-02T17:46:37.631222: step 11017, loss 0.327375, acc 0.859375
2017-03-02T17:46:37.708312: step 11018, loss 0.126935, acc 0.953125
2017-03-02T17:46:37.777075: step 11019, loss 0.0939247, acc 0.96875
2017-03-02T17:46:37.851636: step 11020, loss 0.116213, acc 0.96875
2017-03-02T17:46:37.922476: step 11021, loss 0.181653, acc 0.90625
2017-03-02T17:46:38.000790: step 11022, loss 0.13392, acc 0.9375
2017-03-02T17:46:38.069990: step 11023, loss 0.225831, acc 0.921875
2017-03-02T17:46:38.160669: step 11024, loss 0.204544, acc 0.90625
2017-03-02T17:46:38.241727: step 11025, loss 0.0812574, acc 0.953125
2017-03-02T17:46:38.317897: step 11026, loss 0.133389, acc 0.921875
2017-03-02T17:46:38.391483: step 11027, loss 0.150515, acc 0.9375
2017-03-02T17:46:38.459336: step 11028, loss 0.219876, acc 0.921875
2017-03-02T17:46:38.534614: step 11029, loss 0.172561, acc 0.90625
2017-03-02T17:46:38.612954: step 11030, loss 0.119748, acc 0.953125
2017-03-02T17:46:38.680869: step 11031, loss 0.14902, acc 0.96875
2017-03-02T17:46:38.753725: step 11032, loss 0.147145, acc 0.9375
2017-03-02T17:46:38.826667: step 11033, loss 0.045867, acc 1
2017-03-02T17:46:38.897551: step 11034, loss 0.163903, acc 0.921875
2017-03-02T17:46:38.967314: step 11035, loss 0.23608, acc 0.890625
2017-03-02T17:46:39.041801: step 11036, loss 0.104878, acc 0.953125
2017-03-02T17:46:39.108099: step 11037, loss 0.0734907, acc 0.96875
2017-03-02T17:46:39.176538: step 11038, loss 0.0787017, acc 0.953125
2017-03-02T17:46:39.250511: step 11039, loss 0.0453974, acc 0.984375
2017-03-02T17:46:39.329140: step 11040, loss 0.178795, acc 0.90625
2017-03-02T17:46:39.408792: step 11041, loss 0.107642, acc 0.96875
2017-03-02T17:46:39.476808: step 11042, loss 0.0874777, acc 0.96875
2017-03-02T17:46:39.566518: step 11043, loss 0.212127, acc 0.921875
2017-03-02T17:46:39.648180: step 11044, loss 0.113721, acc 0.984375
2017-03-02T17:46:39.721662: step 11045, loss 0.0721867, acc 0.96875
2017-03-02T17:46:39.796477: step 11046, loss 0.255486, acc 0.890625
2017-03-02T17:46:39.866459: step 11047, loss 0.212824, acc 0.90625
2017-03-02T17:46:39.939296: step 11048, loss 0.0832318, acc 0.96875
2017-03-02T17:46:40.021361: step 11049, loss 0.057335, acc 0.984375
2017-03-02T17:46:40.092479: step 11050, loss 0.201685, acc 0.9375
2017-03-02T17:46:40.159821: step 11051, loss 0.189064, acc 0.953125
2017-03-02T17:46:40.228358: step 11052, loss 0.202427, acc 0.890625
2017-03-02T17:46:40.302089: step 11053, loss 0.0863725, acc 0.953125
2017-03-02T17:46:40.374810: step 11054, loss 0.185626, acc 0.96875
2017-03-02T17:46:40.446968: step 11055, loss 0.109072, acc 0.96875
2017-03-02T17:46:40.533878: step 11056, loss 0.0962809, acc 0.9375
2017-03-02T17:46:40.604315: step 11057, loss 0.186002, acc 0.9375
2017-03-02T17:46:40.681155: step 11058, loss 0.126652, acc 0.921875
2017-03-02T17:46:40.753048: step 11059, loss 0.193429, acc 0.90625
2017-03-02T17:46:40.825949: step 11060, loss 0.135495, acc 0.953125
2017-03-02T17:46:40.907258: step 11061, loss 0.0933264, acc 0.953125
2017-03-02T17:46:40.985599: step 11062, loss 0.100808, acc 0.96875
2017-03-02T17:46:41.068589: step 11063, loss 0.0856232, acc 0.953125
2017-03-02T17:46:41.142657: step 11064, loss 0.269111, acc 0.921875
2017-03-02T17:46:41.215537: step 11065, loss 0.193066, acc 0.9375
2017-03-02T17:46:41.282918: step 11066, loss 0.257509, acc 0.90625
2017-03-02T17:46:41.358461: step 11067, loss 0.189219, acc 0.9375
2017-03-02T17:46:41.434397: step 11068, loss 0.180732, acc 0.890625
2017-03-02T17:46:41.505424: step 11069, loss 0.26029, acc 0.90625
2017-03-02T17:46:41.594523: step 11070, loss 0.163784, acc 0.90625
2017-03-02T17:46:41.677318: step 11071, loss 0.161076, acc 0.9375
2017-03-02T17:46:41.750250: step 11072, loss 0.224598, acc 0.890625
2017-03-02T17:46:41.823474: step 11073, loss 0.138711, acc 0.953125
2017-03-02T17:46:41.899935: step 11074, loss 0.273513, acc 0.921875
2017-03-02T17:46:41.970161: step 11075, loss 0.190919, acc 0.921875
2017-03-02T17:46:42.046238: step 11076, loss 0.204995, acc 0.90625
2017-03-02T17:46:42.122491: step 11077, loss 0.305738, acc 0.953125
2017-03-02T17:46:42.192134: step 11078, loss 0.176287, acc 0.9375
2017-03-02T17:46:42.266924: step 11079, loss 0.141135, acc 0.9375
2017-03-02T17:46:42.342864: step 11080, loss 0.117519, acc 0.984375
2017-03-02T17:46:42.420114: step 11081, loss 0.143513, acc 0.953125
2017-03-02T17:46:42.492159: step 11082, loss 0.228191, acc 0.921875
2017-03-02T17:46:42.567973: step 11083, loss 0.134052, acc 0.96875
2017-03-02T17:46:42.634045: step 11084, loss 0.0513099, acc 1
2017-03-02T17:46:42.713334: step 11085, loss 0.176503, acc 0.953125
2017-03-02T17:46:42.787127: step 11086, loss 0.215858, acc 0.890625
2017-03-02T17:46:42.851935: step 11087, loss 0.247577, acc 0.90625
2017-03-02T17:46:42.921538: step 11088, loss 0.140027, acc 0.953125
2017-03-02T17:46:42.999177: step 11089, loss 0.128568, acc 0.921875
2017-03-02T17:46:43.071248: step 11090, loss 0.1255, acc 0.953125
2017-03-02T17:46:43.143147: step 11091, loss 0.1514, acc 0.9375
2017-03-02T17:46:43.216533: step 11092, loss 0.167501, acc 0.9375
2017-03-02T17:46:43.289493: step 11093, loss 0.2004, acc 0.90625
2017-03-02T17:46:43.362339: step 11094, loss 0.117077, acc 0.9375
2017-03-02T17:46:43.440890: step 11095, loss 0.116235, acc 0.953125
2017-03-02T17:46:43.505340: step 11096, loss 0.134334, acc 0.96875
2017-03-02T17:46:43.574570: step 11097, loss 0.20742, acc 0.921875
2017-03-02T17:46:43.647950: step 11098, loss 0.126902, acc 0.9375
2017-03-02T17:46:43.722826: step 11099, loss 0.174845, acc 0.921875
2017-03-02T17:46:43.796638: step 11100, loss 0.180404, acc 0.921875

Evaluation:
2017-03-02T17:46:43.830173: step 11100, loss 1.52456, acc 0.663302

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11100

2017-03-02T17:46:44.272732: step 11101, loss 0.140679, acc 0.9375
2017-03-02T17:46:44.348616: step 11102, loss 0.340652, acc 0.84375
2017-03-02T17:46:44.420148: step 11103, loss 0.118663, acc 0.953125
2017-03-02T17:46:44.494209: step 11104, loss 0.135849, acc 0.96875
2017-03-02T17:46:44.570288: step 11105, loss 0.158688, acc 0.90625
2017-03-02T17:46:44.637711: step 11106, loss 0.241955, acc 0.875
2017-03-02T17:46:44.705838: step 11107, loss 0.172548, acc 0.9375
2017-03-02T17:46:44.779240: step 11108, loss 0.114006, acc 0.9375
2017-03-02T17:46:44.848644: step 11109, loss 0.166016, acc 0.953125
2017-03-02T17:46:44.916221: step 11110, loss 0.12286, acc 0.9375
2017-03-02T17:46:44.983527: step 11111, loss 0.253249, acc 0.890625
2017-03-02T17:46:45.053544: step 11112, loss 0.115089, acc 0.96875
2017-03-02T17:46:45.126052: step 11113, loss 0.186284, acc 0.9375
2017-03-02T17:46:45.197730: step 11114, loss 0.203071, acc 0.875
2017-03-02T17:46:45.272810: step 11115, loss 0.123667, acc 0.90625
2017-03-02T17:46:45.345371: step 11116, loss 0.179907, acc 0.9375
2017-03-02T17:46:45.416406: step 11117, loss 0.174911, acc 0.90625
2017-03-02T17:46:45.491099: step 11118, loss 0.295619, acc 0.890625
2017-03-02T17:46:45.566715: step 11119, loss 0.202864, acc 0.90625
2017-03-02T17:46:45.638996: step 11120, loss 0.120638, acc 0.9375
2017-03-02T17:46:45.719787: step 11121, loss 0.270686, acc 0.875
2017-03-02T17:46:45.795331: step 11122, loss 0.306501, acc 0.875
2017-03-02T17:46:45.868665: step 11123, loss 0.154426, acc 0.9375
2017-03-02T17:46:45.944667: step 11124, loss 0.193681, acc 0.90625
2017-03-02T17:46:46.015825: step 11125, loss 0.225435, acc 0.890625
2017-03-02T17:46:46.087503: step 11126, loss 0.0971863, acc 0.953125
2017-03-02T17:46:46.166913: step 11127, loss 0.103205, acc 0.953125
2017-03-02T17:46:46.238847: step 11128, loss 0.191603, acc 0.921875
2017-03-02T17:46:46.307555: step 11129, loss 0.143707, acc 0.953125
2017-03-02T17:46:46.383161: step 11130, loss 0.200739, acc 0.90625
2017-03-02T17:46:46.456751: step 11131, loss 0.232422, acc 0.90625
2017-03-02T17:46:46.528359: step 11132, loss 0.355239, acc 0.875
2017-03-02T17:46:46.599853: step 11133, loss 0.16155, acc 0.953125
2017-03-02T17:46:46.674943: step 11134, loss 0.107896, acc 0.96875
2017-03-02T17:46:46.746901: step 11135, loss 0.191483, acc 0.9375
2017-03-02T17:46:46.834080: step 11136, loss 0.071131, acc 0.953125
2017-03-02T17:46:46.907775: step 11137, loss 0.215792, acc 0.90625
2017-03-02T17:46:46.974853: step 11138, loss 0.297271, acc 0.921875
2017-03-02T17:46:47.046478: step 11139, loss 0.190401, acc 0.921875
2017-03-02T17:46:47.122618: step 11140, loss 0.32466, acc 0.859375
2017-03-02T17:46:47.191634: step 11141, loss 0.281108, acc 0.875
2017-03-02T17:46:47.270089: step 11142, loss 0.0207818, acc 1
2017-03-02T17:46:47.346640: step 11143, loss 0.182074, acc 0.921875
2017-03-02T17:46:47.426672: step 11144, loss 0.176848, acc 0.9375
2017-03-02T17:46:47.500309: step 11145, loss 0.316691, acc 0.890625
2017-03-02T17:46:47.572901: step 11146, loss 0.180166, acc 0.921875
2017-03-02T17:46:47.657809: step 11147, loss 0.205981, acc 0.921875
2017-03-02T17:46:47.733491: step 11148, loss 0.216188, acc 0.890625
2017-03-02T17:46:47.812906: step 11149, loss 0.104743, acc 0.953125
2017-03-02T17:46:47.897782: step 11150, loss 0.283836, acc 0.890625
2017-03-02T17:46:47.966840: step 11151, loss 0.311037, acc 0.859375
2017-03-02T17:46:48.041606: step 11152, loss 0.10999, acc 0.953125
2017-03-02T17:46:48.114328: step 11153, loss 0.177781, acc 0.921875
2017-03-02T17:46:48.184043: step 11154, loss 0.328702, acc 0.828125
2017-03-02T17:46:48.255649: step 11155, loss 0.220615, acc 0.9375
2017-03-02T17:46:48.319335: step 11156, loss 0.214953, acc 0.875
2017-03-02T17:46:48.388749: step 11157, loss 0.172383, acc 0.90625
2017-03-02T17:46:48.460289: step 11158, loss 0.151494, acc 0.9375
2017-03-02T17:46:48.534218: step 11159, loss 0.0565349, acc 0.984375
2017-03-02T17:46:48.620539: step 11160, loss 0.226265, acc 0.90625
2017-03-02T17:46:48.692090: step 11161, loss 0.205054, acc 0.9375
2017-03-02T17:46:48.760732: step 11162, loss 0.181381, acc 0.90625
2017-03-02T17:46:48.830990: step 11163, loss 0.146979, acc 0.921875
2017-03-02T17:46:48.906672: step 11164, loss 0.343971, acc 0.875
2017-03-02T17:46:48.977357: step 11165, loss 0.123141, acc 0.9375
2017-03-02T17:46:49.045477: step 11166, loss 0.247244, acc 0.921875
2017-03-02T17:46:49.111400: step 11167, loss 0.220018, acc 0.890625
2017-03-02T17:46:49.182735: step 11168, loss 0.224448, acc 0.90625
2017-03-02T17:46:49.253448: step 11169, loss 0.154009, acc 0.9375
2017-03-02T17:46:49.329125: step 11170, loss 0.345228, acc 0.8125
2017-03-02T17:46:49.401129: step 11171, loss 0.18471, acc 0.90625
2017-03-02T17:46:49.473199: step 11172, loss 0.083715, acc 1
2017-03-02T17:46:49.551375: step 11173, loss 0.0897975, acc 0.96875
2017-03-02T17:46:49.625763: step 11174, loss 0.113322, acc 0.9375
2017-03-02T17:46:49.697453: step 11175, loss 0.0695154, acc 0.984375
2017-03-02T17:46:49.768237: step 11176, loss 0.0717474, acc 0.96875
2017-03-02T17:46:49.832969: step 11177, loss 0.079947, acc 0.984375
2017-03-02T17:46:49.908832: step 11178, loss 0.0950942, acc 0.96875
2017-03-02T17:46:49.982735: step 11179, loss 0.124715, acc 0.9375
2017-03-02T17:46:50.056662: step 11180, loss 0.102159, acc 0.984375
2017-03-02T17:46:50.139768: step 11181, loss 0.167539, acc 0.953125
2017-03-02T17:46:50.216689: step 11182, loss 0.0185253, acc 1
2017-03-02T17:46:50.287467: step 11183, loss 0.335261, acc 0.859375
2017-03-02T17:46:50.357639: step 11184, loss 0.151796, acc 0.921875
2017-03-02T17:46:50.426802: step 11185, loss 0.123777, acc 0.984375
2017-03-02T17:46:50.492329: step 11186, loss 0.203108, acc 0.890625
2017-03-02T17:46:50.562489: step 11187, loss 0.226307, acc 0.90625
2017-03-02T17:46:50.636939: step 11188, loss 0.251033, acc 0.890625
2017-03-02T17:46:50.720088: step 11189, loss 0.101184, acc 0.953125
2017-03-02T17:46:50.793927: step 11190, loss 0.0851695, acc 0.984375
2017-03-02T17:46:50.869674: step 11191, loss 0.153191, acc 0.9375
2017-03-02T17:46:50.934315: step 11192, loss 0.140513, acc 0.921875
2017-03-02T17:46:51.007477: step 11193, loss 0.0652477, acc 0.96875
2017-03-02T17:46:51.078146: step 11194, loss 0.153837, acc 0.9375
2017-03-02T17:46:51.148238: step 11195, loss 0.181903, acc 0.90625
2017-03-02T17:46:51.218655: step 11196, loss 0.138043, acc 0.953125
2017-03-02T17:46:51.292216: step 11197, loss 0.157809, acc 0.953125
2017-03-02T17:46:51.355878: step 11198, loss 0.0408037, acc 0.984375
2017-03-02T17:46:51.435158: step 11199, loss 0.212558, acc 0.90625
2017-03-02T17:46:51.508909: step 11200, loss 0.0574362, acc 1

Evaluation:
2017-03-02T17:46:51.547234: step 11200, loss 1.52389, acc 0.660418

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11200

2017-03-02T17:46:52.023623: step 11201, loss 0.294979, acc 0.90625
2017-03-02T17:46:52.097989: step 11202, loss 0.245312, acc 0.859375
2017-03-02T17:46:52.176610: step 11203, loss 0.160281, acc 0.921875
2017-03-02T17:46:52.251199: step 11204, loss 0.142832, acc 0.9375
2017-03-02T17:46:52.318913: step 11205, loss 0.0843668, acc 0.96875
2017-03-02T17:46:52.401745: step 11206, loss 0.141733, acc 0.921875
2017-03-02T17:46:52.463661: step 11207, loss 0.228567, acc 0.921875
2017-03-02T17:46:52.534434: step 11208, loss 0.232605, acc 0.890625
2017-03-02T17:46:52.604432: step 11209, loss 0.218137, acc 0.921875
2017-03-02T17:46:52.682139: step 11210, loss 0.199065, acc 0.90625
2017-03-02T17:46:52.754487: step 11211, loss 0.0817451, acc 0.96875
2017-03-02T17:46:52.829888: step 11212, loss 0.121737, acc 0.9375
2017-03-02T17:46:52.902691: step 11213, loss 0.214419, acc 0.890625
2017-03-02T17:46:52.975457: step 11214, loss 0.0801638, acc 0.953125
2017-03-02T17:46:53.051172: step 11215, loss 0.215072, acc 0.953125
2017-03-02T17:46:53.111796: step 11216, loss 0.200267, acc 0.921875
2017-03-02T17:46:53.172740: step 11217, loss 0.0942236, acc 0.984375
2017-03-02T17:46:53.239148: step 11218, loss 0.186245, acc 0.9375
2017-03-02T17:46:53.310856: step 11219, loss 0.220382, acc 0.921875
2017-03-02T17:46:53.386636: step 11220, loss 0.129644, acc 0.9375
2017-03-02T17:46:53.458658: step 11221, loss 0.122831, acc 0.953125
2017-03-02T17:46:53.529646: step 11222, loss 0.161083, acc 0.921875
2017-03-02T17:46:53.609175: step 11223, loss 0.155909, acc 0.9375
2017-03-02T17:46:53.679654: step 11224, loss 0.283621, acc 0.859375
2017-03-02T17:46:53.753038: step 11225, loss 0.116835, acc 0.96875
2017-03-02T17:46:53.825437: step 11226, loss 0.107269, acc 0.9375
2017-03-02T17:46:53.900048: step 11227, loss 0.210722, acc 0.90625
2017-03-02T17:46:53.972413: step 11228, loss 0.220234, acc 0.90625
2017-03-02T17:46:54.046878: step 11229, loss 0.103923, acc 0.96875
2017-03-02T17:46:54.121317: step 11230, loss 0.240733, acc 0.921875
2017-03-02T17:46:54.200699: step 11231, loss 0.139212, acc 0.9375
2017-03-02T17:46:54.281253: step 11232, loss 0.345676, acc 0.828125
2017-03-02T17:46:54.346932: step 11233, loss 0.131272, acc 0.953125
2017-03-02T17:46:54.415879: step 11234, loss 0.293168, acc 0.90625
2017-03-02T17:46:54.482516: step 11235, loss 0.198908, acc 0.875
2017-03-02T17:46:54.558868: step 11236, loss 0.167621, acc 0.953125
2017-03-02T17:46:54.635253: step 11237, loss 0.11667, acc 0.9375
2017-03-02T17:46:54.705437: step 11238, loss 0.117596, acc 0.953125
2017-03-02T17:46:54.785649: step 11239, loss 0.246054, acc 0.921875
2017-03-02T17:46:54.859202: step 11240, loss 0.226698, acc 0.890625
2017-03-02T17:46:54.935774: step 11241, loss 0.246949, acc 0.921875
2017-03-02T17:46:55.008733: step 11242, loss 0.186062, acc 0.875
2017-03-02T17:46:55.089450: step 11243, loss 0.0489817, acc 0.984375
2017-03-02T17:46:55.159452: step 11244, loss 0.255898, acc 0.9375
2017-03-02T17:46:55.232762: step 11245, loss 0.139704, acc 0.9375
2017-03-02T17:46:55.300640: step 11246, loss 0.107177, acc 0.96875
2017-03-02T17:46:55.372440: step 11247, loss 0.11453, acc 0.96875
2017-03-02T17:46:55.443141: step 11248, loss 0.173005, acc 0.921875
2017-03-02T17:46:55.524796: step 11249, loss 0.239883, acc 0.921875
2017-03-02T17:46:55.600933: step 11250, loss 0.270575, acc 0.875
2017-03-02T17:46:55.674947: step 11251, loss 0.110927, acc 0.953125
2017-03-02T17:46:55.747293: step 11252, loss 0.23612, acc 0.953125
2017-03-02T17:46:55.820143: step 11253, loss 0.256265, acc 0.9375
2017-03-02T17:46:55.895458: step 11254, loss 0.228984, acc 0.921875
2017-03-02T17:46:55.967713: step 11255, loss 0.148866, acc 0.921875
2017-03-02T17:46:56.037493: step 11256, loss 0.116309, acc 0.953125
2017-03-02T17:46:56.107250: step 11257, loss 0.106299, acc 0.953125
2017-03-02T17:46:56.179103: step 11258, loss 0.140323, acc 0.921875
2017-03-02T17:46:56.252536: step 11259, loss 0.163049, acc 0.9375
2017-03-02T17:46:56.318147: step 11260, loss 0.177447, acc 0.90625
2017-03-02T17:46:56.404341: step 11261, loss 0.155697, acc 0.953125
2017-03-02T17:46:56.477011: step 11262, loss 0.207409, acc 0.890625
2017-03-02T17:46:56.553359: step 11263, loss 0.131982, acc 0.96875
2017-03-02T17:46:56.621603: step 11264, loss 0.131559, acc 0.953125
2017-03-02T17:46:56.689683: step 11265, loss 0.19667, acc 0.9375
2017-03-02T17:46:56.764508: step 11266, loss 0.0896292, acc 0.984375
2017-03-02T17:46:56.852074: step 11267, loss 0.243953, acc 0.890625
2017-03-02T17:46:56.938594: step 11268, loss 0.11178, acc 0.984375
2017-03-02T17:46:57.008990: step 11269, loss 0.210725, acc 0.9375
2017-03-02T17:46:57.079199: step 11270, loss 0.107787, acc 0.953125
2017-03-02T17:46:57.158335: step 11271, loss 0.0647666, acc 0.96875
2017-03-02T17:46:57.232599: step 11272, loss 0.0980976, acc 0.953125
2017-03-02T17:46:57.301206: step 11273, loss 0.263845, acc 0.875
2017-03-02T17:46:57.365264: step 11274, loss 0.0824826, acc 0.96875
2017-03-02T17:46:57.444497: step 11275, loss 0.0802943, acc 0.96875
2017-03-02T17:46:57.525253: step 11276, loss 0.213299, acc 0.90625
2017-03-02T17:46:57.601205: step 11277, loss 0.0944012, acc 0.96875
2017-03-02T17:46:57.671352: step 11278, loss 0.107335, acc 0.953125
2017-03-02T17:46:57.763474: step 11279, loss 0.230264, acc 0.875
2017-03-02T17:46:57.824537: step 11280, loss 0.225764, acc 0.921875
2017-03-02T17:46:57.897224: step 11281, loss 0.183645, acc 0.921875
2017-03-02T17:46:57.971181: step 11282, loss 0.161923, acc 0.90625
2017-03-02T17:46:58.037188: step 11283, loss 0.201435, acc 0.90625
2017-03-02T17:46:58.107102: step 11284, loss 0.20644, acc 0.90625
2017-03-02T17:46:58.179550: step 11285, loss 0.257316, acc 0.890625
2017-03-02T17:46:58.252940: step 11286, loss 0.153795, acc 0.890625
2017-03-02T17:46:58.325575: step 11287, loss 0.209813, acc 0.9375
2017-03-02T17:46:58.398063: step 11288, loss 0.125467, acc 0.9375
2017-03-02T17:46:58.467173: step 11289, loss 0.13187, acc 0.953125
2017-03-02T17:46:58.551111: step 11290, loss 0.0523671, acc 1
2017-03-02T17:46:58.629881: step 11291, loss 0.18238, acc 0.921875
2017-03-02T17:46:58.700514: step 11292, loss 0.107578, acc 0.96875
2017-03-02T17:46:58.769269: step 11293, loss 0.357929, acc 0.875
2017-03-02T17:46:58.842247: step 11294, loss 0.159215, acc 0.921875
2017-03-02T17:46:58.913256: step 11295, loss 0.150839, acc 0.953125
2017-03-02T17:46:58.983597: step 11296, loss 0.158453, acc 0.9375
2017-03-02T17:46:59.056948: step 11297, loss 0.220954, acc 0.921875
2017-03-02T17:46:59.136731: step 11298, loss 0.113399, acc 0.953125
2017-03-02T17:46:59.212411: step 11299, loss 0.138218, acc 0.953125
2017-03-02T17:46:59.286847: step 11300, loss 0.0834456, acc 0.953125

Evaluation:
2017-03-02T17:46:59.315893: step 11300, loss 1.54483, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11300

2017-03-02T17:46:59.812703: step 11301, loss 0.133519, acc 0.921875
2017-03-02T17:46:59.888404: step 11302, loss 0.126367, acc 0.953125
2017-03-02T17:46:59.959460: step 11303, loss 0.143513, acc 0.96875
2017-03-02T17:47:00.018434: step 11304, loss 0.206557, acc 0.890625
2017-03-02T17:47:00.083119: step 11305, loss 0.330238, acc 0.875
2017-03-02T17:47:00.152578: step 11306, loss 0.105743, acc 0.921875
2017-03-02T17:47:00.216340: step 11307, loss 0.188723, acc 0.953125
2017-03-02T17:47:00.290894: step 11308, loss 0.0652674, acc 0.96875
2017-03-02T17:47:00.366285: step 11309, loss 0.217808, acc 0.890625
2017-03-02T17:47:00.437184: step 11310, loss 0.120214, acc 0.9375
2017-03-02T17:47:00.510612: step 11311, loss 0.165323, acc 0.953125
2017-03-02T17:47:00.584227: step 11312, loss 0.244652, acc 0.9375
2017-03-02T17:47:00.655441: step 11313, loss 0.222223, acc 0.859375
2017-03-02T17:47:00.730750: step 11314, loss 0.126192, acc 0.953125
2017-03-02T17:47:00.798236: step 11315, loss 0.231893, acc 0.9375
2017-03-02T17:47:00.871136: step 11316, loss 0.237897, acc 0.90625
2017-03-02T17:47:00.940845: step 11317, loss 0.0760306, acc 0.953125
2017-03-02T17:47:01.014192: step 11318, loss 0.28703, acc 0.890625
2017-03-02T17:47:01.095180: step 11319, loss 0.231185, acc 0.921875
2017-03-02T17:47:01.180940: step 11320, loss 0.15318, acc 0.921875
2017-03-02T17:47:01.253137: step 11321, loss 0.307375, acc 0.890625
2017-03-02T17:47:01.331915: step 11322, loss 0.222073, acc 0.90625
2017-03-02T17:47:01.401067: step 11323, loss 0.207077, acc 0.90625
2017-03-02T17:47:01.470204: step 11324, loss 0.0915997, acc 0.953125
2017-03-02T17:47:01.551864: step 11325, loss 0.161113, acc 0.9375
2017-03-02T17:47:01.625573: step 11326, loss 0.19503, acc 0.9375
2017-03-02T17:47:01.691629: step 11327, loss 0.146057, acc 0.953125
2017-03-02T17:47:01.765036: step 11328, loss 0.11438, acc 0.953125
2017-03-02T17:47:01.837866: step 11329, loss 0.0838954, acc 0.984375
2017-03-02T17:47:01.913982: step 11330, loss 0.193923, acc 0.890625
2017-03-02T17:47:01.989238: step 11331, loss 0.309837, acc 0.859375
2017-03-02T17:47:02.066973: step 11332, loss 0.0989343, acc 0.953125
2017-03-02T17:47:02.133440: step 11333, loss 0.0977203, acc 0.984375
2017-03-02T17:47:02.201021: step 11334, loss 0.100705, acc 0.96875
2017-03-02T17:47:02.274076: step 11335, loss 0.171717, acc 0.953125
2017-03-02T17:47:02.352787: step 11336, loss 0.260651, acc 0.859375
2017-03-02T17:47:02.436630: step 11337, loss 0.108517, acc 0.953125
2017-03-02T17:47:02.508908: step 11338, loss 0.292453, acc 0.90625
2017-03-02T17:47:02.587600: step 11339, loss 0.312542, acc 0.875
2017-03-02T17:47:02.660996: step 11340, loss 0.207129, acc 0.875
2017-03-02T17:47:02.741378: step 11341, loss 0.274446, acc 0.90625
2017-03-02T17:47:02.814850: step 11342, loss 0.139811, acc 0.9375
2017-03-02T17:47:02.889836: step 11343, loss 0.233428, acc 0.953125
2017-03-02T17:47:02.967635: step 11344, loss 0.163158, acc 0.9375
2017-03-02T17:47:03.045834: step 11345, loss 0.213299, acc 0.875
2017-03-02T17:47:03.120369: step 11346, loss 0.116906, acc 0.96875
2017-03-02T17:47:03.193812: step 11347, loss 0.208909, acc 0.9375
2017-03-02T17:47:03.265235: step 11348, loss 0.226847, acc 0.90625
2017-03-02T17:47:03.341063: step 11349, loss 0.130953, acc 0.96875
2017-03-02T17:47:03.417403: step 11350, loss 0.264063, acc 0.859375
2017-03-02T17:47:03.484604: step 11351, loss 0.207158, acc 0.90625
2017-03-02T17:47:03.558424: step 11352, loss 0.188419, acc 0.890625
2017-03-02T17:47:03.634150: step 11353, loss 0.21877, acc 0.90625
2017-03-02T17:47:03.717765: step 11354, loss 0.375632, acc 0.859375
2017-03-02T17:47:03.798128: step 11355, loss 0.325426, acc 0.890625
2017-03-02T17:47:03.863370: step 11356, loss 0.195992, acc 0.890625
2017-03-02T17:47:03.938190: step 11357, loss 0.122786, acc 0.921875
2017-03-02T17:47:04.011310: step 11358, loss 0.157605, acc 0.90625
2017-03-02T17:47:04.084460: step 11359, loss 0.161679, acc 0.96875
2017-03-02T17:47:04.154785: step 11360, loss 0.114615, acc 0.953125
2017-03-02T17:47:04.230249: step 11361, loss 0.35284, acc 0.875
2017-03-02T17:47:04.307351: step 11362, loss 0.313664, acc 0.84375
2017-03-02T17:47:04.382889: step 11363, loss 0.127007, acc 0.953125
2017-03-02T17:47:04.458859: step 11364, loss 0.238571, acc 0.875
2017-03-02T17:47:04.538606: step 11365, loss 0.30504, acc 0.890625
2017-03-02T17:47:04.622447: step 11366, loss 0.188662, acc 0.9375
2017-03-02T17:47:04.695550: step 11367, loss 0.313642, acc 0.90625
2017-03-02T17:47:04.755952: step 11368, loss 0.238383, acc 0.75
2017-03-02T17:47:04.828996: step 11369, loss 0.218629, acc 0.90625
2017-03-02T17:47:04.892823: step 11370, loss 0.192212, acc 0.90625
2017-03-02T17:47:04.966575: step 11371, loss 0.163646, acc 0.953125
2017-03-02T17:47:05.034942: step 11372, loss 0.154141, acc 0.96875
2017-03-02T17:47:05.111737: step 11373, loss 0.156261, acc 0.921875
2017-03-02T17:47:05.180226: step 11374, loss 0.304321, acc 0.828125
2017-03-02T17:47:05.257438: step 11375, loss 0.0693233, acc 0.96875
2017-03-02T17:47:05.331062: step 11376, loss 0.117511, acc 0.953125
2017-03-02T17:47:05.401561: step 11377, loss 0.144852, acc 0.9375
2017-03-02T17:47:05.474885: step 11378, loss 0.161856, acc 0.953125
2017-03-02T17:47:05.547386: step 11379, loss 0.223624, acc 0.921875
2017-03-02T17:47:05.621128: step 11380, loss 0.155653, acc 0.953125
2017-03-02T17:47:05.696468: step 11381, loss 0.155611, acc 0.9375
2017-03-02T17:47:05.773601: step 11382, loss 0.127016, acc 0.953125
2017-03-02T17:47:05.850213: step 11383, loss 0.115629, acc 0.953125
2017-03-02T17:47:05.924760: step 11384, loss 0.125213, acc 0.953125
2017-03-02T17:47:05.999712: step 11385, loss 0.1232, acc 0.96875
2017-03-02T17:47:06.077706: step 11386, loss 0.191192, acc 0.921875
2017-03-02T17:47:06.152050: step 11387, loss 0.127725, acc 0.9375
2017-03-02T17:47:06.219698: step 11388, loss 0.128229, acc 0.921875
2017-03-02T17:47:06.288897: step 11389, loss 0.0869438, acc 0.953125
2017-03-02T17:47:06.358074: step 11390, loss 0.214448, acc 0.921875
2017-03-02T17:47:06.428879: step 11391, loss 0.104874, acc 0.953125
2017-03-02T17:47:06.504292: step 11392, loss 0.200665, acc 0.9375
2017-03-02T17:47:06.577851: step 11393, loss 0.244726, acc 0.921875
2017-03-02T17:47:06.666035: step 11394, loss 0.0888344, acc 0.9375
2017-03-02T17:47:06.737080: step 11395, loss 0.0653307, acc 0.96875
2017-03-02T17:47:06.810490: step 11396, loss 0.162662, acc 0.9375
2017-03-02T17:47:06.880710: step 11397, loss 0.085043, acc 0.96875
2017-03-02T17:47:06.955471: step 11398, loss 0.200067, acc 0.9375
2017-03-02T17:47:07.018995: step 11399, loss 0.229566, acc 0.90625
2017-03-02T17:47:07.088767: step 11400, loss 0.129737, acc 0.9375

Evaluation:
2017-03-02T17:47:07.124820: step 11400, loss 1.56115, acc 0.673396

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11400

2017-03-02T17:47:07.583963: step 11401, loss 0.119599, acc 0.953125
2017-03-02T17:47:07.655058: step 11402, loss 0.108347, acc 0.9375
2017-03-02T17:47:07.721095: step 11403, loss 0.113486, acc 0.96875
2017-03-02T17:47:07.790823: step 11404, loss 0.199553, acc 0.921875
2017-03-02T17:47:07.866544: step 11405, loss 0.293999, acc 0.890625
2017-03-02T17:47:07.948983: step 11406, loss 0.218755, acc 0.90625
2017-03-02T17:47:08.020700: step 11407, loss 0.212602, acc 0.9375
2017-03-02T17:47:08.091626: step 11408, loss 0.114515, acc 0.921875
2017-03-02T17:47:08.164678: step 11409, loss 0.200344, acc 0.921875
2017-03-02T17:47:08.229481: step 11410, loss 0.13191, acc 0.921875
2017-03-02T17:47:08.301421: step 11411, loss 0.163985, acc 0.921875
2017-03-02T17:47:08.370141: step 11412, loss 0.0896672, acc 0.953125
2017-03-02T17:47:08.440459: step 11413, loss 0.0642865, acc 0.984375
2017-03-02T17:47:08.513445: step 11414, loss 0.100195, acc 0.953125
2017-03-02T17:47:08.591681: step 11415, loss 0.0778072, acc 0.953125
2017-03-02T17:47:08.668359: step 11416, loss 0.355352, acc 0.84375
2017-03-02T17:47:08.751699: step 11417, loss 0.351886, acc 0.828125
2017-03-02T17:47:08.828188: step 11418, loss 0.107158, acc 0.9375
2017-03-02T17:47:08.900676: step 11419, loss 0.157908, acc 0.90625
2017-03-02T17:47:08.971007: step 11420, loss 0.276315, acc 0.921875
2017-03-02T17:47:09.041055: step 11421, loss 0.0801492, acc 0.96875
2017-03-02T17:47:09.115015: step 11422, loss 0.0814007, acc 0.96875
2017-03-02T17:47:09.197345: step 11423, loss 0.146963, acc 0.921875
2017-03-02T17:47:09.271364: step 11424, loss 0.115114, acc 0.9375
2017-03-02T17:47:09.346579: step 11425, loss 0.222187, acc 0.921875
2017-03-02T17:47:09.417008: step 11426, loss 0.178368, acc 0.90625
2017-03-02T17:47:09.497866: step 11427, loss 0.121329, acc 0.9375
2017-03-02T17:47:09.569781: step 11428, loss 0.14055, acc 0.921875
2017-03-02T17:47:09.635494: step 11429, loss 0.11023, acc 0.953125
2017-03-02T17:47:09.710037: step 11430, loss 0.0889733, acc 0.953125
2017-03-02T17:47:09.803206: step 11431, loss 0.195538, acc 0.875
2017-03-02T17:47:09.871398: step 11432, loss 0.233932, acc 0.921875
2017-03-02T17:47:09.941542: step 11433, loss 0.281845, acc 0.859375
2017-03-02T17:47:10.013309: step 11434, loss 0.228884, acc 0.875
2017-03-02T17:47:10.083492: step 11435, loss 0.066757, acc 0.984375
2017-03-02T17:47:10.147186: step 11436, loss 0.0789122, acc 0.953125
2017-03-02T17:47:10.218660: step 11437, loss 0.204675, acc 0.9375
2017-03-02T17:47:10.293694: step 11438, loss 0.180851, acc 0.90625
2017-03-02T17:47:10.365701: step 11439, loss 0.268514, acc 0.90625
2017-03-02T17:47:10.441673: step 11440, loss 0.1469, acc 0.9375
2017-03-02T17:47:10.513884: step 11441, loss 0.18249, acc 0.921875
2017-03-02T17:47:10.588131: step 11442, loss 0.154095, acc 0.9375
2017-03-02T17:47:10.658914: step 11443, loss 0.0996444, acc 0.96875
2017-03-02T17:47:10.733250: step 11444, loss 0.201791, acc 0.90625
2017-03-02T17:47:10.808397: step 11445, loss 0.156543, acc 0.921875
2017-03-02T17:47:10.878095: step 11446, loss 0.210534, acc 0.921875
2017-03-02T17:47:10.947389: step 11447, loss 0.158849, acc 0.953125
2017-03-02T17:47:11.021788: step 11448, loss 0.145466, acc 0.9375
2017-03-02T17:47:11.082439: step 11449, loss 0.178709, acc 0.921875
2017-03-02T17:47:11.153551: step 11450, loss 0.135527, acc 0.921875
2017-03-02T17:47:11.244636: step 11451, loss 0.197314, acc 0.890625
2017-03-02T17:47:11.318819: step 11452, loss 0.222518, acc 0.9375
2017-03-02T17:47:11.390234: step 11453, loss 0.0800829, acc 0.96875
2017-03-02T17:47:11.460400: step 11454, loss 0.174332, acc 0.890625
2017-03-02T17:47:11.532158: step 11455, loss 0.196888, acc 0.90625
2017-03-02T17:47:11.607466: step 11456, loss 0.094214, acc 0.953125
2017-03-02T17:47:11.678021: step 11457, loss 0.281247, acc 0.84375
2017-03-02T17:47:11.745889: step 11458, loss 0.119214, acc 0.953125
2017-03-02T17:47:11.818266: step 11459, loss 0.105673, acc 0.953125
2017-03-02T17:47:11.896998: step 11460, loss 0.158787, acc 0.9375
2017-03-02T17:47:11.970518: step 11461, loss 0.137375, acc 0.953125
2017-03-02T17:47:12.049938: step 11462, loss 0.12223, acc 0.9375
2017-03-02T17:47:12.126034: step 11463, loss 0.264138, acc 0.921875
2017-03-02T17:47:12.198174: step 11464, loss 0.160065, acc 0.96875
2017-03-02T17:47:12.270390: step 11465, loss 0.108647, acc 0.96875
2017-03-02T17:47:12.342351: step 11466, loss 0.197728, acc 0.953125
2017-03-02T17:47:12.411235: step 11467, loss 0.160397, acc 0.953125
2017-03-02T17:47:12.484350: step 11468, loss 0.214241, acc 0.921875
2017-03-02T17:47:12.557682: step 11469, loss 0.127954, acc 0.9375
2017-03-02T17:47:12.631946: step 11470, loss 0.190241, acc 0.90625
2017-03-02T17:47:12.700462: step 11471, loss 0.14016, acc 0.921875
2017-03-02T17:47:12.772447: step 11472, loss 0.141179, acc 0.9375
2017-03-02T17:47:12.846380: step 11473, loss 0.165583, acc 0.90625
2017-03-02T17:47:12.922945: step 11474, loss 0.295995, acc 0.921875
2017-03-02T17:47:13.001988: step 11475, loss 0.165191, acc 0.953125
2017-03-02T17:47:13.075984: step 11476, loss 0.191381, acc 0.90625
2017-03-02T17:47:13.145304: step 11477, loss 0.0876559, acc 0.9375
2017-03-02T17:47:13.219318: step 11478, loss 0.115012, acc 0.96875
2017-03-02T17:47:13.290892: step 11479, loss 0.42056, acc 0.796875
2017-03-02T17:47:13.361824: step 11480, loss 0.386067, acc 0.859375
2017-03-02T17:47:13.453989: step 11481, loss 0.252595, acc 0.90625
2017-03-02T17:47:13.527307: step 11482, loss 0.113133, acc 0.984375
2017-03-02T17:47:13.592640: step 11483, loss 0.161065, acc 0.921875
2017-03-02T17:47:13.671250: step 11484, loss 0.192097, acc 0.921875
2017-03-02T17:47:13.737213: step 11485, loss 0.111022, acc 0.96875
2017-03-02T17:47:13.801782: step 11486, loss 0.28164, acc 0.859375
2017-03-02T17:47:13.869597: step 11487, loss 0.131572, acc 0.953125
2017-03-02T17:47:13.946873: step 11488, loss 0.17847, acc 0.921875
2017-03-02T17:47:14.026434: step 11489, loss 0.150442, acc 0.953125
2017-03-02T17:47:14.103762: step 11490, loss 0.0902705, acc 0.96875
2017-03-02T17:47:14.175889: step 11491, loss 0.231221, acc 0.921875
2017-03-02T17:47:14.240952: step 11492, loss 0.126718, acc 0.9375
2017-03-02T17:47:14.303542: step 11493, loss 0.141265, acc 0.9375
2017-03-02T17:47:14.373968: step 11494, loss 0.1454, acc 0.9375
2017-03-02T17:47:14.441250: step 11495, loss 0.275822, acc 0.921875
2017-03-02T17:47:14.511480: step 11496, loss 0.0616303, acc 1
2017-03-02T17:47:14.585418: step 11497, loss 0.149578, acc 0.9375
2017-03-02T17:47:14.660637: step 11498, loss 0.234666, acc 0.90625
2017-03-02T17:47:14.733235: step 11499, loss 0.0680066, acc 1
2017-03-02T17:47:14.803092: step 11500, loss 0.114175, acc 0.9375

Evaluation:
2017-03-02T17:47:14.838927: step 11500, loss 1.5449, acc 0.659697

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11500

2017-03-02T17:47:15.299134: step 11501, loss 0.246205, acc 0.90625
2017-03-02T17:47:15.375034: step 11502, loss 0.0484257, acc 0.984375
2017-03-02T17:47:15.452995: step 11503, loss 0.0637776, acc 0.96875
2017-03-02T17:47:15.529702: step 11504, loss 0.131215, acc 0.953125
2017-03-02T17:47:15.598962: step 11505, loss 0.229224, acc 0.921875
2017-03-02T17:47:15.667450: step 11506, loss 0.121389, acc 0.96875
2017-03-02T17:47:15.740294: step 11507, loss 0.166524, acc 0.921875
2017-03-02T17:47:15.812420: step 11508, loss 0.276801, acc 0.859375
2017-03-02T17:47:15.880651: step 11509, loss 0.254983, acc 0.890625
2017-03-02T17:47:15.950254: step 11510, loss 0.188205, acc 0.921875
2017-03-02T17:47:16.023682: step 11511, loss 0.103403, acc 0.96875
2017-03-02T17:47:16.104813: step 11512, loss 0.159741, acc 0.9375
2017-03-02T17:47:16.178402: step 11513, loss 0.14532, acc 0.921875
2017-03-02T17:47:16.255308: step 11514, loss 0.183995, acc 0.890625
2017-03-02T17:47:16.334751: step 11515, loss 0.157666, acc 0.921875
2017-03-02T17:47:16.413435: step 11516, loss 0.0742799, acc 0.953125
2017-03-02T17:47:16.483881: step 11517, loss 0.176933, acc 0.9375
2017-03-02T17:47:16.553504: step 11518, loss 0.106995, acc 0.96875
2017-03-02T17:47:16.623443: step 11519, loss 0.125155, acc 0.953125
2017-03-02T17:47:16.699520: step 11520, loss 0.135052, acc 0.9375
2017-03-02T17:47:16.770554: step 11521, loss 0.172446, acc 0.96875
2017-03-02T17:47:16.842512: step 11522, loss 0.26407, acc 0.90625
2017-03-02T17:47:16.913572: step 11523, loss 0.0894281, acc 0.984375
2017-03-02T17:47:16.989577: step 11524, loss 0.2005, acc 0.921875
2017-03-02T17:47:17.059588: step 11525, loss 0.208681, acc 0.9375
2017-03-02T17:47:17.131104: step 11526, loss 0.044394, acc 0.984375
2017-03-02T17:47:17.200191: step 11527, loss 0.173141, acc 0.9375
2017-03-02T17:47:17.266533: step 11528, loss 0.0638147, acc 0.96875
2017-03-02T17:47:17.335131: step 11529, loss 0.185559, acc 0.9375
2017-03-02T17:47:17.411427: step 11530, loss 0.0901194, acc 0.96875
2017-03-02T17:47:17.478915: step 11531, loss 0.216537, acc 0.9375
2017-03-02T17:47:17.565349: step 11532, loss 0.339662, acc 0.921875
2017-03-02T17:47:17.641026: step 11533, loss 0.225704, acc 0.890625
2017-03-02T17:47:17.717127: step 11534, loss 0.167304, acc 0.921875
2017-03-02T17:47:17.783901: step 11535, loss 0.222904, acc 0.890625
2017-03-02T17:47:17.857256: step 11536, loss 0.244806, acc 0.890625
2017-03-02T17:47:17.926400: step 11537, loss 0.178723, acc 0.9375
2017-03-02T17:47:17.996002: step 11538, loss 0.270769, acc 0.875
2017-03-02T17:47:18.070854: step 11539, loss 0.317331, acc 0.890625
2017-03-02T17:47:18.141389: step 11540, loss 0.365656, acc 0.890625
2017-03-02T17:47:18.210427: step 11541, loss 0.164655, acc 0.953125
2017-03-02T17:47:18.290770: step 11542, loss 0.13467, acc 0.953125
2017-03-02T17:47:18.370035: step 11543, loss 0.136715, acc 0.953125
2017-03-02T17:47:18.447539: step 11544, loss 0.314009, acc 0.890625
2017-03-02T17:47:18.523762: step 11545, loss 0.167149, acc 0.953125
2017-03-02T17:47:18.593099: step 11546, loss 0.289646, acc 0.859375
2017-03-02T17:47:18.662182: step 11547, loss 0.221406, acc 0.890625
2017-03-02T17:47:18.736762: step 11548, loss 0.163295, acc 0.96875
2017-03-02T17:47:18.811227: step 11549, loss 0.15066, acc 0.96875
2017-03-02T17:47:18.883966: step 11550, loss 0.123532, acc 0.9375
2017-03-02T17:47:18.956019: step 11551, loss 0.129325, acc 0.953125
2017-03-02T17:47:19.030177: step 11552, loss 0.209458, acc 0.9375
2017-03-02T17:47:19.096996: step 11553, loss 0.145193, acc 0.953125
2017-03-02T17:47:19.173510: step 11554, loss 0.196704, acc 0.921875
2017-03-02T17:47:19.262247: step 11555, loss 0.217166, acc 0.890625
2017-03-02T17:47:19.338896: step 11556, loss 0.114747, acc 0.953125
2017-03-02T17:47:19.408350: step 11557, loss 0.231556, acc 0.890625
2017-03-02T17:47:19.493746: step 11558, loss 0.168962, acc 0.921875
2017-03-02T17:47:19.567323: step 11559, loss 0.254798, acc 0.875
2017-03-02T17:47:19.653014: step 11560, loss 0.248434, acc 0.890625
2017-03-02T17:47:19.714909: step 11561, loss 0.164831, acc 0.90625
2017-03-02T17:47:19.781413: step 11562, loss 0.178404, acc 0.90625
2017-03-02T17:47:19.853317: step 11563, loss 0.158732, acc 0.96875
2017-03-02T17:47:19.920459: step 11564, loss 0.0657861, acc 1
2017-03-02T17:47:19.988291: step 11565, loss 0.116754, acc 0.96875
2017-03-02T17:47:20.056623: step 11566, loss 0.184565, acc 0.90625
2017-03-02T17:47:20.132735: step 11567, loss 0.104522, acc 0.96875
2017-03-02T17:47:20.204854: step 11568, loss 0.156535, acc 0.9375
2017-03-02T17:47:20.275147: step 11569, loss 0.124713, acc 0.9375
2017-03-02T17:47:20.347130: step 11570, loss 0.132072, acc 0.9375
2017-03-02T17:47:20.421131: step 11571, loss 0.125497, acc 0.9375
2017-03-02T17:47:20.487449: step 11572, loss 0.195324, acc 0.875
2017-03-02T17:47:20.564575: step 11573, loss 0.116952, acc 0.953125
2017-03-02T17:47:20.647915: step 11574, loss 0.0721843, acc 0.96875
2017-03-02T17:47:20.722960: step 11575, loss 0.0431324, acc 1
2017-03-02T17:47:20.793419: step 11576, loss 0.169451, acc 0.9375
2017-03-02T17:47:20.871281: step 11577, loss 0.149135, acc 0.953125
2017-03-02T17:47:20.948892: step 11578, loss 0.174116, acc 0.9375
2017-03-02T17:47:21.018192: step 11579, loss 0.0809711, acc 0.953125
2017-03-02T17:47:21.105156: step 11580, loss 0.107943, acc 0.984375
2017-03-02T17:47:21.174361: step 11581, loss 0.117273, acc 0.9375
2017-03-02T17:47:21.253646: step 11582, loss 0.0737205, acc 1
2017-03-02T17:47:21.328738: step 11583, loss 0.095187, acc 0.9375
2017-03-02T17:47:21.408285: step 11584, loss 0.126981, acc 0.9375
2017-03-02T17:47:21.474328: step 11585, loss 0.06473, acc 0.953125
2017-03-02T17:47:21.545931: step 11586, loss 0.112293, acc 0.96875
2017-03-02T17:47:21.623171: step 11587, loss 0.0672718, acc 0.96875
2017-03-02T17:47:21.698485: step 11588, loss 0.0600875, acc 0.96875
2017-03-02T17:47:21.773551: step 11589, loss 0.145807, acc 0.90625
2017-03-02T17:47:21.843649: step 11590, loss 0.190919, acc 0.9375
2017-03-02T17:47:21.909844: step 11591, loss 0.0918892, acc 0.953125
2017-03-02T17:47:21.981891: step 11592, loss 0.126841, acc 0.953125
2017-03-02T17:47:22.048778: step 11593, loss 0.150855, acc 0.9375
2017-03-02T17:47:22.119568: step 11594, loss 0.222452, acc 0.90625
2017-03-02T17:47:22.194137: step 11595, loss 0.124142, acc 0.9375
2017-03-02T17:47:22.266720: step 11596, loss 0.116265, acc 0.9375
2017-03-02T17:47:22.339781: step 11597, loss 0.207975, acc 0.9375
2017-03-02T17:47:22.413727: step 11598, loss 0.296123, acc 0.90625
2017-03-02T17:47:22.495804: step 11599, loss 0.283341, acc 0.90625
2017-03-02T17:47:22.572473: step 11600, loss 0.159113, acc 0.9375

Evaluation:
2017-03-02T17:47:22.607005: step 11600, loss 1.59378, acc 0.668349

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11600

2017-03-02T17:47:23.099591: step 11601, loss 0.186984, acc 0.921875
2017-03-02T17:47:23.181041: step 11602, loss 0.161908, acc 0.953125
2017-03-02T17:47:23.259221: step 11603, loss 0.214611, acc 0.921875
2017-03-02T17:47:23.351784: step 11604, loss 0.150942, acc 0.9375
2017-03-02T17:47:23.424108: step 11605, loss 0.0772429, acc 0.96875
2017-03-02T17:47:23.490174: step 11606, loss 0.184449, acc 0.921875
2017-03-02T17:47:23.553856: step 11607, loss 0.110551, acc 0.96875
2017-03-02T17:47:23.621682: step 11608, loss 0.202842, acc 0.921875
2017-03-02T17:47:23.694394: step 11609, loss 0.167125, acc 0.921875
2017-03-02T17:47:23.771434: step 11610, loss 0.0543113, acc 1
2017-03-02T17:47:23.845681: step 11611, loss 0.132463, acc 0.96875
2017-03-02T17:47:23.911759: step 11612, loss 0.124204, acc 0.9375
2017-03-02T17:47:23.990837: step 11613, loss 0.153657, acc 0.90625
2017-03-02T17:47:24.063819: step 11614, loss 0.164914, acc 0.921875
2017-03-02T17:47:24.139358: step 11615, loss 0.192377, acc 0.921875
2017-03-02T17:47:24.209454: step 11616, loss 0.164762, acc 0.9375
2017-03-02T17:47:24.288450: step 11617, loss 0.229841, acc 0.921875
2017-03-02T17:47:24.362731: step 11618, loss 0.0994783, acc 0.921875
2017-03-02T17:47:24.447105: step 11619, loss 0.323838, acc 0.90625
2017-03-02T17:47:24.535065: step 11620, loss 0.0971351, acc 0.96875
2017-03-02T17:47:24.601857: step 11621, loss 0.149156, acc 0.96875
2017-03-02T17:47:24.676274: step 11622, loss 0.168465, acc 0.921875
2017-03-02T17:47:24.754041: step 11623, loss 0.157877, acc 0.953125
2017-03-02T17:47:24.822230: step 11624, loss 0.137838, acc 0.921875
2017-03-02T17:47:24.894778: step 11625, loss 0.272735, acc 0.859375
2017-03-02T17:47:24.979701: step 11626, loss 0.338613, acc 0.875
2017-03-02T17:47:25.064449: step 11627, loss 0.123586, acc 0.953125
2017-03-02T17:47:25.136175: step 11628, loss 0.290374, acc 0.890625
2017-03-02T17:47:25.208644: step 11629, loss 0.163911, acc 0.921875
2017-03-02T17:47:25.289410: step 11630, loss 0.126667, acc 0.9375
2017-03-02T17:47:25.361427: step 11631, loss 0.215743, acc 0.921875
2017-03-02T17:47:25.432649: step 11632, loss 0.202421, acc 0.90625
2017-03-02T17:47:25.505711: step 11633, loss 0.191306, acc 0.953125
2017-03-02T17:47:25.579330: step 11634, loss 0.252229, acc 0.9375
2017-03-02T17:47:25.651279: step 11635, loss 0.159795, acc 0.953125
2017-03-02T17:47:25.723593: step 11636, loss 0.128601, acc 0.953125
2017-03-02T17:47:25.798523: step 11637, loss 0.0978604, acc 0.953125
2017-03-02T17:47:25.867728: step 11638, loss 0.130479, acc 0.953125
2017-03-02T17:47:25.939810: step 11639, loss 0.196938, acc 0.9375
2017-03-02T17:47:26.010824: step 11640, loss 0.0527803, acc 0.984375
2017-03-02T17:47:26.077464: step 11641, loss 0.226232, acc 0.890625
2017-03-02T17:47:26.142549: step 11642, loss 0.150823, acc 0.921875
2017-03-02T17:47:26.216675: step 11643, loss 0.168355, acc 0.921875
2017-03-02T17:47:26.283796: step 11644, loss 0.107482, acc 0.953125
2017-03-02T17:47:26.355902: step 11645, loss 0.101751, acc 0.953125
2017-03-02T17:47:26.432253: step 11646, loss 0.183425, acc 0.921875
2017-03-02T17:47:26.505528: step 11647, loss 0.191565, acc 0.90625
2017-03-02T17:47:26.577479: step 11648, loss 0.188481, acc 0.953125
2017-03-02T17:47:26.658948: step 11649, loss 0.0528121, acc 0.984375
2017-03-02T17:47:26.742217: step 11650, loss 0.141154, acc 0.9375
2017-03-02T17:47:26.815166: step 11651, loss 0.075115, acc 0.953125
2017-03-02T17:47:26.879886: step 11652, loss 0.106449, acc 0.953125
2017-03-02T17:47:26.948338: step 11653, loss 0.125592, acc 0.953125
2017-03-02T17:47:27.022270: step 11654, loss 0.216904, acc 0.9375
2017-03-02T17:47:27.101700: step 11655, loss 0.194746, acc 0.921875
2017-03-02T17:47:27.187567: step 11656, loss 0.348996, acc 0.890625
2017-03-02T17:47:27.266613: step 11657, loss 0.342181, acc 0.90625
2017-03-02T17:47:27.337642: step 11658, loss 0.204759, acc 0.890625
2017-03-02T17:47:27.405629: step 11659, loss 0.254445, acc 0.890625
2017-03-02T17:47:27.491759: step 11660, loss 0.219983, acc 0.890625
2017-03-02T17:47:27.573120: step 11661, loss 0.186306, acc 0.921875
2017-03-02T17:47:27.652346: step 11662, loss 0.125667, acc 0.953125
2017-03-02T17:47:27.734136: step 11663, loss 0.240083, acc 0.90625
2017-03-02T17:47:27.805582: step 11664, loss 0.13814, acc 0.9375
2017-03-02T17:47:27.879097: step 11665, loss 0.155417, acc 0.9375
2017-03-02T17:47:27.960449: step 11666, loss 0.148383, acc 0.921875
2017-03-02T17:47:28.031232: step 11667, loss 0.226569, acc 0.90625
2017-03-02T17:47:28.098099: step 11668, loss 0.170296, acc 0.953125
2017-03-02T17:47:28.176231: step 11669, loss 0.180807, acc 0.90625
2017-03-02T17:47:28.245226: step 11670, loss 0.130848, acc 0.921875
2017-03-02T17:47:28.317557: step 11671, loss 0.138218, acc 0.9375
2017-03-02T17:47:28.388453: step 11672, loss 0.116521, acc 0.921875
2017-03-02T17:47:28.460946: step 11673, loss 0.163635, acc 0.9375
2017-03-02T17:47:28.536465: step 11674, loss 0.205563, acc 0.90625
2017-03-02T17:47:28.612836: step 11675, loss 0.0991478, acc 0.984375
2017-03-02T17:47:28.681494: step 11676, loss 0.0935875, acc 0.96875
2017-03-02T17:47:28.754834: step 11677, loss 0.140989, acc 0.921875
2017-03-02T17:47:28.823730: step 11678, loss 0.138072, acc 0.9375
2017-03-02T17:47:28.906089: step 11679, loss 0.117958, acc 0.9375
2017-03-02T17:47:28.976070: step 11680, loss 0.207998, acc 0.921875
2017-03-02T17:47:29.053738: step 11681, loss 0.0967078, acc 0.96875
2017-03-02T17:47:29.127788: step 11682, loss 0.354042, acc 0.859375
2017-03-02T17:47:29.210330: step 11683, loss 0.169431, acc 0.921875
2017-03-02T17:47:29.294338: step 11684, loss 0.125023, acc 0.953125
2017-03-02T17:47:29.374015: step 11685, loss 0.150751, acc 0.953125
2017-03-02T17:47:29.444404: step 11686, loss 0.168741, acc 0.9375
2017-03-02T17:47:29.516589: step 11687, loss 0.116224, acc 0.96875
2017-03-02T17:47:29.582234: step 11688, loss 0.131599, acc 0.9375
2017-03-02T17:47:29.647460: step 11689, loss 0.158335, acc 0.921875
2017-03-02T17:47:29.716551: step 11690, loss 0.189011, acc 0.9375
2017-03-02T17:47:29.794843: step 11691, loss 0.151178, acc 0.9375
2017-03-02T17:47:29.884250: step 11692, loss 0.312382, acc 0.890625
2017-03-02T17:47:29.956912: step 11693, loss 0.109487, acc 0.953125
2017-03-02T17:47:30.036728: step 11694, loss 0.163138, acc 0.9375
2017-03-02T17:47:30.105862: step 11695, loss 0.155114, acc 0.90625
2017-03-02T17:47:30.185734: step 11696, loss 0.150585, acc 0.953125
2017-03-02T17:47:30.252875: step 11697, loss 0.0744078, acc 0.96875
2017-03-02T17:47:30.326110: step 11698, loss 0.0773398, acc 0.96875
2017-03-02T17:47:30.399322: step 11699, loss 0.0782031, acc 0.984375
2017-03-02T17:47:30.474195: step 11700, loss 0.160054, acc 0.9375

Evaluation:
2017-03-02T17:47:30.513709: step 11700, loss 1.55606, acc 0.659697

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11700

2017-03-02T17:47:30.976019: step 11701, loss 0.140862, acc 0.921875
2017-03-02T17:47:31.047252: step 11702, loss 0.162542, acc 0.90625
2017-03-02T17:47:31.120865: step 11703, loss 0.151233, acc 0.921875
2017-03-02T17:47:31.193175: step 11704, loss 0.170375, acc 0.921875
2017-03-02T17:47:31.271149: step 11705, loss 0.14903, acc 0.921875
2017-03-02T17:47:31.339715: step 11706, loss 0.328103, acc 0.84375
2017-03-02T17:47:31.413720: step 11707, loss 0.189045, acc 0.953125
2017-03-02T17:47:31.485885: step 11708, loss 0.194178, acc 0.921875
2017-03-02T17:47:31.557484: step 11709, loss 0.101638, acc 0.96875
2017-03-02T17:47:31.627592: step 11710, loss 0.211189, acc 0.90625
2017-03-02T17:47:31.693882: step 11711, loss 0.183235, acc 0.921875
2017-03-02T17:47:31.767521: step 11712, loss 0.198853, acc 0.890625
2017-03-02T17:47:31.847199: step 11713, loss 0.328664, acc 0.859375
2017-03-02T17:47:31.923165: step 11714, loss 0.146644, acc 0.9375
2017-03-02T17:47:32.001150: step 11715, loss 0.167489, acc 0.9375
2017-03-02T17:47:32.080247: step 11716, loss 0.108933, acc 0.953125
2017-03-02T17:47:32.156875: step 11717, loss 0.0237818, acc 1
2017-03-02T17:47:32.233930: step 11718, loss 0.378632, acc 0.859375
2017-03-02T17:47:32.309614: step 11719, loss 0.261808, acc 0.90625
2017-03-02T17:47:32.379903: step 11720, loss 0.12985, acc 0.9375
2017-03-02T17:47:32.479286: step 11721, loss 0.160747, acc 0.953125
2017-03-02T17:47:32.550162: step 11722, loss 0.26205, acc 0.875
2017-03-02T17:47:32.623935: step 11723, loss 0.312357, acc 0.90625
2017-03-02T17:47:32.702650: step 11724, loss 0.0975119, acc 0.953125
2017-03-02T17:47:32.779808: step 11725, loss 0.218794, acc 0.9375
2017-03-02T17:47:32.851786: step 11726, loss 0.198322, acc 0.921875
2017-03-02T17:47:32.924298: step 11727, loss 0.202904, acc 0.90625
2017-03-02T17:47:32.991168: step 11728, loss 0.232622, acc 0.921875
2017-03-02T17:47:33.060064: step 11729, loss 0.114057, acc 0.96875
2017-03-02T17:47:33.127991: step 11730, loss 0.284913, acc 0.859375
2017-03-02T17:47:33.204598: step 11731, loss 0.129036, acc 0.9375
2017-03-02T17:47:33.279457: step 11732, loss 0.209688, acc 0.921875
2017-03-02T17:47:33.357969: step 11733, loss 0.178731, acc 0.921875
2017-03-02T17:47:33.432491: step 11734, loss 0.196306, acc 0.90625
2017-03-02T17:47:33.505403: step 11735, loss 0.235931, acc 0.921875
2017-03-02T17:47:33.571413: step 11736, loss 0.152712, acc 0.90625
2017-03-02T17:47:33.657708: step 11737, loss 0.11903, acc 0.953125
2017-03-02T17:47:33.732727: step 11738, loss 0.0962338, acc 0.96875
2017-03-02T17:47:33.802963: step 11739, loss 0.217473, acc 0.921875
2017-03-02T17:47:33.884935: step 11740, loss 0.194766, acc 0.921875
2017-03-02T17:47:33.955977: step 11741, loss 0.112461, acc 0.9375
2017-03-02T17:47:34.026929: step 11742, loss 0.299097, acc 0.84375
2017-03-02T17:47:34.108884: step 11743, loss 0.17118, acc 0.9375
2017-03-02T17:47:34.183653: step 11744, loss 0.0797096, acc 0.984375
2017-03-02T17:47:34.254817: step 11745, loss 0.106184, acc 0.953125
2017-03-02T17:47:34.330168: step 11746, loss 0.0984045, acc 0.953125
2017-03-02T17:47:34.403541: step 11747, loss 0.241348, acc 0.890625
2017-03-02T17:47:34.475264: step 11748, loss 0.245232, acc 0.84375
2017-03-02T17:47:34.553201: step 11749, loss 0.0780671, acc 0.96875
2017-03-02T17:47:34.627444: step 11750, loss 0.387676, acc 0.859375
2017-03-02T17:47:34.700804: step 11751, loss 0.217668, acc 0.90625
2017-03-02T17:47:34.779509: step 11752, loss 0.317968, acc 0.890625
2017-03-02T17:47:34.854779: step 11753, loss 0.290673, acc 0.90625
2017-03-02T17:47:34.926098: step 11754, loss 0.335077, acc 0.84375
2017-03-02T17:47:34.999174: step 11755, loss 0.216902, acc 0.921875
2017-03-02T17:47:35.070387: step 11756, loss 0.261243, acc 0.890625
2017-03-02T17:47:35.141034: step 11757, loss 0.278542, acc 0.84375
2017-03-02T17:47:35.220672: step 11758, loss 0.237827, acc 0.90625
2017-03-02T17:47:35.295016: step 11759, loss 0.051036, acc 0.984375
2017-03-02T17:47:35.364539: step 11760, loss 0.491482, acc 0.75
2017-03-02T17:47:35.441539: step 11761, loss 0.21463, acc 0.921875
2017-03-02T17:47:35.517256: step 11762, loss 0.201817, acc 0.875
2017-03-02T17:47:35.590285: step 11763, loss 0.13389, acc 0.96875
2017-03-02T17:47:35.670722: step 11764, loss 0.450545, acc 0.78125
2017-03-02T17:47:35.742404: step 11765, loss 0.195833, acc 0.875
2017-03-02T17:47:35.817214: step 11766, loss 0.14037, acc 0.9375
2017-03-02T17:47:35.886064: step 11767, loss 0.171258, acc 0.9375
2017-03-02T17:47:35.959358: step 11768, loss 0.191138, acc 0.921875
2017-03-02T17:47:36.030150: step 11769, loss 0.174399, acc 0.90625
2017-03-02T17:47:36.101505: step 11770, loss 0.170105, acc 0.9375
2017-03-02T17:47:36.173488: step 11771, loss 0.180075, acc 0.9375
2017-03-02T17:47:36.244523: step 11772, loss 0.215505, acc 0.859375
2017-03-02T17:47:36.312223: step 11773, loss 0.174823, acc 0.953125
2017-03-02T17:47:36.381795: step 11774, loss 0.0828274, acc 0.96875
2017-03-02T17:47:36.456725: step 11775, loss 0.234893, acc 0.890625
2017-03-02T17:47:36.521898: step 11776, loss 0.157267, acc 0.9375
2017-03-02T17:47:36.593844: step 11777, loss 0.143419, acc 0.96875
2017-03-02T17:47:36.670500: step 11778, loss 0.154535, acc 0.921875
2017-03-02T17:47:36.749365: step 11779, loss 0.128837, acc 0.953125
2017-03-02T17:47:36.822386: step 11780, loss 0.146914, acc 0.953125
2017-03-02T17:47:36.896984: step 11781, loss 0.14483, acc 0.9375
2017-03-02T17:47:36.973080: step 11782, loss 0.132249, acc 0.9375
2017-03-02T17:47:37.049081: step 11783, loss 0.0980853, acc 0.984375
2017-03-02T17:47:37.127104: step 11784, loss 0.0515241, acc 0.96875
2017-03-02T17:47:37.202938: step 11785, loss 0.145242, acc 0.953125
2017-03-02T17:47:37.267124: step 11786, loss 0.224315, acc 0.921875
2017-03-02T17:47:37.341875: step 11787, loss 0.229953, acc 0.90625
2017-03-02T17:47:37.411143: step 11788, loss 0.182772, acc 0.9375
2017-03-02T17:47:37.485763: step 11789, loss 0.0439674, acc 1
2017-03-02T17:47:37.555611: step 11790, loss 0.181079, acc 0.921875
2017-03-02T17:47:37.627500: step 11791, loss 0.142268, acc 0.90625
2017-03-02T17:47:37.700213: step 11792, loss 0.154623, acc 0.9375
2017-03-02T17:47:37.778058: step 11793, loss 0.142143, acc 0.953125
2017-03-02T17:47:37.855666: step 11794, loss 0.236444, acc 0.90625
2017-03-02T17:47:37.923889: step 11795, loss 0.0945872, acc 0.96875
2017-03-02T17:47:37.996975: step 11796, loss 0.18528, acc 0.9375
2017-03-02T17:47:38.069837: step 11797, loss 0.229522, acc 0.875
2017-03-02T17:47:38.147455: step 11798, loss 0.224948, acc 0.921875
2017-03-02T17:47:38.219412: step 11799, loss 0.0742308, acc 0.96875
2017-03-02T17:47:38.297923: step 11800, loss 0.245118, acc 0.921875

Evaluation:
2017-03-02T17:47:38.334984: step 11800, loss 1.59354, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11800

2017-03-02T17:47:38.790788: step 11801, loss 0.0878127, acc 0.953125
2017-03-02T17:47:38.866582: step 11802, loss 0.210078, acc 0.921875
2017-03-02T17:47:38.940836: step 11803, loss 0.209845, acc 0.9375
2017-03-02T17:47:39.023402: step 11804, loss 0.269922, acc 0.890625
2017-03-02T17:47:39.129247: step 11805, loss 0.143804, acc 0.9375
2017-03-02T17:47:39.200704: step 11806, loss 0.185006, acc 0.921875
2017-03-02T17:47:39.268319: step 11807, loss 0.146074, acc 0.953125
2017-03-02T17:47:39.364682: step 11808, loss 0.103307, acc 0.953125
2017-03-02T17:47:39.435622: step 11809, loss 0.201692, acc 0.90625
2017-03-02T17:47:39.504446: step 11810, loss 0.238137, acc 0.859375
2017-03-02T17:47:39.587904: step 11811, loss 0.183843, acc 0.890625
2017-03-02T17:47:39.661094: step 11812, loss 0.0856438, acc 0.96875
2017-03-02T17:47:39.728921: step 11813, loss 0.183187, acc 0.921875
2017-03-02T17:47:39.793883: step 11814, loss 0.226038, acc 0.890625
2017-03-02T17:47:39.867130: step 11815, loss 0.349895, acc 0.90625
2017-03-02T17:47:39.934792: step 11816, loss 0.216096, acc 0.90625
2017-03-02T17:47:40.002020: step 11817, loss 0.20718, acc 0.90625
2017-03-02T17:47:40.092175: step 11818, loss 0.201636, acc 0.890625
2017-03-02T17:47:40.161632: step 11819, loss 0.0925208, acc 0.953125
2017-03-02T17:47:40.233585: step 11820, loss 0.147615, acc 0.9375
2017-03-02T17:47:40.318373: step 11821, loss 0.114293, acc 0.9375
2017-03-02T17:47:40.391381: step 11822, loss 0.267987, acc 0.84375
2017-03-02T17:47:40.458992: step 11823, loss 0.11769, acc 0.96875
2017-03-02T17:47:40.532663: step 11824, loss 0.106198, acc 0.96875
2017-03-02T17:47:40.591806: step 11825, loss 0.221146, acc 0.953125
2017-03-02T17:47:40.659674: step 11826, loss 0.177312, acc 0.9375
2017-03-02T17:47:40.730848: step 11827, loss 0.17835, acc 0.890625
2017-03-02T17:47:40.804769: step 11828, loss 0.100047, acc 0.953125
2017-03-02T17:47:40.876652: step 11829, loss 0.0955054, acc 0.96875
2017-03-02T17:47:40.947587: step 11830, loss 0.143994, acc 0.9375
2017-03-02T17:47:41.037243: step 11831, loss 0.110851, acc 0.953125
2017-03-02T17:47:41.113946: step 11832, loss 0.184406, acc 0.921875
2017-03-02T17:47:41.186614: step 11833, loss 0.136389, acc 0.9375
2017-03-02T17:47:41.263266: step 11834, loss 0.168669, acc 0.921875
2017-03-02T17:47:41.330280: step 11835, loss 0.14349, acc 0.9375
2017-03-02T17:47:41.397725: step 11836, loss 0.233616, acc 0.890625
2017-03-02T17:47:41.477176: step 11837, loss 0.107867, acc 0.9375
2017-03-02T17:47:41.555784: step 11838, loss 0.168291, acc 0.9375
2017-03-02T17:47:41.628840: step 11839, loss 0.356159, acc 0.96875
2017-03-02T17:47:41.706129: step 11840, loss 0.13532, acc 0.9375
2017-03-02T17:47:41.779020: step 11841, loss 0.181519, acc 0.921875
2017-03-02T17:47:41.848356: step 11842, loss 0.13108, acc 0.953125
2017-03-02T17:47:41.927481: step 11843, loss 0.196789, acc 0.921875
2017-03-02T17:47:42.010660: step 11844, loss 0.305575, acc 0.921875
2017-03-02T17:47:42.078764: step 11845, loss 0.157443, acc 0.9375
2017-03-02T17:47:42.148619: step 11846, loss 0.14354, acc 0.90625
2017-03-02T17:47:42.239633: step 11847, loss 0.305122, acc 0.890625
2017-03-02T17:47:42.324388: step 11848, loss 0.189175, acc 0.890625
2017-03-02T17:47:42.398846: step 11849, loss 0.264531, acc 0.859375
2017-03-02T17:47:42.478778: step 11850, loss 0.115619, acc 0.9375
2017-03-02T17:47:42.553162: step 11851, loss 0.126586, acc 0.953125
2017-03-02T17:47:42.619408: step 11852, loss 0.175307, acc 0.921875
2017-03-02T17:47:42.685003: step 11853, loss 0.209899, acc 0.9375
2017-03-02T17:47:42.753481: step 11854, loss 0.160166, acc 0.9375
2017-03-02T17:47:42.821266: step 11855, loss 0.13571, acc 0.9375
2017-03-02T17:47:42.892072: step 11856, loss 0.13346, acc 0.9375
2017-03-02T17:47:42.957475: step 11857, loss 0.147103, acc 0.953125
2017-03-02T17:47:43.034478: step 11858, loss 0.195065, acc 0.890625
2017-03-02T17:47:43.111703: step 11859, loss 0.173809, acc 0.953125
2017-03-02T17:47:43.183443: step 11860, loss 0.106886, acc 0.953125
2017-03-02T17:47:43.263557: step 11861, loss 0.072835, acc 0.953125
2017-03-02T17:47:43.349653: step 11862, loss 0.180998, acc 0.921875
2017-03-02T17:47:43.422326: step 11863, loss 0.144174, acc 0.921875
2017-03-02T17:47:43.491320: step 11864, loss 0.0813526, acc 0.96875
2017-03-02T17:47:43.561924: step 11865, loss 0.297956, acc 0.90625
2017-03-02T17:47:43.636172: step 11866, loss 0.219145, acc 0.9375
2017-03-02T17:47:43.717457: step 11867, loss 0.193136, acc 0.953125
2017-03-02T17:47:43.793843: step 11868, loss 0.236443, acc 0.90625
2017-03-02T17:47:43.868419: step 11869, loss 0.0991988, acc 0.96875
2017-03-02T17:47:43.946969: step 11870, loss 0.139038, acc 0.953125
2017-03-02T17:47:44.017010: step 11871, loss 0.148192, acc 0.9375
2017-03-02T17:47:44.086700: step 11872, loss 0.20425, acc 0.890625
2017-03-02T17:47:44.161662: step 11873, loss 0.190392, acc 0.90625
2017-03-02T17:47:44.243154: step 11874, loss 0.203415, acc 0.9375
2017-03-02T17:47:44.348842: step 11875, loss 0.146983, acc 0.9375
2017-03-02T17:47:44.420543: step 11876, loss 0.134958, acc 0.9375
2017-03-02T17:47:44.495948: step 11877, loss 0.104477, acc 0.9375
2017-03-02T17:47:44.583297: step 11878, loss 0.236069, acc 0.921875
2017-03-02T17:47:44.657509: step 11879, loss 0.31022, acc 0.90625
2017-03-02T17:47:44.729907: step 11880, loss 0.149527, acc 0.953125
2017-03-02T17:47:44.800369: step 11881, loss 0.117098, acc 0.96875
2017-03-02T17:47:44.871482: step 11882, loss 0.145065, acc 0.921875
2017-03-02T17:47:44.944548: step 11883, loss 0.101879, acc 0.953125
2017-03-02T17:47:45.016531: step 11884, loss 0.133032, acc 0.9375
2017-03-02T17:47:45.090332: step 11885, loss 0.129967, acc 0.953125
2017-03-02T17:47:45.160558: step 11886, loss 0.133305, acc 0.96875
2017-03-02T17:47:45.234834: step 11887, loss 0.329685, acc 0.890625
2017-03-02T17:47:45.307086: step 11888, loss 0.247505, acc 0.90625
2017-03-02T17:47:45.383848: step 11889, loss 0.141758, acc 0.921875
2017-03-02T17:47:45.449920: step 11890, loss 0.135051, acc 0.9375
2017-03-02T17:47:45.505478: step 11891, loss 0.134319, acc 0.9375
2017-03-02T17:47:45.584487: step 11892, loss 0.0753906, acc 0.96875
2017-03-02T17:47:45.661913: step 11893, loss 0.126502, acc 0.9375
2017-03-02T17:47:45.737524: step 11894, loss 0.125494, acc 0.921875
2017-03-02T17:47:45.810140: step 11895, loss 0.143708, acc 0.90625
2017-03-02T17:47:45.882385: step 11896, loss 0.192242, acc 0.9375
2017-03-02T17:47:45.958686: step 11897, loss 0.120225, acc 0.921875
2017-03-02T17:47:46.035243: step 11898, loss 0.147572, acc 0.953125
2017-03-02T17:47:46.109569: step 11899, loss 0.284067, acc 0.9375
2017-03-02T17:47:46.185803: step 11900, loss 0.229661, acc 0.921875

Evaluation:
2017-03-02T17:47:46.222667: step 11900, loss 1.58791, acc 0.662581

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-11900

2017-03-02T17:47:46.713102: step 11901, loss 0.199611, acc 0.921875
2017-03-02T17:47:46.783756: step 11902, loss 0.0812514, acc 0.96875
2017-03-02T17:47:46.854157: step 11903, loss 0.0840427, acc 0.96875
2017-03-02T17:47:46.925349: step 11904, loss 0.174134, acc 0.9375
2017-03-02T17:47:46.998505: step 11905, loss 0.0680488, acc 0.96875
2017-03-02T17:47:47.069689: step 11906, loss 0.218583, acc 0.90625
2017-03-02T17:47:47.149633: step 11907, loss 0.128476, acc 0.921875
2017-03-02T17:47:47.226321: step 11908, loss 0.169921, acc 0.921875
2017-03-02T17:47:47.306869: step 11909, loss 0.279345, acc 0.890625
2017-03-02T17:47:47.386468: step 11910, loss 0.20705, acc 0.921875
2017-03-02T17:47:47.458132: step 11911, loss 0.118753, acc 0.953125
2017-03-02T17:47:47.524816: step 11912, loss 0.330588, acc 0.875
2017-03-02T17:47:47.597013: step 11913, loss 0.191604, acc 0.890625
2017-03-02T17:47:47.675244: step 11914, loss 0.148486, acc 0.921875
2017-03-02T17:47:47.746498: step 11915, loss 0.0876037, acc 0.984375
2017-03-02T17:47:47.808182: step 11916, loss 0.133723, acc 0.953125
2017-03-02T17:47:47.887708: step 11917, loss 0.213283, acc 0.921875
2017-03-02T17:47:47.958312: step 11918, loss 0.122136, acc 0.953125
2017-03-02T17:47:48.031423: step 11919, loss 0.290595, acc 0.890625
2017-03-02T17:47:48.111442: step 11920, loss 0.238475, acc 0.859375
2017-03-02T17:47:48.183017: step 11921, loss 0.122072, acc 0.9375
2017-03-02T17:47:48.251468: step 11922, loss 0.213146, acc 0.875
2017-03-02T17:47:48.344424: step 11923, loss 0.175475, acc 0.90625
2017-03-02T17:47:48.421689: step 11924, loss 0.066073, acc 0.96875
2017-03-02T17:47:48.495518: step 11925, loss 0.105172, acc 0.9375
2017-03-02T17:47:48.567685: step 11926, loss 0.131637, acc 0.9375
2017-03-02T17:47:48.652312: step 11927, loss 0.177255, acc 0.9375
2017-03-02T17:47:48.723838: step 11928, loss 0.156336, acc 0.9375
2017-03-02T17:47:48.802829: step 11929, loss 0.111694, acc 0.9375
2017-03-02T17:47:48.876505: step 11930, loss 0.212622, acc 0.9375
2017-03-02T17:47:48.954607: step 11931, loss 0.140648, acc 0.953125
2017-03-02T17:47:49.029229: step 11932, loss 0.0940901, acc 0.953125
2017-03-02T17:47:49.102698: step 11933, loss 0.129056, acc 0.96875
2017-03-02T17:47:49.179838: step 11934, loss 0.293537, acc 0.90625
2017-03-02T17:47:49.270414: step 11935, loss 0.235015, acc 0.890625
2017-03-02T17:47:49.378789: step 11936, loss 0.279365, acc 0.84375
2017-03-02T17:47:49.460872: step 11937, loss 0.212466, acc 0.90625
2017-03-02T17:47:49.534541: step 11938, loss 0.0969388, acc 0.9375
2017-03-02T17:47:49.601416: step 11939, loss 0.207799, acc 0.890625
2017-03-02T17:47:49.669746: step 11940, loss 0.196928, acc 0.953125
2017-03-02T17:47:49.744817: step 11941, loss 0.28317, acc 0.890625
2017-03-02T17:47:49.815466: step 11942, loss 0.164336, acc 0.921875
2017-03-02T17:47:49.879003: step 11943, loss 0.211846, acc 0.921875
2017-03-02T17:47:49.957198: step 11944, loss 0.131746, acc 0.9375
2017-03-02T17:47:50.034166: step 11945, loss 0.159689, acc 0.9375
2017-03-02T17:47:50.106896: step 11946, loss 0.259633, acc 0.90625
2017-03-02T17:47:50.179608: step 11947, loss 0.28952, acc 0.90625
2017-03-02T17:47:50.252238: step 11948, loss 0.141955, acc 0.9375
2017-03-02T17:47:50.321590: step 11949, loss 0.0901117, acc 0.96875
2017-03-02T17:47:50.398345: step 11950, loss 0.22416, acc 0.875
2017-03-02T17:47:50.473670: step 11951, loss 0.296979, acc 0.890625
2017-03-02T17:47:50.555695: step 11952, loss 0.172813, acc 0.9375
2017-03-02T17:47:50.627875: step 11953, loss 0.17876, acc 0.921875
2017-03-02T17:47:50.694737: step 11954, loss 0.168572, acc 0.953125
2017-03-02T17:47:50.754914: step 11955, loss 0.144355, acc 0.953125
2017-03-02T17:47:50.824512: step 11956, loss 0.334098, acc 0.75
2017-03-02T17:47:50.906130: step 11957, loss 0.113375, acc 0.953125
2017-03-02T17:47:50.973992: step 11958, loss 0.230994, acc 0.859375
2017-03-02T17:47:51.050355: step 11959, loss 0.102773, acc 0.953125
2017-03-02T17:47:51.131468: step 11960, loss 0.401022, acc 0.859375
2017-03-02T17:47:51.207258: step 11961, loss 0.0771067, acc 0.984375
2017-03-02T17:47:51.279962: step 11962, loss 0.224426, acc 0.921875
2017-03-02T17:47:51.345585: step 11963, loss 0.163375, acc 0.90625
2017-03-02T17:47:51.419103: step 11964, loss 0.257179, acc 0.90625
2017-03-02T17:47:51.494216: step 11965, loss 0.191686, acc 0.890625
2017-03-02T17:47:51.567554: step 11966, loss 0.225451, acc 0.90625
2017-03-02T17:47:51.638617: step 11967, loss 0.194324, acc 0.90625
2017-03-02T17:47:51.707225: step 11968, loss 0.184185, acc 0.921875
2017-03-02T17:47:51.779520: step 11969, loss 0.19649, acc 0.921875
2017-03-02T17:47:51.850494: step 11970, loss 0.209132, acc 0.90625
2017-03-02T17:47:51.921424: step 11971, loss 0.19142, acc 0.921875
2017-03-02T17:47:51.992977: step 11972, loss 0.187628, acc 0.90625
2017-03-02T17:47:52.071636: step 11973, loss 0.117863, acc 0.953125
2017-03-02T17:47:52.149372: step 11974, loss 0.0978691, acc 0.96875
2017-03-02T17:47:52.222342: step 11975, loss 0.0810458, acc 0.96875
2017-03-02T17:47:52.297210: step 11976, loss 0.164852, acc 0.9375
2017-03-02T17:47:52.373875: step 11977, loss 0.0815956, acc 0.953125
2017-03-02T17:47:52.446800: step 11978, loss 0.13115, acc 0.953125
2017-03-02T17:47:52.518587: step 11979, loss 0.200352, acc 0.921875
2017-03-02T17:47:52.594284: step 11980, loss 0.216217, acc 0.890625
2017-03-02T17:47:52.673043: step 11981, loss 0.116222, acc 0.921875
2017-03-02T17:47:52.744878: step 11982, loss 0.100218, acc 0.96875
2017-03-02T17:47:52.817271: step 11983, loss 0.139179, acc 0.9375
2017-03-02T17:47:52.896449: step 11984, loss 0.194356, acc 0.890625
2017-03-02T17:47:52.974773: step 11985, loss 0.216411, acc 0.921875
2017-03-02T17:47:53.044582: step 11986, loss 0.100836, acc 0.953125
2017-03-02T17:47:53.113235: step 11987, loss 0.112474, acc 0.9375
2017-03-02T17:47:53.186419: step 11988, loss 0.0880451, acc 0.96875
2017-03-02T17:47:53.252775: step 11989, loss 0.096615, acc 0.953125
2017-03-02T17:47:53.319032: step 11990, loss 0.220234, acc 0.890625
2017-03-02T17:47:53.396185: step 11991, loss 0.208339, acc 0.90625
2017-03-02T17:47:53.468963: step 11992, loss 0.178362, acc 0.90625
2017-03-02T17:47:53.532860: step 11993, loss 0.177035, acc 0.921875
2017-03-02T17:47:53.619243: step 11994, loss 0.147949, acc 0.9375
2017-03-02T17:47:53.692031: step 11995, loss 0.275379, acc 0.953125
2017-03-02T17:47:53.761218: step 11996, loss 0.095275, acc 0.96875
2017-03-02T17:47:53.829833: step 11997, loss 0.0833511, acc 0.953125
2017-03-02T17:47:53.899003: step 11998, loss 0.24656, acc 0.921875
2017-03-02T17:47:53.970073: step 11999, loss 0.208415, acc 0.90625
2017-03-02T17:47:54.043764: step 12000, loss 0.174912, acc 0.921875

Evaluation:
2017-03-02T17:47:54.076129: step 12000, loss 1.55164, acc 0.662581

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12000

2017-03-02T17:47:54.533603: step 12001, loss 0.141428, acc 0.9375
2017-03-02T17:47:54.606644: step 12002, loss 0.122213, acc 0.953125
2017-03-02T17:47:54.678475: step 12003, loss 0.182059, acc 0.9375
2017-03-02T17:47:54.751157: step 12004, loss 0.188039, acc 0.9375
2017-03-02T17:47:54.823161: step 12005, loss 0.224229, acc 0.921875
2017-03-02T17:47:54.894403: step 12006, loss 0.214228, acc 0.875
2017-03-02T17:47:54.965476: step 12007, loss 0.0349979, acc 1
2017-03-02T17:47:55.042384: step 12008, loss 0.0974086, acc 0.96875
2017-03-02T17:47:55.121218: step 12009, loss 0.1549, acc 0.9375
2017-03-02T17:47:55.192147: step 12010, loss 0.260945, acc 0.890625
2017-03-02T17:47:55.262707: step 12011, loss 0.0587945, acc 1
2017-03-02T17:47:55.334822: step 12012, loss 0.214629, acc 0.9375
2017-03-02T17:47:55.408643: step 12013, loss 0.168501, acc 0.9375
2017-03-02T17:47:55.481589: step 12014, loss 0.248698, acc 0.921875
2017-03-02T17:47:55.558003: step 12015, loss 0.136313, acc 0.921875
2017-03-02T17:47:55.634791: step 12016, loss 0.214613, acc 0.921875
2017-03-02T17:47:55.708569: step 12017, loss 0.0848005, acc 0.96875
2017-03-02T17:47:55.784982: step 12018, loss 0.0812519, acc 0.96875
2017-03-02T17:47:55.850540: step 12019, loss 0.0446616, acc 0.984375
2017-03-02T17:47:55.919008: step 12020, loss 0.212888, acc 0.921875
2017-03-02T17:47:55.987567: step 12021, loss 0.115957, acc 0.96875
2017-03-02T17:47:56.057825: step 12022, loss 0.0980878, acc 0.953125
2017-03-02T17:47:56.129328: step 12023, loss 0.184686, acc 0.90625
2017-03-02T17:47:56.201985: step 12024, loss 0.172363, acc 0.953125
2017-03-02T17:47:56.275169: step 12025, loss 0.290758, acc 0.84375
2017-03-02T17:47:56.348360: step 12026, loss 0.169412, acc 0.921875
2017-03-02T17:47:56.417796: step 12027, loss 0.0785957, acc 0.96875
2017-03-02T17:47:56.488698: step 12028, loss 0.140832, acc 0.96875
2017-03-02T17:47:56.554224: step 12029, loss 0.0406607, acc 0.984375
2017-03-02T17:47:56.619177: step 12030, loss 0.11415, acc 0.953125
2017-03-02T17:47:56.693359: step 12031, loss 0.218842, acc 0.875
2017-03-02T17:47:56.764706: step 12032, loss 0.117708, acc 0.9375
2017-03-02T17:47:56.842139: step 12033, loss 0.162834, acc 0.921875
2017-03-02T17:47:56.911310: step 12034, loss 0.138017, acc 0.953125
2017-03-02T17:47:56.992637: step 12035, loss 0.148159, acc 0.921875
2017-03-02T17:47:57.079828: step 12036, loss 0.148823, acc 0.9375
2017-03-02T17:47:57.159348: step 12037, loss 0.234569, acc 0.90625
2017-03-02T17:47:57.233351: step 12038, loss 0.183762, acc 0.890625
2017-03-02T17:47:57.306390: step 12039, loss 0.10945, acc 0.953125
2017-03-02T17:47:57.378660: step 12040, loss 0.157167, acc 0.90625
2017-03-02T17:47:57.450396: step 12041, loss 0.104338, acc 0.96875
2017-03-02T17:47:57.519824: step 12042, loss 0.211353, acc 0.9375
2017-03-02T17:47:57.591171: step 12043, loss 0.110032, acc 0.953125
2017-03-02T17:47:57.662253: step 12044, loss 0.139461, acc 0.921875
2017-03-02T17:47:57.741662: step 12045, loss 0.256518, acc 0.859375
2017-03-02T17:47:57.813466: step 12046, loss 0.166965, acc 0.9375
2017-03-02T17:47:57.884677: step 12047, loss 0.0812555, acc 0.96875
2017-03-02T17:47:57.950094: step 12048, loss 0.0802713, acc 0.984375
2017-03-02T17:47:58.021298: step 12049, loss 0.17474, acc 0.9375
2017-03-02T17:47:58.115680: step 12050, loss 0.266801, acc 0.859375
2017-03-02T17:47:58.187558: step 12051, loss 0.0985469, acc 0.953125
2017-03-02T17:47:58.263900: step 12052, loss 0.288418, acc 0.890625
2017-03-02T17:47:58.338867: step 12053, loss 0.176352, acc 0.890625
2017-03-02T17:47:58.411413: step 12054, loss 0.16196, acc 0.9375
2017-03-02T17:47:58.485139: step 12055, loss 0.190765, acc 0.90625
2017-03-02T17:47:58.567486: step 12056, loss 0.111471, acc 0.953125
2017-03-02T17:47:58.641312: step 12057, loss 0.268467, acc 0.890625
2017-03-02T17:47:58.708629: step 12058, loss 0.155472, acc 0.953125
2017-03-02T17:47:58.781523: step 12059, loss 0.136162, acc 0.9375
2017-03-02T17:47:58.857697: step 12060, loss 0.313697, acc 0.859375
2017-03-02T17:47:58.932840: step 12061, loss 0.0306772, acc 0.984375
2017-03-02T17:47:59.009787: step 12062, loss 0.110426, acc 0.953125
2017-03-02T17:47:59.084737: step 12063, loss 0.172284, acc 0.921875
2017-03-02T17:47:59.156804: step 12064, loss 0.312179, acc 0.921875
2017-03-02T17:47:59.236150: step 12065, loss 0.153535, acc 0.9375
2017-03-02T17:47:59.305530: step 12066, loss 0.219367, acc 0.90625
2017-03-02T17:47:59.377107: step 12067, loss 0.130349, acc 0.9375
2017-03-02T17:47:59.450864: step 12068, loss 0.135388, acc 0.984375
2017-03-02T17:47:59.528134: step 12069, loss 0.255574, acc 0.90625
2017-03-02T17:47:59.599657: step 12070, loss 0.197417, acc 0.90625
2017-03-02T17:47:59.674575: step 12071, loss 0.206382, acc 0.90625
2017-03-02T17:47:59.757216: step 12072, loss 0.156329, acc 0.9375
2017-03-02T17:47:59.839318: step 12073, loss 0.191729, acc 0.921875
2017-03-02T17:47:59.910596: step 12074, loss 0.113723, acc 0.953125
2017-03-02T17:47:59.992425: step 12075, loss 0.21555, acc 0.875
2017-03-02T17:48:00.060635: step 12076, loss 0.125522, acc 0.953125
2017-03-02T17:48:00.135631: step 12077, loss 0.254743, acc 0.90625
2017-03-02T17:48:00.208528: step 12078, loss 0.118013, acc 0.9375
2017-03-02T17:48:00.280022: step 12079, loss 0.152902, acc 0.90625
2017-03-02T17:48:00.352893: step 12080, loss 0.174053, acc 0.9375
2017-03-02T17:48:00.427928: step 12081, loss 0.111049, acc 0.953125
2017-03-02T17:48:00.499460: step 12082, loss 0.18975, acc 0.9375
2017-03-02T17:48:00.581067: step 12083, loss 0.156549, acc 0.9375
2017-03-02T17:48:00.653263: step 12084, loss 0.053664, acc 1
2017-03-02T17:48:00.726672: step 12085, loss 0.248128, acc 0.90625
2017-03-02T17:48:00.803776: step 12086, loss 0.166016, acc 0.90625
2017-03-02T17:48:00.879743: step 12087, loss 0.174168, acc 0.9375
2017-03-02T17:48:00.948545: step 12088, loss 0.185944, acc 0.9375
2017-03-02T17:48:01.025464: step 12089, loss 0.12142, acc 0.9375
2017-03-02T17:48:01.108983: step 12090, loss 0.272251, acc 0.890625
2017-03-02T17:48:01.187963: step 12091, loss 0.173207, acc 0.921875
2017-03-02T17:48:01.262235: step 12092, loss 0.159351, acc 0.953125
2017-03-02T17:48:01.324369: step 12093, loss 0.113092, acc 0.921875
2017-03-02T17:48:01.392469: step 12094, loss 0.206654, acc 0.9375
2017-03-02T17:48:01.465723: step 12095, loss 0.0931893, acc 0.9375
2017-03-02T17:48:01.539914: step 12096, loss 0.178694, acc 0.9375
2017-03-02T17:48:01.619624: step 12097, loss 0.19517, acc 0.890625
2017-03-02T17:48:01.690839: step 12098, loss 0.139621, acc 0.921875
2017-03-02T17:48:01.768848: step 12099, loss 0.159085, acc 0.9375
2017-03-02T17:48:01.846687: step 12100, loss 0.16178, acc 0.90625

Evaluation:
2017-03-02T17:48:01.882484: step 12100, loss 1.5688, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12100

2017-03-02T17:48:02.330439: step 12101, loss 0.203715, acc 0.875
2017-03-02T17:48:02.401962: step 12102, loss 0.191606, acc 0.921875
2017-03-02T17:48:02.475856: step 12103, loss 0.15676, acc 0.9375
2017-03-02T17:48:02.551524: step 12104, loss 0.28634, acc 0.859375
2017-03-02T17:48:02.616085: step 12105, loss 0.177198, acc 0.90625
2017-03-02T17:48:02.679520: step 12106, loss 0.202468, acc 0.890625
2017-03-02T17:48:02.748989: step 12107, loss 0.152723, acc 0.921875
2017-03-02T17:48:02.810123: step 12108, loss 0.134341, acc 0.921875
2017-03-02T17:48:02.891368: step 12109, loss 0.12133, acc 0.9375
2017-03-02T17:48:02.970782: step 12110, loss 0.216378, acc 0.90625
2017-03-02T17:48:03.041010: step 12111, loss 0.0979644, acc 0.9375
2017-03-02T17:48:03.115848: step 12112, loss 0.212696, acc 0.921875
2017-03-02T17:48:03.179852: step 12113, loss 0.213625, acc 0.921875
2017-03-02T17:48:03.254422: step 12114, loss 0.191893, acc 0.890625
2017-03-02T17:48:03.331854: step 12115, loss 0.430728, acc 0.875
2017-03-02T17:48:03.409193: step 12116, loss 0.133642, acc 0.921875
2017-03-02T17:48:03.474936: step 12117, loss 0.215362, acc 0.90625
2017-03-02T17:48:03.549261: step 12118, loss 0.206806, acc 0.90625
2017-03-02T17:48:03.625371: step 12119, loss 0.337157, acc 0.890625
2017-03-02T17:48:03.710113: step 12120, loss 0.115968, acc 0.953125
2017-03-02T17:48:03.783393: step 12121, loss 0.208255, acc 0.921875
2017-03-02T17:48:03.858988: step 12122, loss 0.0948514, acc 0.9375
2017-03-02T17:48:03.938163: step 12123, loss 0.208641, acc 0.90625
2017-03-02T17:48:04.016006: step 12124, loss 0.285241, acc 0.890625
2017-03-02T17:48:04.083665: step 12125, loss 0.0813126, acc 0.953125
2017-03-02T17:48:04.147552: step 12126, loss 0.270967, acc 0.921875
2017-03-02T17:48:04.222594: step 12127, loss 0.251956, acc 0.890625
2017-03-02T17:48:04.328309: step 12128, loss 0.1387, acc 0.921875
2017-03-02T17:48:04.402310: step 12129, loss 0.105576, acc 0.953125
2017-03-02T17:48:04.481666: step 12130, loss 0.109849, acc 0.953125
2017-03-02T17:48:04.553369: step 12131, loss 0.154105, acc 0.921875
2017-03-02T17:48:04.629121: step 12132, loss 0.189863, acc 0.890625
2017-03-02T17:48:04.699935: step 12133, loss 0.156271, acc 0.953125
2017-03-02T17:48:04.772886: step 12134, loss 0.185336, acc 0.9375
2017-03-02T17:48:04.841258: step 12135, loss 0.126743, acc 0.953125
2017-03-02T17:48:04.911573: step 12136, loss 0.126427, acc 0.921875
2017-03-02T17:48:04.994217: step 12137, loss 0.197932, acc 0.953125
2017-03-02T17:48:05.067933: step 12138, loss 0.261448, acc 0.875
2017-03-02T17:48:05.154703: step 12139, loss 0.0872698, acc 0.96875
2017-03-02T17:48:05.225222: step 12140, loss 0.213533, acc 0.890625
2017-03-02T17:48:05.312563: step 12141, loss 0.214799, acc 0.90625
2017-03-02T17:48:05.388570: step 12142, loss 0.334414, acc 0.875
2017-03-02T17:48:05.467576: step 12143, loss 0.269453, acc 0.875
2017-03-02T17:48:05.541643: step 12144, loss 0.109825, acc 0.96875
2017-03-02T17:48:05.613726: step 12145, loss 0.17419, acc 0.90625
2017-03-02T17:48:05.695001: step 12146, loss 0.188624, acc 0.9375
2017-03-02T17:48:05.766978: step 12147, loss 0.118723, acc 0.96875
2017-03-02T17:48:05.846464: step 12148, loss 0.196696, acc 0.9375
2017-03-02T17:48:05.919080: step 12149, loss 0.136959, acc 0.921875
2017-03-02T17:48:05.991136: step 12150, loss 0.11742, acc 0.9375
2017-03-02T17:48:06.068161: step 12151, loss 0.226364, acc 0.90625
2017-03-02T17:48:06.146643: step 12152, loss 0.169899, acc 1
2017-03-02T17:48:06.222689: step 12153, loss 0.103955, acc 0.96875
2017-03-02T17:48:06.292681: step 12154, loss 0.235187, acc 0.875
2017-03-02T17:48:06.365680: step 12155, loss 0.184791, acc 0.90625
2017-03-02T17:48:06.435458: step 12156, loss 0.150819, acc 0.90625
2017-03-02T17:48:06.509387: step 12157, loss 0.202065, acc 0.9375
2017-03-02T17:48:06.583876: step 12158, loss 0.172954, acc 0.953125
2017-03-02T17:48:06.654468: step 12159, loss 0.219827, acc 0.921875
2017-03-02T17:48:06.727866: step 12160, loss 0.148433, acc 0.96875
2017-03-02T17:48:06.798648: step 12161, loss 0.127591, acc 0.9375
2017-03-02T17:48:06.870243: step 12162, loss 0.216411, acc 0.921875
2017-03-02T17:48:06.938695: step 12163, loss 0.102146, acc 0.953125
2017-03-02T17:48:07.019806: step 12164, loss 0.229039, acc 0.921875
2017-03-02T17:48:07.093744: step 12165, loss 0.117243, acc 0.9375
2017-03-02T17:48:07.171432: step 12166, loss 0.231304, acc 0.9375
2017-03-02T17:48:07.249367: step 12167, loss 0.11012, acc 0.953125
2017-03-02T17:48:07.324023: step 12168, loss 0.11756, acc 0.984375
2017-03-02T17:48:07.399547: step 12169, loss 0.137111, acc 0.953125
2017-03-02T17:48:07.474044: step 12170, loss 0.114096, acc 0.9375
2017-03-02T17:48:07.546096: step 12171, loss 0.129672, acc 0.9375
2017-03-02T17:48:07.613895: step 12172, loss 0.115026, acc 0.921875
2017-03-02T17:48:07.676142: step 12173, loss 0.144204, acc 0.953125
2017-03-02T17:48:07.751126: step 12174, loss 0.185754, acc 0.90625
2017-03-02T17:48:07.826696: step 12175, loss 0.0729832, acc 0.984375
2017-03-02T17:48:07.899484: step 12176, loss 0.167804, acc 0.953125
2017-03-02T17:48:07.974571: step 12177, loss 0.204238, acc 0.90625
2017-03-02T17:48:08.048939: step 12178, loss 0.175196, acc 0.9375
2017-03-02T17:48:08.130853: step 12179, loss 0.183123, acc 0.953125
2017-03-02T17:48:08.201929: step 12180, loss 0.0350259, acc 1
2017-03-02T17:48:08.278567: step 12181, loss 0.139294, acc 0.921875
2017-03-02T17:48:08.346336: step 12182, loss 0.131319, acc 0.9375
2017-03-02T17:48:08.418467: step 12183, loss 0.110278, acc 0.9375
2017-03-02T17:48:08.507771: step 12184, loss 0.149816, acc 0.9375
2017-03-02T17:48:08.583320: step 12185, loss 0.194086, acc 0.921875
2017-03-02T17:48:08.655827: step 12186, loss 0.19871, acc 0.9375
2017-03-02T17:48:08.728124: step 12187, loss 0.0768806, acc 0.984375
2017-03-02T17:48:08.804714: step 12188, loss 0.133804, acc 0.953125
2017-03-02T17:48:08.872641: step 12189, loss 0.202613, acc 0.921875
2017-03-02T17:48:08.942892: step 12190, loss 0.140635, acc 0.9375
2017-03-02T17:48:09.013458: step 12191, loss 0.266622, acc 0.875
2017-03-02T17:48:09.077148: step 12192, loss 0.15011, acc 0.953125
2017-03-02T17:48:09.167086: step 12193, loss 0.143585, acc 0.9375
2017-03-02T17:48:09.243870: step 12194, loss 0.107498, acc 0.953125
2017-03-02T17:48:09.320168: step 12195, loss 0.151627, acc 0.953125
2017-03-02T17:48:09.391009: step 12196, loss 0.310157, acc 0.796875
2017-03-02T17:48:09.461652: step 12197, loss 0.170636, acc 0.921875
2017-03-02T17:48:09.535880: step 12198, loss 0.288481, acc 0.859375
2017-03-02T17:48:09.615109: step 12199, loss 0.188021, acc 0.9375
2017-03-02T17:48:09.687583: step 12200, loss 0.169254, acc 0.9375

Evaluation:
2017-03-02T17:48:09.716794: step 12200, loss 1.57275, acc 0.645998

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12200

2017-03-02T17:48:10.168812: step 12201, loss 0.254677, acc 0.921875
2017-03-02T17:48:10.247650: step 12202, loss 0.205147, acc 0.90625
2017-03-02T17:48:10.323689: step 12203, loss 0.149187, acc 0.921875
2017-03-02T17:48:10.392990: step 12204, loss 0.187417, acc 0.90625
2017-03-02T17:48:10.480998: step 12205, loss 0.207088, acc 0.90625
2017-03-02T17:48:10.552819: step 12206, loss 0.165129, acc 0.953125
2017-03-02T17:48:10.625231: step 12207, loss 0.128602, acc 0.96875
2017-03-02T17:48:10.698935: step 12208, loss 0.196467, acc 0.90625
2017-03-02T17:48:10.772567: step 12209, loss 0.166688, acc 0.90625
2017-03-02T17:48:10.848432: step 12210, loss 0.0821853, acc 0.9375
2017-03-02T17:48:10.937824: step 12211, loss 0.185425, acc 0.921875
2017-03-02T17:48:11.009808: step 12212, loss 0.152642, acc 0.953125
2017-03-02T17:48:11.078141: step 12213, loss 0.242494, acc 0.890625
2017-03-02T17:48:11.150217: step 12214, loss 0.192156, acc 0.9375
2017-03-02T17:48:11.224372: step 12215, loss 0.22891, acc 0.90625
2017-03-02T17:48:11.297010: step 12216, loss 0.176744, acc 0.921875
2017-03-02T17:48:11.371729: step 12217, loss 0.165391, acc 0.953125
2017-03-02T17:48:11.453823: step 12218, loss 0.0906938, acc 0.953125
2017-03-02T17:48:11.526387: step 12219, loss 0.0728496, acc 0.96875
2017-03-02T17:48:11.596900: step 12220, loss 0.193631, acc 0.875
2017-03-02T17:48:11.673226: step 12221, loss 0.199489, acc 0.921875
2017-03-02T17:48:11.745266: step 12222, loss 0.212066, acc 0.90625
2017-03-02T17:48:11.837353: step 12223, loss 0.229097, acc 0.90625
2017-03-02T17:48:11.910463: step 12224, loss 0.132232, acc 0.96875
2017-03-02T17:48:11.985959: step 12225, loss 0.174874, acc 0.921875
2017-03-02T17:48:12.066695: step 12226, loss 0.144258, acc 0.9375
2017-03-02T17:48:12.141132: step 12227, loss 0.12051, acc 0.953125
2017-03-02T17:48:12.211810: step 12228, loss 0.108373, acc 0.953125
2017-03-02T17:48:12.289858: step 12229, loss 0.157504, acc 0.90625
2017-03-02T17:48:12.363776: step 12230, loss 0.0894095, acc 0.96875
2017-03-02T17:48:12.433609: step 12231, loss 0.15952, acc 0.953125
2017-03-02T17:48:12.517078: step 12232, loss 0.166147, acc 0.96875
2017-03-02T17:48:12.589578: step 12233, loss 0.138328, acc 0.9375
2017-03-02T17:48:12.667195: step 12234, loss 0.2353, acc 0.90625
2017-03-02T17:48:12.739353: step 12235, loss 0.210207, acc 0.890625
2017-03-02T17:48:12.815198: step 12236, loss 0.118714, acc 0.953125
2017-03-02T17:48:12.884964: step 12237, loss 0.0948176, acc 0.953125
2017-03-02T17:48:12.954070: step 12238, loss 0.233711, acc 0.90625
2017-03-02T17:48:13.027133: step 12239, loss 0.132758, acc 0.953125
2017-03-02T17:48:13.103604: step 12240, loss 0.164344, acc 0.953125
2017-03-02T17:48:13.172412: step 12241, loss 0.191193, acc 0.890625
2017-03-02T17:48:13.243313: step 12242, loss 0.126044, acc 0.96875
2017-03-02T17:48:13.315129: step 12243, loss 0.111862, acc 0.953125
2017-03-02T17:48:13.387016: step 12244, loss 0.0984274, acc 0.9375
2017-03-02T17:48:13.461106: step 12245, loss 0.102041, acc 0.953125
2017-03-02T17:48:13.531488: step 12246, loss 0.140405, acc 0.921875
2017-03-02T17:48:13.609359: step 12247, loss 0.0776874, acc 0.96875
2017-03-02T17:48:13.694864: step 12248, loss 0.214972, acc 0.875
2017-03-02T17:48:13.769073: step 12249, loss 0.282303, acc 0.9375
2017-03-02T17:48:13.846825: step 12250, loss 0.121016, acc 0.96875
2017-03-02T17:48:13.917528: step 12251, loss 0.182847, acc 0.9375
2017-03-02T17:48:13.987384: step 12252, loss 0.337045, acc 0.890625
2017-03-02T17:48:14.059384: step 12253, loss 0.179613, acc 0.9375
2017-03-02T17:48:14.133200: step 12254, loss 0.0850208, acc 0.953125
2017-03-02T17:48:14.204381: step 12255, loss 0.173783, acc 0.953125
2017-03-02T17:48:14.285701: step 12256, loss 0.144672, acc 0.9375
2017-03-02T17:48:14.359004: step 12257, loss 0.143212, acc 0.921875
2017-03-02T17:48:14.435778: step 12258, loss 0.101285, acc 0.9375
2017-03-02T17:48:14.517962: step 12259, loss 0.211053, acc 0.90625
2017-03-02T17:48:14.595839: step 12260, loss 0.102877, acc 0.953125
2017-03-02T17:48:14.671896: step 12261, loss 0.212491, acc 0.90625
2017-03-02T17:48:14.744543: step 12262, loss 0.209499, acc 0.890625
2017-03-02T17:48:14.829622: step 12263, loss 0.212453, acc 0.890625
2017-03-02T17:48:14.906183: step 12264, loss 0.19255, acc 0.921875
2017-03-02T17:48:14.981982: step 12265, loss 0.152147, acc 0.90625
2017-03-02T17:48:15.063813: step 12266, loss 0.0988931, acc 0.953125
2017-03-02T17:48:15.139570: step 12267, loss 0.17018, acc 0.953125
2017-03-02T17:48:15.218472: step 12268, loss 0.13675, acc 0.9375
2017-03-02T17:48:15.286427: step 12269, loss 0.307441, acc 0.859375
2017-03-02T17:48:15.357371: step 12270, loss 0.141743, acc 0.953125
2017-03-02T17:48:15.438241: step 12271, loss 0.271785, acc 0.890625
2017-03-02T17:48:15.518547: step 12272, loss 0.173411, acc 0.9375
2017-03-02T17:48:15.593951: step 12273, loss 0.192604, acc 0.921875
2017-03-02T17:48:15.668109: step 12274, loss 0.222733, acc 0.9375
2017-03-02T17:48:15.739671: step 12275, loss 0.198245, acc 0.9375
2017-03-02T17:48:15.811557: step 12276, loss 0.137076, acc 0.953125
2017-03-02T17:48:15.882459: step 12277, loss 0.113218, acc 0.9375
2017-03-02T17:48:15.959861: step 12278, loss 0.156426, acc 0.921875
2017-03-02T17:48:16.027753: step 12279, loss 0.106993, acc 0.9375
2017-03-02T17:48:16.106264: step 12280, loss 0.375346, acc 0.875
2017-03-02T17:48:16.183136: step 12281, loss 0.146982, acc 0.9375
2017-03-02T17:48:16.253189: step 12282, loss 0.157517, acc 0.921875
2017-03-02T17:48:16.339625: step 12283, loss 0.242921, acc 0.921875
2017-03-02T17:48:16.411573: step 12284, loss 0.142812, acc 0.9375
2017-03-02T17:48:16.483656: step 12285, loss 0.225028, acc 0.921875
2017-03-02T17:48:16.552351: step 12286, loss 0.184929, acc 0.90625
2017-03-02T17:48:16.631166: step 12287, loss 0.0548894, acc 0.984375
2017-03-02T17:48:16.707247: step 12288, loss 0.0642719, acc 1
2017-03-02T17:48:16.781358: step 12289, loss 0.14728, acc 0.921875
2017-03-02T17:48:16.857191: step 12290, loss 0.121437, acc 0.953125
2017-03-02T17:48:16.928215: step 12291, loss 0.0869147, acc 0.984375
2017-03-02T17:48:16.999146: step 12292, loss 0.209743, acc 0.9375
2017-03-02T17:48:17.070422: step 12293, loss 0.135832, acc 0.953125
2017-03-02T17:48:17.140334: step 12294, loss 0.0901003, acc 0.96875
2017-03-02T17:48:17.221366: step 12295, loss 0.0796834, acc 0.96875
2017-03-02T17:48:17.287865: step 12296, loss 0.159886, acc 0.921875
2017-03-02T17:48:17.356663: step 12297, loss 0.145966, acc 0.921875
2017-03-02T17:48:17.428514: step 12298, loss 0.202364, acc 0.90625
2017-03-02T17:48:17.507652: step 12299, loss 0.146521, acc 0.921875
2017-03-02T17:48:17.593620: step 12300, loss 0.151262, acc 0.921875

Evaluation:
2017-03-02T17:48:17.633646: step 12300, loss 1.60663, acc 0.665465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12300

2017-03-02T17:48:18.141811: step 12301, loss 0.131553, acc 0.953125
2017-03-02T17:48:18.222932: step 12302, loss 0.141405, acc 0.9375
2017-03-02T17:48:18.296872: step 12303, loss 0.18963, acc 0.90625
2017-03-02T17:48:18.368578: step 12304, loss 0.211932, acc 0.890625
2017-03-02T17:48:18.439532: step 12305, loss 0.231048, acc 0.875
2017-03-02T17:48:18.510763: step 12306, loss 0.193978, acc 0.9375
2017-03-02T17:48:18.589123: step 12307, loss 0.218996, acc 0.921875
2017-03-02T17:48:18.657102: step 12308, loss 0.140159, acc 0.9375
2017-03-02T17:48:18.728219: step 12309, loss 0.247536, acc 0.90625
2017-03-02T17:48:18.799875: step 12310, loss 0.0859432, acc 0.96875
2017-03-02T17:48:18.869718: step 12311, loss 0.0754583, acc 0.953125
2017-03-02T17:48:18.952633: step 12312, loss 0.180726, acc 0.90625
2017-03-02T17:48:19.034312: step 12313, loss 0.0916343, acc 0.984375
2017-03-02T17:48:19.114172: step 12314, loss 0.19087, acc 0.953125
2017-03-02T17:48:19.190110: step 12315, loss 0.0779892, acc 0.96875
2017-03-02T17:48:19.270757: step 12316, loss 0.187668, acc 0.9375
2017-03-02T17:48:19.342947: step 12317, loss 0.300336, acc 0.859375
2017-03-02T17:48:19.410600: step 12318, loss 0.207179, acc 0.921875
2017-03-02T17:48:19.486346: step 12319, loss 0.129289, acc 0.9375
2017-03-02T17:48:19.559604: step 12320, loss 0.165156, acc 0.9375
2017-03-02T17:48:19.634541: step 12321, loss 0.241156, acc 0.90625
2017-03-02T17:48:19.706477: step 12322, loss 0.185727, acc 0.90625
2017-03-02T17:48:19.781720: step 12323, loss 0.207161, acc 0.921875
2017-03-02T17:48:19.851788: step 12324, loss 0.0844003, acc 0.953125
2017-03-02T17:48:19.926329: step 12325, loss 0.201395, acc 0.9375
2017-03-02T17:48:19.996801: step 12326, loss 0.180518, acc 0.9375
2017-03-02T17:48:20.066526: step 12327, loss 0.0327394, acc 0.984375
2017-03-02T17:48:20.141206: step 12328, loss 0.112073, acc 0.9375
2017-03-02T17:48:20.216201: step 12329, loss 0.173348, acc 0.90625
2017-03-02T17:48:20.295671: step 12330, loss 0.117156, acc 0.953125
2017-03-02T17:48:20.368473: step 12331, loss 0.0595629, acc 0.96875
2017-03-02T17:48:20.442168: step 12332, loss 0.236354, acc 0.90625
2017-03-02T17:48:20.518794: step 12333, loss 0.142796, acc 0.96875
2017-03-02T17:48:20.588786: step 12334, loss 0.174534, acc 0.953125
2017-03-02T17:48:20.663726: step 12335, loss 0.0784177, acc 0.96875
2017-03-02T17:48:20.735801: step 12336, loss 0.30995, acc 0.875
2017-03-02T17:48:20.810176: step 12337, loss 0.129346, acc 0.9375
2017-03-02T17:48:20.883723: step 12338, loss 0.167541, acc 0.9375
2017-03-02T17:48:20.955103: step 12339, loss 0.658222, acc 0.859375
2017-03-02T17:48:21.036670: step 12340, loss 0.155476, acc 0.90625
2017-03-02T17:48:21.113571: step 12341, loss 0.130833, acc 0.953125
2017-03-02T17:48:21.186978: step 12342, loss 0.231795, acc 0.9375
2017-03-02T17:48:21.263863: step 12343, loss 0.226787, acc 0.9375
2017-03-02T17:48:21.344620: step 12344, loss 0.195662, acc 0.9375
2017-03-02T17:48:21.411533: step 12345, loss 0.145038, acc 0.921875
2017-03-02T17:48:21.480202: step 12346, loss 0.155316, acc 0.921875
2017-03-02T17:48:21.554784: step 12347, loss 0.129246, acc 0.9375
2017-03-02T17:48:21.632627: step 12348, loss 0.00160281, acc 1
2017-03-02T17:48:21.728331: step 12349, loss 0.192968, acc 0.953125
2017-03-02T17:48:21.803724: step 12350, loss 0.106708, acc 0.96875
2017-03-02T17:48:21.874780: step 12351, loss 0.109271, acc 0.953125
2017-03-02T17:48:21.962811: step 12352, loss 0.164458, acc 0.921875
2017-03-02T17:48:22.046505: step 12353, loss 0.152547, acc 0.9375
2017-03-02T17:48:22.121897: step 12354, loss 0.106326, acc 0.9375
2017-03-02T17:48:22.189706: step 12355, loss 0.266015, acc 0.875
2017-03-02T17:48:22.257382: step 12356, loss 0.09082, acc 0.953125
2017-03-02T17:48:22.345900: step 12357, loss 0.146908, acc 0.90625
2017-03-02T17:48:22.421254: step 12358, loss 0.155243, acc 0.96875
2017-03-02T17:48:22.490465: step 12359, loss 0.0770228, acc 0.96875
2017-03-02T17:48:22.556688: step 12360, loss 0.177695, acc 0.9375
2017-03-02T17:48:22.628047: step 12361, loss 0.0907845, acc 0.984375
2017-03-02T17:48:22.702197: step 12362, loss 0.195499, acc 0.9375
2017-03-02T17:48:22.772710: step 12363, loss 0.200898, acc 0.921875
2017-03-02T17:48:22.839358: step 12364, loss 0.175261, acc 0.9375
2017-03-02T17:48:22.904147: step 12365, loss 0.164012, acc 0.9375
2017-03-02T17:48:22.967325: step 12366, loss 0.158905, acc 0.921875
2017-03-02T17:48:23.039404: step 12367, loss 0.242019, acc 0.875
2017-03-02T17:48:23.117524: step 12368, loss 0.149718, acc 0.9375
2017-03-02T17:48:23.219688: step 12369, loss 0.0878035, acc 0.96875
2017-03-02T17:48:23.286963: step 12370, loss 0.20093, acc 0.9375
2017-03-02T17:48:23.357836: step 12371, loss 0.125777, acc 0.9375
2017-03-02T17:48:23.428767: step 12372, loss 0.240197, acc 0.84375
2017-03-02T17:48:23.496280: step 12373, loss 0.148155, acc 0.96875
2017-03-02T17:48:23.563239: step 12374, loss 0.0983773, acc 0.9375
2017-03-02T17:48:23.638357: step 12375, loss 0.167364, acc 0.921875
2017-03-02T17:48:23.711779: step 12376, loss 0.417005, acc 0.84375
2017-03-02T17:48:23.790934: step 12377, loss 0.137383, acc 0.9375
2017-03-02T17:48:23.868034: step 12378, loss 0.177854, acc 0.9375
2017-03-02T17:48:23.942979: step 12379, loss 0.0972163, acc 0.96875
2017-03-02T17:48:24.018373: step 12380, loss 0.116057, acc 0.953125
2017-03-02T17:48:24.092813: step 12381, loss 0.127411, acc 0.953125
2017-03-02T17:48:24.162221: step 12382, loss 0.176542, acc 0.90625
2017-03-02T17:48:24.230192: step 12383, loss 0.225681, acc 0.9375
2017-03-02T17:48:24.317136: step 12384, loss 0.146775, acc 0.921875
2017-03-02T17:48:24.391508: step 12385, loss 0.217991, acc 0.90625
2017-03-02T17:48:24.465640: step 12386, loss 0.271164, acc 0.9375
2017-03-02T17:48:24.543564: step 12387, loss 0.081703, acc 0.984375
2017-03-02T17:48:24.618653: step 12388, loss 0.107438, acc 0.96875
2017-03-02T17:48:24.691705: step 12389, loss 0.168318, acc 0.921875
2017-03-02T17:48:24.774443: step 12390, loss 0.200553, acc 0.921875
2017-03-02T17:48:24.847027: step 12391, loss 0.282959, acc 0.890625
2017-03-02T17:48:24.917111: step 12392, loss 0.116254, acc 0.96875
2017-03-02T17:48:24.979849: step 12393, loss 0.177428, acc 0.921875
2017-03-02T17:48:25.051557: step 12394, loss 0.120293, acc 0.953125
2017-03-02T17:48:25.121297: step 12395, loss 0.206117, acc 0.921875
2017-03-02T17:48:25.193919: step 12396, loss 0.255652, acc 0.875
2017-03-02T17:48:25.264876: step 12397, loss 0.0774179, acc 0.96875
2017-03-02T17:48:25.336475: step 12398, loss 0.0508645, acc 1
2017-03-02T17:48:25.410050: step 12399, loss 0.184557, acc 0.90625
2017-03-02T17:48:25.481860: step 12400, loss 0.0881249, acc 0.953125

Evaluation:
2017-03-02T17:48:25.509093: step 12400, loss 1.60658, acc 0.670512

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12400

2017-03-02T17:48:25.967118: step 12401, loss 0.206413, acc 0.90625
2017-03-02T17:48:26.034227: step 12402, loss 0.16417, acc 0.90625
2017-03-02T17:48:26.108102: step 12403, loss 0.112767, acc 0.921875
2017-03-02T17:48:26.202640: step 12404, loss 0.189206, acc 0.953125
2017-03-02T17:48:26.273423: step 12405, loss 0.138215, acc 0.953125
2017-03-02T17:48:26.352007: step 12406, loss 0.0338764, acc 1
2017-03-02T17:48:26.427467: step 12407, loss 0.211586, acc 0.921875
2017-03-02T17:48:26.502870: step 12408, loss 0.196866, acc 0.90625
2017-03-02T17:48:26.573903: step 12409, loss 0.154692, acc 0.9375
2017-03-02T17:48:26.645783: step 12410, loss 0.108275, acc 0.953125
2017-03-02T17:48:26.725453: step 12411, loss 0.155907, acc 0.953125
2017-03-02T17:48:26.805421: step 12412, loss 0.245899, acc 0.9375
2017-03-02T17:48:26.874060: step 12413, loss 0.125812, acc 0.953125
2017-03-02T17:48:26.939646: step 12414, loss 0.32564, acc 0.875
2017-03-02T17:48:27.047691: step 12415, loss 0.0776496, acc 0.984375
2017-03-02T17:48:27.124431: step 12416, loss 0.0952944, acc 0.96875
2017-03-02T17:48:27.204337: step 12417, loss 0.196567, acc 0.921875
2017-03-02T17:48:27.281950: step 12418, loss 0.154351, acc 0.921875
2017-03-02T17:48:27.356557: step 12419, loss 0.127554, acc 0.9375
2017-03-02T17:48:27.429729: step 12420, loss 0.210523, acc 0.921875
2017-03-02T17:48:27.504039: step 12421, loss 0.126355, acc 0.9375
2017-03-02T17:48:27.572228: step 12422, loss 0.0873184, acc 0.96875
2017-03-02T17:48:27.639651: step 12423, loss 0.149497, acc 0.9375
2017-03-02T17:48:27.712026: step 12424, loss 0.152108, acc 0.9375
2017-03-02T17:48:27.788803: step 12425, loss 0.0592729, acc 0.96875
2017-03-02T17:48:27.859234: step 12426, loss 0.264384, acc 0.890625
2017-03-02T17:48:27.938720: step 12427, loss 0.245987, acc 0.90625
2017-03-02T17:48:28.013260: step 12428, loss 0.27204, acc 0.875
2017-03-02T17:48:28.085432: step 12429, loss 0.261328, acc 0.890625
2017-03-02T17:48:28.150305: step 12430, loss 0.036519, acc 1
2017-03-02T17:48:28.225777: step 12431, loss 0.207659, acc 0.921875
2017-03-02T17:48:28.297215: step 12432, loss 0.178741, acc 0.9375
2017-03-02T17:48:28.394947: step 12433, loss 0.201826, acc 0.921875
2017-03-02T17:48:28.463995: step 12434, loss 0.134318, acc 0.9375
2017-03-02T17:48:28.560888: step 12435, loss 0.0414539, acc 0.984375
2017-03-02T17:48:28.634599: step 12436, loss 0.144249, acc 0.90625
2017-03-02T17:48:28.708288: step 12437, loss 0.194627, acc 0.921875
2017-03-02T17:48:28.784915: step 12438, loss 0.18625, acc 0.921875
2017-03-02T17:48:28.863484: step 12439, loss 0.218208, acc 0.875
2017-03-02T17:48:28.936349: step 12440, loss 0.157611, acc 0.9375
2017-03-02T17:48:29.003853: step 12441, loss 0.0930867, acc 0.96875
2017-03-02T17:48:29.073610: step 12442, loss 0.0866927, acc 0.953125
2017-03-02T17:48:29.142598: step 12443, loss 0.18391, acc 0.890625
2017-03-02T17:48:29.212737: step 12444, loss 0.133455, acc 0.9375
2017-03-02T17:48:29.285290: step 12445, loss 0.0967624, acc 0.96875
2017-03-02T17:48:29.357207: step 12446, loss 0.109242, acc 0.9375
2017-03-02T17:48:29.432573: step 12447, loss 0.148225, acc 0.953125
2017-03-02T17:48:29.503444: step 12448, loss 0.16575, acc 0.953125
2017-03-02T17:48:29.575694: step 12449, loss 0.0884433, acc 0.96875
2017-03-02T17:48:29.641087: step 12450, loss 0.253894, acc 0.890625
2017-03-02T17:48:29.709578: step 12451, loss 0.191591, acc 0.890625
2017-03-02T17:48:29.792953: step 12452, loss 0.2074, acc 0.90625
2017-03-02T17:48:29.867857: step 12453, loss 0.158382, acc 0.90625
2017-03-02T17:48:29.944028: step 12454, loss 0.167417, acc 0.90625
2017-03-02T17:48:30.017412: step 12455, loss 0.181558, acc 0.9375
2017-03-02T17:48:30.090704: step 12456, loss 0.114466, acc 0.96875
2017-03-02T17:48:30.161736: step 12457, loss 0.155371, acc 0.953125
2017-03-02T17:48:30.236014: step 12458, loss 0.251474, acc 0.859375
2017-03-02T17:48:30.308831: step 12459, loss 0.201952, acc 0.9375
2017-03-02T17:48:30.377263: step 12460, loss 0.211844, acc 0.90625
2017-03-02T17:48:30.472679: step 12461, loss 0.0894786, acc 0.984375
2017-03-02T17:48:30.546389: step 12462, loss 0.146792, acc 0.921875
2017-03-02T17:48:30.621172: step 12463, loss 0.0958726, acc 0.96875
2017-03-02T17:48:30.692448: step 12464, loss 0.159183, acc 0.9375
2017-03-02T17:48:30.763621: step 12465, loss 0.0650088, acc 0.96875
2017-03-02T17:48:30.835363: step 12466, loss 0.198754, acc 0.90625
2017-03-02T17:48:30.905835: step 12467, loss 0.106398, acc 0.96875
2017-03-02T17:48:30.984752: step 12468, loss 0.250257, acc 0.921875
2017-03-02T17:48:31.053036: step 12469, loss 0.131144, acc 0.953125
2017-03-02T17:48:31.124755: step 12470, loss 0.143577, acc 0.953125
2017-03-02T17:48:31.206096: step 12471, loss 0.149329, acc 0.921875
2017-03-02T17:48:31.290973: step 12472, loss 0.148426, acc 0.953125
2017-03-02T17:48:31.365694: step 12473, loss 0.0869677, acc 0.953125
2017-03-02T17:48:31.435227: step 12474, loss 0.184899, acc 0.90625
2017-03-02T17:48:31.519267: step 12475, loss 0.226238, acc 0.875
2017-03-02T17:48:31.589760: step 12476, loss 0.234919, acc 0.921875
2017-03-02T17:48:31.664297: step 12477, loss 0.101055, acc 0.96875
2017-03-02T17:48:31.735786: step 12478, loss 0.302931, acc 0.890625
2017-03-02T17:48:31.806147: step 12479, loss 0.0803742, acc 0.984375
2017-03-02T17:48:31.879849: step 12480, loss 0.0456876, acc 0.984375
2017-03-02T17:48:31.952824: step 12481, loss 0.0697167, acc 0.984375
2017-03-02T17:48:32.023811: step 12482, loss 0.0755031, acc 0.96875
2017-03-02T17:48:32.095204: step 12483, loss 0.0955232, acc 0.9375
2017-03-02T17:48:32.166522: step 12484, loss 0.158078, acc 0.953125
2017-03-02T17:48:32.232098: step 12485, loss 0.135954, acc 0.9375
2017-03-02T17:48:32.335967: step 12486, loss 0.115116, acc 0.9375
2017-03-02T17:48:32.405419: step 12487, loss 0.213727, acc 0.875
2017-03-02T17:48:32.485254: step 12488, loss 0.188212, acc 0.90625
2017-03-02T17:48:32.561604: step 12489, loss 0.222512, acc 0.90625
2017-03-02T17:48:32.631942: step 12490, loss 0.222724, acc 0.90625
2017-03-02T17:48:32.710007: step 12491, loss 0.342701, acc 0.875
2017-03-02T17:48:32.796526: step 12492, loss 0.158759, acc 0.953125
2017-03-02T17:48:32.876086: step 12493, loss 0.195124, acc 0.9375
2017-03-02T17:48:32.946285: step 12494, loss 0.151711, acc 0.953125
2017-03-02T17:48:33.026495: step 12495, loss 0.138004, acc 0.9375
2017-03-02T17:48:33.095338: step 12496, loss 0.218121, acc 0.921875
2017-03-02T17:48:33.166194: step 12497, loss 0.153162, acc 0.953125
2017-03-02T17:48:33.233410: step 12498, loss 0.19683, acc 0.953125
2017-03-02T17:48:33.306957: step 12499, loss 0.203047, acc 0.90625
2017-03-02T17:48:33.378959: step 12500, loss 0.125548, acc 0.9375

Evaluation:
2017-03-02T17:48:33.414596: step 12500, loss 1.61505, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12500

2017-03-02T17:48:33.919858: step 12501, loss 0.274474, acc 0.890625
2017-03-02T17:48:33.992751: step 12502, loss 0.0981255, acc 0.9375
2017-03-02T17:48:34.053688: step 12503, loss 0.125198, acc 0.9375
2017-03-02T17:48:34.133043: step 12504, loss 0.297945, acc 0.890625
2017-03-02T17:48:34.206799: step 12505, loss 0.201835, acc 0.9375
2017-03-02T17:48:34.274213: step 12506, loss 0.111481, acc 0.9375
2017-03-02T17:48:34.353113: step 12507, loss 0.197183, acc 0.953125
2017-03-02T17:48:34.430011: step 12508, loss 0.237603, acc 0.875
2017-03-02T17:48:34.506578: step 12509, loss 0.291552, acc 0.890625
2017-03-02T17:48:34.579248: step 12510, loss 0.242097, acc 0.90625
2017-03-02T17:48:34.652421: step 12511, loss 0.123572, acc 0.953125
2017-03-02T17:48:34.721047: step 12512, loss 0.12911, acc 0.96875
2017-03-02T17:48:34.801996: step 12513, loss 0.183109, acc 0.921875
2017-03-02T17:48:34.879144: step 12514, loss 0.148211, acc 0.953125
2017-03-02T17:48:34.948850: step 12515, loss 0.186349, acc 0.921875
2017-03-02T17:48:35.020212: step 12516, loss 0.258411, acc 0.84375
2017-03-02T17:48:35.092454: step 12517, loss 0.14036, acc 0.9375
2017-03-02T17:48:35.163662: step 12518, loss 0.193176, acc 0.953125
2017-03-02T17:48:35.249540: step 12519, loss 0.181815, acc 0.9375
2017-03-02T17:48:35.322279: step 12520, loss 0.323218, acc 0.84375
2017-03-02T17:48:35.398850: step 12521, loss 0.146724, acc 0.921875
2017-03-02T17:48:35.473891: step 12522, loss 0.152409, acc 0.90625
2017-03-02T17:48:35.561281: step 12523, loss 0.21592, acc 0.90625
2017-03-02T17:48:35.638540: step 12524, loss 0.162254, acc 0.921875
2017-03-02T17:48:35.708334: step 12525, loss 0.162294, acc 0.953125
2017-03-02T17:48:35.786526: step 12526, loss 0.26721, acc 0.859375
2017-03-02T17:48:35.859365: step 12527, loss 0.174957, acc 0.921875
2017-03-02T17:48:35.935534: step 12528, loss 0.0449061, acc 0.984375
2017-03-02T17:48:36.008130: step 12529, loss 0.111381, acc 0.96875
2017-03-02T17:48:36.085355: step 12530, loss 0.100382, acc 0.953125
2017-03-02T17:48:36.165003: step 12531, loss 0.113576, acc 0.953125
2017-03-02T17:48:36.242350: step 12532, loss 0.207443, acc 0.921875
2017-03-02T17:48:36.316555: step 12533, loss 0.290146, acc 0.875
2017-03-02T17:48:36.388934: step 12534, loss 0.157796, acc 0.953125
2017-03-02T17:48:36.463142: step 12535, loss 0.0896722, acc 0.953125
2017-03-02T17:48:36.529867: step 12536, loss 0.184244, acc 0.9375
2017-03-02T17:48:36.595652: step 12537, loss 0.218235, acc 0.921875
2017-03-02T17:48:36.654648: step 12538, loss 0.188536, acc 0.9375
2017-03-02T17:48:36.723016: step 12539, loss 0.206353, acc 0.859375
2017-03-02T17:48:36.798863: step 12540, loss 0.182662, acc 0.90625
2017-03-02T17:48:36.870327: step 12541, loss 0.169969, acc 0.921875
2017-03-02T17:48:36.941937: step 12542, loss 0.102203, acc 0.96875
2017-03-02T17:48:37.007833: step 12543, loss 0.148686, acc 0.90625
2017-03-02T17:48:37.078038: step 12544, loss 0.583392, acc 0.75
2017-03-02T17:48:37.156145: step 12545, loss 0.17858, acc 0.921875
2017-03-02T17:48:37.246964: step 12546, loss 0.180401, acc 0.90625
2017-03-02T17:48:37.321957: step 12547, loss 0.203891, acc 0.9375
2017-03-02T17:48:37.398495: step 12548, loss 0.255538, acc 0.90625
2017-03-02T17:48:37.471185: step 12549, loss 0.303192, acc 0.859375
2017-03-02T17:48:37.551720: step 12550, loss 0.187036, acc 0.921875
2017-03-02T17:48:37.632836: step 12551, loss 0.354701, acc 0.890625
2017-03-02T17:48:37.709068: step 12552, loss 0.186924, acc 0.9375
2017-03-02T17:48:37.775651: step 12553, loss 0.208194, acc 0.890625
2017-03-02T17:48:37.847822: step 12554, loss 0.188457, acc 0.90625
2017-03-02T17:48:37.915385: step 12555, loss 0.406467, acc 0.859375
2017-03-02T17:48:37.982574: step 12556, loss 0.159012, acc 0.9375
2017-03-02T17:48:38.052623: step 12557, loss 0.0365557, acc 1
2017-03-02T17:48:38.122093: step 12558, loss 0.199863, acc 0.90625
2017-03-02T17:48:38.190806: step 12559, loss 0.210188, acc 0.890625
2017-03-02T17:48:38.262844: step 12560, loss 0.194611, acc 0.890625
2017-03-02T17:48:38.330076: step 12561, loss 0.136652, acc 0.921875
2017-03-02T17:48:38.406874: step 12562, loss 0.168283, acc 0.90625
2017-03-02T17:48:38.479658: step 12563, loss 0.202486, acc 0.90625
2017-03-02T17:48:38.548489: step 12564, loss 0.20114, acc 0.875
2017-03-02T17:48:38.613604: step 12565, loss 0.151831, acc 0.9375
2017-03-02T17:48:38.684986: step 12566, loss 0.147455, acc 0.96875
2017-03-02T17:48:38.753696: step 12567, loss 0.0919171, acc 0.96875
2017-03-02T17:48:38.827059: step 12568, loss 0.112017, acc 0.953125
2017-03-02T17:48:38.899015: step 12569, loss 0.18375, acc 0.953125
2017-03-02T17:48:38.972471: step 12570, loss 0.125259, acc 0.921875
2017-03-02T17:48:39.050695: step 12571, loss 0.0836224, acc 0.96875
2017-03-02T17:48:39.135025: step 12572, loss 0.198528, acc 0.875
2017-03-02T17:48:39.209856: step 12573, loss 0.180522, acc 0.890625
2017-03-02T17:48:39.282696: step 12574, loss 0.168469, acc 0.953125
2017-03-02T17:48:39.347553: step 12575, loss 0.0732766, acc 0.96875
2017-03-02T17:48:39.427714: step 12576, loss 0.0987806, acc 0.953125
2017-03-02T17:48:39.495827: step 12577, loss 0.13353, acc 0.953125
2017-03-02T17:48:39.568363: step 12578, loss 0.239308, acc 0.90625
2017-03-02T17:48:39.646523: step 12579, loss 0.303101, acc 0.84375
2017-03-02T17:48:39.730527: step 12580, loss 0.123719, acc 0.921875
2017-03-02T17:48:39.799919: step 12581, loss 0.0905348, acc 0.96875
2017-03-02T17:48:39.870962: step 12582, loss 0.0760565, acc 0.96875
2017-03-02T17:48:39.950600: step 12583, loss 0.114503, acc 0.96875
2017-03-02T17:48:40.023980: step 12584, loss 0.155778, acc 0.921875
2017-03-02T17:48:40.100883: step 12585, loss 0.051563, acc 0.984375
2017-03-02T17:48:40.173817: step 12586, loss 0.121513, acc 0.921875
2017-03-02T17:48:40.255041: step 12587, loss 0.316511, acc 0.890625
2017-03-02T17:48:40.326842: step 12588, loss 0.163979, acc 0.921875
2017-03-02T17:48:40.407134: step 12589, loss 0.166037, acc 0.953125
2017-03-02T17:48:40.479794: step 12590, loss 0.183027, acc 0.90625
2017-03-02T17:48:40.549550: step 12591, loss 0.180719, acc 0.9375
2017-03-02T17:48:40.625144: step 12592, loss 0.16436, acc 0.90625
2017-03-02T17:48:40.690828: step 12593, loss 0.205918, acc 0.90625
2017-03-02T17:48:40.760170: step 12594, loss 0.161421, acc 0.953125
2017-03-02T17:48:40.837707: step 12595, loss 0.266646, acc 0.859375
2017-03-02T17:48:40.913530: step 12596, loss 0.301398, acc 0.90625
2017-03-02T17:48:40.985625: step 12597, loss 0.20763, acc 0.90625
2017-03-02T17:48:41.058262: step 12598, loss 0.133614, acc 0.953125
2017-03-02T17:48:41.137588: step 12599, loss 0.181782, acc 0.90625
2017-03-02T17:48:41.212191: step 12600, loss 0.156741, acc 0.953125

Evaluation:
2017-03-02T17:48:41.245811: step 12600, loss 1.6841, acc 0.656813

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12600

2017-03-02T17:48:43.276939: step 12601, loss 0.161282, acc 0.921875
2017-03-02T17:48:43.351545: step 12602, loss 0.291368, acc 0.921875
2017-03-02T17:48:43.421217: step 12603, loss 0.116445, acc 0.9375
2017-03-02T17:48:43.489997: step 12604, loss 0.184299, acc 0.890625
2017-03-02T17:48:43.561757: step 12605, loss 0.21576, acc 0.921875
2017-03-02T17:48:43.638329: step 12606, loss 0.171423, acc 0.890625
2017-03-02T17:48:43.703557: step 12607, loss 0.0713375, acc 0.984375
2017-03-02T17:48:43.781126: step 12608, loss 0.163971, acc 0.921875
2017-03-02T17:48:43.855113: step 12609, loss 0.150159, acc 0.953125
2017-03-02T17:48:43.933868: step 12610, loss 0.134599, acc 0.953125
2017-03-02T17:48:44.002494: step 12611, loss 0.104806, acc 0.96875
2017-03-02T17:48:44.083541: step 12612, loss 0.224975, acc 0.875
2017-03-02T17:48:44.156332: step 12613, loss 0.278373, acc 0.875
2017-03-02T17:48:44.227018: step 12614, loss 0.180282, acc 0.921875
2017-03-02T17:48:44.298789: step 12615, loss 0.212307, acc 0.9375
2017-03-02T17:48:44.373205: step 12616, loss 0.143303, acc 0.921875
2017-03-02T17:48:44.444082: step 12617, loss 0.237314, acc 0.921875
2017-03-02T17:48:44.514798: step 12618, loss 0.0768135, acc 0.96875
2017-03-02T17:48:44.592818: step 12619, loss 0.123153, acc 0.9375
2017-03-02T17:48:44.671179: step 12620, loss 0.17742, acc 0.9375
2017-03-02T17:48:44.741380: step 12621, loss 0.113658, acc 0.96875
2017-03-02T17:48:44.808813: step 12622, loss 0.251822, acc 0.90625
2017-03-02T17:48:44.890191: step 12623, loss 0.100375, acc 0.953125
2017-03-02T17:48:44.966032: step 12624, loss 0.0981101, acc 0.9375
2017-03-02T17:48:45.038084: step 12625, loss 0.283413, acc 0.8125
2017-03-02T17:48:45.122853: step 12626, loss 0.195747, acc 0.9375
2017-03-02T17:48:45.195500: step 12627, loss 0.173521, acc 0.921875
2017-03-02T17:48:45.269603: step 12628, loss 0.156422, acc 0.90625
2017-03-02T17:48:45.346947: step 12629, loss 0.200188, acc 0.953125
2017-03-02T17:48:45.421177: step 12630, loss 0.164516, acc 0.9375
2017-03-02T17:48:45.487610: step 12631, loss 0.341461, acc 0.875
2017-03-02T17:48:45.564331: step 12632, loss 0.270403, acc 0.90625
2017-03-02T17:48:45.640714: step 12633, loss 0.0699716, acc 0.96875
2017-03-02T17:48:45.714569: step 12634, loss 0.115001, acc 0.96875
2017-03-02T17:48:45.790361: step 12635, loss 0.116497, acc 0.953125
2017-03-02T17:48:45.860346: step 12636, loss 0.0812915, acc 0.96875
2017-03-02T17:48:45.937468: step 12637, loss 0.212871, acc 0.921875
2017-03-02T17:48:46.010981: step 12638, loss 0.134927, acc 0.9375
2017-03-02T17:48:46.076492: step 12639, loss 0.172233, acc 0.9375
2017-03-02T17:48:46.144569: step 12640, loss 0.143093, acc 0.921875
2017-03-02T17:48:46.231155: step 12641, loss 0.143989, acc 0.890625
2017-03-02T17:48:46.300321: step 12642, loss 0.246651, acc 0.921875
2017-03-02T17:48:46.373165: step 12643, loss 0.135602, acc 0.9375
2017-03-02T17:48:46.449957: step 12644, loss 0.245585, acc 0.921875
2017-03-02T17:48:46.526927: step 12645, loss 0.153212, acc 0.9375
2017-03-02T17:48:46.601342: step 12646, loss 0.0478705, acc 1
2017-03-02T17:48:46.678996: step 12647, loss 0.198975, acc 0.890625
2017-03-02T17:48:46.750439: step 12648, loss 0.308052, acc 0.859375
2017-03-02T17:48:46.819882: step 12649, loss 0.13004, acc 0.953125
2017-03-02T17:48:46.889302: step 12650, loss 0.101936, acc 0.984375
2017-03-02T17:48:46.961039: step 12651, loss 0.312118, acc 0.875
2017-03-02T17:48:47.038766: step 12652, loss 0.158589, acc 0.9375
2017-03-02T17:48:47.114684: step 12653, loss 0.199469, acc 0.90625
2017-03-02T17:48:47.184772: step 12654, loss 0.140738, acc 0.921875
2017-03-02T17:48:47.262387: step 12655, loss 0.0764573, acc 0.984375
2017-03-02T17:48:47.347134: step 12656, loss 0.189702, acc 0.9375
2017-03-02T17:48:47.423176: step 12657, loss 0.12831, acc 0.96875
2017-03-02T17:48:47.490028: step 12658, loss 0.188451, acc 0.921875
2017-03-02T17:48:47.557366: step 12659, loss 0.24211, acc 0.9375
2017-03-02T17:48:47.626490: step 12660, loss 0.25483, acc 0.9375
2017-03-02T17:48:47.692154: step 12661, loss 0.100443, acc 0.9375
2017-03-02T17:48:47.760008: step 12662, loss 0.475631, acc 0.859375
2017-03-02T17:48:47.834872: step 12663, loss 0.141552, acc 0.953125
2017-03-02T17:48:47.914113: step 12664, loss 0.137191, acc 0.9375
2017-03-02T17:48:47.982529: step 12665, loss 0.135578, acc 0.953125
2017-03-02T17:48:48.060553: step 12666, loss 0.163494, acc 0.921875
2017-03-02T17:48:48.135717: step 12667, loss 0.209048, acc 0.90625
2017-03-02T17:48:48.210861: step 12668, loss 0.189034, acc 0.921875
2017-03-02T17:48:48.280640: step 12669, loss 0.161272, acc 0.921875
2017-03-02T17:48:48.349545: step 12670, loss 0.27225, acc 0.875
2017-03-02T17:48:48.424063: step 12671, loss 0.257744, acc 0.875
2017-03-02T17:48:48.502659: step 12672, loss 0.141973, acc 0.9375
2017-03-02T17:48:48.569552: step 12673, loss 0.299715, acc 0.890625
2017-03-02T17:48:48.643662: step 12674, loss 0.123799, acc 0.921875
2017-03-02T17:48:48.720201: step 12675, loss 0.195367, acc 0.921875
2017-03-02T17:48:48.818285: step 12676, loss 0.144287, acc 0.921875
2017-03-02T17:48:48.888107: step 12677, loss 0.178355, acc 0.921875
2017-03-02T17:48:48.957390: step 12678, loss 0.0962338, acc 0.953125
2017-03-02T17:48:49.032665: step 12679, loss 0.171622, acc 0.921875
2017-03-02T17:48:49.106595: step 12680, loss 0.377977, acc 0.890625
2017-03-02T17:48:49.182711: step 12681, loss 0.110885, acc 0.953125
2017-03-02T17:48:49.258538: step 12682, loss 0.130252, acc 0.96875
2017-03-02T17:48:49.332416: step 12683, loss 0.144477, acc 0.9375
2017-03-02T17:48:49.407412: step 12684, loss 0.160007, acc 0.9375
2017-03-02T17:48:49.484186: step 12685, loss 0.096808, acc 0.96875
2017-03-02T17:48:49.553072: step 12686, loss 0.164298, acc 0.96875
2017-03-02T17:48:49.625351: step 12687, loss 0.319946, acc 0.90625
2017-03-02T17:48:49.697383: step 12688, loss 0.0892708, acc 0.96875
2017-03-02T17:48:49.774221: step 12689, loss 0.170141, acc 0.9375
2017-03-02T17:48:49.859070: step 12690, loss 0.173762, acc 0.921875
2017-03-02T17:48:49.933276: step 12691, loss 0.0781299, acc 0.953125
2017-03-02T17:48:50.008635: step 12692, loss 0.0486528, acc 0.96875
2017-03-02T17:48:50.084769: step 12693, loss 0.0609692, acc 0.984375
2017-03-02T17:48:50.157398: step 12694, loss 0.255047, acc 0.890625
2017-03-02T17:48:50.245977: step 12695, loss 0.232393, acc 0.90625
2017-03-02T17:48:50.309215: step 12696, loss 0.188972, acc 0.90625
2017-03-02T17:48:50.381515: step 12697, loss 0.236669, acc 0.90625
2017-03-02T17:48:50.457590: step 12698, loss 0.151663, acc 0.9375
2017-03-02T17:48:50.533059: step 12699, loss 0.107495, acc 0.953125
2017-03-02T17:48:50.607578: step 12700, loss 0.228794, acc 0.90625

Evaluation:
2017-03-02T17:48:50.645379: step 12700, loss 1.63801, acc 0.660418

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12700

2017-03-02T17:48:51.129009: step 12701, loss 0.0617982, acc 0.984375
2017-03-02T17:48:51.225194: step 12702, loss 0.126631, acc 0.9375
2017-03-02T17:48:51.306052: step 12703, loss 0.156253, acc 0.921875
2017-03-02T17:48:51.377674: step 12704, loss 0.223493, acc 0.875
2017-03-02T17:48:51.453207: step 12705, loss 0.214941, acc 0.9375
2017-03-02T17:48:51.525167: step 12706, loss 0.0965267, acc 0.953125
2017-03-02T17:48:51.597504: step 12707, loss 0.0886168, acc 0.96875
2017-03-02T17:48:51.667513: step 12708, loss 0.0733676, acc 0.953125
2017-03-02T17:48:51.739260: step 12709, loss 0.180666, acc 0.90625
2017-03-02T17:48:51.808403: step 12710, loss 0.244596, acc 0.90625
2017-03-02T17:48:51.886782: step 12711, loss 0.120749, acc 0.9375
2017-03-02T17:48:51.955234: step 12712, loss 0.0903344, acc 0.953125
2017-03-02T17:48:52.028894: step 12713, loss 0.192117, acc 0.9375
2017-03-02T17:48:52.094770: step 12714, loss 0.215328, acc 0.890625
2017-03-02T17:48:52.175821: step 12715, loss 0.244779, acc 0.859375
2017-03-02T17:48:52.245710: step 12716, loss 0.160192, acc 0.9375
2017-03-02T17:48:52.321548: step 12717, loss 0.168344, acc 0.921875
2017-03-02T17:48:52.397162: step 12718, loss 0.232589, acc 0.921875
2017-03-02T17:48:52.465114: step 12719, loss 0.250786, acc 0.921875
2017-03-02T17:48:52.549092: step 12720, loss 0.230104, acc 0.890625
2017-03-02T17:48:52.626492: step 12721, loss 0.0772104, acc 0.96875
2017-03-02T17:48:52.706709: step 12722, loss 0.101793, acc 0.953125
2017-03-02T17:48:52.782107: step 12723, loss 0.168766, acc 0.96875
2017-03-02T17:48:52.855025: step 12724, loss 0.13673, acc 0.9375
2017-03-02T17:48:52.932577: step 12725, loss 0.19368, acc 0.921875
2017-03-02T17:48:53.010680: step 12726, loss 0.202031, acc 0.90625
2017-03-02T17:48:53.084583: step 12727, loss 0.279403, acc 0.890625
2017-03-02T17:48:53.151453: step 12728, loss 0.120301, acc 0.9375
2017-03-02T17:48:53.224159: step 12729, loss 0.195162, acc 0.953125
2017-03-02T17:48:53.297724: step 12730, loss 0.191168, acc 0.921875
2017-03-02T17:48:53.371881: step 12731, loss 0.136463, acc 0.921875
2017-03-02T17:48:53.448725: step 12732, loss 0.198053, acc 0.921875
2017-03-02T17:48:53.523790: step 12733, loss 0.121844, acc 0.921875
2017-03-02T17:48:53.602218: step 12734, loss 0.227661, acc 0.875
2017-03-02T17:48:53.680448: step 12735, loss 0.272466, acc 0.859375
2017-03-02T17:48:53.755713: step 12736, loss 0.188644, acc 0.9375
2017-03-02T17:48:53.823990: step 12737, loss 0.155422, acc 0.9375
2017-03-02T17:48:53.899096: step 12738, loss 0.206454, acc 0.921875
2017-03-02T17:48:53.992165: step 12739, loss 0.103855, acc 0.9375
2017-03-02T17:48:54.060942: step 12740, loss 0.00461638, acc 1
2017-03-02T17:48:54.139655: step 12741, loss 0.163732, acc 0.9375
2017-03-02T17:48:54.212761: step 12742, loss 0.156385, acc 0.9375
2017-03-02T17:48:54.288566: step 12743, loss 0.217878, acc 0.90625
2017-03-02T17:48:54.362780: step 12744, loss 0.102984, acc 0.96875
2017-03-02T17:48:54.431983: step 12745, loss 0.10648, acc 0.953125
2017-03-02T17:48:54.498777: step 12746, loss 0.250951, acc 0.875
2017-03-02T17:48:54.572674: step 12747, loss 0.224802, acc 0.859375
2017-03-02T17:48:54.663502: step 12748, loss 0.231165, acc 0.90625
2017-03-02T17:48:54.740408: step 12749, loss 0.135884, acc 0.953125
2017-03-02T17:48:54.813416: step 12750, loss 0.157119, acc 0.9375
2017-03-02T17:48:54.889655: step 12751, loss 0.158821, acc 0.9375
2017-03-02T17:48:54.960732: step 12752, loss 0.0750311, acc 0.984375
2017-03-02T17:48:55.037750: step 12753, loss 0.179418, acc 0.921875
2017-03-02T17:48:55.119485: step 12754, loss 0.0716016, acc 1
2017-03-02T17:48:55.186273: step 12755, loss 0.124571, acc 0.96875
2017-03-02T17:48:55.257549: step 12756, loss 0.0748899, acc 0.953125
2017-03-02T17:48:55.327613: step 12757, loss 0.161017, acc 0.921875
2017-03-02T17:48:55.397970: step 12758, loss 0.132005, acc 0.921875
2017-03-02T17:48:55.473541: step 12759, loss 0.297508, acc 0.921875
2017-03-02T17:48:55.554198: step 12760, loss 0.140112, acc 0.9375
2017-03-02T17:48:55.628524: step 12761, loss 0.157529, acc 0.953125
2017-03-02T17:48:55.702746: step 12762, loss 0.203085, acc 0.875
2017-03-02T17:48:55.777927: step 12763, loss 0.125303, acc 0.953125
2017-03-02T17:48:55.844959: step 12764, loss 0.143145, acc 0.921875
2017-03-02T17:48:55.920971: step 12765, loss 0.151849, acc 0.953125
2017-03-02T17:48:55.998088: step 12766, loss 0.185976, acc 0.90625
2017-03-02T17:48:56.072685: step 12767, loss 0.215795, acc 0.90625
2017-03-02T17:48:56.149631: step 12768, loss 0.103574, acc 0.953125
2017-03-02T17:48:56.226762: step 12769, loss 0.124753, acc 0.96875
2017-03-02T17:48:56.305879: step 12770, loss 0.169495, acc 0.9375
2017-03-02T17:48:56.381954: step 12771, loss 0.244285, acc 0.875
2017-03-02T17:48:56.472212: step 12772, loss 0.235662, acc 0.875
2017-03-02T17:48:56.543641: step 12773, loss 0.179757, acc 0.90625
2017-03-02T17:48:56.610738: step 12774, loss 0.0908539, acc 0.96875
2017-03-02T17:48:56.687148: step 12775, loss 0.241022, acc 0.921875
2017-03-02T17:48:56.764988: step 12776, loss 0.122327, acc 0.9375
2017-03-02T17:48:56.838211: step 12777, loss 0.131325, acc 0.9375
2017-03-02T17:48:56.908982: step 12778, loss 0.201376, acc 0.90625
2017-03-02T17:48:56.988066: step 12779, loss 0.0862618, acc 0.96875
2017-03-02T17:48:57.061090: step 12780, loss 0.132777, acc 0.96875
2017-03-02T17:48:57.143027: step 12781, loss 0.141714, acc 0.921875
2017-03-02T17:48:57.213276: step 12782, loss 0.105668, acc 0.953125
2017-03-02T17:48:57.281497: step 12783, loss 0.0385504, acc 1
2017-03-02T17:48:57.358777: step 12784, loss 0.095103, acc 0.96875
2017-03-02T17:48:57.434900: step 12785, loss 0.0319764, acc 1
2017-03-02T17:48:57.518598: step 12786, loss 0.127555, acc 0.953125
2017-03-02T17:48:57.591854: step 12787, loss 0.111932, acc 0.953125
2017-03-02T17:48:57.666471: step 12788, loss 0.126217, acc 0.9375
2017-03-02T17:48:57.744139: step 12789, loss 0.20958, acc 0.890625
2017-03-02T17:48:57.821830: step 12790, loss 0.0533423, acc 1
2017-03-02T17:48:57.893172: step 12791, loss 0.156109, acc 0.9375
2017-03-02T17:48:57.966121: step 12792, loss 0.345628, acc 0.859375
2017-03-02T17:48:58.038025: step 12793, loss 0.064738, acc 0.984375
2017-03-02T17:48:58.112180: step 12794, loss 0.127485, acc 0.953125
2017-03-02T17:48:58.188454: step 12795, loss 0.103566, acc 0.953125
2017-03-02T17:48:58.263105: step 12796, loss 0.11218, acc 0.953125
2017-03-02T17:48:58.337032: step 12797, loss 0.216018, acc 0.9375
2017-03-02T17:48:58.403056: step 12798, loss 0.0798414, acc 0.984375
2017-03-02T17:48:58.475507: step 12799, loss 0.0765088, acc 0.984375
2017-03-02T17:48:58.543661: step 12800, loss 0.12174, acc 0.96875

Evaluation:
2017-03-02T17:48:58.569322: step 12800, loss 1.66523, acc 0.669791

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12800

2017-03-02T17:48:59.014521: step 12801, loss 0.156511, acc 0.9375
2017-03-02T17:48:59.083437: step 12802, loss 0.168678, acc 0.921875
2017-03-02T17:48:59.155032: step 12803, loss 0.260592, acc 0.90625
2017-03-02T17:48:59.225800: step 12804, loss 0.182625, acc 0.921875
2017-03-02T17:48:59.296439: step 12805, loss 0.215964, acc 0.890625
2017-03-02T17:48:59.365822: step 12806, loss 0.11968, acc 0.953125
2017-03-02T17:48:59.436420: step 12807, loss 0.209526, acc 0.90625
2017-03-02T17:48:59.507311: step 12808, loss 0.169479, acc 0.921875
2017-03-02T17:48:59.584545: step 12809, loss 0.135125, acc 0.953125
2017-03-02T17:48:59.676379: step 12810, loss 0.0941287, acc 0.96875
2017-03-02T17:48:59.749467: step 12811, loss 0.143012, acc 0.953125
2017-03-02T17:48:59.824452: step 12812, loss 0.259025, acc 0.890625
2017-03-02T17:48:59.907876: step 12813, loss 0.194514, acc 0.921875
2017-03-02T17:48:59.978007: step 12814, loss 0.245345, acc 0.90625
2017-03-02T17:49:00.047646: step 12815, loss 0.154961, acc 0.921875
2017-03-02T17:49:00.121585: step 12816, loss 0.241322, acc 0.875
2017-03-02T17:49:00.194922: step 12817, loss 0.2733, acc 0.9375
2017-03-02T17:49:00.269062: step 12818, loss 0.198831, acc 0.9375
2017-03-02T17:49:00.342824: step 12819, loss 0.214374, acc 0.890625
2017-03-02T17:49:00.413904: step 12820, loss 0.185136, acc 0.90625
2017-03-02T17:49:00.485125: step 12821, loss 0.139189, acc 0.953125
2017-03-02T17:49:00.561836: step 12822, loss 0.084365, acc 0.96875
2017-03-02T17:49:00.629452: step 12823, loss 0.204903, acc 0.921875
2017-03-02T17:49:00.700412: step 12824, loss 0.245598, acc 0.875
2017-03-02T17:49:00.776742: step 12825, loss 0.167243, acc 0.875
2017-03-02T17:49:00.840095: step 12826, loss 0.178694, acc 0.921875
2017-03-02T17:49:00.909542: step 12827, loss 0.12895, acc 0.953125
2017-03-02T17:49:00.977946: step 12828, loss 0.108066, acc 0.96875
2017-03-02T17:49:01.050878: step 12829, loss 0.195796, acc 0.890625
2017-03-02T17:49:01.118401: step 12830, loss 0.122585, acc 0.9375
2017-03-02T17:49:01.206052: step 12831, loss 0.150069, acc 0.921875
2017-03-02T17:49:01.277450: step 12832, loss 0.290004, acc 0.90625
2017-03-02T17:49:01.356704: step 12833, loss 0.0365743, acc 1
2017-03-02T17:49:01.428527: step 12834, loss 0.214921, acc 0.9375
2017-03-02T17:49:01.496977: step 12835, loss 0.216701, acc 0.9375
2017-03-02T17:49:01.571185: step 12836, loss 0.0889265, acc 0.96875
2017-03-02T17:49:01.640962: step 12837, loss 0.206094, acc 0.9375
2017-03-02T17:49:01.723165: step 12838, loss 0.336712, acc 0.890625
2017-03-02T17:49:01.798191: step 12839, loss 0.186753, acc 0.9375
2017-03-02T17:49:01.866184: step 12840, loss 0.133262, acc 0.921875
2017-03-02T17:49:01.940380: step 12841, loss 0.147411, acc 0.9375
2017-03-02T17:49:02.010763: step 12842, loss 0.207393, acc 0.90625
2017-03-02T17:49:02.081493: step 12843, loss 0.29749, acc 0.859375
2017-03-02T17:49:02.159241: step 12844, loss 0.144109, acc 0.96875
2017-03-02T17:49:02.240339: step 12845, loss 0.0683137, acc 0.984375
2017-03-02T17:49:02.349353: step 12846, loss 0.0636535, acc 1
2017-03-02T17:49:02.418550: step 12847, loss 0.15043, acc 0.9375
2017-03-02T17:49:02.496687: step 12848, loss 0.160398, acc 0.953125
2017-03-02T17:49:02.578438: step 12849, loss 0.101183, acc 0.96875
2017-03-02T17:49:02.649474: step 12850, loss 0.137264, acc 0.90625
2017-03-02T17:49:02.731865: step 12851, loss 0.196082, acc 0.9375
2017-03-02T17:49:02.801019: step 12852, loss 0.18516, acc 0.921875
2017-03-02T17:49:02.873169: step 12853, loss 0.191318, acc 0.890625
2017-03-02T17:49:02.947996: step 12854, loss 0.272905, acc 0.921875
2017-03-02T17:49:03.024448: step 12855, loss 0.22512, acc 0.890625
2017-03-02T17:49:03.094052: step 12856, loss 0.157992, acc 0.921875
2017-03-02T17:49:03.151943: step 12857, loss 0.0657398, acc 0.96875
2017-03-02T17:49:03.223435: step 12858, loss 0.152983, acc 0.9375
2017-03-02T17:49:03.297831: step 12859, loss 0.255455, acc 0.90625
2017-03-02T17:49:03.366513: step 12860, loss 0.162864, acc 0.921875
2017-03-02T17:49:03.434430: step 12861, loss 0.0753715, acc 1
2017-03-02T17:49:03.507635: step 12862, loss 0.113376, acc 0.9375
2017-03-02T17:49:03.579475: step 12863, loss 0.295667, acc 0.890625
2017-03-02T17:49:03.651886: step 12864, loss 0.30331, acc 0.859375
2017-03-02T17:49:03.726701: step 12865, loss 0.128087, acc 0.921875
2017-03-02T17:49:03.800304: step 12866, loss 0.154213, acc 0.9375
2017-03-02T17:49:03.878570: step 12867, loss 0.231391, acc 0.90625
2017-03-02T17:49:03.953759: step 12868, loss 0.203912, acc 0.921875
2017-03-02T17:49:04.014481: step 12869, loss 0.0768085, acc 0.96875
2017-03-02T17:49:04.092075: step 12870, loss 0.137971, acc 0.9375
2017-03-02T17:49:04.151060: step 12871, loss 0.127625, acc 0.953125
2017-03-02T17:49:04.228166: step 12872, loss 0.224135, acc 0.90625
2017-03-02T17:49:04.303776: step 12873, loss 0.152408, acc 0.9375
2017-03-02T17:49:04.376940: step 12874, loss 0.255634, acc 0.921875
2017-03-02T17:49:04.454500: step 12875, loss 0.274798, acc 0.875
2017-03-02T17:49:04.530500: step 12876, loss 0.296384, acc 0.921875
2017-03-02T17:49:04.601817: step 12877, loss 0.122517, acc 0.953125
2017-03-02T17:49:04.684381: step 12878, loss 0.107916, acc 0.9375
2017-03-02T17:49:04.756414: step 12879, loss 0.0954711, acc 0.953125
2017-03-02T17:49:04.829797: step 12880, loss 0.0622491, acc 0.984375
2017-03-02T17:49:04.900355: step 12881, loss 0.14189, acc 0.90625
2017-03-02T17:49:04.985554: step 12882, loss 0.261444, acc 0.890625
2017-03-02T17:49:05.057994: step 12883, loss 0.135062, acc 0.9375
2017-03-02T17:49:05.133291: step 12884, loss 0.251417, acc 0.875
2017-03-02T17:49:05.211255: step 12885, loss 0.146075, acc 0.953125
2017-03-02T17:49:05.283645: step 12886, loss 0.222999, acc 0.921875
2017-03-02T17:49:05.355852: step 12887, loss 0.158202, acc 0.96875
2017-03-02T17:49:05.441044: step 12888, loss 0.101922, acc 0.96875
2017-03-02T17:49:05.512695: step 12889, loss 0.264517, acc 0.90625
2017-03-02T17:49:05.581709: step 12890, loss 0.136242, acc 0.921875
2017-03-02T17:49:05.656895: step 12891, loss 0.0794313, acc 0.96875
2017-03-02T17:49:05.734725: step 12892, loss 0.133012, acc 0.9375
2017-03-02T17:49:05.807999: step 12893, loss 0.24352, acc 0.90625
2017-03-02T17:49:05.885469: step 12894, loss 0.171816, acc 0.90625
2017-03-02T17:49:05.952786: step 12895, loss 0.0465682, acc 0.984375
2017-03-02T17:49:06.019798: step 12896, loss 0.0960095, acc 0.953125
2017-03-02T17:49:06.093027: step 12897, loss 0.0760694, acc 0.96875
2017-03-02T17:49:06.171832: step 12898, loss 0.21723, acc 0.921875
2017-03-02T17:49:06.243639: step 12899, loss 0.233334, acc 0.890625
2017-03-02T17:49:06.309444: step 12900, loss 0.150233, acc 0.9375

Evaluation:
2017-03-02T17:49:06.339676: step 12900, loss 1.63092, acc 0.652487

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-12900

2017-03-02T17:49:06.793568: step 12901, loss 0.293845, acc 0.890625
2017-03-02T17:49:06.854854: step 12902, loss 0.268962, acc 0.921875
2017-03-02T17:49:06.934538: step 12903, loss 0.198481, acc 0.921875
2017-03-02T17:49:07.013668: step 12904, loss 0.113335, acc 0.96875
2017-03-02T17:49:07.086938: step 12905, loss 0.176555, acc 0.921875
2017-03-02T17:49:07.173155: step 12906, loss 0.169397, acc 0.921875
2017-03-02T17:49:07.243660: step 12907, loss 0.221357, acc 0.921875
2017-03-02T17:49:07.313068: step 12908, loss 0.0563736, acc 0.96875
2017-03-02T17:49:07.392250: step 12909, loss 0.146819, acc 0.921875
2017-03-02T17:49:07.460442: step 12910, loss 0.0957094, acc 0.953125
2017-03-02T17:49:07.535630: step 12911, loss 0.144193, acc 0.9375
2017-03-02T17:49:07.603416: step 12912, loss 0.227844, acc 0.90625
2017-03-02T17:49:07.675117: step 12913, loss 0.233044, acc 0.90625
2017-03-02T17:49:07.748208: step 12914, loss 0.0517627, acc 0.984375
2017-03-02T17:49:07.817273: step 12915, loss 0.196844, acc 0.90625
2017-03-02T17:49:07.888940: step 12916, loss 0.186374, acc 0.9375
2017-03-02T17:49:07.960332: step 12917, loss 0.195106, acc 0.921875
2017-03-02T17:49:08.036597: step 12918, loss 0.127383, acc 0.953125
2017-03-02T17:49:08.108387: step 12919, loss 0.179658, acc 0.9375
2017-03-02T17:49:08.179147: step 12920, loss 0.174234, acc 0.890625
2017-03-02T17:49:08.245592: step 12921, loss 0.182024, acc 0.9375
2017-03-02T17:49:08.318803: step 12922, loss 0.287154, acc 0.890625
2017-03-02T17:49:08.389529: step 12923, loss 0.225026, acc 0.921875
2017-03-02T17:49:08.462948: step 12924, loss 0.304154, acc 0.875
2017-03-02T17:49:08.538177: step 12925, loss 0.104197, acc 0.953125
2017-03-02T17:49:08.618736: step 12926, loss 0.0970545, acc 0.96875
2017-03-02T17:49:08.693216: step 12927, loss 0.117239, acc 0.9375
2017-03-02T17:49:08.767075: step 12928, loss 0.200794, acc 0.890625
2017-03-02T17:49:08.836011: step 12929, loss 0.284999, acc 0.890625
2017-03-02T17:49:08.910220: step 12930, loss 0.194279, acc 0.890625
2017-03-02T17:49:08.981709: step 12931, loss 0.147358, acc 0.9375
2017-03-02T17:49:09.054380: step 12932, loss 0.243713, acc 0.90625
2017-03-02T17:49:09.126393: step 12933, loss 0.082169, acc 0.984375
2017-03-02T17:49:09.199644: step 12934, loss 0.0765122, acc 0.96875
2017-03-02T17:49:09.273571: step 12935, loss 0.208433, acc 0.90625
2017-03-02T17:49:09.351647: step 12936, loss 0.162754, acc 1
2017-03-02T17:49:09.426602: step 12937, loss 0.200219, acc 0.953125
2017-03-02T17:49:09.499670: step 12938, loss 0.225118, acc 0.890625
2017-03-02T17:49:09.565001: step 12939, loss 0.169577, acc 0.921875
2017-03-02T17:49:09.637996: step 12940, loss 0.16258, acc 0.96875
2017-03-02T17:49:09.727109: step 12941, loss 0.223566, acc 0.890625
2017-03-02T17:49:09.790396: step 12942, loss 0.206222, acc 0.859375
2017-03-02T17:49:09.858924: step 12943, loss 0.207155, acc 0.953125
2017-03-02T17:49:09.961579: step 12944, loss 0.0805066, acc 0.984375
2017-03-02T17:49:10.035938: step 12945, loss 0.161645, acc 0.875
2017-03-02T17:49:10.108529: step 12946, loss 0.166935, acc 0.9375
2017-03-02T17:49:10.182777: step 12947, loss 0.196248, acc 0.890625
2017-03-02T17:49:10.251227: step 12948, loss 0.246752, acc 0.921875
2017-03-02T17:49:10.336930: step 12949, loss 0.307716, acc 0.875
2017-03-02T17:49:10.414801: step 12950, loss 0.184713, acc 0.921875
2017-03-02T17:49:10.490339: step 12951, loss 0.144028, acc 0.953125
2017-03-02T17:49:10.563459: step 12952, loss 0.195392, acc 0.921875
2017-03-02T17:49:10.637404: step 12953, loss 0.142626, acc 0.9375
2017-03-02T17:49:10.708788: step 12954, loss 0.263665, acc 0.953125
2017-03-02T17:49:10.784213: step 12955, loss 0.15665, acc 0.953125
2017-03-02T17:49:10.858452: step 12956, loss 0.300408, acc 0.890625
2017-03-02T17:49:10.932907: step 12957, loss 0.186656, acc 0.90625
2017-03-02T17:49:10.999429: step 12958, loss 0.236701, acc 0.890625
2017-03-02T17:49:11.072773: step 12959, loss 0.132692, acc 0.953125
2017-03-02T17:49:11.142333: step 12960, loss 0.266054, acc 0.890625
2017-03-02T17:49:11.213526: step 12961, loss 0.112918, acc 0.953125
2017-03-02T17:49:11.285758: step 12962, loss 0.0634756, acc 1
2017-03-02T17:49:11.360638: step 12963, loss 0.124767, acc 0.953125
2017-03-02T17:49:11.431673: step 12964, loss 0.151583, acc 0.9375
2017-03-02T17:49:11.501386: step 12965, loss 0.0898572, acc 0.96875
2017-03-02T17:49:11.574955: step 12966, loss 0.252386, acc 0.90625
2017-03-02T17:49:11.646990: step 12967, loss 0.127226, acc 0.953125
2017-03-02T17:49:11.716457: step 12968, loss 0.222312, acc 0.90625
2017-03-02T17:49:11.788857: step 12969, loss 0.177068, acc 0.953125
2017-03-02T17:49:11.860592: step 12970, loss 0.221509, acc 0.890625
2017-03-02T17:49:11.934572: step 12971, loss 0.192612, acc 0.9375
2017-03-02T17:49:12.010501: step 12972, loss 0.144351, acc 0.9375
2017-03-02T17:49:12.085889: step 12973, loss 0.0746472, acc 0.984375
2017-03-02T17:49:12.159729: step 12974, loss 0.1902, acc 0.9375
2017-03-02T17:49:12.235046: step 12975, loss 0.270553, acc 0.890625
2017-03-02T17:49:12.305454: step 12976, loss 0.178396, acc 0.890625
2017-03-02T17:49:12.370988: step 12977, loss 0.0935317, acc 0.953125
2017-03-02T17:49:12.446595: step 12978, loss 0.103643, acc 0.953125
2017-03-02T17:49:12.520049: step 12979, loss 0.101962, acc 0.953125
2017-03-02T17:49:12.596168: step 12980, loss 0.100423, acc 0.953125
2017-03-02T17:49:12.664817: step 12981, loss 0.23298, acc 0.875
2017-03-02T17:49:12.730606: step 12982, loss 0.152937, acc 0.9375
2017-03-02T17:49:12.801886: step 12983, loss 0.113025, acc 0.953125
2017-03-02T17:49:12.875143: step 12984, loss 0.201564, acc 0.953125
2017-03-02T17:49:12.944746: step 12985, loss 0.0698288, acc 0.984375
2017-03-02T17:49:13.052907: step 12986, loss 0.142297, acc 0.921875
2017-03-02T17:49:13.124553: step 12987, loss 0.0703872, acc 0.984375
2017-03-02T17:49:13.204409: step 12988, loss 0.159487, acc 0.90625
2017-03-02T17:49:13.273040: step 12989, loss 0.153765, acc 0.9375
2017-03-02T17:49:13.350674: step 12990, loss 0.0844483, acc 0.984375
2017-03-02T17:49:13.431243: step 12991, loss 0.0534353, acc 0.96875
2017-03-02T17:49:13.505084: step 12992, loss 0.198232, acc 0.90625
2017-03-02T17:49:13.580759: step 12993, loss 0.123895, acc 0.9375
2017-03-02T17:49:13.655413: step 12994, loss 0.213641, acc 0.90625
2017-03-02T17:49:13.724111: step 12995, loss 0.151669, acc 0.921875
2017-03-02T17:49:13.795709: step 12996, loss 0.208202, acc 0.890625
2017-03-02T17:49:13.867340: step 12997, loss 0.153084, acc 0.9375
2017-03-02T17:49:13.938396: step 12998, loss 0.188197, acc 0.921875
2017-03-02T17:49:14.024846: step 12999, loss 0.117711, acc 0.953125
2017-03-02T17:49:14.097858: step 13000, loss 0.285744, acc 0.890625

Evaluation:
2017-03-02T17:49:14.131154: step 13000, loss 1.63103, acc 0.653208

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13000

2017-03-02T17:49:14.596791: step 13001, loss 0.161247, acc 0.9375
2017-03-02T17:49:14.669934: step 13002, loss 0.291761, acc 0.875
2017-03-02T17:49:14.739498: step 13003, loss 0.0543995, acc 0.984375
2017-03-02T17:49:14.830287: step 13004, loss 0.157802, acc 0.9375
2017-03-02T17:49:14.908668: step 13005, loss 0.132186, acc 0.921875
2017-03-02T17:49:14.980547: step 13006, loss 0.128416, acc 0.96875
2017-03-02T17:49:15.061648: step 13007, loss 0.117932, acc 0.9375
2017-03-02T17:49:15.150079: step 13008, loss 0.1115, acc 0.9375
2017-03-02T17:49:15.225354: step 13009, loss 0.223864, acc 0.875
2017-03-02T17:49:15.301748: step 13010, loss 0.19947, acc 0.953125
2017-03-02T17:49:15.377252: step 13011, loss 0.204283, acc 0.9375
2017-03-02T17:49:15.450379: step 13012, loss 0.153978, acc 0.921875
2017-03-02T17:49:15.526914: step 13013, loss 0.128954, acc 0.9375
2017-03-02T17:49:15.592891: step 13014, loss 0.0886364, acc 0.953125
2017-03-02T17:49:15.661512: step 13015, loss 0.176328, acc 0.9375
2017-03-02T17:49:15.736763: step 13016, loss 0.0822231, acc 0.96875
2017-03-02T17:49:15.802736: step 13017, loss 0.157912, acc 0.9375
2017-03-02T17:49:15.872613: step 13018, loss 0.254777, acc 0.9375
2017-03-02T17:49:15.945036: step 13019, loss 0.229539, acc 0.90625
2017-03-02T17:49:16.014318: step 13020, loss 0.11824, acc 0.953125
2017-03-02T17:49:16.086612: step 13021, loss 0.178883, acc 0.921875
2017-03-02T17:49:16.156567: step 13022, loss 0.0568719, acc 0.984375
2017-03-02T17:49:16.230533: step 13023, loss 0.171858, acc 0.90625
2017-03-02T17:49:16.292872: step 13024, loss 0.0919982, acc 0.953125
2017-03-02T17:49:16.373976: step 13025, loss 0.0816244, acc 0.984375
2017-03-02T17:49:16.444769: step 13026, loss 0.106125, acc 0.953125
2017-03-02T17:49:16.516327: step 13027, loss 0.174883, acc 0.9375
2017-03-02T17:49:16.587642: step 13028, loss 0.240417, acc 0.890625
2017-03-02T17:49:16.670235: step 13029, loss 0.209047, acc 0.921875
2017-03-02T17:49:16.743329: step 13030, loss 0.0975282, acc 0.984375
2017-03-02T17:49:16.817882: step 13031, loss 0.0656812, acc 0.984375
2017-03-02T17:49:16.890609: step 13032, loss 0.181157, acc 0.921875
2017-03-02T17:49:16.959532: step 13033, loss 0.118071, acc 0.953125
2017-03-02T17:49:17.025261: step 13034, loss 0.0954375, acc 0.96875
2017-03-02T17:49:17.101340: step 13035, loss 0.130493, acc 0.9375
2017-03-02T17:49:17.170813: step 13036, loss 0.214253, acc 0.921875
2017-03-02T17:49:17.244876: step 13037, loss 0.271708, acc 0.890625
2017-03-02T17:49:17.318602: step 13038, loss 0.12887, acc 0.953125
2017-03-02T17:49:17.394365: step 13039, loss 0.145984, acc 0.9375
2017-03-02T17:49:17.465054: step 13040, loss 0.176036, acc 0.953125
2017-03-02T17:49:17.537044: step 13041, loss 0.197807, acc 0.90625
2017-03-02T17:49:17.607103: step 13042, loss 0.195603, acc 0.90625
2017-03-02T17:49:17.691069: step 13043, loss 0.191076, acc 0.90625
2017-03-02T17:49:17.770955: step 13044, loss 0.19361, acc 0.9375
2017-03-02T17:49:17.845357: step 13045, loss 0.13458, acc 0.953125
2017-03-02T17:49:17.916190: step 13046, loss 0.219571, acc 0.90625
2017-03-02T17:49:17.997736: step 13047, loss 0.103836, acc 0.984375
2017-03-02T17:49:18.076615: step 13048, loss 0.155562, acc 0.921875
2017-03-02T17:49:18.154444: step 13049, loss 0.237198, acc 0.9375
2017-03-02T17:49:18.227626: step 13050, loss 0.221805, acc 0.921875
2017-03-02T17:49:18.298111: step 13051, loss 0.0918577, acc 0.953125
2017-03-02T17:49:18.362026: step 13052, loss 0.261398, acc 0.890625
2017-03-02T17:49:18.436561: step 13053, loss 0.110773, acc 0.953125
2017-03-02T17:49:18.507010: step 13054, loss 0.101226, acc 0.984375
2017-03-02T17:49:18.577853: step 13055, loss 0.315988, acc 0.84375
2017-03-02T17:49:18.649055: step 13056, loss 0.123856, acc 0.96875
2017-03-02T17:49:18.721727: step 13057, loss 0.10632, acc 0.96875
2017-03-02T17:49:18.794427: step 13058, loss 0.196256, acc 0.90625
2017-03-02T17:49:18.867103: step 13059, loss 0.277608, acc 0.890625
2017-03-02T17:49:18.941335: step 13060, loss 0.19032, acc 0.890625
2017-03-02T17:49:19.012459: step 13061, loss 0.163263, acc 0.9375
2017-03-02T17:49:19.094199: step 13062, loss 0.219652, acc 0.890625
2017-03-02T17:49:19.161566: step 13063, loss 0.207469, acc 0.90625
2017-03-02T17:49:19.235841: step 13064, loss 0.124666, acc 0.9375
2017-03-02T17:49:19.313412: step 13065, loss 0.08296, acc 0.953125
2017-03-02T17:49:19.383627: step 13066, loss 0.144009, acc 0.953125
2017-03-02T17:49:19.473239: step 13067, loss 0.148484, acc 0.9375
2017-03-02T17:49:19.543851: step 13068, loss 0.165248, acc 0.90625
2017-03-02T17:49:19.618282: step 13069, loss 0.0990892, acc 0.96875
2017-03-02T17:49:19.690925: step 13070, loss 0.14087, acc 0.953125
2017-03-02T17:49:19.765535: step 13071, loss 0.238034, acc 0.875
2017-03-02T17:49:19.841003: step 13072, loss 0.171802, acc 0.921875
2017-03-02T17:49:19.908874: step 13073, loss 0.155928, acc 0.953125
2017-03-02T17:49:19.980444: step 13074, loss 0.13324, acc 0.9375
2017-03-02T17:49:20.054844: step 13075, loss 0.197147, acc 0.9375
2017-03-02T17:49:20.127943: step 13076, loss 0.0566117, acc 0.984375
2017-03-02T17:49:20.205342: step 13077, loss 0.175872, acc 0.9375
2017-03-02T17:49:20.275820: step 13078, loss 0.221438, acc 0.890625
2017-03-02T17:49:20.348764: step 13079, loss 0.132112, acc 0.953125
2017-03-02T17:49:20.423888: step 13080, loss 0.155582, acc 0.953125
2017-03-02T17:49:20.489836: step 13081, loss 0.140068, acc 0.9375
2017-03-02T17:49:20.562064: step 13082, loss 0.113025, acc 0.9375
2017-03-02T17:49:20.631453: step 13083, loss 0.361908, acc 0.875
2017-03-02T17:49:20.699901: step 13084, loss 0.241861, acc 0.875
2017-03-02T17:49:20.775410: step 13085, loss 0.148177, acc 0.921875
2017-03-02T17:49:20.849439: step 13086, loss 0.16259, acc 0.9375
2017-03-02T17:49:20.942835: step 13087, loss 0.0674049, acc 0.984375
2017-03-02T17:49:21.011347: step 13088, loss 0.055311, acc 0.984375
2017-03-02T17:49:21.082640: step 13089, loss 0.151876, acc 0.953125
2017-03-02T17:49:21.153734: step 13090, loss 0.240896, acc 0.921875
2017-03-02T17:49:21.224691: step 13091, loss 0.261739, acc 0.9375
2017-03-02T17:49:21.293365: step 13092, loss 0.105917, acc 0.96875
2017-03-02T17:49:21.360647: step 13093, loss 0.0987073, acc 0.953125
2017-03-02T17:49:21.432973: step 13094, loss 0.175242, acc 0.90625
2017-03-02T17:49:21.500785: step 13095, loss 0.139739, acc 0.9375
2017-03-02T17:49:21.573571: step 13096, loss 0.11663, acc 0.9375
2017-03-02T17:49:21.643347: step 13097, loss 0.124646, acc 0.953125
2017-03-02T17:49:21.714488: step 13098, loss 0.178073, acc 0.953125
2017-03-02T17:49:21.785893: step 13099, loss 0.297345, acc 0.875
2017-03-02T17:49:21.859975: step 13100, loss 0.252267, acc 0.859375

Evaluation:
2017-03-02T17:49:21.890868: step 13100, loss 1.65729, acc 0.66186

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13100

2017-03-02T17:49:22.373900: step 13101, loss 0.153839, acc 0.953125
2017-03-02T17:49:22.446727: step 13102, loss 0.102155, acc 0.953125
2017-03-02T17:49:22.519881: step 13103, loss 0.13589, acc 0.921875
2017-03-02T17:49:22.590855: step 13104, loss 0.150943, acc 0.953125
2017-03-02T17:49:22.658070: step 13105, loss 0.0632686, acc 0.984375
2017-03-02T17:49:22.725858: step 13106, loss 0.183576, acc 0.9375
2017-03-02T17:49:22.809989: step 13107, loss 0.132862, acc 0.953125
2017-03-02T17:49:22.882637: step 13108, loss 0.20066, acc 0.921875
2017-03-02T17:49:22.952103: step 13109, loss 0.105152, acc 0.953125
2017-03-02T17:49:23.015267: step 13110, loss 0.171776, acc 0.9375
2017-03-02T17:49:23.089000: step 13111, loss 0.195094, acc 0.921875
2017-03-02T17:49:23.156698: step 13112, loss 0.206258, acc 0.890625
2017-03-02T17:49:23.226679: step 13113, loss 0.27618, acc 0.90625
2017-03-02T17:49:23.305271: step 13114, loss 0.0899474, acc 0.96875
2017-03-02T17:49:23.379956: step 13115, loss 0.153839, acc 0.921875
2017-03-02T17:49:23.458787: step 13116, loss 0.361295, acc 0.8125
2017-03-02T17:49:23.535872: step 13117, loss 0.124386, acc 0.9375
2017-03-02T17:49:23.622026: step 13118, loss 0.186383, acc 0.921875
2017-03-02T17:49:23.703173: step 13119, loss 0.208597, acc 0.9375
2017-03-02T17:49:23.776489: step 13120, loss 0.215257, acc 0.921875
2017-03-02T17:49:23.847974: step 13121, loss 0.163714, acc 0.90625
2017-03-02T17:49:23.920093: step 13122, loss 0.135753, acc 0.953125
2017-03-02T17:49:23.995199: step 13123, loss 0.101307, acc 0.953125
2017-03-02T17:49:24.064118: step 13124, loss 0.392389, acc 0.875
2017-03-02T17:49:24.136934: step 13125, loss 0.0837502, acc 0.984375
2017-03-02T17:49:24.203448: step 13126, loss 0.242913, acc 0.875
2017-03-02T17:49:24.284520: step 13127, loss 0.0747179, acc 0.96875
2017-03-02T17:49:24.358575: step 13128, loss 0.0935568, acc 0.9375
2017-03-02T17:49:24.440458: step 13129, loss 0.191241, acc 0.953125
2017-03-02T17:49:24.521077: step 13130, loss 0.200277, acc 0.890625
2017-03-02T17:49:24.593830: step 13131, loss 0.0921913, acc 0.953125
2017-03-02T17:49:24.663785: step 13132, loss 0.000585015, acc 1
2017-03-02T17:49:24.736185: step 13133, loss 0.128795, acc 0.953125
2017-03-02T17:49:24.809740: step 13134, loss 0.156239, acc 0.953125
2017-03-02T17:49:24.892325: step 13135, loss 0.0679397, acc 0.984375
2017-03-02T17:49:24.967389: step 13136, loss 0.120972, acc 0.953125
2017-03-02T17:49:25.044597: step 13137, loss 0.150145, acc 0.90625
2017-03-02T17:49:25.121317: step 13138, loss 0.136208, acc 0.9375
2017-03-02T17:49:25.199770: step 13139, loss 0.102167, acc 0.953125
2017-03-02T17:49:25.272147: step 13140, loss 0.0665097, acc 0.96875
2017-03-02T17:49:25.347177: step 13141, loss 0.120873, acc 0.953125
2017-03-02T17:49:25.420297: step 13142, loss 0.139677, acc 0.9375
2017-03-02T17:49:25.486767: step 13143, loss 0.066532, acc 0.984375
2017-03-02T17:49:25.558610: step 13144, loss 0.129816, acc 0.9375
2017-03-02T17:49:25.627472: step 13145, loss 0.154921, acc 0.90625
2017-03-02T17:49:25.698214: step 13146, loss 0.0927689, acc 0.96875
2017-03-02T17:49:25.780882: step 13147, loss 0.322753, acc 0.859375
2017-03-02T17:49:25.854869: step 13148, loss 0.267, acc 0.890625
2017-03-02T17:49:25.932105: step 13149, loss 0.037199, acc 0.96875
2017-03-02T17:49:25.999757: step 13150, loss 0.123124, acc 0.9375
2017-03-02T17:49:26.072454: step 13151, loss 0.166384, acc 0.9375
2017-03-02T17:49:26.155128: step 13152, loss 0.139404, acc 0.921875
2017-03-02T17:49:26.225326: step 13153, loss 0.136117, acc 0.9375
2017-03-02T17:49:26.298269: step 13154, loss 0.115803, acc 0.953125
2017-03-02T17:49:26.374362: step 13155, loss 0.0900982, acc 0.953125
2017-03-02T17:49:26.440347: step 13156, loss 0.205852, acc 0.875
2017-03-02T17:49:26.511003: step 13157, loss 0.192277, acc 0.96875
2017-03-02T17:49:26.583519: step 13158, loss 0.185224, acc 0.9375
2017-03-02T17:49:26.662532: step 13159, loss 0.0799844, acc 0.984375
2017-03-02T17:49:26.731634: step 13160, loss 0.155367, acc 0.9375
2017-03-02T17:49:26.802660: step 13161, loss 0.0903813, acc 0.96875
2017-03-02T17:49:26.864096: step 13162, loss 0.181183, acc 0.90625
2017-03-02T17:49:26.933428: step 13163, loss 0.248353, acc 0.921875
2017-03-02T17:49:27.004151: step 13164, loss 0.202817, acc 0.875
2017-03-02T17:49:27.080734: step 13165, loss 0.116138, acc 0.96875
2017-03-02T17:49:27.159674: step 13166, loss 0.208986, acc 0.921875
2017-03-02T17:49:27.233844: step 13167, loss 0.0980506, acc 0.96875
2017-03-02T17:49:27.309242: step 13168, loss 0.0998184, acc 0.953125
2017-03-02T17:49:27.380950: step 13169, loss 0.109065, acc 0.96875
2017-03-02T17:49:27.460771: step 13170, loss 0.0873025, acc 0.953125
2017-03-02T17:49:27.532481: step 13171, loss 0.195916, acc 0.90625
2017-03-02T17:49:27.604746: step 13172, loss 0.121626, acc 0.953125
2017-03-02T17:49:27.680850: step 13173, loss 0.232805, acc 0.90625
2017-03-02T17:49:27.754017: step 13174, loss 0.136022, acc 0.921875
2017-03-02T17:49:27.840328: step 13175, loss 0.210317, acc 0.9375
2017-03-02T17:49:27.917005: step 13176, loss 0.0502833, acc 1
2017-03-02T17:49:27.989964: step 13177, loss 0.082174, acc 0.96875
2017-03-02T17:49:28.060062: step 13178, loss 0.181829, acc 0.890625
2017-03-02T17:49:28.132713: step 13179, loss 0.173511, acc 0.921875
2017-03-02T17:49:28.206623: step 13180, loss 0.23473, acc 0.875
2017-03-02T17:49:28.273469: step 13181, loss 0.270047, acc 0.875
2017-03-02T17:49:28.358965: step 13182, loss 0.100354, acc 0.953125
2017-03-02T17:49:28.447457: step 13183, loss 0.214799, acc 0.921875
2017-03-02T17:49:28.531600: step 13184, loss 0.105158, acc 0.9375
2017-03-02T17:49:28.605924: step 13185, loss 0.117213, acc 0.953125
2017-03-02T17:49:28.676503: step 13186, loss 0.130216, acc 0.953125
2017-03-02T17:49:28.748419: step 13187, loss 0.117735, acc 0.921875
2017-03-02T17:49:28.820368: step 13188, loss 0.149465, acc 0.921875
2017-03-02T17:49:28.893547: step 13189, loss 0.153317, acc 0.953125
2017-03-02T17:49:28.961474: step 13190, loss 0.0990458, acc 0.9375
2017-03-02T17:49:29.042657: step 13191, loss 0.120887, acc 0.9375
2017-03-02T17:49:29.117620: step 13192, loss 0.172507, acc 0.90625
2017-03-02T17:49:29.190693: step 13193, loss 0.149615, acc 0.90625
2017-03-02T17:49:29.263581: step 13194, loss 0.146156, acc 0.984375
2017-03-02T17:49:29.341228: step 13195, loss 0.122626, acc 0.953125
2017-03-02T17:49:29.414110: step 13196, loss 0.301362, acc 0.859375
2017-03-02T17:49:29.488735: step 13197, loss 0.140359, acc 0.953125
2017-03-02T17:49:29.560424: step 13198, loss 0.223698, acc 0.9375
2017-03-02T17:49:29.634132: step 13199, loss 0.144494, acc 0.953125
2017-03-02T17:49:29.701728: step 13200, loss 0.171014, acc 0.921875

Evaluation:
2017-03-02T17:49:29.740464: step 13200, loss 1.70516, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13200

2017-03-02T17:49:30.213917: step 13201, loss 0.17485, acc 0.90625
2017-03-02T17:49:30.287414: step 13202, loss 0.198721, acc 0.90625
2017-03-02T17:49:30.358167: step 13203, loss 0.214061, acc 0.90625
2017-03-02T17:49:30.442108: step 13204, loss 0.118025, acc 0.953125
2017-03-02T17:49:30.517059: step 13205, loss 0.192212, acc 0.890625
2017-03-02T17:49:30.604415: step 13206, loss 0.0894831, acc 0.953125
2017-03-02T17:49:30.674171: step 13207, loss 0.210522, acc 0.921875
2017-03-02T17:49:30.748167: step 13208, loss 0.173372, acc 0.9375
2017-03-02T17:49:30.825475: step 13209, loss 0.176835, acc 0.9375
2017-03-02T17:49:30.902400: step 13210, loss 0.0657398, acc 0.96875
2017-03-02T17:49:30.974672: step 13211, loss 0.196849, acc 0.9375
2017-03-02T17:49:31.046450: step 13212, loss 0.134991, acc 0.9375
2017-03-02T17:49:31.113157: step 13213, loss 0.205296, acc 0.9375
2017-03-02T17:49:31.188650: step 13214, loss 0.174571, acc 0.921875
2017-03-02T17:49:31.261406: step 13215, loss 0.196991, acc 0.921875
2017-03-02T17:49:31.330903: step 13216, loss 0.161537, acc 0.9375
2017-03-02T17:49:31.409526: step 13217, loss 0.154736, acc 0.890625
2017-03-02T17:49:31.483230: step 13218, loss 0.158145, acc 0.921875
2017-03-02T17:49:31.569557: step 13219, loss 0.0805401, acc 0.96875
2017-03-02T17:49:31.646449: step 13220, loss 0.100862, acc 0.953125
2017-03-02T17:49:31.723129: step 13221, loss 0.137298, acc 0.9375
2017-03-02T17:49:31.791011: step 13222, loss 0.169889, acc 0.90625
2017-03-02T17:49:31.870864: step 13223, loss 0.158603, acc 0.953125
2017-03-02T17:49:31.959275: step 13224, loss 0.132539, acc 0.953125
2017-03-02T17:49:32.033684: step 13225, loss 0.181728, acc 0.890625
2017-03-02T17:49:32.103892: step 13226, loss 0.241863, acc 0.921875
2017-03-02T17:49:32.176212: step 13227, loss 0.150748, acc 0.921875
2017-03-02T17:49:32.248433: step 13228, loss 0.137119, acc 0.9375
2017-03-02T17:49:32.323943: step 13229, loss 0.199075, acc 0.90625
2017-03-02T17:49:32.398816: step 13230, loss 0.160012, acc 0.9375
2017-03-02T17:49:32.463940: step 13231, loss 0.260287, acc 0.890625
2017-03-02T17:49:32.538116: step 13232, loss 0.176868, acc 0.90625
2017-03-02T17:49:32.621107: step 13233, loss 0.285859, acc 0.921875
2017-03-02T17:49:32.698313: step 13234, loss 0.0976555, acc 0.953125
2017-03-02T17:49:32.768893: step 13235, loss 0.0825114, acc 0.96875
2017-03-02T17:49:32.839203: step 13236, loss 0.0606526, acc 0.96875
2017-03-02T17:49:32.900063: step 13237, loss 0.241591, acc 0.921875
2017-03-02T17:49:32.974567: step 13238, loss 0.156862, acc 0.9375
2017-03-02T17:49:33.041654: step 13239, loss 0.172727, acc 0.9375
2017-03-02T17:49:33.118689: step 13240, loss 0.127386, acc 0.953125
2017-03-02T17:49:33.187659: step 13241, loss 0.173944, acc 0.890625
2017-03-02T17:49:33.256157: step 13242, loss 0.160747, acc 0.90625
2017-03-02T17:49:33.335240: step 13243, loss 0.322065, acc 0.890625
2017-03-02T17:49:33.410077: step 13244, loss 0.306378, acc 0.875
2017-03-02T17:49:33.481241: step 13245, loss 0.107247, acc 0.953125
2017-03-02T17:49:33.556115: step 13246, loss 0.17832, acc 0.921875
2017-03-02T17:49:33.629561: step 13247, loss 0.165795, acc 0.90625
2017-03-02T17:49:33.705116: step 13248, loss 0.168992, acc 0.90625
2017-03-02T17:49:33.773406: step 13249, loss 0.16371, acc 0.90625
2017-03-02T17:49:33.844527: step 13250, loss 0.151291, acc 0.9375
2017-03-02T17:49:33.918443: step 13251, loss 0.064782, acc 0.984375
2017-03-02T17:49:33.992448: step 13252, loss 0.0918665, acc 0.953125
2017-03-02T17:49:34.062513: step 13253, loss 0.1165, acc 0.953125
2017-03-02T17:49:34.133967: step 13254, loss 0.251511, acc 0.921875
2017-03-02T17:49:34.208297: step 13255, loss 0.0794047, acc 0.984375
2017-03-02T17:49:34.286133: step 13256, loss 0.0999913, acc 0.953125
2017-03-02T17:49:34.359405: step 13257, loss 0.0855983, acc 0.96875
2017-03-02T17:49:34.424370: step 13258, loss 0.244175, acc 0.921875
2017-03-02T17:49:34.493965: step 13259, loss 0.205798, acc 0.921875
2017-03-02T17:49:34.565242: step 13260, loss 0.241179, acc 0.890625
2017-03-02T17:49:34.642894: step 13261, loss 0.188439, acc 0.90625
2017-03-02T17:49:34.731411: step 13262, loss 0.196739, acc 0.953125
2017-03-02T17:49:34.805296: step 13263, loss 0.0964356, acc 0.953125
2017-03-02T17:49:34.882433: step 13264, loss 0.217894, acc 0.921875
2017-03-02T17:49:34.956196: step 13265, loss 0.139965, acc 0.953125
2017-03-02T17:49:35.033271: step 13266, loss 0.17438, acc 0.921875
2017-03-02T17:49:35.108667: step 13267, loss 0.116418, acc 0.984375
2017-03-02T17:49:35.177577: step 13268, loss 0.173663, acc 0.90625
2017-03-02T17:49:35.251928: step 13269, loss 0.30432, acc 0.890625
2017-03-02T17:49:35.324475: step 13270, loss 0.0852378, acc 0.96875
2017-03-02T17:49:35.395969: step 13271, loss 0.217867, acc 0.890625
2017-03-02T17:49:35.469780: step 13272, loss 0.216942, acc 0.890625
2017-03-02T17:49:35.541352: step 13273, loss 0.189666, acc 0.890625
2017-03-02T17:49:35.611528: step 13274, loss 0.250573, acc 0.875
2017-03-02T17:49:35.680715: step 13275, loss 0.107539, acc 0.953125
2017-03-02T17:49:35.751551: step 13276, loss 0.119475, acc 0.96875
2017-03-02T17:49:35.823177: step 13277, loss 0.105622, acc 0.953125
2017-03-02T17:49:35.891375: step 13278, loss 0.342007, acc 0.890625
2017-03-02T17:49:35.957628: step 13279, loss 0.197981, acc 0.90625
2017-03-02T17:49:36.029999: step 13280, loss 0.139539, acc 0.9375
2017-03-02T17:49:36.104805: step 13281, loss 0.107467, acc 0.953125
2017-03-02T17:49:36.174356: step 13282, loss 0.24726, acc 0.890625
2017-03-02T17:49:36.246049: step 13283, loss 0.254644, acc 0.859375
2017-03-02T17:49:36.321700: step 13284, loss 0.211201, acc 0.921875
2017-03-02T17:49:36.419863: step 13285, loss 0.09854, acc 0.953125
2017-03-02T17:49:36.505809: step 13286, loss 0.104168, acc 0.9375
2017-03-02T17:49:36.579178: step 13287, loss 0.221014, acc 0.875
2017-03-02T17:49:36.648464: step 13288, loss 0.225233, acc 0.875
2017-03-02T17:49:36.711791: step 13289, loss 0.34183, acc 0.921875
2017-03-02T17:49:36.787841: step 13290, loss 0.139776, acc 0.921875
2017-03-02T17:49:36.855136: step 13291, loss 0.256821, acc 0.9375
2017-03-02T17:49:36.933790: step 13292, loss 0.1482, acc 0.9375
2017-03-02T17:49:37.009731: step 13293, loss 0.257383, acc 0.890625
2017-03-02T17:49:37.088093: step 13294, loss 0.164182, acc 0.9375
2017-03-02T17:49:37.158818: step 13295, loss 0.138335, acc 0.953125
2017-03-02T17:49:37.232019: step 13296, loss 0.159786, acc 0.921875
2017-03-02T17:49:37.302775: step 13297, loss 0.299063, acc 0.859375
2017-03-02T17:49:37.375304: step 13298, loss 0.0901136, acc 0.953125
2017-03-02T17:49:37.447364: step 13299, loss 0.192554, acc 0.921875
2017-03-02T17:49:37.518004: step 13300, loss 0.151792, acc 0.96875

Evaluation:
2017-03-02T17:49:37.555545: step 13300, loss 1.67456, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13300

2017-03-02T17:49:38.009895: step 13301, loss 0.24015, acc 0.890625
2017-03-02T17:49:38.081977: step 13302, loss 0.159418, acc 0.9375
2017-03-02T17:49:38.156422: step 13303, loss 0.170803, acc 0.96875
2017-03-02T17:49:38.228511: step 13304, loss 0.0827892, acc 0.96875
2017-03-02T17:49:38.300044: step 13305, loss 0.168189, acc 0.953125
2017-03-02T17:49:38.378334: step 13306, loss 0.176009, acc 0.9375
2017-03-02T17:49:38.450726: step 13307, loss 0.076555, acc 0.96875
2017-03-02T17:49:38.528491: step 13308, loss 0.134063, acc 0.9375
2017-03-02T17:49:38.596578: step 13309, loss 0.252561, acc 0.890625
2017-03-02T17:49:38.669607: step 13310, loss 0.199856, acc 0.90625
2017-03-02T17:49:38.735961: step 13311, loss 0.148616, acc 0.9375
2017-03-02T17:49:38.812213: step 13312, loss 0.188671, acc 0.921875
2017-03-02T17:49:38.889303: step 13313, loss 0.204072, acc 0.90625
2017-03-02T17:49:38.960345: step 13314, loss 0.126732, acc 0.9375
2017-03-02T17:49:39.037040: step 13315, loss 0.182638, acc 0.90625
2017-03-02T17:49:39.104248: step 13316, loss 0.201145, acc 0.90625
2017-03-02T17:49:39.179393: step 13317, loss 0.188495, acc 0.890625
2017-03-02T17:49:39.247085: step 13318, loss 0.128262, acc 0.9375
2017-03-02T17:49:39.323923: step 13319, loss 0.247002, acc 0.890625
2017-03-02T17:49:39.391938: step 13320, loss 0.238772, acc 0.90625
2017-03-02T17:49:39.464645: step 13321, loss 0.164375, acc 0.9375
2017-03-02T17:49:39.535630: step 13322, loss 0.16218, acc 0.921875
2017-03-02T17:49:39.614010: step 13323, loss 0.140523, acc 0.953125
2017-03-02T17:49:39.682351: step 13324, loss 0.10784, acc 0.96875
2017-03-02T17:49:39.754323: step 13325, loss 0.280922, acc 0.90625
2017-03-02T17:49:39.825768: step 13326, loss 0.0187254, acc 1
2017-03-02T17:49:39.901821: step 13327, loss 0.105743, acc 0.96875
2017-03-02T17:49:39.974709: step 13328, loss 0.133222, acc 1
2017-03-02T17:49:40.055861: step 13329, loss 0.114418, acc 0.96875
2017-03-02T17:49:40.131545: step 13330, loss 0.154259, acc 0.953125
2017-03-02T17:49:40.208336: step 13331, loss 0.0773848, acc 0.96875
2017-03-02T17:49:40.280739: step 13332, loss 0.105853, acc 0.921875
2017-03-02T17:49:40.350653: step 13333, loss 0.111692, acc 0.953125
2017-03-02T17:49:40.429141: step 13334, loss 0.107763, acc 0.96875
2017-03-02T17:49:40.503519: step 13335, loss 0.175968, acc 0.921875
2017-03-02T17:49:40.576412: step 13336, loss 0.132305, acc 0.921875
2017-03-02T17:49:40.651214: step 13337, loss 0.181402, acc 0.9375
2017-03-02T17:49:40.725245: step 13338, loss 0.186947, acc 0.921875
2017-03-02T17:49:40.798693: step 13339, loss 0.340498, acc 0.875
2017-03-02T17:49:40.868801: step 13340, loss 0.217887, acc 0.9375
2017-03-02T17:49:40.939404: step 13341, loss 0.0794362, acc 0.984375
2017-03-02T17:49:41.012524: step 13342, loss 0.188124, acc 0.890625
2017-03-02T17:49:41.083182: step 13343, loss 0.201361, acc 0.9375
2017-03-02T17:49:41.163622: step 13344, loss 0.250428, acc 0.875
2017-03-02T17:49:41.240688: step 13345, loss 0.190964, acc 0.921875
2017-03-02T17:49:41.317767: step 13346, loss 0.193508, acc 0.953125
2017-03-02T17:49:41.394105: step 13347, loss 0.194273, acc 0.890625
2017-03-02T17:49:41.462681: step 13348, loss 0.121714, acc 0.953125
2017-03-02T17:49:41.534589: step 13349, loss 0.156979, acc 0.921875
2017-03-02T17:49:41.609882: step 13350, loss 0.17631, acc 0.9375
2017-03-02T17:49:41.682601: step 13351, loss 0.328653, acc 0.859375
2017-03-02T17:49:41.760683: step 13352, loss 0.153857, acc 0.9375
2017-03-02T17:49:41.835079: step 13353, loss 0.0984986, acc 0.953125
2017-03-02T17:49:41.908814: step 13354, loss 0.21635, acc 0.953125
2017-03-02T17:49:41.982828: step 13355, loss 0.177992, acc 0.890625
2017-03-02T17:49:42.051487: step 13356, loss 0.144147, acc 0.9375
2017-03-02T17:49:42.120885: step 13357, loss 0.155241, acc 0.9375
2017-03-02T17:49:42.200008: step 13358, loss 0.156477, acc 0.9375
2017-03-02T17:49:42.269474: step 13359, loss 0.0924167, acc 0.96875
2017-03-02T17:49:42.341125: step 13360, loss 0.109047, acc 0.953125
2017-03-02T17:49:42.414317: step 13361, loss 0.1847, acc 0.90625
2017-03-02T17:49:42.484630: step 13362, loss 0.199002, acc 0.921875
2017-03-02T17:49:42.556662: step 13363, loss 0.0801285, acc 0.953125
2017-03-02T17:49:42.628937: step 13364, loss 0.177223, acc 0.921875
2017-03-02T17:49:42.706766: step 13365, loss 0.185725, acc 0.921875
2017-03-02T17:49:42.780716: step 13366, loss 0.191474, acc 0.90625
2017-03-02T17:49:42.853543: step 13367, loss 0.174582, acc 0.96875
2017-03-02T17:49:42.946807: step 13368, loss 0.151766, acc 0.921875
2017-03-02T17:49:43.014738: step 13369, loss 0.232596, acc 0.890625
2017-03-02T17:49:43.086106: step 13370, loss 0.173422, acc 0.90625
2017-03-02T17:49:43.159857: step 13371, loss 0.224052, acc 0.875
2017-03-02T17:49:43.230107: step 13372, loss 0.136908, acc 0.9375
2017-03-02T17:49:43.333884: step 13373, loss 0.0408428, acc 1
2017-03-02T17:49:43.401807: step 13374, loss 0.238357, acc 0.890625
2017-03-02T17:49:43.468829: step 13375, loss 0.176701, acc 0.875
2017-03-02T17:49:43.545418: step 13376, loss 0.19888, acc 0.90625
2017-03-02T17:49:43.615128: step 13377, loss 0.182353, acc 0.9375
2017-03-02T17:49:43.689196: step 13378, loss 0.149632, acc 0.953125
2017-03-02T17:49:43.759468: step 13379, loss 0.157947, acc 0.953125
2017-03-02T17:49:43.833564: step 13380, loss 0.258947, acc 0.84375
2017-03-02T17:49:43.905309: step 13381, loss 0.205107, acc 0.9375
2017-03-02T17:49:43.976328: step 13382, loss 0.179161, acc 0.921875
2017-03-02T17:49:44.052735: step 13383, loss 0.11206, acc 0.953125
2017-03-02T17:49:44.119137: step 13384, loss 0.270295, acc 0.875
2017-03-02T17:49:44.199821: step 13385, loss 0.232967, acc 0.90625
2017-03-02T17:49:44.273692: step 13386, loss 0.0721581, acc 0.96875
2017-03-02T17:49:44.355364: step 13387, loss 0.129983, acc 0.953125
2017-03-02T17:49:44.437166: step 13388, loss 0.061306, acc 0.984375
2017-03-02T17:49:44.515263: step 13389, loss 0.294713, acc 0.90625
2017-03-02T17:49:44.587461: step 13390, loss 0.341041, acc 0.859375
2017-03-02T17:49:44.658690: step 13391, loss 0.150726, acc 0.9375
2017-03-02T17:49:44.733811: step 13392, loss 0.226634, acc 0.890625
2017-03-02T17:49:44.814093: step 13393, loss 0.124931, acc 0.953125
2017-03-02T17:49:44.881812: step 13394, loss 0.103698, acc 0.953125
2017-03-02T17:49:44.954894: step 13395, loss 0.202378, acc 0.90625
2017-03-02T17:49:45.024414: step 13396, loss 0.0412326, acc 1
2017-03-02T17:49:45.102341: step 13397, loss 0.15809, acc 0.9375
2017-03-02T17:49:45.173964: step 13398, loss 0.141863, acc 0.921875
2017-03-02T17:49:45.239067: step 13399, loss 0.160982, acc 0.921875
2017-03-02T17:49:45.307153: step 13400, loss 0.130682, acc 0.921875

Evaluation:
2017-03-02T17:49:45.339269: step 13400, loss 1.66984, acc 0.64672

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13400

2017-03-02T17:49:45.829637: step 13401, loss 0.186313, acc 0.96875
2017-03-02T17:49:45.906391: step 13402, loss 0.12296, acc 0.953125
2017-03-02T17:49:45.993895: step 13403, loss 0.116648, acc 0.953125
2017-03-02T17:49:46.064020: step 13404, loss 0.117688, acc 0.9375
2017-03-02T17:49:46.145796: step 13405, loss 0.183677, acc 0.890625
2017-03-02T17:49:46.217012: step 13406, loss 0.209506, acc 0.921875
2017-03-02T17:49:46.289829: step 13407, loss 0.176705, acc 0.90625
2017-03-02T17:49:46.361606: step 13408, loss 0.0834549, acc 0.96875
2017-03-02T17:49:46.432102: step 13409, loss 0.166699, acc 0.921875
2017-03-02T17:49:46.505428: step 13410, loss 0.182483, acc 0.9375
2017-03-02T17:49:46.578483: step 13411, loss 0.141281, acc 0.921875
2017-03-02T17:49:46.648357: step 13412, loss 0.178024, acc 0.9375
2017-03-02T17:49:46.723713: step 13413, loss 0.151522, acc 0.921875
2017-03-02T17:49:46.795810: step 13414, loss 0.0632966, acc 0.96875
2017-03-02T17:49:46.864010: step 13415, loss 0.130079, acc 0.953125
2017-03-02T17:49:46.933848: step 13416, loss 0.18424, acc 0.90625
2017-03-02T17:49:47.031733: step 13417, loss 0.119826, acc 0.9375
2017-03-02T17:49:47.104620: step 13418, loss 0.162368, acc 0.9375
2017-03-02T17:49:47.181752: step 13419, loss 0.137796, acc 0.921875
2017-03-02T17:49:47.252597: step 13420, loss 0.11767, acc 0.96875
2017-03-02T17:49:47.323692: step 13421, loss 0.215824, acc 0.90625
2017-03-02T17:49:47.402067: step 13422, loss 0.0772815, acc 0.96875
2017-03-02T17:49:47.477329: step 13423, loss 0.190793, acc 0.921875
2017-03-02T17:49:47.545659: step 13424, loss 0.132899, acc 0.921875
2017-03-02T17:49:47.623612: step 13425, loss 0.237161, acc 0.921875
2017-03-02T17:49:47.694540: step 13426, loss 0.106355, acc 0.96875
2017-03-02T17:49:47.765377: step 13427, loss 0.132922, acc 0.953125
2017-03-02T17:49:47.839816: step 13428, loss 0.149809, acc 0.9375
2017-03-02T17:49:47.910177: step 13429, loss 0.154389, acc 0.953125
2017-03-02T17:49:47.986726: step 13430, loss 0.258927, acc 0.90625
2017-03-02T17:49:48.062134: step 13431, loss 0.104629, acc 0.953125
2017-03-02T17:49:48.140994: step 13432, loss 0.182356, acc 0.9375
2017-03-02T17:49:48.219128: step 13433, loss 0.144675, acc 0.953125
2017-03-02T17:49:48.289457: step 13434, loss 0.107629, acc 0.9375
2017-03-02T17:49:48.357479: step 13435, loss 0.174774, acc 0.921875
2017-03-02T17:49:48.429319: step 13436, loss 0.109262, acc 0.9375
2017-03-02T17:49:48.502361: step 13437, loss 0.183362, acc 0.9375
2017-03-02T17:49:48.580195: step 13438, loss 0.0317816, acc 1
2017-03-02T17:49:48.657029: step 13439, loss 0.111997, acc 0.96875
2017-03-02T17:49:48.745159: step 13440, loss 0.127908, acc 0.9375
2017-03-02T17:49:48.828771: step 13441, loss 0.205737, acc 0.890625
2017-03-02T17:49:48.905329: step 13442, loss 0.23795, acc 0.875
2017-03-02T17:49:48.984826: step 13443, loss 0.167785, acc 0.9375
2017-03-02T17:49:49.058833: step 13444, loss 0.180629, acc 0.921875
2017-03-02T17:49:49.131423: step 13445, loss 0.273854, acc 0.875
2017-03-02T17:49:49.204410: step 13446, loss 0.120433, acc 0.921875
2017-03-02T17:49:49.289439: step 13447, loss 0.227692, acc 0.890625
2017-03-02T17:49:49.361789: step 13448, loss 0.25815, acc 0.921875
2017-03-02T17:49:49.434690: step 13449, loss 0.0680097, acc 0.96875
2017-03-02T17:49:49.505409: step 13450, loss 0.121109, acc 0.9375
2017-03-02T17:49:49.583275: step 13451, loss 0.0549543, acc 0.984375
2017-03-02T17:49:49.654803: step 13452, loss 0.128188, acc 0.953125
2017-03-02T17:49:49.728878: step 13453, loss 0.166672, acc 0.90625
2017-03-02T17:49:49.804369: step 13454, loss 0.274119, acc 0.921875
2017-03-02T17:49:49.885085: step 13455, loss 0.276079, acc 0.890625
2017-03-02T17:49:49.961801: step 13456, loss 0.166648, acc 0.953125
2017-03-02T17:49:50.034580: step 13457, loss 0.0914229, acc 0.984375
2017-03-02T17:49:50.108764: step 13458, loss 0.0699592, acc 1
2017-03-02T17:49:50.183740: step 13459, loss 0.221657, acc 0.9375
2017-03-02T17:49:50.263466: step 13460, loss 0.186015, acc 0.921875
2017-03-02T17:49:50.331689: step 13461, loss 0.219205, acc 0.890625
2017-03-02T17:49:50.400318: step 13462, loss 0.154928, acc 0.953125
2017-03-02T17:49:50.479414: step 13463, loss 0.174081, acc 0.921875
2017-03-02T17:49:50.552987: step 13464, loss 0.149047, acc 0.921875
2017-03-02T17:49:50.626085: step 13465, loss 0.171104, acc 0.9375
2017-03-02T17:49:50.700933: step 13466, loss 0.205496, acc 0.921875
2017-03-02T17:49:50.776022: step 13467, loss 0.180892, acc 0.921875
2017-03-02T17:49:50.865383: step 13468, loss 0.183101, acc 0.9375
2017-03-02T17:49:50.941483: step 13469, loss 0.151194, acc 0.953125
2017-03-02T17:49:51.015641: step 13470, loss 0.169762, acc 0.90625
2017-03-02T17:49:51.085501: step 13471, loss 0.123249, acc 0.953125
2017-03-02T17:49:51.155525: step 13472, loss 0.0983215, acc 0.9375
2017-03-02T17:49:51.232077: step 13473, loss 0.0595156, acc 0.96875
2017-03-02T17:49:51.311244: step 13474, loss 0.0934054, acc 0.96875
2017-03-02T17:49:51.388884: step 13475, loss 0.164511, acc 0.953125
2017-03-02T17:49:51.494541: step 13476, loss 0.0995781, acc 0.9375
2017-03-02T17:49:51.596500: step 13477, loss 0.2049, acc 0.90625
2017-03-02T17:49:51.670141: step 13478, loss 0.181397, acc 0.921875
2017-03-02T17:49:51.737185: step 13479, loss 0.100899, acc 0.96875
2017-03-02T17:49:51.805628: step 13480, loss 0.0908997, acc 0.953125
2017-03-02T17:49:51.875873: step 13481, loss 0.194575, acc 0.90625
2017-03-02T17:49:51.950169: step 13482, loss 0.116933, acc 0.9375
2017-03-02T17:49:52.028073: step 13483, loss 0.154481, acc 0.921875
2017-03-02T17:49:52.106629: step 13484, loss 0.13783, acc 0.9375
2017-03-02T17:49:52.177978: step 13485, loss 0.245069, acc 0.890625
2017-03-02T17:49:52.252712: step 13486, loss 0.16586, acc 0.96875
2017-03-02T17:49:52.329860: step 13487, loss 0.133538, acc 0.9375
2017-03-02T17:49:52.415804: step 13488, loss 0.320417, acc 0.875
2017-03-02T17:49:52.484814: step 13489, loss 0.209665, acc 0.859375
2017-03-02T17:49:52.559418: step 13490, loss 0.12448, acc 0.921875
2017-03-02T17:49:52.647151: step 13491, loss 0.173415, acc 0.9375
2017-03-02T17:49:52.721742: step 13492, loss 0.13818, acc 0.9375
2017-03-02T17:49:52.804572: step 13493, loss 0.176974, acc 0.890625
2017-03-02T17:49:52.884839: step 13494, loss 0.241646, acc 0.9375
2017-03-02T17:49:52.958162: step 13495, loss 0.185783, acc 0.890625
2017-03-02T17:49:53.030610: step 13496, loss 0.104415, acc 0.953125
2017-03-02T17:49:53.109658: step 13497, loss 0.156476, acc 0.953125
2017-03-02T17:49:53.180427: step 13498, loss 0.119565, acc 0.9375
2017-03-02T17:49:53.249247: step 13499, loss 0.209194, acc 0.90625
2017-03-02T17:49:53.323392: step 13500, loss 0.259694, acc 0.890625

Evaluation:
2017-03-02T17:49:53.361749: step 13500, loss 1.69024, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13500

2017-03-02T17:49:53.830966: step 13501, loss 0.194731, acc 0.953125
2017-03-02T17:49:53.899747: step 13502, loss 0.0816949, acc 0.953125
2017-03-02T17:49:53.970793: step 13503, loss 0.366437, acc 0.90625
2017-03-02T17:49:54.049479: step 13504, loss 0.12144, acc 0.96875
2017-03-02T17:49:54.123849: step 13505, loss 0.0700305, acc 0.984375
2017-03-02T17:49:54.206850: step 13506, loss 0.136675, acc 0.953125
2017-03-02T17:49:54.279051: step 13507, loss 0.109713, acc 0.9375
2017-03-02T17:49:54.354878: step 13508, loss 0.13679, acc 0.921875
2017-03-02T17:49:54.431139: step 13509, loss 0.322747, acc 0.875
2017-03-02T17:49:54.506300: step 13510, loss 0.163485, acc 0.90625
2017-03-02T17:49:54.575876: step 13511, loss 0.104127, acc 0.96875
2017-03-02T17:49:54.647262: step 13512, loss 0.260163, acc 0.90625
2017-03-02T17:49:54.723526: step 13513, loss 0.0897417, acc 0.953125
2017-03-02T17:49:54.796215: step 13514, loss 0.130518, acc 0.9375
2017-03-02T17:49:54.866537: step 13515, loss 0.0874926, acc 0.953125
2017-03-02T17:49:54.942421: step 13516, loss 0.191572, acc 0.921875
2017-03-02T17:49:55.019851: step 13517, loss 0.134981, acc 0.953125
2017-03-02T17:49:55.088222: step 13518, loss 0.169261, acc 0.90625
2017-03-02T17:49:55.164823: step 13519, loss 0.10927, acc 0.953125
2017-03-02T17:49:55.228739: step 13520, loss 0.21434, acc 0.890625
2017-03-02T17:49:55.303889: step 13521, loss 0.147982, acc 0.953125
2017-03-02T17:49:55.379106: step 13522, loss 0.182093, acc 0.9375
2017-03-02T17:49:55.459183: step 13523, loss 0.16141, acc 0.9375
2017-03-02T17:49:55.527220: step 13524, loss 0.13514, acc 1
2017-03-02T17:49:55.601377: step 13525, loss 0.283264, acc 0.890625
2017-03-02T17:49:55.674379: step 13526, loss 0.205156, acc 0.921875
2017-03-02T17:49:55.749637: step 13527, loss 0.0994116, acc 0.96875
2017-03-02T17:49:55.822634: step 13528, loss 0.167712, acc 0.90625
2017-03-02T17:49:55.899977: step 13529, loss 0.18072, acc 0.9375
2017-03-02T17:49:55.967065: step 13530, loss 0.108167, acc 0.953125
2017-03-02T17:49:56.034686: step 13531, loss 0.124198, acc 0.96875
2017-03-02T17:49:56.104456: step 13532, loss 0.215305, acc 0.890625
2017-03-02T17:49:56.181820: step 13533, loss 0.1203, acc 0.9375
2017-03-02T17:49:56.250366: step 13534, loss 0.102957, acc 0.9375
2017-03-02T17:49:56.334699: step 13535, loss 0.0957931, acc 0.984375
2017-03-02T17:49:56.414567: step 13536, loss 0.165934, acc 0.921875
2017-03-02T17:49:56.486828: step 13537, loss 0.0925855, acc 0.953125
2017-03-02T17:49:56.555405: step 13538, loss 0.199304, acc 0.90625
2017-03-02T17:49:56.626910: step 13539, loss 0.0927206, acc 0.9375
2017-03-02T17:49:56.697067: step 13540, loss 0.204827, acc 0.921875
2017-03-02T17:49:56.776901: step 13541, loss 0.068429, acc 0.984375
2017-03-02T17:49:56.859720: step 13542, loss 0.110937, acc 0.953125
2017-03-02T17:49:56.927133: step 13543, loss 0.235093, acc 0.875
2017-03-02T17:49:56.988044: step 13544, loss 0.0844079, acc 0.96875
2017-03-02T17:49:57.053732: step 13545, loss 0.22599, acc 0.890625
2017-03-02T17:49:57.127249: step 13546, loss 0.175639, acc 0.921875
2017-03-02T17:49:57.199525: step 13547, loss 0.164775, acc 0.9375
2017-03-02T17:49:57.267786: step 13548, loss 0.2694, acc 0.9375
2017-03-02T17:49:57.341988: step 13549, loss 0.125488, acc 0.953125
2017-03-02T17:49:57.422799: step 13550, loss 0.153046, acc 0.953125
2017-03-02T17:49:57.499372: step 13551, loss 0.0802015, acc 0.953125
2017-03-02T17:49:57.576780: step 13552, loss 0.125487, acc 0.953125
2017-03-02T17:49:57.649880: step 13553, loss 0.13625, acc 0.953125
2017-03-02T17:49:57.721703: step 13554, loss 0.167522, acc 0.90625
2017-03-02T17:49:57.801832: step 13555, loss 0.129309, acc 0.9375
2017-03-02T17:49:57.882331: step 13556, loss 0.0988298, acc 0.9375
2017-03-02T17:49:57.959882: step 13557, loss 0.317001, acc 0.875
2017-03-02T17:49:58.026804: step 13558, loss 0.151403, acc 0.9375
2017-03-02T17:49:58.091279: step 13559, loss 0.0502512, acc 0.984375
2017-03-02T17:49:58.162561: step 13560, loss 0.125338, acc 0.96875
2017-03-02T17:49:58.222370: step 13561, loss 0.11401, acc 0.953125
2017-03-02T17:49:58.289005: step 13562, loss 0.367975, acc 0.8125
2017-03-02T17:49:58.356139: step 13563, loss 0.108504, acc 0.96875
2017-03-02T17:49:58.429616: step 13564, loss 0.199076, acc 0.90625
2017-03-02T17:49:58.504586: step 13565, loss 0.126923, acc 0.9375
2017-03-02T17:49:58.578596: step 13566, loss 0.195401, acc 0.921875
2017-03-02T17:49:58.659254: step 13567, loss 0.150054, acc 0.921875
2017-03-02T17:49:58.726105: step 13568, loss 0.136871, acc 0.953125
2017-03-02T17:49:58.794689: step 13569, loss 0.181877, acc 0.890625
2017-03-02T17:49:58.866230: step 13570, loss 0.0946041, acc 0.953125
2017-03-02T17:49:58.936395: step 13571, loss 0.237721, acc 0.890625
2017-03-02T17:49:59.010049: step 13572, loss 0.23882, acc 0.890625
2017-03-02T17:49:59.084031: step 13573, loss 0.105528, acc 0.953125
2017-03-02T17:49:59.157272: step 13574, loss 0.0736801, acc 0.96875
2017-03-02T17:49:59.228106: step 13575, loss 0.139471, acc 0.953125
2017-03-02T17:49:59.302906: step 13576, loss 0.0717006, acc 0.96875
2017-03-02T17:49:59.372596: step 13577, loss 0.113033, acc 0.9375
2017-03-02T17:49:59.441602: step 13578, loss 0.119785, acc 0.921875
2017-03-02T17:49:59.517885: step 13579, loss 0.182277, acc 0.921875
2017-03-02T17:49:59.595794: step 13580, loss 0.181249, acc 0.921875
2017-03-02T17:49:59.667225: step 13581, loss 0.162555, acc 0.953125
2017-03-02T17:49:59.743812: step 13582, loss 0.11915, acc 0.953125
2017-03-02T17:49:59.823161: step 13583, loss 0.180685, acc 0.953125
2017-03-02T17:49:59.901475: step 13584, loss 0.150762, acc 0.90625
2017-03-02T17:49:59.980022: step 13585, loss 0.175252, acc 0.921875
2017-03-02T17:50:00.063995: step 13586, loss 0.190405, acc 0.921875
2017-03-02T17:50:00.137447: step 13587, loss 0.183386, acc 0.9375
2017-03-02T17:50:00.226432: step 13588, loss 0.179026, acc 0.921875
2017-03-02T17:50:00.294389: step 13589, loss 0.13624, acc 0.9375
2017-03-02T17:50:00.367910: step 13590, loss 0.237202, acc 0.890625
2017-03-02T17:50:00.452832: step 13591, loss 0.145038, acc 0.921875
2017-03-02T17:50:00.533891: step 13592, loss 0.118851, acc 0.96875
2017-03-02T17:50:00.607090: step 13593, loss 0.141446, acc 0.921875
2017-03-02T17:50:00.686505: step 13594, loss 0.130813, acc 0.96875
2017-03-02T17:50:00.760853: step 13595, loss 0.110543, acc 0.984375
2017-03-02T17:50:00.831090: step 13596, loss 0.101251, acc 0.953125
2017-03-02T17:50:00.904173: step 13597, loss 0.184687, acc 0.9375
2017-03-02T17:50:00.983515: step 13598, loss 0.104294, acc 0.96875
2017-03-02T17:50:01.051522: step 13599, loss 0.235995, acc 0.90625
2017-03-02T17:50:01.123529: step 13600, loss 0.186548, acc 0.890625

Evaluation:
2017-03-02T17:50:01.154111: step 13600, loss 1.74177, acc 0.664744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13600

2017-03-02T17:50:01.608653: step 13601, loss 0.201666, acc 0.921875
2017-03-02T17:50:01.680901: step 13602, loss 0.07323, acc 0.96875
2017-03-02T17:50:01.752561: step 13603, loss 0.106527, acc 0.953125
2017-03-02T17:50:01.825595: step 13604, loss 0.0721096, acc 0.96875
2017-03-02T17:50:01.903268: step 13605, loss 0.152433, acc 0.9375
2017-03-02T17:50:01.979427: step 13606, loss 0.400118, acc 0.84375
2017-03-02T17:50:02.062799: step 13607, loss 0.254768, acc 0.890625
2017-03-02T17:50:02.131549: step 13608, loss 0.108566, acc 0.9375
2017-03-02T17:50:02.198057: step 13609, loss 0.189388, acc 0.9375
2017-03-02T17:50:02.271008: step 13610, loss 0.132631, acc 0.921875
2017-03-02T17:50:02.344963: step 13611, loss 0.220091, acc 0.875
2017-03-02T17:50:02.417506: step 13612, loss 0.218726, acc 0.921875
2017-03-02T17:50:02.488365: step 13613, loss 0.10367, acc 0.953125
2017-03-02T17:50:02.561913: step 13614, loss 0.119314, acc 0.953125
2017-03-02T17:50:02.643868: step 13615, loss 0.237995, acc 0.875
2017-03-02T17:50:02.723919: step 13616, loss 0.115682, acc 0.9375
2017-03-02T17:50:02.791907: step 13617, loss 0.132328, acc 0.921875
2017-03-02T17:50:02.860716: step 13618, loss 0.256504, acc 0.859375
2017-03-02T17:50:02.935737: step 13619, loss 0.146365, acc 0.953125
2017-03-02T17:50:03.011582: step 13620, loss 0.204383, acc 0.921875
2017-03-02T17:50:03.088679: step 13621, loss 0.132779, acc 0.953125
2017-03-02T17:50:03.162162: step 13622, loss 0.172863, acc 0.953125
2017-03-02T17:50:03.234405: step 13623, loss 0.141733, acc 0.921875
2017-03-02T17:50:03.309180: step 13624, loss 0.226251, acc 0.890625
2017-03-02T17:50:03.378607: step 13625, loss 0.221636, acc 0.921875
2017-03-02T17:50:03.451718: step 13626, loss 0.16516, acc 0.921875
2017-03-02T17:50:03.524733: step 13627, loss 0.151356, acc 0.953125
2017-03-02T17:50:03.596968: step 13628, loss 0.240853, acc 0.90625
2017-03-02T17:50:03.697823: step 13629, loss 0.142923, acc 0.921875
2017-03-02T17:50:03.802120: step 13630, loss 0.415654, acc 0.828125
2017-03-02T17:50:03.880906: step 13631, loss 0.061261, acc 0.96875
2017-03-02T17:50:03.953899: step 13632, loss 0.159727, acc 0.9375
2017-03-02T17:50:04.023452: step 13633, loss 0.307321, acc 0.890625
2017-03-02T17:50:04.099345: step 13634, loss 0.217209, acc 0.921875
2017-03-02T17:50:04.170954: step 13635, loss 0.190771, acc 0.90625
2017-03-02T17:50:04.241036: step 13636, loss 0.0797378, acc 0.96875
2017-03-02T17:50:04.312668: step 13637, loss 0.158934, acc 0.9375
2017-03-02T17:50:04.385156: step 13638, loss 0.092999, acc 0.96875
2017-03-02T17:50:04.459964: step 13639, loss 0.0785381, acc 0.96875
2017-03-02T17:50:04.531027: step 13640, loss 0.141426, acc 0.921875
2017-03-02T17:50:04.601153: step 13641, loss 0.115957, acc 0.921875
2017-03-02T17:50:04.672099: step 13642, loss 0.0928912, acc 0.96875
2017-03-02T17:50:04.747201: step 13643, loss 0.160059, acc 0.921875
2017-03-02T17:50:04.817537: step 13644, loss 0.189435, acc 0.90625
2017-03-02T17:50:04.890636: step 13645, loss 0.150212, acc 0.921875
2017-03-02T17:50:04.951856: step 13646, loss 0.158186, acc 0.953125
2017-03-02T17:50:05.018795: step 13647, loss 0.131566, acc 0.9375
2017-03-02T17:50:05.092724: step 13648, loss 0.210618, acc 0.921875
2017-03-02T17:50:05.163233: step 13649, loss 0.31716, acc 0.875
2017-03-02T17:50:05.238558: step 13650, loss 0.165308, acc 0.921875
2017-03-02T17:50:05.309629: step 13651, loss 0.224607, acc 0.90625
2017-03-02T17:50:05.384369: step 13652, loss 0.0639493, acc 0.96875
2017-03-02T17:50:05.457017: step 13653, loss 0.0808887, acc 0.96875
2017-03-02T17:50:05.530543: step 13654, loss 0.211464, acc 0.90625
2017-03-02T17:50:05.597169: step 13655, loss 0.160404, acc 0.90625
2017-03-02T17:50:05.670734: step 13656, loss 0.115414, acc 0.921875
2017-03-02T17:50:05.752563: step 13657, loss 0.0498353, acc 0.984375
2017-03-02T17:50:05.822530: step 13658, loss 0.132261, acc 0.9375
2017-03-02T17:50:05.898338: step 13659, loss 0.12321, acc 0.953125
2017-03-02T17:50:05.968811: step 13660, loss 0.201659, acc 0.953125
2017-03-02T17:50:06.040003: step 13661, loss 0.0331496, acc 1
2017-03-02T17:50:06.108574: step 13662, loss 0.0932198, acc 0.9375
2017-03-02T17:50:06.187060: step 13663, loss 0.263003, acc 0.859375
2017-03-02T17:50:06.259599: step 13664, loss 0.210747, acc 0.9375
2017-03-02T17:50:06.330875: step 13665, loss 0.181382, acc 0.921875
2017-03-02T17:50:06.405209: step 13666, loss 0.150585, acc 0.90625
2017-03-02T17:50:06.494050: step 13667, loss 0.223556, acc 0.875
2017-03-02T17:50:06.570035: step 13668, loss 0.108607, acc 0.9375
2017-03-02T17:50:06.658167: step 13669, loss 0.221364, acc 0.9375
2017-03-02T17:50:06.730905: step 13670, loss 0.0807987, acc 0.953125
2017-03-02T17:50:06.799490: step 13671, loss 0.143736, acc 0.9375
2017-03-02T17:50:06.871770: step 13672, loss 0.215225, acc 0.9375
2017-03-02T17:50:06.958437: step 13673, loss 0.149875, acc 0.9375
2017-03-02T17:50:07.026869: step 13674, loss 0.373284, acc 0.84375
2017-03-02T17:50:07.091291: step 13675, loss 0.193655, acc 0.90625
2017-03-02T17:50:07.164975: step 13676, loss 0.198163, acc 0.890625
2017-03-02T17:50:07.239261: step 13677, loss 0.12892, acc 0.9375
2017-03-02T17:50:07.318116: step 13678, loss 0.061973, acc 0.96875
2017-03-02T17:50:07.389745: step 13679, loss 0.0434371, acc 1
2017-03-02T17:50:07.468325: step 13680, loss 0.125961, acc 0.921875
2017-03-02T17:50:07.539677: step 13681, loss 0.153779, acc 0.953125
2017-03-02T17:50:07.614240: step 13682, loss 0.197437, acc 0.875
2017-03-02T17:50:07.688092: step 13683, loss 0.0988996, acc 0.953125
2017-03-02T17:50:07.756642: step 13684, loss 0.203404, acc 0.875
2017-03-02T17:50:07.827358: step 13685, loss 0.168214, acc 0.90625
2017-03-02T17:50:07.902959: step 13686, loss 0.170473, acc 0.921875
2017-03-02T17:50:07.974879: step 13687, loss 0.126266, acc 0.90625
2017-03-02T17:50:08.047904: step 13688, loss 0.128196, acc 0.9375
2017-03-02T17:50:08.118700: step 13689, loss 0.130928, acc 0.953125
2017-03-02T17:50:08.206852: step 13690, loss 0.188704, acc 0.890625
2017-03-02T17:50:08.281498: step 13691, loss 0.184779, acc 0.921875
2017-03-02T17:50:08.358116: step 13692, loss 0.119152, acc 0.96875
2017-03-02T17:50:08.435156: step 13693, loss 0.0550917, acc 1
2017-03-02T17:50:08.504197: step 13694, loss 0.215509, acc 0.90625
2017-03-02T17:50:08.572131: step 13695, loss 0.0879768, acc 0.953125
2017-03-02T17:50:08.662280: step 13696, loss 0.204822, acc 0.90625
2017-03-02T17:50:08.728601: step 13697, loss 0.252904, acc 0.875
2017-03-02T17:50:08.806488: step 13698, loss 0.112141, acc 0.9375
2017-03-02T17:50:08.877529: step 13699, loss 0.182549, acc 0.890625
2017-03-02T17:50:08.947276: step 13700, loss 0.258293, acc 0.84375

Evaluation:
2017-03-02T17:50:08.979847: step 13700, loss 1.75854, acc 0.667628

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13700

2017-03-02T17:50:09.457847: step 13701, loss 0.194602, acc 0.921875
2017-03-02T17:50:09.547086: step 13702, loss 0.255364, acc 0.9375
2017-03-02T17:50:09.619162: step 13703, loss 0.114038, acc 0.9375
2017-03-02T17:50:09.691220: step 13704, loss 0.138023, acc 0.953125
2017-03-02T17:50:09.765318: step 13705, loss 0.139432, acc 0.953125
2017-03-02T17:50:09.833033: step 13706, loss 0.248445, acc 0.921875
2017-03-02T17:50:09.906290: step 13707, loss 0.104226, acc 0.96875
2017-03-02T17:50:09.976940: step 13708, loss 0.203344, acc 0.90625
2017-03-02T17:50:10.048003: step 13709, loss 0.238298, acc 0.90625
2017-03-02T17:50:10.126497: step 13710, loss 0.123521, acc 0.953125
2017-03-02T17:50:10.195363: step 13711, loss 0.0507125, acc 0.984375
2017-03-02T17:50:10.270331: step 13712, loss 0.244126, acc 0.921875
2017-03-02T17:50:10.344728: step 13713, loss 0.349219, acc 0.859375
2017-03-02T17:50:10.422882: step 13714, loss 0.213371, acc 0.90625
2017-03-02T17:50:10.497241: step 13715, loss 0.214598, acc 0.875
2017-03-02T17:50:10.567153: step 13716, loss 0.303569, acc 0.859375
2017-03-02T17:50:10.637291: step 13717, loss 0.130815, acc 0.9375
2017-03-02T17:50:10.710947: step 13718, loss 0.126885, acc 0.984375
2017-03-02T17:50:10.782416: step 13719, loss 0.13544, acc 0.953125
2017-03-02T17:50:10.855739: step 13720, loss 0.700686, acc 0.75
2017-03-02T17:50:10.932875: step 13721, loss 0.158626, acc 0.9375
2017-03-02T17:50:11.001961: step 13722, loss 0.0997839, acc 0.953125
2017-03-02T17:50:11.072081: step 13723, loss 0.0719012, acc 1
2017-03-02T17:50:11.145368: step 13724, loss 0.133903, acc 0.9375
2017-03-02T17:50:11.225919: step 13725, loss 0.0965417, acc 0.953125
2017-03-02T17:50:11.297291: step 13726, loss 0.134712, acc 0.9375
2017-03-02T17:50:11.375613: step 13727, loss 0.175808, acc 0.96875
2017-03-02T17:50:11.453456: step 13728, loss 0.125016, acc 0.921875
2017-03-02T17:50:11.531291: step 13729, loss 0.118231, acc 0.921875
2017-03-02T17:50:11.601656: step 13730, loss 0.159072, acc 0.921875
2017-03-02T17:50:11.670443: step 13731, loss 0.150474, acc 0.921875
2017-03-02T17:50:11.740579: step 13732, loss 0.22079, acc 0.90625
2017-03-02T17:50:11.821460: step 13733, loss 0.160688, acc 0.953125
2017-03-02T17:50:11.888111: step 13734, loss 0.19804, acc 0.90625
2017-03-02T17:50:11.944602: step 13735, loss 0.178104, acc 0.90625
2017-03-02T17:50:12.026833: step 13736, loss 0.125779, acc 0.921875
2017-03-02T17:50:12.099466: step 13737, loss 0.171042, acc 0.921875
2017-03-02T17:50:12.173370: step 13738, loss 0.157334, acc 0.890625
2017-03-02T17:50:12.245270: step 13739, loss 0.146729, acc 0.9375
2017-03-02T17:50:12.317626: step 13740, loss 0.293947, acc 0.875
2017-03-02T17:50:12.384815: step 13741, loss 0.189161, acc 0.921875
2017-03-02T17:50:12.453564: step 13742, loss 0.101857, acc 0.953125
2017-03-02T17:50:12.529090: step 13743, loss 0.108777, acc 0.96875
2017-03-02T17:50:12.596039: step 13744, loss 0.121517, acc 0.9375
2017-03-02T17:50:12.659650: step 13745, loss 0.17866, acc 0.921875
2017-03-02T17:50:12.729140: step 13746, loss 0.179508, acc 0.921875
2017-03-02T17:50:12.800186: step 13747, loss 0.250102, acc 0.890625
2017-03-02T17:50:12.877891: step 13748, loss 0.140169, acc 0.921875
2017-03-02T17:50:12.945516: step 13749, loss 0.165994, acc 0.921875
2017-03-02T17:50:13.019481: step 13750, loss 0.0936151, acc 0.96875
2017-03-02T17:50:13.094372: step 13751, loss 0.223, acc 0.890625
2017-03-02T17:50:13.173577: step 13752, loss 0.210724, acc 0.90625
2017-03-02T17:50:13.250631: step 13753, loss 0.154655, acc 0.953125
2017-03-02T17:50:13.326572: step 13754, loss 0.139607, acc 0.9375
2017-03-02T17:50:13.399960: step 13755, loss 0.147107, acc 0.9375
2017-03-02T17:50:13.474608: step 13756, loss 0.0981863, acc 0.953125
2017-03-02T17:50:13.550434: step 13757, loss 0.151213, acc 0.921875
2017-03-02T17:50:13.625774: step 13758, loss 0.0822758, acc 0.96875
2017-03-02T17:50:13.698097: step 13759, loss 0.161367, acc 0.90625
2017-03-02T17:50:13.776743: step 13760, loss 0.146121, acc 0.953125
2017-03-02T17:50:13.846971: step 13761, loss 0.103554, acc 0.9375
2017-03-02T17:50:13.924388: step 13762, loss 0.12108, acc 0.96875
2017-03-02T17:50:14.001398: step 13763, loss 0.115511, acc 0.984375
2017-03-02T17:50:14.066162: step 13764, loss 0.146937, acc 0.9375
2017-03-02T17:50:14.138072: step 13765, loss 0.153763, acc 0.953125
2017-03-02T17:50:14.212122: step 13766, loss 0.119486, acc 0.96875
2017-03-02T17:50:14.285606: step 13767, loss 0.157338, acc 0.96875
2017-03-02T17:50:14.361091: step 13768, loss 0.0776699, acc 0.984375
2017-03-02T17:50:14.436897: step 13769, loss 0.082271, acc 0.953125
2017-03-02T17:50:14.504001: step 13770, loss 0.0396398, acc 1
2017-03-02T17:50:14.586336: step 13771, loss 0.169448, acc 0.9375
2017-03-02T17:50:14.655548: step 13772, loss 0.132856, acc 0.953125
2017-03-02T17:50:14.726399: step 13773, loss 0.177601, acc 0.921875
2017-03-02T17:50:14.802397: step 13774, loss 0.137404, acc 0.9375
2017-03-02T17:50:14.882532: step 13775, loss 0.141774, acc 0.90625
2017-03-02T17:50:14.959368: step 13776, loss 0.123012, acc 0.96875
2017-03-02T17:50:15.026718: step 13777, loss 0.136558, acc 0.96875
2017-03-02T17:50:15.107328: step 13778, loss 0.107404, acc 0.96875
2017-03-02T17:50:15.183894: step 13779, loss 0.0996652, acc 0.9375
2017-03-02T17:50:15.258540: step 13780, loss 0.262677, acc 0.9375
2017-03-02T17:50:15.328359: step 13781, loss 0.077465, acc 0.984375
2017-03-02T17:50:15.400050: step 13782, loss 0.0990243, acc 0.953125
2017-03-02T17:50:15.472686: step 13783, loss 0.152963, acc 0.9375
2017-03-02T17:50:15.544725: step 13784, loss 0.0564733, acc 0.984375
2017-03-02T17:50:15.620930: step 13785, loss 0.0519727, acc 0.96875
2017-03-02T17:50:15.696406: step 13786, loss 0.129434, acc 0.9375
2017-03-02T17:50:15.769779: step 13787, loss 0.111419, acc 0.9375
2017-03-02T17:50:15.842906: step 13788, loss 0.240109, acc 0.875
2017-03-02T17:50:15.918367: step 13789, loss 0.289629, acc 0.859375
2017-03-02T17:50:15.989753: step 13790, loss 0.0996488, acc 0.953125
2017-03-02T17:50:16.066821: step 13791, loss 0.11168, acc 0.96875
2017-03-02T17:50:16.139956: step 13792, loss 0.272342, acc 0.890625
2017-03-02T17:50:16.216214: step 13793, loss 0.148834, acc 0.9375
2017-03-02T17:50:16.298480: step 13794, loss 0.15852, acc 0.921875
2017-03-02T17:50:16.376407: step 13795, loss 0.145455, acc 0.9375
2017-03-02T17:50:16.451353: step 13796, loss 0.215613, acc 0.921875
2017-03-02T17:50:16.529888: step 13797, loss 0.177238, acc 0.90625
2017-03-02T17:50:16.603416: step 13798, loss 0.155763, acc 0.90625
2017-03-02T17:50:16.675443: step 13799, loss 0.194036, acc 0.921875
2017-03-02T17:50:16.746726: step 13800, loss 0.134234, acc 0.953125

Evaluation:
2017-03-02T17:50:16.781484: step 13800, loss 1.74582, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13800

2017-03-02T17:50:17.238656: step 13801, loss 0.177128, acc 0.90625
2017-03-02T17:50:17.308761: step 13802, loss 0.0629771, acc 0.984375
2017-03-02T17:50:17.376737: step 13803, loss 0.133825, acc 0.9375
2017-03-02T17:50:17.454408: step 13804, loss 0.0810863, acc 0.96875
2017-03-02T17:50:17.531376: step 13805, loss 0.194802, acc 0.90625
2017-03-02T17:50:17.609699: step 13806, loss 0.112898, acc 0.984375
2017-03-02T17:50:17.717444: step 13807, loss 0.0690474, acc 0.96875
2017-03-02T17:50:17.806772: step 13808, loss 0.155262, acc 0.9375
2017-03-02T17:50:17.878952: step 13809, loss 0.137406, acc 0.921875
2017-03-02T17:50:17.956446: step 13810, loss 0.114786, acc 0.953125
2017-03-02T17:50:18.035336: step 13811, loss 0.176638, acc 0.921875
2017-03-02T17:50:18.110586: step 13812, loss 0.20084, acc 0.9375
2017-03-02T17:50:18.183127: step 13813, loss 0.1455, acc 0.9375
2017-03-02T17:50:18.256753: step 13814, loss 0.221601, acc 0.9375
2017-03-02T17:50:18.333648: step 13815, loss 0.166246, acc 0.953125
2017-03-02T17:50:18.413287: step 13816, loss 0.12512, acc 0.9375
2017-03-02T17:50:18.486853: step 13817, loss 0.211423, acc 0.9375
2017-03-02T17:50:18.559832: step 13818, loss 0.171375, acc 0.953125
2017-03-02T17:50:18.636643: step 13819, loss 0.156503, acc 0.9375
2017-03-02T17:50:18.710376: step 13820, loss 0.120165, acc 0.9375
2017-03-02T17:50:18.786587: step 13821, loss 0.106414, acc 0.984375
2017-03-02T17:50:18.858407: step 13822, loss 0.357307, acc 0.859375
2017-03-02T17:50:18.930418: step 13823, loss 0.12809, acc 0.9375
2017-03-02T17:50:19.003205: step 13824, loss 0.160721, acc 0.9375
2017-03-02T17:50:19.073875: step 13825, loss 0.138545, acc 0.96875
2017-03-02T17:50:19.156368: step 13826, loss 0.140151, acc 0.9375
2017-03-02T17:50:19.232659: step 13827, loss 0.167247, acc 0.984375
2017-03-02T17:50:19.303707: step 13828, loss 0.104303, acc 0.984375
2017-03-02T17:50:19.385071: step 13829, loss 0.220231, acc 0.90625
2017-03-02T17:50:19.453690: step 13830, loss 0.176408, acc 0.9375
2017-03-02T17:50:19.527640: step 13831, loss 0.0793041, acc 0.96875
2017-03-02T17:50:19.600707: step 13832, loss 0.108309, acc 0.96875
2017-03-02T17:50:19.670310: step 13833, loss 0.0982709, acc 0.953125
2017-03-02T17:50:19.744287: step 13834, loss 0.161595, acc 0.90625
2017-03-02T17:50:19.820645: step 13835, loss 0.182348, acc 0.921875
2017-03-02T17:50:19.895055: step 13836, loss 0.134494, acc 0.9375
2017-03-02T17:50:19.966295: step 13837, loss 0.0916308, acc 0.953125
2017-03-02T17:50:20.039399: step 13838, loss 0.144727, acc 0.90625
2017-03-02T17:50:20.104860: step 13839, loss 0.0412023, acc 1
2017-03-02T17:50:20.170354: step 13840, loss 0.098267, acc 0.96875
2017-03-02T17:50:20.250299: step 13841, loss 0.195375, acc 0.890625
2017-03-02T17:50:20.324431: step 13842, loss 0.109089, acc 0.96875
2017-03-02T17:50:20.403306: step 13843, loss 0.129983, acc 0.9375
2017-03-02T17:50:20.479542: step 13844, loss 0.199021, acc 0.890625
2017-03-02T17:50:20.565150: step 13845, loss 0.233148, acc 0.875
2017-03-02T17:50:20.644796: step 13846, loss 0.258139, acc 0.921875
2017-03-02T17:50:20.716600: step 13847, loss 0.216548, acc 0.921875
2017-03-02T17:50:20.789485: step 13848, loss 0.0736417, acc 0.984375
2017-03-02T17:50:20.860286: step 13849, loss 0.273016, acc 0.859375
2017-03-02T17:50:20.924292: step 13850, loss 0.313416, acc 0.875
2017-03-02T17:50:20.995673: step 13851, loss 0.155041, acc 0.921875
2017-03-02T17:50:21.076305: step 13852, loss 0.127681, acc 0.953125
2017-03-02T17:50:21.147823: step 13853, loss 0.233805, acc 0.90625
2017-03-02T17:50:21.223119: step 13854, loss 0.219887, acc 0.890625
2017-03-02T17:50:21.308168: step 13855, loss 0.138484, acc 0.96875
2017-03-02T17:50:21.385807: step 13856, loss 0.226377, acc 0.90625
2017-03-02T17:50:21.465212: step 13857, loss 0.218557, acc 0.890625
2017-03-02T17:50:21.533518: step 13858, loss 0.208214, acc 0.90625
2017-03-02T17:50:21.602889: step 13859, loss 0.146453, acc 0.9375
2017-03-02T17:50:21.675499: step 13860, loss 0.274479, acc 0.859375
2017-03-02T17:50:21.748392: step 13861, loss 0.220324, acc 0.921875
2017-03-02T17:50:21.822595: step 13862, loss 0.19765, acc 0.9375
2017-03-02T17:50:21.893584: step 13863, loss 0.14831, acc 0.9375
2017-03-02T17:50:21.966943: step 13864, loss 0.175457, acc 0.921875
2017-03-02T17:50:22.039016: step 13865, loss 0.15511, acc 0.90625
2017-03-02T17:50:22.116141: step 13866, loss 0.0425337, acc 0.984375
2017-03-02T17:50:22.182426: step 13867, loss 0.114765, acc 0.921875
2017-03-02T17:50:22.251058: step 13868, loss 0.213215, acc 0.9375
2017-03-02T17:50:22.328521: step 13869, loss 0.139298, acc 0.90625
2017-03-02T17:50:22.410959: step 13870, loss 0.193203, acc 0.921875
2017-03-02T17:50:22.484698: step 13871, loss 0.115878, acc 0.953125
2017-03-02T17:50:22.553616: step 13872, loss 0.229608, acc 0.90625
2017-03-02T17:50:22.626327: step 13873, loss 0.106079, acc 0.953125
2017-03-02T17:50:22.701450: step 13874, loss 0.177819, acc 0.90625
2017-03-02T17:50:22.775004: step 13875, loss 0.108254, acc 0.9375
2017-03-02T17:50:22.838115: step 13876, loss 0.129522, acc 0.921875
2017-03-02T17:50:22.907749: step 13877, loss 0.331212, acc 0.84375
2017-03-02T17:50:22.977129: step 13878, loss 0.0770129, acc 0.96875
2017-03-02T17:50:23.054108: step 13879, loss 0.182096, acc 0.953125
2017-03-02T17:50:23.130923: step 13880, loss 0.270781, acc 0.90625
2017-03-02T17:50:23.211986: step 13881, loss 0.219346, acc 0.875
2017-03-02T17:50:23.288032: step 13882, loss 0.209645, acc 0.953125
2017-03-02T17:50:23.364872: step 13883, loss 0.225748, acc 0.890625
2017-03-02T17:50:23.442624: step 13884, loss 0.109466, acc 0.9375
2017-03-02T17:50:23.514534: step 13885, loss 0.110054, acc 1
2017-03-02T17:50:23.582298: step 13886, loss 0.288227, acc 0.875
2017-03-02T17:50:23.652948: step 13887, loss 0.139478, acc 0.90625
2017-03-02T17:50:23.726153: step 13888, loss 0.209401, acc 0.890625
2017-03-02T17:50:23.798930: step 13889, loss 0.131691, acc 0.953125
2017-03-02T17:50:23.877838: step 13890, loss 0.1302, acc 0.9375
2017-03-02T17:50:23.981416: step 13891, loss 0.288856, acc 0.859375
2017-03-02T17:50:24.048790: step 13892, loss 0.243572, acc 0.9375
2017-03-02T17:50:24.125304: step 13893, loss 0.177474, acc 0.9375
2017-03-02T17:50:24.194291: step 13894, loss 0.197152, acc 0.890625
2017-03-02T17:50:24.261172: step 13895, loss 0.307235, acc 0.875
2017-03-02T17:50:24.330102: step 13896, loss 0.127667, acc 0.96875
2017-03-02T17:50:24.401191: step 13897, loss 0.162709, acc 0.953125
2017-03-02T17:50:24.476131: step 13898, loss 0.113836, acc 0.9375
2017-03-02T17:50:24.548480: step 13899, loss 0.10564, acc 0.96875
2017-03-02T17:50:24.623690: step 13900, loss 0.134556, acc 0.90625

Evaluation:
2017-03-02T17:50:24.660767: step 13900, loss 1.7208, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-13900

2017-03-02T17:50:25.105561: step 13901, loss 0.233299, acc 0.890625
2017-03-02T17:50:25.184070: step 13902, loss 0.130034, acc 0.953125
2017-03-02T17:50:25.255883: step 13903, loss 0.158726, acc 0.90625
2017-03-02T17:50:25.331385: step 13904, loss 0.285539, acc 0.859375
2017-03-02T17:50:25.409392: step 13905, loss 0.275747, acc 0.90625
2017-03-02T17:50:25.485787: step 13906, loss 0.0951557, acc 0.96875
2017-03-02T17:50:25.555201: step 13907, loss 0.192316, acc 0.9375
2017-03-02T17:50:25.623934: step 13908, loss 0.110994, acc 0.953125
2017-03-02T17:50:25.696840: step 13909, loss 0.232534, acc 0.890625
2017-03-02T17:50:25.776036: step 13910, loss 0.106521, acc 0.953125
2017-03-02T17:50:25.853230: step 13911, loss 0.274489, acc 0.84375
2017-03-02T17:50:25.931286: step 13912, loss 0.1145, acc 0.96875
2017-03-02T17:50:26.006657: step 13913, loss 0.247661, acc 0.90625
2017-03-02T17:50:26.089242: step 13914, loss 0.365082, acc 0.90625
2017-03-02T17:50:26.165652: step 13915, loss 0.180815, acc 0.96875
2017-03-02T17:50:26.240855: step 13916, loss 0.00296629, acc 1
2017-03-02T17:50:26.305606: step 13917, loss 0.178434, acc 0.921875
2017-03-02T17:50:26.377539: step 13918, loss 0.175857, acc 0.90625
2017-03-02T17:50:26.456912: step 13919, loss 0.0670474, acc 0.96875
2017-03-02T17:50:26.522944: step 13920, loss 0.0814781, acc 0.953125
2017-03-02T17:50:26.599918: step 13921, loss 0.147417, acc 0.9375
2017-03-02T17:50:26.677381: step 13922, loss 0.0746232, acc 0.96875
2017-03-02T17:50:26.763231: step 13923, loss 0.132512, acc 0.953125
2017-03-02T17:50:26.841806: step 13924, loss 0.120042, acc 0.953125
2017-03-02T17:50:26.912702: step 13925, loss 0.0568158, acc 0.96875
2017-03-02T17:50:26.977661: step 13926, loss 0.143372, acc 0.9375
2017-03-02T17:50:27.037462: step 13927, loss 0.111056, acc 0.921875
2017-03-02T17:50:27.113117: step 13928, loss 0.119635, acc 0.953125
2017-03-02T17:50:27.187018: step 13929, loss 0.173734, acc 0.890625
2017-03-02T17:50:27.261761: step 13930, loss 0.240902, acc 0.921875
2017-03-02T17:50:27.344194: step 13931, loss 0.193583, acc 0.921875
2017-03-02T17:50:27.422510: step 13932, loss 0.0825187, acc 0.96875
2017-03-02T17:50:27.496374: step 13933, loss 0.194207, acc 0.9375
2017-03-02T17:50:27.569337: step 13934, loss 0.147139, acc 0.90625
2017-03-02T17:50:27.634724: step 13935, loss 0.153471, acc 0.9375
2017-03-02T17:50:27.708282: step 13936, loss 0.19935, acc 0.921875
2017-03-02T17:50:27.772351: step 13937, loss 0.126053, acc 0.921875
2017-03-02T17:50:27.846249: step 13938, loss 0.0779221, acc 0.96875
2017-03-02T17:50:27.915962: step 13939, loss 0.154443, acc 0.921875
2017-03-02T17:50:27.985866: step 13940, loss 0.256302, acc 0.84375
2017-03-02T17:50:28.058881: step 13941, loss 0.222627, acc 0.9375
2017-03-02T17:50:28.134362: step 13942, loss 0.138309, acc 0.9375
2017-03-02T17:50:28.205497: step 13943, loss 0.0609923, acc 1
2017-03-02T17:50:28.274021: step 13944, loss 0.089473, acc 0.953125
2017-03-02T17:50:28.343551: step 13945, loss 0.119954, acc 0.96875
2017-03-02T17:50:28.411381: step 13946, loss 0.156816, acc 0.921875
2017-03-02T17:50:28.495424: step 13947, loss 0.355767, acc 0.875
2017-03-02T17:50:28.567524: step 13948, loss 0.0769175, acc 0.984375
2017-03-02T17:50:28.638052: step 13949, loss 0.0867452, acc 1
2017-03-02T17:50:28.710159: step 13950, loss 0.13585, acc 0.90625
2017-03-02T17:50:28.781554: step 13951, loss 0.225584, acc 0.9375
2017-03-02T17:50:28.849136: step 13952, loss 0.160628, acc 0.921875
2017-03-02T17:50:28.925676: step 13953, loss 0.129047, acc 0.953125
2017-03-02T17:50:28.997327: step 13954, loss 0.130463, acc 0.9375
2017-03-02T17:50:29.065701: step 13955, loss 0.302977, acc 0.921875
2017-03-02T17:50:29.134708: step 13956, loss 0.194757, acc 0.890625
2017-03-02T17:50:29.210124: step 13957, loss 0.10853, acc 0.953125
2017-03-02T17:50:29.285221: step 13958, loss 0.187414, acc 0.890625
2017-03-02T17:50:29.361265: step 13959, loss 0.123676, acc 0.9375
2017-03-02T17:50:29.442255: step 13960, loss 0.132736, acc 0.953125
2017-03-02T17:50:29.523114: step 13961, loss 0.0960783, acc 0.953125
2017-03-02T17:50:29.596679: step 13962, loss 0.0716447, acc 0.96875
2017-03-02T17:50:29.669103: step 13963, loss 0.148061, acc 0.953125
2017-03-02T17:50:29.731698: step 13964, loss 0.155093, acc 0.953125
2017-03-02T17:50:29.799468: step 13965, loss 0.177497, acc 0.90625
2017-03-02T17:50:29.887288: step 13966, loss 0.174875, acc 0.921875
2017-03-02T17:50:29.957514: step 13967, loss 0.189424, acc 0.890625
2017-03-02T17:50:30.021740: step 13968, loss 0.0941739, acc 0.96875
2017-03-02T17:50:30.097400: step 13969, loss 0.0986275, acc 0.96875
2017-03-02T17:50:30.171484: step 13970, loss 0.073976, acc 0.96875
2017-03-02T17:50:30.243319: step 13971, loss 0.0666539, acc 0.96875
2017-03-02T17:50:30.317402: step 13972, loss 0.182261, acc 0.921875
2017-03-02T17:50:30.390728: step 13973, loss 0.198902, acc 0.921875
2017-03-02T17:50:30.463839: step 13974, loss 0.142379, acc 0.9375
2017-03-02T17:50:30.533793: step 13975, loss 0.0944223, acc 0.96875
2017-03-02T17:50:30.605240: step 13976, loss 0.146261, acc 0.96875
2017-03-02T17:50:30.682328: step 13977, loss 0.106789, acc 0.9375
2017-03-02T17:50:30.752086: step 13978, loss 0.219599, acc 0.890625
2017-03-02T17:50:30.825041: step 13979, loss 0.203002, acc 0.921875
2017-03-02T17:50:30.895386: step 13980, loss 0.0801125, acc 0.96875
2017-03-02T17:50:30.967721: step 13981, loss 0.189266, acc 0.953125
2017-03-02T17:50:31.038525: step 13982, loss 0.117114, acc 0.9375
2017-03-02T17:50:31.108322: step 13983, loss 0.147782, acc 0.9375
2017-03-02T17:50:31.179584: step 13984, loss 0.110928, acc 0.953125
2017-03-02T17:50:31.244836: step 13985, loss 0.0759286, acc 0.96875
2017-03-02T17:50:31.334930: step 13986, loss 0.153444, acc 0.921875
2017-03-02T17:50:31.408860: step 13987, loss 0.140968, acc 0.921875
2017-03-02T17:50:31.482277: step 13988, loss 0.181326, acc 0.90625
2017-03-02T17:50:31.562010: step 13989, loss 0.132793, acc 0.9375
2017-03-02T17:50:31.636347: step 13990, loss 0.113687, acc 0.9375
2017-03-02T17:50:31.702595: step 13991, loss 0.161711, acc 0.9375
2017-03-02T17:50:31.782713: step 13992, loss 0.181994, acc 0.90625
2017-03-02T17:50:31.854600: step 13993, loss 0.0836009, acc 0.96875
2017-03-02T17:50:31.927283: step 13994, loss 0.197603, acc 0.921875
2017-03-02T17:50:31.990450: step 13995, loss 0.0876215, acc 0.96875
2017-03-02T17:50:32.064938: step 13996, loss 0.166096, acc 0.90625
2017-03-02T17:50:32.141197: step 13997, loss 0.0657318, acc 0.984375
2017-03-02T17:50:32.214315: step 13998, loss 0.104157, acc 0.96875
2017-03-02T17:50:32.294568: step 13999, loss 0.344818, acc 0.890625
2017-03-02T17:50:32.367935: step 14000, loss 0.17647, acc 0.921875

Evaluation:
2017-03-02T17:50:32.405102: step 14000, loss 1.81676, acc 0.664023

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14000

2017-03-02T17:50:32.848939: step 14001, loss 0.103504, acc 0.96875
2017-03-02T17:50:32.933014: step 14002, loss 0.0675797, acc 0.953125
2017-03-02T17:50:33.009418: step 14003, loss 0.342368, acc 0.859375
2017-03-02T17:50:33.074180: step 14004, loss 0.260376, acc 0.9375
2017-03-02T17:50:33.164691: step 14005, loss 0.172633, acc 0.921875
2017-03-02T17:50:33.231640: step 14006, loss 0.181677, acc 0.9375
2017-03-02T17:50:33.324664: step 14007, loss 0.231487, acc 0.9375
2017-03-02T17:50:33.407272: step 14008, loss 0.15891, acc 0.9375
2017-03-02T17:50:33.484964: step 14009, loss 0.179857, acc 0.921875
2017-03-02T17:50:33.556157: step 14010, loss 0.303998, acc 0.875
2017-03-02T17:50:33.630218: step 14011, loss 0.19824, acc 0.90625
2017-03-02T17:50:33.703427: step 14012, loss 0.168979, acc 0.921875
2017-03-02T17:50:33.778134: step 14013, loss 0.117294, acc 0.9375
2017-03-02T17:50:33.856231: step 14014, loss 0.202195, acc 0.90625
2017-03-02T17:50:33.936027: step 14015, loss 0.0811139, acc 0.96875
2017-03-02T17:50:34.005010: step 14016, loss 0.271643, acc 0.90625
2017-03-02T17:50:34.087856: step 14017, loss 0.210642, acc 0.90625
2017-03-02T17:50:34.165478: step 14018, loss 0.296768, acc 0.90625
2017-03-02T17:50:34.237461: step 14019, loss 0.186874, acc 0.921875
2017-03-02T17:50:34.308719: step 14020, loss 0.112046, acc 0.9375
2017-03-02T17:50:34.391263: step 14021, loss 0.259442, acc 0.875
2017-03-02T17:50:34.466215: step 14022, loss 0.0533899, acc 0.984375
2017-03-02T17:50:34.542659: step 14023, loss 0.347831, acc 0.84375
2017-03-02T17:50:34.618820: step 14024, loss 0.167605, acc 0.9375
2017-03-02T17:50:34.694891: step 14025, loss 0.270754, acc 0.890625
2017-03-02T17:50:34.767932: step 14026, loss 0.226088, acc 0.90625
2017-03-02T17:50:34.843599: step 14027, loss 0.223456, acc 0.90625
2017-03-02T17:50:34.912420: step 14028, loss 0.205877, acc 0.90625
2017-03-02T17:50:34.987405: step 14029, loss 0.181504, acc 0.890625
2017-03-02T17:50:35.061563: step 14030, loss 0.209263, acc 0.890625
2017-03-02T17:50:35.129786: step 14031, loss 0.0702717, acc 0.96875
2017-03-02T17:50:35.207952: step 14032, loss 0.177775, acc 0.921875
2017-03-02T17:50:35.278262: step 14033, loss 0.230202, acc 0.9375
2017-03-02T17:50:35.340514: step 14034, loss 0.109333, acc 0.96875
2017-03-02T17:50:35.415420: step 14035, loss 0.218168, acc 0.890625
2017-03-02T17:50:35.493528: step 14036, loss 0.163018, acc 0.9375
2017-03-02T17:50:35.569144: step 14037, loss 0.153975, acc 0.921875
2017-03-02T17:50:35.640687: step 14038, loss 0.195393, acc 0.921875
2017-03-02T17:50:35.715717: step 14039, loss 0.125752, acc 0.953125
2017-03-02T17:50:35.787103: step 14040, loss 0.199593, acc 0.90625
2017-03-02T17:50:35.857990: step 14041, loss 0.104768, acc 0.96875
2017-03-02T17:50:35.924695: step 14042, loss 0.182053, acc 0.90625
2017-03-02T17:50:35.993856: step 14043, loss 0.126901, acc 0.953125
2017-03-02T17:50:36.068569: step 14044, loss 0.242617, acc 0.890625
2017-03-02T17:50:36.156434: step 14045, loss 0.18785, acc 0.9375
2017-03-02T17:50:36.238847: step 14046, loss 0.144993, acc 0.953125
2017-03-02T17:50:36.308477: step 14047, loss 0.06889, acc 0.96875
2017-03-02T17:50:36.375659: step 14048, loss 0.160312, acc 0.9375
2017-03-02T17:50:36.452846: step 14049, loss 0.251202, acc 0.890625
2017-03-02T17:50:36.526200: step 14050, loss 0.0595607, acc 0.984375
2017-03-02T17:50:36.596780: step 14051, loss 0.0796542, acc 0.984375
2017-03-02T17:50:36.661826: step 14052, loss 0.147518, acc 0.9375
2017-03-02T17:50:36.729306: step 14053, loss 0.165091, acc 0.9375
2017-03-02T17:50:36.805308: step 14054, loss 0.201584, acc 0.921875
2017-03-02T17:50:36.900322: step 14055, loss 0.212246, acc 0.90625
2017-03-02T17:50:36.974692: step 14056, loss 0.229324, acc 0.875
2017-03-02T17:50:37.055263: step 14057, loss 0.137085, acc 0.953125
2017-03-02T17:50:37.132722: step 14058, loss 0.159188, acc 0.9375
2017-03-02T17:50:37.207903: step 14059, loss 0.220395, acc 0.890625
2017-03-02T17:50:37.279209: step 14060, loss 0.171659, acc 0.90625
2017-03-02T17:50:37.342921: step 14061, loss 0.227266, acc 0.875
2017-03-02T17:50:37.412858: step 14062, loss 0.141066, acc 0.953125
2017-03-02T17:50:37.483484: step 14063, loss 0.196314, acc 0.890625
2017-03-02T17:50:37.564195: step 14064, loss 0.0622107, acc 0.96875
2017-03-02T17:50:37.637224: step 14065, loss 0.120773, acc 0.96875
2017-03-02T17:50:37.710328: step 14066, loss 0.0977665, acc 0.9375
2017-03-02T17:50:37.785484: step 14067, loss 0.221231, acc 0.859375
2017-03-02T17:50:37.854240: step 14068, loss 0.15534, acc 0.921875
2017-03-02T17:50:37.927119: step 14069, loss 0.165584, acc 0.921875
2017-03-02T17:50:37.997692: step 14070, loss 0.131686, acc 0.953125
2017-03-02T17:50:38.064323: step 14071, loss 0.117536, acc 0.9375
2017-03-02T17:50:38.140192: step 14072, loss 0.243532, acc 0.921875
2017-03-02T17:50:38.213020: step 14073, loss 0.195757, acc 0.921875
2017-03-02T17:50:38.284419: step 14074, loss 0.168524, acc 0.875
2017-03-02T17:50:38.358631: step 14075, loss 0.137749, acc 0.9375
2017-03-02T17:50:38.430871: step 14076, loss 0.188812, acc 0.90625
2017-03-02T17:50:38.506555: step 14077, loss 0.149404, acc 0.96875
2017-03-02T17:50:38.585828: step 14078, loss 0.377029, acc 0.859375
2017-03-02T17:50:38.657518: step 14079, loss 0.0980932, acc 0.984375
2017-03-02T17:50:38.724332: step 14080, loss 0.251255, acc 0.875
2017-03-02T17:50:38.794234: step 14081, loss 0.128472, acc 0.984375
2017-03-02T17:50:38.862306: step 14082, loss 0.0651955, acc 0.984375
2017-03-02T17:50:38.940058: step 14083, loss 0.139503, acc 0.90625
2017-03-02T17:50:39.011642: step 14084, loss 0.162404, acc 0.921875
2017-03-02T17:50:39.086993: step 14085, loss 0.0806635, acc 0.984375
2017-03-02T17:50:39.163919: step 14086, loss 0.11238, acc 0.96875
2017-03-02T17:50:39.234775: step 14087, loss 0.181142, acc 0.9375
2017-03-02T17:50:39.304822: step 14088, loss 0.137127, acc 0.9375
2017-03-02T17:50:39.375478: step 14089, loss 0.111149, acc 0.984375
2017-03-02T17:50:39.443093: step 14090, loss 0.124839, acc 0.953125
2017-03-02T17:50:39.515175: step 14091, loss 0.113428, acc 0.953125
2017-03-02T17:50:39.597472: step 14092, loss 0.101481, acc 0.96875
2017-03-02T17:50:39.670137: step 14093, loss 0.221658, acc 0.921875
2017-03-02T17:50:39.751445: step 14094, loss 0.177827, acc 0.953125
2017-03-02T17:50:39.860607: step 14095, loss 0.0805729, acc 1
2017-03-02T17:50:39.936859: step 14096, loss 0.29861, acc 0.859375
2017-03-02T17:50:40.005175: step 14097, loss 0.220827, acc 0.921875
2017-03-02T17:50:40.072679: step 14098, loss 0.115956, acc 0.9375
2017-03-02T17:50:40.148161: step 14099, loss 0.156745, acc 0.9375
2017-03-02T17:50:40.217855: step 14100, loss 0.157666, acc 0.9375

Evaluation:
2017-03-02T17:50:40.254828: step 14100, loss 1.81647, acc 0.675559

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14100

2017-03-02T17:50:40.695846: step 14101, loss 0.129583, acc 0.921875
2017-03-02T17:50:40.768030: step 14102, loss 0.17802, acc 0.90625
2017-03-02T17:50:40.839114: step 14103, loss 0.213642, acc 0.875
2017-03-02T17:50:40.923688: step 14104, loss 0.176025, acc 0.90625
2017-03-02T17:50:41.009688: step 14105, loss 0.136571, acc 0.953125
2017-03-02T17:50:41.084682: step 14106, loss 0.264103, acc 0.875
2017-03-02T17:50:41.156343: step 14107, loss 0.271707, acc 0.875
2017-03-02T17:50:41.234077: step 14108, loss 0.173675, acc 0.921875
2017-03-02T17:50:41.309981: step 14109, loss 0.164936, acc 0.921875
2017-03-02T17:50:41.388897: step 14110, loss 0.196312, acc 0.921875
2017-03-02T17:50:41.457605: step 14111, loss 0.26905, acc 0.921875
2017-03-02T17:50:41.521799: step 14112, loss 0.179896, acc 1
2017-03-02T17:50:41.600106: step 14113, loss 0.170062, acc 0.90625
2017-03-02T17:50:41.678301: step 14114, loss 0.219894, acc 0.90625
2017-03-02T17:50:41.752102: step 14115, loss 0.106022, acc 0.96875
2017-03-02T17:50:41.826042: step 14116, loss 0.268816, acc 0.921875
2017-03-02T17:50:41.895611: step 14117, loss 0.120501, acc 0.9375
2017-03-02T17:50:41.968509: step 14118, loss 0.107027, acc 0.953125
2017-03-02T17:50:42.043053: step 14119, loss 0.164451, acc 0.953125
2017-03-02T17:50:42.134373: step 14120, loss 0.115939, acc 0.953125
2017-03-02T17:50:42.206887: step 14121, loss 0.148885, acc 0.9375
2017-03-02T17:50:42.279805: step 14122, loss 0.19755, acc 0.953125
2017-03-02T17:50:42.362820: step 14123, loss 0.157612, acc 0.9375
2017-03-02T17:50:42.443908: step 14124, loss 0.279222, acc 0.90625
2017-03-02T17:50:42.519201: step 14125, loss 0.0997049, acc 0.96875
2017-03-02T17:50:42.608316: step 14126, loss 0.132759, acc 0.9375
2017-03-02T17:50:42.677671: step 14127, loss 0.23126, acc 0.921875
2017-03-02T17:50:42.750725: step 14128, loss 0.153239, acc 0.953125
2017-03-02T17:50:42.820769: step 14129, loss 0.202251, acc 0.90625
2017-03-02T17:50:42.898564: step 14130, loss 0.172172, acc 0.9375
2017-03-02T17:50:42.967013: step 14131, loss 0.131836, acc 0.921875
2017-03-02T17:50:43.036513: step 14132, loss 0.105919, acc 0.9375
2017-03-02T17:50:43.112854: step 14133, loss 0.216083, acc 0.859375
2017-03-02T17:50:43.189435: step 14134, loss 0.141712, acc 0.9375
2017-03-02T17:50:43.262499: step 14135, loss 0.250251, acc 0.890625
2017-03-02T17:50:43.349316: step 14136, loss 0.182919, acc 0.9375
2017-03-02T17:50:43.420740: step 14137, loss 0.0567794, acc 0.96875
2017-03-02T17:50:43.493066: step 14138, loss 0.12293, acc 0.921875
2017-03-02T17:50:43.561544: step 14139, loss 0.316283, acc 0.84375
2017-03-02T17:50:43.641072: step 14140, loss 0.047271, acc 0.984375
2017-03-02T17:50:43.711806: step 14141, loss 0.139562, acc 0.953125
2017-03-02T17:50:43.790664: step 14142, loss 0.182055, acc 0.921875
2017-03-02T17:50:43.871344: step 14143, loss 0.168312, acc 0.9375
2017-03-02T17:50:43.945908: step 14144, loss 0.193292, acc 0.9375
2017-03-02T17:50:44.023399: step 14145, loss 0.0714622, acc 0.984375
2017-03-02T17:50:44.101138: step 14146, loss 0.100593, acc 0.96875
2017-03-02T17:50:44.186199: step 14147, loss 0.066314, acc 0.953125
2017-03-02T17:50:44.260901: step 14148, loss 0.192854, acc 0.9375
2017-03-02T17:50:44.329256: step 14149, loss 0.109365, acc 0.9375
2017-03-02T17:50:44.406017: step 14150, loss 0.133932, acc 0.921875
2017-03-02T17:50:44.479773: step 14151, loss 0.237818, acc 0.890625
2017-03-02T17:50:44.557533: step 14152, loss 0.118004, acc 0.9375
2017-03-02T17:50:44.636497: step 14153, loss 0.306771, acc 0.921875
2017-03-02T17:50:44.713821: step 14154, loss 0.0609664, acc 0.953125
2017-03-02T17:50:44.787121: step 14155, loss 0.115174, acc 0.96875
2017-03-02T17:50:44.858995: step 14156, loss 0.0879529, acc 0.953125
2017-03-02T17:50:44.932770: step 14157, loss 0.333487, acc 0.828125
2017-03-02T17:50:45.000756: step 14158, loss 0.262241, acc 0.84375
2017-03-02T17:50:45.073557: step 14159, loss 0.108419, acc 0.9375
2017-03-02T17:50:45.149344: step 14160, loss 0.11219, acc 0.953125
2017-03-02T17:50:45.225581: step 14161, loss 0.10408, acc 0.96875
2017-03-02T17:50:45.302712: step 14162, loss 0.23771, acc 0.875
2017-03-02T17:50:45.371784: step 14163, loss 0.0273125, acc 1
2017-03-02T17:50:45.451591: step 14164, loss 0.118575, acc 0.953125
2017-03-02T17:50:45.522614: step 14165, loss 0.210507, acc 0.9375
2017-03-02T17:50:45.589476: step 14166, loss 0.341662, acc 0.921875
2017-03-02T17:50:45.656438: step 14167, loss 0.0571832, acc 1
2017-03-02T17:50:45.724763: step 14168, loss 0.285278, acc 0.875
2017-03-02T17:50:45.799568: step 14169, loss 0.242062, acc 0.890625
2017-03-02T17:50:45.872092: step 14170, loss 0.233208, acc 0.921875
2017-03-02T17:50:45.946990: step 14171, loss 0.176374, acc 0.890625
2017-03-02T17:50:46.017974: step 14172, loss 0.101527, acc 0.9375
2017-03-02T17:50:46.094118: step 14173, loss 0.175815, acc 0.9375
2017-03-02T17:50:46.171173: step 14174, loss 0.0683931, acc 0.984375
2017-03-02T17:50:46.246792: step 14175, loss 0.163006, acc 0.953125
2017-03-02T17:50:46.323188: step 14176, loss 0.079279, acc 0.953125
2017-03-02T17:50:46.392344: step 14177, loss 0.141511, acc 0.953125
2017-03-02T17:50:46.462881: step 14178, loss 0.312579, acc 0.90625
2017-03-02T17:50:46.535219: step 14179, loss 0.111813, acc 0.9375
2017-03-02T17:50:46.609800: step 14180, loss 0.0762043, acc 0.96875
2017-03-02T17:50:46.683272: step 14181, loss 0.11558, acc 0.953125
2017-03-02T17:50:46.756585: step 14182, loss 0.128079, acc 0.96875
2017-03-02T17:50:46.824432: step 14183, loss 0.20911, acc 0.921875
2017-03-02T17:50:46.893627: step 14184, loss 0.177877, acc 0.921875
2017-03-02T17:50:46.965106: step 14185, loss 0.203057, acc 0.921875
2017-03-02T17:50:47.050198: step 14186, loss 0.161812, acc 0.9375
2017-03-02T17:50:47.117061: step 14187, loss 0.258889, acc 0.875
2017-03-02T17:50:47.186087: step 14188, loss 0.148942, acc 0.9375
2017-03-02T17:50:47.259841: step 14189, loss 0.144681, acc 0.921875
2017-03-02T17:50:47.332885: step 14190, loss 0.192119, acc 0.953125
2017-03-02T17:50:47.399885: step 14191, loss 0.115692, acc 0.9375
2017-03-02T17:50:47.479757: step 14192, loss 0.164922, acc 0.9375
2017-03-02T17:50:47.554810: step 14193, loss 0.156915, acc 0.9375
2017-03-02T17:50:47.620785: step 14194, loss 0.195927, acc 0.921875
2017-03-02T17:50:47.693537: step 14195, loss 0.107809, acc 0.9375
2017-03-02T17:50:47.763347: step 14196, loss 0.131779, acc 0.9375
2017-03-02T17:50:47.839361: step 14197, loss 0.120649, acc 0.921875
2017-03-02T17:50:47.916622: step 14198, loss 0.111468, acc 0.984375
2017-03-02T17:50:47.991334: step 14199, loss 0.17923, acc 0.921875
2017-03-02T17:50:48.065827: step 14200, loss 0.196063, acc 0.9375

Evaluation:
2017-03-02T17:50:48.101567: step 14200, loss 1.7767, acc 0.652487

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14200

2017-03-02T17:50:48.557528: step 14201, loss 0.12814, acc 0.953125
2017-03-02T17:50:48.626809: step 14202, loss 0.0870707, acc 0.953125
2017-03-02T17:50:48.699395: step 14203, loss 0.157138, acc 0.90625
2017-03-02T17:50:48.771760: step 14204, loss 0.154358, acc 0.953125
2017-03-02T17:50:48.840852: step 14205, loss 0.236527, acc 0.90625
2017-03-02T17:50:48.911073: step 14206, loss 0.105487, acc 0.953125
2017-03-02T17:50:48.987234: step 14207, loss 0.0867693, acc 0.9375
2017-03-02T17:50:49.058145: step 14208, loss 0.0496267, acc 0.984375
2017-03-02T17:50:49.133307: step 14209, loss 0.194312, acc 0.90625
2017-03-02T17:50:49.202933: step 14210, loss 0.109378, acc 0.953125
2017-03-02T17:50:49.276150: step 14211, loss 0.207971, acc 0.90625
2017-03-02T17:50:49.354146: step 14212, loss 0.116634, acc 0.9375
2017-03-02T17:50:49.433511: step 14213, loss 0.102515, acc 0.9375
2017-03-02T17:50:49.506413: step 14214, loss 0.258686, acc 0.9375
2017-03-02T17:50:49.578273: step 14215, loss 0.17548, acc 0.953125
2017-03-02T17:50:49.653063: step 14216, loss 0.145113, acc 0.921875
2017-03-02T17:50:49.722987: step 14217, loss 0.113733, acc 0.953125
2017-03-02T17:50:49.790459: step 14218, loss 0.119162, acc 0.953125
2017-03-02T17:50:49.859169: step 14219, loss 0.167355, acc 0.9375
2017-03-02T17:50:49.924797: step 14220, loss 0.13044, acc 0.953125
2017-03-02T17:50:49.998837: step 14221, loss 0.183381, acc 0.921875
2017-03-02T17:50:50.071495: step 14222, loss 0.139845, acc 0.953125
2017-03-02T17:50:50.149206: step 14223, loss 0.0621812, acc 0.96875
2017-03-02T17:50:50.224003: step 14224, loss 0.193852, acc 0.90625
2017-03-02T17:50:50.309429: step 14225, loss 0.134628, acc 0.921875
2017-03-02T17:50:50.385596: step 14226, loss 0.181211, acc 0.90625
2017-03-02T17:50:50.465645: step 14227, loss 0.176672, acc 0.9375
2017-03-02T17:50:50.532881: step 14228, loss 0.212343, acc 0.890625
2017-03-02T17:50:50.600509: step 14229, loss 0.119323, acc 0.96875
2017-03-02T17:50:50.673680: step 14230, loss 0.247953, acc 0.90625
2017-03-02T17:50:50.752532: step 14231, loss 0.105318, acc 0.96875
2017-03-02T17:50:50.827100: step 14232, loss 0.0825099, acc 0.953125
2017-03-02T17:50:50.902579: step 14233, loss 0.254859, acc 0.890625
2017-03-02T17:50:50.989146: step 14234, loss 0.122289, acc 0.9375
2017-03-02T17:50:51.063960: step 14235, loss 0.244664, acc 0.890625
2017-03-02T17:50:51.141651: step 14236, loss 0.20431, acc 0.90625
2017-03-02T17:50:51.206496: step 14237, loss 0.12307, acc 0.953125
2017-03-02T17:50:51.276611: step 14238, loss 0.129782, acc 0.96875
2017-03-02T17:50:51.352581: step 14239, loss 0.130152, acc 0.953125
2017-03-02T17:50:51.428949: step 14240, loss 0.282254, acc 0.890625
2017-03-02T17:50:51.506692: step 14241, loss 0.0832698, acc 0.953125
2017-03-02T17:50:51.580882: step 14242, loss 0.0935958, acc 0.953125
2017-03-02T17:50:51.656151: step 14243, loss 0.265308, acc 0.9375
2017-03-02T17:50:51.725851: step 14244, loss 0.326151, acc 0.875
2017-03-02T17:50:51.799654: step 14245, loss 0.237631, acc 0.953125
2017-03-02T17:50:51.870406: step 14246, loss 0.128429, acc 0.96875
2017-03-02T17:50:51.939689: step 14247, loss 0.0875955, acc 0.953125
2017-03-02T17:50:52.010655: step 14248, loss 0.151485, acc 0.9375
2017-03-02T17:50:52.084420: step 14249, loss 0.105401, acc 0.953125
2017-03-02T17:50:52.158235: step 14250, loss 0.267977, acc 0.90625
2017-03-02T17:50:52.230592: step 14251, loss 0.244341, acc 0.875
2017-03-02T17:50:52.301576: step 14252, loss 0.148806, acc 0.921875
2017-03-02T17:50:52.393124: step 14253, loss 0.163649, acc 0.921875
2017-03-02T17:50:52.466680: step 14254, loss 0.0711588, acc 0.984375
2017-03-02T17:50:52.537553: step 14255, loss 0.178828, acc 0.921875
2017-03-02T17:50:52.601802: step 14256, loss 0.193772, acc 0.9375
2017-03-02T17:50:52.669308: step 14257, loss 0.223055, acc 0.90625
2017-03-02T17:50:52.748152: step 14258, loss 0.21802, acc 0.921875
2017-03-02T17:50:52.813658: step 14259, loss 0.16247, acc 0.9375
2017-03-02T17:50:52.892887: step 14260, loss 0.100713, acc 0.953125
2017-03-02T17:50:52.972310: step 14261, loss 0.102489, acc 0.96875
2017-03-02T17:50:53.047312: step 14262, loss 0.0621053, acc 0.984375
2017-03-02T17:50:53.118613: step 14263, loss 0.25154, acc 0.90625
2017-03-02T17:50:53.208149: step 14264, loss 0.192479, acc 0.953125
2017-03-02T17:50:53.282046: step 14265, loss 0.169923, acc 0.9375
2017-03-02T17:50:53.362170: step 14266, loss 0.0842175, acc 0.953125
2017-03-02T17:50:53.425485: step 14267, loss 0.244177, acc 0.875
2017-03-02T17:50:53.499403: step 14268, loss 0.225713, acc 0.90625
2017-03-02T17:50:53.572964: step 14269, loss 0.17617, acc 0.953125
2017-03-02T17:50:53.644833: step 14270, loss 0.247554, acc 0.921875
2017-03-02T17:50:53.718583: step 14271, loss 0.120153, acc 0.953125
2017-03-02T17:50:53.789581: step 14272, loss 0.0965882, acc 0.953125
2017-03-02T17:50:53.869967: step 14273, loss 0.189426, acc 0.875
2017-03-02T17:50:53.936971: step 14274, loss 0.161605, acc 0.953125
2017-03-02T17:50:54.005319: step 14275, loss 0.169899, acc 0.90625
2017-03-02T17:50:54.086864: step 14276, loss 0.17646, acc 0.921875
2017-03-02T17:50:54.162094: step 14277, loss 0.170535, acc 0.890625
2017-03-02T17:50:54.241593: step 14278, loss 0.198381, acc 0.890625
2017-03-02T17:50:54.319269: step 14279, loss 0.203431, acc 0.9375
2017-03-02T17:50:54.391476: step 14280, loss 0.242613, acc 0.90625
2017-03-02T17:50:54.472230: step 14281, loss 0.0875256, acc 0.984375
2017-03-02T17:50:54.553026: step 14282, loss 0.147544, acc 0.921875
2017-03-02T17:50:54.619191: step 14283, loss 0.0948642, acc 0.953125
2017-03-02T17:50:54.690491: step 14284, loss 0.258326, acc 0.921875
2017-03-02T17:50:54.762006: step 14285, loss 0.167237, acc 0.953125
2017-03-02T17:50:54.834743: step 14286, loss 0.187227, acc 0.921875
2017-03-02T17:50:54.918352: step 14287, loss 0.184125, acc 0.90625
2017-03-02T17:50:54.997314: step 14288, loss 0.185832, acc 0.890625
2017-03-02T17:50:55.079364: step 14289, loss 0.197357, acc 0.9375
2017-03-02T17:50:55.141795: step 14290, loss 0.116014, acc 0.953125
2017-03-02T17:50:55.218957: step 14291, loss 0.229481, acc 0.859375
2017-03-02T17:50:55.296623: step 14292, loss 0.0888889, acc 0.953125
2017-03-02T17:50:55.363333: step 14293, loss 0.233065, acc 0.921875
2017-03-02T17:50:55.439923: step 14294, loss 0.11946, acc 0.96875
2017-03-02T17:50:55.514713: step 14295, loss 0.24018, acc 0.890625
2017-03-02T17:50:55.590197: step 14296, loss 0.241912, acc 0.90625
2017-03-02T17:50:55.663843: step 14297, loss 0.173907, acc 0.921875
2017-03-02T17:50:55.739183: step 14298, loss 0.235371, acc 0.921875
2017-03-02T17:50:55.814596: step 14299, loss 0.128817, acc 0.953125
2017-03-02T17:50:55.905560: step 14300, loss 0.162305, acc 0.953125

Evaluation:
2017-03-02T17:50:55.933887: step 14300, loss 1.73199, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14300

2017-03-02T17:50:56.431291: step 14301, loss 0.35194, acc 0.890625
2017-03-02T17:50:56.503269: step 14302, loss 0.142792, acc 0.9375
2017-03-02T17:50:56.577438: step 14303, loss 0.0886176, acc 0.9375
2017-03-02T17:50:56.653564: step 14304, loss 0.388073, acc 0.875
2017-03-02T17:50:56.723488: step 14305, loss 0.144343, acc 0.96875
2017-03-02T17:50:56.788230: step 14306, loss 0.323949, acc 0.859375
2017-03-02T17:50:56.863867: step 14307, loss 0.127854, acc 0.953125
2017-03-02T17:50:56.947219: step 14308, loss 0.250831, acc 0.75
2017-03-02T17:50:57.026614: step 14309, loss 0.0654967, acc 0.984375
2017-03-02T17:50:57.109338: step 14310, loss 0.129819, acc 0.9375
2017-03-02T17:50:57.184153: step 14311, loss 0.121259, acc 0.9375
2017-03-02T17:50:57.259224: step 14312, loss 0.109179, acc 0.953125
2017-03-02T17:50:57.340412: step 14313, loss 0.165215, acc 0.921875
2017-03-02T17:50:57.410911: step 14314, loss 0.254893, acc 0.890625
2017-03-02T17:50:57.482322: step 14315, loss 0.13182, acc 0.9375
2017-03-02T17:50:57.556227: step 14316, loss 0.143548, acc 0.9375
2017-03-02T17:50:57.633248: step 14317, loss 0.0528763, acc 0.984375
2017-03-02T17:50:57.713555: step 14318, loss 0.144053, acc 0.921875
2017-03-02T17:50:57.785548: step 14319, loss 0.192625, acc 0.890625
2017-03-02T17:50:57.868137: step 14320, loss 0.0852357, acc 0.96875
2017-03-02T17:50:57.935626: step 14321, loss 0.231247, acc 0.875
2017-03-02T17:50:58.013019: step 14322, loss 0.164672, acc 0.921875
2017-03-02T17:50:58.083417: step 14323, loss 0.279507, acc 0.890625
2017-03-02T17:50:58.152125: step 14324, loss 0.184002, acc 0.9375
2017-03-02T17:50:58.219675: step 14325, loss 0.110697, acc 0.953125
2017-03-02T17:50:58.291765: step 14326, loss 0.21406, acc 0.90625
2017-03-02T17:50:58.364801: step 14327, loss 0.173343, acc 0.9375
2017-03-02T17:50:58.444084: step 14328, loss 0.0640686, acc 0.984375
2017-03-02T17:50:58.520072: step 14329, loss 0.0933898, acc 0.953125
2017-03-02T17:50:58.604950: step 14330, loss 0.16193, acc 0.9375
2017-03-02T17:50:58.676040: step 14331, loss 0.197203, acc 0.953125
2017-03-02T17:50:58.749936: step 14332, loss 0.144546, acc 0.9375
2017-03-02T17:50:58.816429: step 14333, loss 0.111967, acc 0.96875
2017-03-02T17:50:58.886099: step 14334, loss 0.246854, acc 0.90625
2017-03-02T17:50:58.960989: step 14335, loss 0.186879, acc 0.921875
2017-03-02T17:50:59.041184: step 14336, loss 0.0968366, acc 0.953125
2017-03-02T17:50:59.115032: step 14337, loss 0.114927, acc 0.953125
2017-03-02T17:50:59.190860: step 14338, loss 0.224178, acc 0.9375
2017-03-02T17:50:59.263754: step 14339, loss 0.0801309, acc 0.953125
2017-03-02T17:50:59.341761: step 14340, loss 0.218942, acc 0.921875
2017-03-02T17:50:59.417670: step 14341, loss 0.139524, acc 0.9375
2017-03-02T17:50:59.490356: step 14342, loss 0.231982, acc 0.90625
2017-03-02T17:50:59.560079: step 14343, loss 0.283168, acc 0.90625
2017-03-02T17:50:59.638586: step 14344, loss 0.109365, acc 0.953125
2017-03-02T17:50:59.715579: step 14345, loss 0.121625, acc 0.9375
2017-03-02T17:50:59.786705: step 14346, loss 0.103474, acc 0.984375
2017-03-02T17:50:59.854764: step 14347, loss 0.100767, acc 0.96875
2017-03-02T17:50:59.930056: step 14348, loss 0.212828, acc 0.875
2017-03-02T17:51:00.006416: step 14349, loss 0.175788, acc 0.921875
2017-03-02T17:51:00.080731: step 14350, loss 0.104432, acc 0.9375
2017-03-02T17:51:00.159387: step 14351, loss 0.217743, acc 0.890625
2017-03-02T17:51:00.231989: step 14352, loss 0.0794324, acc 0.953125
2017-03-02T17:51:00.304552: step 14353, loss 0.15014, acc 0.953125
2017-03-02T17:51:00.377617: step 14354, loss 0.194575, acc 0.921875
2017-03-02T17:51:00.453333: step 14355, loss 0.157979, acc 0.953125
2017-03-02T17:51:00.530360: step 14356, loss 0.185452, acc 0.953125
2017-03-02T17:51:00.631393: step 14357, loss 0.126914, acc 0.96875
2017-03-02T17:51:00.702526: step 14358, loss 0.163301, acc 0.921875
2017-03-02T17:51:00.778172: step 14359, loss 0.207609, acc 0.90625
2017-03-02T17:51:00.850897: step 14360, loss 0.149315, acc 0.9375
2017-03-02T17:51:00.917383: step 14361, loss 0.153288, acc 0.921875
2017-03-02T17:51:00.990685: step 14362, loss 0.227001, acc 0.921875
2017-03-02T17:51:01.064912: step 14363, loss 0.203868, acc 0.921875
2017-03-02T17:51:01.145014: step 14364, loss 0.20492, acc 0.90625
2017-03-02T17:51:01.214059: step 14365, loss 0.123682, acc 0.9375
2017-03-02T17:51:01.278035: step 14366, loss 0.19718, acc 0.90625
2017-03-02T17:51:01.357328: step 14367, loss 0.25085, acc 0.890625
2017-03-02T17:51:01.430639: step 14368, loss 0.301221, acc 0.875
2017-03-02T17:51:01.501118: step 14369, loss 0.0565106, acc 0.984375
2017-03-02T17:51:01.563725: step 14370, loss 0.234716, acc 0.90625
2017-03-02T17:51:01.626997: step 14371, loss 0.1982, acc 0.890625
2017-03-02T17:51:01.698427: step 14372, loss 0.235975, acc 0.890625
2017-03-02T17:51:01.769312: step 14373, loss 0.162015, acc 0.9375
2017-03-02T17:51:01.843284: step 14374, loss 0.137186, acc 0.953125
2017-03-02T17:51:01.915037: step 14375, loss 0.598371, acc 0.84375
2017-03-02T17:51:01.988708: step 14376, loss 0.141173, acc 0.953125
2017-03-02T17:51:02.063057: step 14377, loss 0.263334, acc 0.90625
2017-03-02T17:51:02.140092: step 14378, loss 0.159938, acc 0.96875
2017-03-02T17:51:02.207883: step 14379, loss 0.12963, acc 0.9375
2017-03-02T17:51:02.275945: step 14380, loss 0.0846795, acc 0.953125
2017-03-02T17:51:02.350631: step 14381, loss 0.0774137, acc 0.96875
2017-03-02T17:51:02.435606: step 14382, loss 0.150553, acc 0.9375
2017-03-02T17:51:02.505919: step 14383, loss 0.0740684, acc 0.96875
2017-03-02T17:51:02.579104: step 14384, loss 0.15189, acc 0.953125
2017-03-02T17:51:02.653116: step 14385, loss 0.188622, acc 0.90625
2017-03-02T17:51:02.736626: step 14386, loss 0.143671, acc 0.953125
2017-03-02T17:51:02.814761: step 14387, loss 0.103918, acc 0.984375
2017-03-02T17:51:02.896773: step 14388, loss 0.122029, acc 0.953125
2017-03-02T17:51:02.970081: step 14389, loss 0.137763, acc 0.953125
2017-03-02T17:51:03.043018: step 14390, loss 0.197872, acc 0.90625
2017-03-02T17:51:03.114295: step 14391, loss 0.10341, acc 0.953125
2017-03-02T17:51:03.188423: step 14392, loss 0.347754, acc 0.859375
2017-03-02T17:51:03.257803: step 14393, loss 0.212211, acc 0.90625
2017-03-02T17:51:03.334696: step 14394, loss 0.272204, acc 0.890625
2017-03-02T17:51:03.405093: step 14395, loss 0.132657, acc 0.96875
2017-03-02T17:51:03.475054: step 14396, loss 0.108286, acc 0.96875
2017-03-02T17:51:03.545768: step 14397, loss 0.104099, acc 0.953125
2017-03-02T17:51:03.615551: step 14398, loss 0.284735, acc 0.875
2017-03-02T17:51:03.685649: step 14399, loss 0.182736, acc 0.953125
2017-03-02T17:51:03.765692: step 14400, loss 0.0702866, acc 0.984375

Evaluation:
2017-03-02T17:51:03.799097: step 14400, loss 1.79086, acc 0.66186

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14400

2017-03-02T17:51:04.255341: step 14401, loss 0.13757, acc 0.921875
2017-03-02T17:51:04.327551: step 14402, loss 0.112704, acc 0.9375
2017-03-02T17:51:04.410122: step 14403, loss 0.28904, acc 0.890625
2017-03-02T17:51:04.485440: step 14404, loss 0.182484, acc 0.953125
2017-03-02T17:51:04.555796: step 14405, loss 0.249484, acc 0.890625
2017-03-02T17:51:04.620981: step 14406, loss 0.161791, acc 0.9375
2017-03-02T17:51:04.711747: step 14407, loss 0.160082, acc 0.953125
2017-03-02T17:51:04.781918: step 14408, loss 0.347323, acc 0.84375
2017-03-02T17:51:04.862705: step 14409, loss 0.128349, acc 0.9375
2017-03-02T17:51:04.944757: step 14410, loss 0.136042, acc 0.9375
2017-03-02T17:51:05.007746: step 14411, loss 0.133106, acc 0.953125
2017-03-02T17:51:05.073839: step 14412, loss 0.179764, acc 0.875
2017-03-02T17:51:05.142279: step 14413, loss 0.169494, acc 0.953125
2017-03-02T17:51:05.213828: step 14414, loss 0.0832437, acc 0.953125
2017-03-02T17:51:05.296573: step 14415, loss 0.19483, acc 0.921875
2017-03-02T17:51:05.379097: step 14416, loss 0.0632693, acc 0.984375
2017-03-02T17:51:05.449982: step 14417, loss 0.169387, acc 0.90625
2017-03-02T17:51:05.523561: step 14418, loss 0.0945016, acc 0.96875
2017-03-02T17:51:05.603272: step 14419, loss 0.0899589, acc 0.953125
2017-03-02T17:51:05.669252: step 14420, loss 0.141429, acc 0.921875
2017-03-02T17:51:05.742886: step 14421, loss 0.123774, acc 0.953125
2017-03-02T17:51:05.815526: step 14422, loss 0.16232, acc 0.921875
2017-03-02T17:51:05.906558: step 14423, loss 0.324946, acc 0.84375
2017-03-02T17:51:05.986171: step 14424, loss 0.115405, acc 0.953125
2017-03-02T17:51:06.062320: step 14425, loss 0.275852, acc 0.875
2017-03-02T17:51:06.136785: step 14426, loss 0.135035, acc 0.921875
2017-03-02T17:51:06.211083: step 14427, loss 0.151495, acc 0.953125
2017-03-02T17:51:06.291231: step 14428, loss 0.183009, acc 0.90625
2017-03-02T17:51:06.364384: step 14429, loss 0.156978, acc 0.9375
2017-03-02T17:51:06.437212: step 14430, loss 0.123821, acc 0.921875
2017-03-02T17:51:06.508442: step 14431, loss 0.214854, acc 0.921875
2017-03-02T17:51:06.590262: step 14432, loss 0.0590734, acc 0.96875
2017-03-02T17:51:06.663774: step 14433, loss 0.123781, acc 0.9375
2017-03-02T17:51:06.736635: step 14434, loss 0.136479, acc 0.953125
2017-03-02T17:51:06.818300: step 14435, loss 0.0543841, acc 1
2017-03-02T17:51:06.907811: step 14436, loss 0.116839, acc 0.953125
2017-03-02T17:51:06.985090: step 14437, loss 0.203738, acc 0.90625
2017-03-02T17:51:07.053746: step 14438, loss 0.217355, acc 0.921875
2017-03-02T17:51:07.127244: step 14439, loss 0.205522, acc 0.890625
2017-03-02T17:51:07.202555: step 14440, loss 0.121533, acc 0.9375
2017-03-02T17:51:07.276538: step 14441, loss 0.130789, acc 0.9375
2017-03-02T17:51:07.349536: step 14442, loss 0.0723251, acc 0.96875
2017-03-02T17:51:07.422242: step 14443, loss 0.0791447, acc 0.984375
2017-03-02T17:51:07.502927: step 14444, loss 0.149056, acc 0.921875
2017-03-02T17:51:07.570955: step 14445, loss 0.207731, acc 0.921875
2017-03-02T17:51:07.645424: step 14446, loss 0.0412502, acc 0.984375
2017-03-02T17:51:07.713567: step 14447, loss 0.242582, acc 0.890625
2017-03-02T17:51:07.777957: step 14448, loss 0.240611, acc 0.890625
2017-03-02T17:51:07.843341: step 14449, loss 0.143723, acc 0.921875
2017-03-02T17:51:07.917285: step 14450, loss 0.179871, acc 0.890625
2017-03-02T17:51:07.997253: step 14451, loss 0.0635912, acc 0.96875
2017-03-02T17:51:08.077545: step 14452, loss 0.110087, acc 0.953125
2017-03-02T17:51:08.151230: step 14453, loss 0.182167, acc 0.953125
2017-03-02T17:51:08.224318: step 14454, loss 0.10564, acc 0.96875
2017-03-02T17:51:08.293589: step 14455, loss 0.168948, acc 0.921875
2017-03-02T17:51:08.368868: step 14456, loss 0.205026, acc 0.90625
2017-03-02T17:51:08.446837: step 14457, loss 0.142393, acc 0.9375
2017-03-02T17:51:08.514880: step 14458, loss 0.191729, acc 0.921875
2017-03-02T17:51:08.588088: step 14459, loss 0.109988, acc 0.953125
2017-03-02T17:51:08.656627: step 14460, loss 0.0792825, acc 0.96875
2017-03-02T17:51:08.728313: step 14461, loss 0.205516, acc 0.9375
2017-03-02T17:51:08.800782: step 14462, loss 0.200258, acc 0.921875
2017-03-02T17:51:08.875574: step 14463, loss 0.118475, acc 0.953125
2017-03-02T17:51:08.950015: step 14464, loss 0.181424, acc 0.921875
2017-03-02T17:51:09.021991: step 14465, loss 0.271783, acc 0.890625
2017-03-02T17:51:09.089631: step 14466, loss 0.168991, acc 0.9375
2017-03-02T17:51:09.156878: step 14467, loss 0.0663613, acc 0.984375
2017-03-02T17:51:09.224036: step 14468, loss 0.140887, acc 0.9375
2017-03-02T17:51:09.294477: step 14469, loss 0.154762, acc 0.921875
2017-03-02T17:51:09.371360: step 14470, loss 0.0555841, acc 0.984375
2017-03-02T17:51:09.450362: step 14471, loss 0.195866, acc 0.890625
2017-03-02T17:51:09.526791: step 14472, loss 0.214843, acc 0.9375
2017-03-02T17:51:09.599200: step 14473, loss 0.119592, acc 0.96875
2017-03-02T17:51:09.669524: step 14474, loss 0.260654, acc 0.875
2017-03-02T17:51:09.745830: step 14475, loss 0.104, acc 0.953125
2017-03-02T17:51:09.810607: step 14476, loss 0.261443, acc 0.90625
2017-03-02T17:51:09.877013: step 14477, loss 0.127153, acc 0.984375
2017-03-02T17:51:09.951119: step 14478, loss 0.199946, acc 0.921875
2017-03-02T17:51:10.026020: step 14479, loss 0.122157, acc 0.921875
2017-03-02T17:51:10.100318: step 14480, loss 0.2368, acc 0.9375
2017-03-02T17:51:10.172960: step 14481, loss 0.144711, acc 0.9375
2017-03-02T17:51:10.254944: step 14482, loss 0.111258, acc 0.953125
2017-03-02T17:51:10.334737: step 14483, loss 0.147064, acc 0.953125
2017-03-02T17:51:10.407874: step 14484, loss 0.286163, acc 0.890625
2017-03-02T17:51:10.478717: step 14485, loss 0.214331, acc 0.9375
2017-03-02T17:51:10.557795: step 14486, loss 0.19058, acc 0.921875
2017-03-02T17:51:10.632659: step 14487, loss 0.150901, acc 0.921875
2017-03-02T17:51:10.706524: step 14488, loss 0.108037, acc 0.953125
2017-03-02T17:51:10.780063: step 14489, loss 0.138475, acc 0.921875
2017-03-02T17:51:10.858232: step 14490, loss 0.420583, acc 0.828125
2017-03-02T17:51:10.934852: step 14491, loss 0.161068, acc 0.921875
2017-03-02T17:51:11.007369: step 14492, loss 0.143381, acc 0.9375
2017-03-02T17:51:11.080918: step 14493, loss 0.079117, acc 0.953125
2017-03-02T17:51:11.155789: step 14494, loss 0.132999, acc 0.9375
2017-03-02T17:51:11.223465: step 14495, loss 0.122377, acc 0.96875
2017-03-02T17:51:11.292858: step 14496, loss 0.155363, acc 0.9375
2017-03-02T17:51:11.373546: step 14497, loss 0.232867, acc 0.875
2017-03-02T17:51:11.450338: step 14498, loss 0.0468421, acc 0.984375
2017-03-02T17:51:11.523104: step 14499, loss 0.0569298, acc 0.984375
2017-03-02T17:51:11.595117: step 14500, loss 0.208814, acc 0.921875

Evaluation:
2017-03-02T17:51:11.628313: step 14500, loss 1.8043, acc 0.664744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14500

2017-03-02T17:51:12.081379: step 14501, loss 0.153585, acc 0.921875
2017-03-02T17:51:12.149641: step 14502, loss 0.14947, acc 0.9375
2017-03-02T17:51:12.221351: step 14503, loss 0.288551, acc 0.90625
2017-03-02T17:51:12.288588: step 14504, loss 0.00623435, acc 1
2017-03-02T17:51:12.365413: step 14505, loss 0.104927, acc 0.953125
2017-03-02T17:51:12.440566: step 14506, loss 0.135977, acc 0.921875
2017-03-02T17:51:12.515789: step 14507, loss 0.322119, acc 0.859375
2017-03-02T17:51:12.586414: step 14508, loss 0.109588, acc 0.9375
2017-03-02T17:51:12.663873: step 14509, loss 0.0764302, acc 0.96875
2017-03-02T17:51:12.739863: step 14510, loss 0.123876, acc 0.96875
2017-03-02T17:51:12.814306: step 14511, loss 0.223364, acc 0.890625
2017-03-02T17:51:12.888287: step 14512, loss 0.0779536, acc 0.96875
2017-03-02T17:51:12.971586: step 14513, loss 0.181624, acc 0.953125
2017-03-02T17:51:13.047666: step 14514, loss 0.0557735, acc 0.984375
2017-03-02T17:51:13.120932: step 14515, loss 0.262196, acc 0.921875
2017-03-02T17:51:13.196894: step 14516, loss 0.170251, acc 0.921875
2017-03-02T17:51:13.266533: step 14517, loss 0.0810019, acc 1
2017-03-02T17:51:13.338635: step 14518, loss 0.159477, acc 0.921875
2017-03-02T17:51:13.412628: step 14519, loss 0.181673, acc 0.90625
2017-03-02T17:51:13.492889: step 14520, loss 0.239411, acc 0.890625
2017-03-02T17:51:13.571978: step 14521, loss 0.164848, acc 0.9375
2017-03-02T17:51:13.645719: step 14522, loss 0.0382223, acc 0.984375
2017-03-02T17:51:13.727922: step 14523, loss 0.222094, acc 0.875
2017-03-02T17:51:13.811119: step 14524, loss 0.225147, acc 0.9375
2017-03-02T17:51:13.902062: step 14525, loss 0.221744, acc 0.921875
2017-03-02T17:51:13.969998: step 14526, loss 0.111823, acc 0.96875
2017-03-02T17:51:14.047694: step 14527, loss 0.221053, acc 0.9375
2017-03-02T17:51:14.126835: step 14528, loss 0.102116, acc 0.953125
2017-03-02T17:51:14.206167: step 14529, loss 0.0635426, acc 0.953125
2017-03-02T17:51:14.284019: step 14530, loss 0.211338, acc 0.921875
2017-03-02T17:51:14.359447: step 14531, loss 0.0860287, acc 0.984375
2017-03-02T17:51:14.439542: step 14532, loss 0.191631, acc 0.90625
2017-03-02T17:51:14.513971: step 14533, loss 0.190443, acc 0.921875
2017-03-02T17:51:14.595249: step 14534, loss 0.338462, acc 0.90625
2017-03-02T17:51:14.662680: step 14535, loss 0.108278, acc 0.96875
2017-03-02T17:51:14.724566: step 14536, loss 0.183076, acc 0.90625
2017-03-02T17:51:14.795608: step 14537, loss 0.13184, acc 0.9375
2017-03-02T17:51:14.864473: step 14538, loss 0.149213, acc 0.921875
2017-03-02T17:51:14.948593: step 14539, loss 0.0682211, acc 0.96875
2017-03-02T17:51:15.026552: step 14540, loss 0.115725, acc 0.96875
2017-03-02T17:51:15.102174: step 14541, loss 0.189886, acc 0.921875
2017-03-02T17:51:15.173173: step 14542, loss 0.101465, acc 0.953125
2017-03-02T17:51:15.247258: step 14543, loss 0.0312186, acc 1
2017-03-02T17:51:15.324690: step 14544, loss 0.164209, acc 0.9375
2017-03-02T17:51:15.408897: step 14545, loss 0.129272, acc 0.96875
2017-03-02T17:51:15.481063: step 14546, loss 0.237915, acc 0.890625
2017-03-02T17:51:15.558516: step 14547, loss 0.138893, acc 0.921875
2017-03-02T17:51:15.648823: step 14548, loss 0.137597, acc 0.953125
2017-03-02T17:51:15.718868: step 14549, loss 0.200832, acc 0.890625
2017-03-02T17:51:15.789461: step 14550, loss 0.141758, acc 0.9375
2017-03-02T17:51:15.866201: step 14551, loss 0.0913051, acc 0.96875
2017-03-02T17:51:15.940558: step 14552, loss 0.251252, acc 0.890625
2017-03-02T17:51:16.013949: step 14553, loss 0.156598, acc 0.9375
2017-03-02T17:51:16.084379: step 14554, loss 0.112458, acc 0.96875
2017-03-02T17:51:16.156550: step 14555, loss 0.112641, acc 0.96875
2017-03-02T17:51:16.228299: step 14556, loss 0.0275863, acc 0.984375
2017-03-02T17:51:16.300818: step 14557, loss 0.175053, acc 0.921875
2017-03-02T17:51:16.374541: step 14558, loss 0.214961, acc 0.921875
2017-03-02T17:51:16.444678: step 14559, loss 0.164384, acc 0.9375
2017-03-02T17:51:16.517368: step 14560, loss 0.124452, acc 0.921875
2017-03-02T17:51:16.588631: step 14561, loss 0.130808, acc 0.953125
2017-03-02T17:51:16.666177: step 14562, loss 0.21478, acc 0.90625
2017-03-02T17:51:16.735349: step 14563, loss 0.405281, acc 0.921875
2017-03-02T17:51:16.804643: step 14564, loss 0.212332, acc 0.921875
2017-03-02T17:51:16.881648: step 14565, loss 0.203966, acc 0.921875
2017-03-02T17:51:16.957746: step 14566, loss 0.10468, acc 0.96875
2017-03-02T17:51:17.033187: step 14567, loss 0.187905, acc 0.921875
2017-03-02T17:51:17.107315: step 14568, loss 0.13028, acc 0.953125
2017-03-02T17:51:17.183664: step 14569, loss 0.190225, acc 0.921875
2017-03-02T17:51:17.257430: step 14570, loss 0.177862, acc 0.90625
2017-03-02T17:51:17.331174: step 14571, loss 0.212128, acc 0.921875
2017-03-02T17:51:17.396099: step 14572, loss 0.179269, acc 0.921875
2017-03-02T17:51:17.463189: step 14573, loss 0.157075, acc 0.9375
2017-03-02T17:51:17.538169: step 14574, loss 0.246756, acc 0.90625
2017-03-02T17:51:17.610442: step 14575, loss 0.141816, acc 0.9375
2017-03-02T17:51:17.675529: step 14576, loss 0.092941, acc 0.953125
2017-03-02T17:51:17.750553: step 14577, loss 0.087433, acc 0.96875
2017-03-02T17:51:17.823884: step 14578, loss 0.100664, acc 0.953125
2017-03-02T17:51:17.905716: step 14579, loss 0.124327, acc 0.953125
2017-03-02T17:51:17.980824: step 14580, loss 0.113282, acc 0.953125
2017-03-02T17:51:18.047497: step 14581, loss 0.0669951, acc 0.953125
2017-03-02T17:51:18.111756: step 14582, loss 0.119637, acc 0.9375
2017-03-02T17:51:18.181466: step 14583, loss 0.208379, acc 0.890625
2017-03-02T17:51:18.257177: step 14584, loss 0.0634626, acc 0.984375
2017-03-02T17:51:18.331282: step 14585, loss 0.196637, acc 0.921875
2017-03-02T17:51:18.402756: step 14586, loss 0.142899, acc 0.921875
2017-03-02T17:51:18.476965: step 14587, loss 0.134461, acc 0.9375
2017-03-02T17:51:18.544440: step 14588, loss 0.221687, acc 0.875
2017-03-02T17:51:18.619756: step 14589, loss 0.135063, acc 0.9375
2017-03-02T17:51:18.690611: step 14590, loss 0.420901, acc 0.84375
2017-03-02T17:51:18.760482: step 14591, loss 0.0575204, acc 0.96875
2017-03-02T17:51:18.831878: step 14592, loss 0.178304, acc 0.921875
2017-03-02T17:51:18.905178: step 14593, loss 0.180399, acc 0.90625
2017-03-02T17:51:18.979377: step 14594, loss 0.0944297, acc 0.953125
2017-03-02T17:51:19.051184: step 14595, loss 0.101026, acc 0.953125
2017-03-02T17:51:19.125488: step 14596, loss 0.0973719, acc 0.953125
2017-03-02T17:51:19.203059: step 14597, loss 0.109559, acc 0.953125
2017-03-02T17:51:19.276165: step 14598, loss 0.171356, acc 0.921875
2017-03-02T17:51:19.350097: step 14599, loss 0.0883799, acc 0.96875
2017-03-02T17:51:19.427284: step 14600, loss 0.133609, acc 0.953125

Evaluation:
2017-03-02T17:51:19.450567: step 14600, loss 1.80461, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14600

2017-03-02T17:51:19.892749: step 14601, loss 0.132581, acc 0.921875
2017-03-02T17:51:19.967264: step 14602, loss 0.101292, acc 0.96875
2017-03-02T17:51:20.042904: step 14603, loss 0.0922549, acc 0.984375
2017-03-02T17:51:20.124079: step 14604, loss 0.0761249, acc 0.984375
2017-03-02T17:51:20.207479: step 14605, loss 0.118636, acc 0.9375
2017-03-02T17:51:20.277341: step 14606, loss 0.309425, acc 0.84375
2017-03-02T17:51:20.349611: step 14607, loss 0.257986, acc 0.921875
2017-03-02T17:51:20.420741: step 14608, loss 0.107034, acc 0.96875
2017-03-02T17:51:20.496254: step 14609, loss 0.127454, acc 0.921875
2017-03-02T17:51:20.569732: step 14610, loss 0.0938345, acc 0.9375
2017-03-02T17:51:20.642782: step 14611, loss 0.175747, acc 0.9375
2017-03-02T17:51:20.728675: step 14612, loss 0.113763, acc 0.96875
2017-03-02T17:51:20.802793: step 14613, loss 0.309922, acc 0.859375
2017-03-02T17:51:20.868432: step 14614, loss 0.146772, acc 0.921875
2017-03-02T17:51:20.933377: step 14615, loss 0.173094, acc 0.90625
2017-03-02T17:51:21.012721: step 14616, loss 0.110732, acc 0.9375
2017-03-02T17:51:21.086762: step 14617, loss 0.231314, acc 0.9375
2017-03-02T17:51:21.165110: step 14618, loss 0.140084, acc 0.953125
2017-03-02T17:51:21.248702: step 14619, loss 0.194066, acc 0.921875
2017-03-02T17:51:21.321764: step 14620, loss 0.229244, acc 0.9375
2017-03-02T17:51:21.391147: step 14621, loss 0.164629, acc 0.921875
2017-03-02T17:51:21.463115: step 14622, loss 0.190561, acc 0.9375
2017-03-02T17:51:21.539142: step 14623, loss 0.154641, acc 0.921875
2017-03-02T17:51:21.608323: step 14624, loss 0.201695, acc 0.953125
2017-03-02T17:51:21.684340: step 14625, loss 0.279066, acc 0.859375
2017-03-02T17:51:21.751383: step 14626, loss 0.247434, acc 0.90625
2017-03-02T17:51:21.823717: step 14627, loss 0.169792, acc 0.921875
2017-03-02T17:51:21.896895: step 14628, loss 0.138091, acc 0.9375
2017-03-02T17:51:21.969242: step 14629, loss 0.121679, acc 0.953125
2017-03-02T17:51:22.036859: step 14630, loss 0.194165, acc 0.921875
2017-03-02T17:51:22.110521: step 14631, loss 0.1109, acc 0.9375
2017-03-02T17:51:22.179741: step 14632, loss 0.117703, acc 0.921875
2017-03-02T17:51:22.250167: step 14633, loss 0.397527, acc 0.859375
2017-03-02T17:51:22.317014: step 14634, loss 0.297877, acc 0.90625
2017-03-02T17:51:22.400820: step 14635, loss 0.137334, acc 0.921875
2017-03-02T17:51:22.473974: step 14636, loss 0.122821, acc 0.96875
2017-03-02T17:51:22.541532: step 14637, loss 0.101573, acc 0.953125
2017-03-02T17:51:22.613375: step 14638, loss 0.211284, acc 0.921875
2017-03-02T17:51:22.689143: step 14639, loss 0.222526, acc 0.90625
2017-03-02T17:51:22.762376: step 14640, loss 0.167634, acc 0.96875
2017-03-02T17:51:22.839839: step 14641, loss 0.247014, acc 0.90625
2017-03-02T17:51:22.914595: step 14642, loss 0.116727, acc 0.953125
2017-03-02T17:51:22.983348: step 14643, loss 0.126984, acc 0.953125
2017-03-02T17:51:23.056795: step 14644, loss 0.0774873, acc 0.953125
2017-03-02T17:51:23.133225: step 14645, loss 0.12197, acc 0.96875
2017-03-02T17:51:23.227472: step 14646, loss 0.134998, acc 0.9375
2017-03-02T17:51:23.299483: step 14647, loss 0.172379, acc 0.921875
2017-03-02T17:51:23.382408: step 14648, loss 0.170094, acc 0.90625
2017-03-02T17:51:23.457537: step 14649, loss 0.203039, acc 0.90625
2017-03-02T17:51:23.528602: step 14650, loss 0.262336, acc 0.90625
2017-03-02T17:51:23.612660: step 14651, loss 0.119149, acc 0.9375
2017-03-02T17:51:23.687128: step 14652, loss 0.141516, acc 0.90625
2017-03-02T17:51:23.762020: step 14653, loss 0.164126, acc 0.9375
2017-03-02T17:51:23.833979: step 14654, loss 0.142693, acc 0.921875
2017-03-02T17:51:23.905874: step 14655, loss 0.148741, acc 0.953125
2017-03-02T17:51:23.992176: step 14656, loss 0.220573, acc 0.890625
2017-03-02T17:51:24.087623: step 14657, loss 0.185133, acc 0.90625
2017-03-02T17:51:24.157052: step 14658, loss 0.237122, acc 0.921875
2017-03-02T17:51:24.232281: step 14659, loss 0.11883, acc 0.953125
2017-03-02T17:51:24.314071: step 14660, loss 0.0872608, acc 0.984375
2017-03-02T17:51:24.395973: step 14661, loss 0.187871, acc 0.9375
2017-03-02T17:51:24.461429: step 14662, loss 0.320548, acc 0.859375
2017-03-02T17:51:24.538544: step 14663, loss 0.0909187, acc 0.984375
2017-03-02T17:51:24.624055: step 14664, loss 0.179874, acc 0.90625
2017-03-02T17:51:24.702184: step 14665, loss 0.149236, acc 0.953125
2017-03-02T17:51:24.777351: step 14666, loss 0.147388, acc 0.9375
2017-03-02T17:51:24.852854: step 14667, loss 0.174662, acc 0.921875
2017-03-02T17:51:24.938074: step 14668, loss 0.231271, acc 0.90625
2017-03-02T17:51:25.018915: step 14669, loss 0.183615, acc 0.9375
2017-03-02T17:51:25.095340: step 14670, loss 0.175863, acc 0.90625
2017-03-02T17:51:25.176308: step 14671, loss 0.140858, acc 0.921875
2017-03-02T17:51:25.251184: step 14672, loss 0.174362, acc 0.953125
2017-03-02T17:51:25.342060: step 14673, loss 0.159852, acc 0.890625
2017-03-02T17:51:25.425706: step 14674, loss 0.261895, acc 0.875
2017-03-02T17:51:25.502999: step 14675, loss 0.197128, acc 0.90625
2017-03-02T17:51:25.581648: step 14676, loss 0.18709, acc 0.921875
2017-03-02T17:51:25.651526: step 14677, loss 0.230339, acc 0.875
2017-03-02T17:51:25.721647: step 14678, loss 0.186351, acc 0.90625
2017-03-02T17:51:25.802290: step 14679, loss 0.131151, acc 0.953125
2017-03-02T17:51:25.895350: step 14680, loss 0.225389, acc 0.859375
2017-03-02T17:51:25.974805: step 14681, loss 0.0934154, acc 0.984375
2017-03-02T17:51:26.051668: step 14682, loss 0.110011, acc 0.984375
2017-03-02T17:51:26.128090: step 14683, loss 0.168738, acc 0.9375
2017-03-02T17:51:26.200560: step 14684, loss 0.180733, acc 0.90625
2017-03-02T17:51:26.274596: step 14685, loss 0.315782, acc 0.859375
2017-03-02T17:51:26.342735: step 14686, loss 0.224784, acc 0.875
2017-03-02T17:51:26.407824: step 14687, loss 0.245347, acc 0.90625
2017-03-02T17:51:26.475260: step 14688, loss 0.238109, acc 0.90625
2017-03-02T17:51:26.550720: step 14689, loss 0.145805, acc 0.953125
2017-03-02T17:51:26.623339: step 14690, loss 0.117293, acc 0.953125
2017-03-02T17:51:26.691737: step 14691, loss 0.112598, acc 0.953125
2017-03-02T17:51:26.760053: step 14692, loss 0.238822, acc 0.875
2017-03-02T17:51:26.836403: step 14693, loss 0.122712, acc 0.9375
2017-03-02T17:51:26.910509: step 14694, loss 0.111146, acc 0.9375
2017-03-02T17:51:26.990335: step 14695, loss 0.0846726, acc 0.96875
2017-03-02T17:51:27.050730: step 14696, loss 0.190776, acc 0.921875
2017-03-02T17:51:27.117924: step 14697, loss 0.159051, acc 0.90625
2017-03-02T17:51:27.186867: step 14698, loss 0.131248, acc 0.953125
2017-03-02T17:51:27.261422: step 14699, loss 0.254451, acc 0.921875
2017-03-02T17:51:27.341842: step 14700, loss 7.09819e-05, acc 1

Evaluation:
2017-03-02T17:51:27.378482: step 14700, loss 1.78225, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14700

2017-03-02T17:51:27.845765: step 14701, loss 0.177408, acc 0.90625
2017-03-02T17:51:27.920355: step 14702, loss 0.208768, acc 0.9375
2017-03-02T17:51:27.991102: step 14703, loss 0.177257, acc 0.90625
2017-03-02T17:51:28.062585: step 14704, loss 0.078204, acc 0.984375
2017-03-02T17:51:28.140824: step 14705, loss 0.188823, acc 0.90625
2017-03-02T17:51:28.215635: step 14706, loss 0.124375, acc 0.9375
2017-03-02T17:51:28.293901: step 14707, loss 0.0951882, acc 0.96875
2017-03-02T17:51:28.363426: step 14708, loss 0.112137, acc 0.953125
2017-03-02T17:51:28.435861: step 14709, loss 0.191858, acc 0.921875
2017-03-02T17:51:28.505570: step 14710, loss 0.138626, acc 0.9375
2017-03-02T17:51:28.580952: step 14711, loss 0.117991, acc 0.953125
2017-03-02T17:51:28.654328: step 14712, loss 0.224345, acc 0.84375
2017-03-02T17:51:28.725879: step 14713, loss 0.107753, acc 0.953125
2017-03-02T17:51:28.805480: step 14714, loss 0.194855, acc 0.875
2017-03-02T17:51:28.877888: step 14715, loss 0.171491, acc 0.890625
2017-03-02T17:51:28.951590: step 14716, loss 0.204084, acc 0.953125
2017-03-02T17:51:29.030261: step 14717, loss 0.255146, acc 0.90625
2017-03-02T17:51:29.096634: step 14718, loss 0.179674, acc 0.90625
2017-03-02T17:51:29.164797: step 14719, loss 0.102138, acc 0.953125
2017-03-02T17:51:29.237014: step 14720, loss 0.18261, acc 0.90625
2017-03-02T17:51:29.309452: step 14721, loss 0.21523, acc 0.921875
2017-03-02T17:51:29.384639: step 14722, loss 0.0673091, acc 0.984375
2017-03-02T17:51:29.457098: step 14723, loss 0.131731, acc 0.96875
2017-03-02T17:51:29.526959: step 14724, loss 0.128476, acc 0.953125
2017-03-02T17:51:29.597643: step 14725, loss 0.12979, acc 0.921875
2017-03-02T17:51:29.664593: step 14726, loss 0.125722, acc 0.953125
2017-03-02T17:51:29.736418: step 14727, loss 0.173334, acc 0.9375
2017-03-02T17:51:29.817116: step 14728, loss 0.155241, acc 0.9375
2017-03-02T17:51:29.893579: step 14729, loss 0.214029, acc 0.921875
2017-03-02T17:51:29.962618: step 14730, loss 0.174351, acc 0.9375
2017-03-02T17:51:30.033293: step 14731, loss 0.117683, acc 0.9375
2017-03-02T17:51:30.101600: step 14732, loss 0.113849, acc 0.953125
2017-03-02T17:51:30.177707: step 14733, loss 0.137822, acc 0.9375
2017-03-02T17:51:30.254034: step 14734, loss 0.10808, acc 0.9375
2017-03-02T17:51:30.332050: step 14735, loss 0.0582012, acc 1
2017-03-02T17:51:30.402898: step 14736, loss 0.104197, acc 0.9375
2017-03-02T17:51:30.471378: step 14737, loss 0.223202, acc 0.921875
2017-03-02T17:51:30.549082: step 14738, loss 0.152535, acc 0.953125
2017-03-02T17:51:30.623452: step 14739, loss 0.177258, acc 0.90625
2017-03-02T17:51:30.695914: step 14740, loss 0.0984214, acc 0.953125
2017-03-02T17:51:30.769438: step 14741, loss 0.174754, acc 0.921875
2017-03-02T17:51:30.844184: step 14742, loss 0.200765, acc 0.921875
2017-03-02T17:51:30.918176: step 14743, loss 0.145354, acc 0.921875
2017-03-02T17:51:30.989512: step 14744, loss 0.134296, acc 0.953125
2017-03-02T17:51:31.066407: step 14745, loss 0.0970345, acc 0.953125
2017-03-02T17:51:31.134420: step 14746, loss 0.183992, acc 0.890625
2017-03-02T17:51:31.208095: step 14747, loss 0.214777, acc 0.90625
2017-03-02T17:51:31.278480: step 14748, loss 0.165561, acc 0.953125
2017-03-02T17:51:31.350393: step 14749, loss 0.232328, acc 0.921875
2017-03-02T17:51:31.427675: step 14750, loss 0.078636, acc 0.96875
2017-03-02T17:51:31.503003: step 14751, loss 0.228629, acc 0.84375
2017-03-02T17:51:31.576467: step 14752, loss 0.202138, acc 0.90625
2017-03-02T17:51:31.650479: step 14753, loss 0.156528, acc 0.96875
2017-03-02T17:51:31.723024: step 14754, loss 0.105972, acc 0.96875
2017-03-02T17:51:31.804050: step 14755, loss 0.160074, acc 0.9375
2017-03-02T17:51:31.879703: step 14756, loss 0.109069, acc 0.96875
2017-03-02T17:51:31.952024: step 14757, loss 0.0957382, acc 0.96875
2017-03-02T17:51:32.023280: step 14758, loss 0.14456, acc 0.953125
2017-03-02T17:51:32.106716: step 14759, loss 0.116103, acc 0.921875
2017-03-02T17:51:32.181747: step 14760, loss 0.157897, acc 0.890625
2017-03-02T17:51:32.260839: step 14761, loss 0.107427, acc 0.921875
2017-03-02T17:51:32.334222: step 14762, loss 0.18931, acc 0.90625
2017-03-02T17:51:32.406443: step 14763, loss 0.249402, acc 0.875
2017-03-02T17:51:32.481360: step 14764, loss 0.195385, acc 0.921875
2017-03-02T17:51:32.550866: step 14765, loss 0.139843, acc 0.953125
2017-03-02T17:51:32.622428: step 14766, loss 0.0527561, acc 0.984375
2017-03-02T17:51:32.702844: step 14767, loss 0.132543, acc 0.9375
2017-03-02T17:51:32.780114: step 14768, loss 0.19307, acc 0.921875
2017-03-02T17:51:32.856324: step 14769, loss 0.1584, acc 0.921875
2017-03-02T17:51:32.930768: step 14770, loss 0.178752, acc 0.921875
2017-03-02T17:51:33.007504: step 14771, loss 0.0570103, acc 0.984375
2017-03-02T17:51:33.082536: step 14772, loss 0.227488, acc 0.90625
2017-03-02T17:51:33.160717: step 14773, loss 0.133995, acc 0.90625
2017-03-02T17:51:33.235967: step 14774, loss 0.0736141, acc 0.96875
2017-03-02T17:51:33.304321: step 14775, loss 0.189655, acc 0.921875
2017-03-02T17:51:33.380369: step 14776, loss 0.129768, acc 0.9375
2017-03-02T17:51:33.450578: step 14777, loss 0.0923546, acc 0.984375
2017-03-02T17:51:33.524540: step 14778, loss 0.155599, acc 0.921875
2017-03-02T17:51:33.595985: step 14779, loss 0.22902, acc 0.921875
2017-03-02T17:51:33.675733: step 14780, loss 0.0895209, acc 0.96875
2017-03-02T17:51:33.755197: step 14781, loss 0.117599, acc 0.96875
2017-03-02T17:51:33.829318: step 14782, loss 0.176889, acc 0.921875
2017-03-02T17:51:33.894625: step 14783, loss 0.0283069, acc 1
2017-03-02T17:51:33.984334: step 14784, loss 0.111197, acc 0.96875
2017-03-02T17:51:34.057998: step 14785, loss 0.340017, acc 0.859375
2017-03-02T17:51:34.129419: step 14786, loss 0.146742, acc 0.9375
2017-03-02T17:51:34.207118: step 14787, loss 0.105785, acc 0.953125
2017-03-02T17:51:34.278667: step 14788, loss 0.0811379, acc 0.953125
2017-03-02T17:51:34.344321: step 14789, loss 0.117604, acc 0.953125
2017-03-02T17:51:34.422178: step 14790, loss 0.0323215, acc 0.984375
2017-03-02T17:51:34.498033: step 14791, loss 0.179808, acc 0.9375
2017-03-02T17:51:34.565330: step 14792, loss 0.243821, acc 0.9375
2017-03-02T17:51:34.634169: step 14793, loss 0.0781831, acc 0.96875
2017-03-02T17:51:34.701261: step 14794, loss 0.241384, acc 0.890625
2017-03-02T17:51:34.775260: step 14795, loss 0.251772, acc 0.875
2017-03-02T17:51:34.847462: step 14796, loss 0.197915, acc 0.9375
2017-03-02T17:51:34.921396: step 14797, loss 0.0704283, acc 0.96875
2017-03-02T17:51:34.989330: step 14798, loss 0.0852189, acc 0.953125
2017-03-02T17:51:35.073184: step 14799, loss 0.165324, acc 0.921875
2017-03-02T17:51:35.144428: step 14800, loss 0.139306, acc 0.953125

Evaluation:
2017-03-02T17:51:35.174308: step 14800, loss 1.8801, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14800

2017-03-02T17:51:35.643425: step 14801, loss 0.220787, acc 0.875
2017-03-02T17:51:35.717915: step 14802, loss 0.353912, acc 0.875
2017-03-02T17:51:35.792106: step 14803, loss 0.0964694, acc 0.984375
2017-03-02T17:51:35.865410: step 14804, loss 0.129424, acc 0.953125
2017-03-02T17:51:35.937601: step 14805, loss 0.165314, acc 0.9375
2017-03-02T17:51:36.005063: step 14806, loss 0.265521, acc 0.859375
2017-03-02T17:51:36.076476: step 14807, loss 0.151966, acc 0.921875
2017-03-02T17:51:36.151300: step 14808, loss 0.179813, acc 0.9375
2017-03-02T17:51:36.226615: step 14809, loss 0.175624, acc 0.921875
2017-03-02T17:51:36.304048: step 14810, loss 0.0887904, acc 0.96875
2017-03-02T17:51:36.378449: step 14811, loss 0.255432, acc 0.875
2017-03-02T17:51:36.449676: step 14812, loss 0.177956, acc 0.90625
2017-03-02T17:51:36.521381: step 14813, loss 0.0567812, acc 0.984375
2017-03-02T17:51:36.599080: step 14814, loss 0.0536539, acc 0.984375
2017-03-02T17:51:36.670289: step 14815, loss 0.215516, acc 0.90625
2017-03-02T17:51:36.740102: step 14816, loss 0.0961118, acc 0.96875
2017-03-02T17:51:36.826978: step 14817, loss 0.176969, acc 0.890625
2017-03-02T17:51:36.905632: step 14818, loss 0.134302, acc 0.921875
2017-03-02T17:51:36.978293: step 14819, loss 0.166063, acc 0.90625
2017-03-02T17:51:37.042876: step 14820, loss 0.285087, acc 0.859375
2017-03-02T17:51:37.127055: step 14821, loss 0.187282, acc 0.9375
2017-03-02T17:51:37.199512: step 14822, loss 0.168804, acc 0.921875
2017-03-02T17:51:37.278859: step 14823, loss 0.0956052, acc 0.953125
2017-03-02T17:51:37.342223: step 14824, loss 0.145068, acc 0.921875
2017-03-02T17:51:37.407813: step 14825, loss 0.177093, acc 0.875
2017-03-02T17:51:37.500163: step 14826, loss 0.319036, acc 0.84375
2017-03-02T17:51:37.580533: step 14827, loss 0.179632, acc 0.921875
2017-03-02T17:51:37.651895: step 14828, loss 0.249616, acc 0.921875
2017-03-02T17:51:37.724005: step 14829, loss 0.222159, acc 0.921875
2017-03-02T17:51:37.800959: step 14830, loss 0.17928, acc 0.921875
2017-03-02T17:51:37.874266: step 14831, loss 0.191902, acc 0.921875
2017-03-02T17:51:37.958064: step 14832, loss 0.131117, acc 0.921875
2017-03-02T17:51:38.029144: step 14833, loss 0.246885, acc 0.890625
2017-03-02T17:51:38.096004: step 14834, loss 0.197599, acc 0.890625
2017-03-02T17:51:38.166394: step 14835, loss 0.157456, acc 0.9375
2017-03-02T17:51:38.238719: step 14836, loss 0.166142, acc 0.9375
2017-03-02T17:51:38.312865: step 14837, loss 0.160942, acc 0.921875
2017-03-02T17:51:38.390374: step 14838, loss 0.0714434, acc 0.984375
2017-03-02T17:51:38.463700: step 14839, loss 0.121622, acc 0.953125
2017-03-02T17:51:38.535201: step 14840, loss 0.096949, acc 0.953125
2017-03-02T17:51:38.611663: step 14841, loss 0.167254, acc 0.953125
2017-03-02T17:51:38.692014: step 14842, loss 0.1213, acc 0.953125
2017-03-02T17:51:38.767588: step 14843, loss 0.270425, acc 0.84375
2017-03-02T17:51:38.843181: step 14844, loss 0.123591, acc 0.9375
2017-03-02T17:51:38.915741: step 14845, loss 0.267476, acc 0.9375
2017-03-02T17:51:38.984809: step 14846, loss 0.153306, acc 0.953125
2017-03-02T17:51:39.056326: step 14847, loss 0.0567954, acc 1
2017-03-02T17:51:39.127554: step 14848, loss 0.170744, acc 0.9375
2017-03-02T17:51:39.199610: step 14849, loss 0.157826, acc 0.96875
2017-03-02T17:51:39.271280: step 14850, loss 0.137068, acc 0.9375
2017-03-02T17:51:39.344693: step 14851, loss 0.239578, acc 0.859375
2017-03-02T17:51:39.416327: step 14852, loss 0.097564, acc 0.96875
2017-03-02T17:51:39.487799: step 14853, loss 0.0976277, acc 0.984375
2017-03-02T17:51:39.568761: step 14854, loss 0.214811, acc 0.921875
2017-03-02T17:51:39.644610: step 14855, loss 0.316467, acc 0.875
2017-03-02T17:51:39.716746: step 14856, loss 0.145735, acc 0.9375
2017-03-02T17:51:39.803159: step 14857, loss 0.139031, acc 0.921875
2017-03-02T17:51:39.874800: step 14858, loss 0.175913, acc 0.9375
2017-03-02T17:51:39.956448: step 14859, loss 0.159233, acc 0.9375
2017-03-02T17:51:40.024803: step 14860, loss 0.13055, acc 0.953125
2017-03-02T17:51:40.096675: step 14861, loss 0.255138, acc 0.890625
2017-03-02T17:51:40.172889: step 14862, loss 0.0597487, acc 0.984375
2017-03-02T17:51:40.241470: step 14863, loss 0.110482, acc 0.953125
2017-03-02T17:51:40.303582: step 14864, loss 0.207815, acc 0.921875
2017-03-02T17:51:40.377249: step 14865, loss 0.318963, acc 0.875
2017-03-02T17:51:40.451584: step 14866, loss 0.352165, acc 0.859375
2017-03-02T17:51:40.527504: step 14867, loss 0.143141, acc 0.9375
2017-03-02T17:51:40.611299: step 14868, loss 0.178813, acc 0.890625
2017-03-02T17:51:40.691123: step 14869, loss 0.0959257, acc 0.953125
2017-03-02T17:51:40.767142: step 14870, loss 0.187607, acc 0.921875
2017-03-02T17:51:40.841098: step 14871, loss 0.260751, acc 0.890625
2017-03-02T17:51:40.911662: step 14872, loss 0.198323, acc 0.96875
2017-03-02T17:51:40.990593: step 14873, loss 0.0822411, acc 0.953125
2017-03-02T17:51:41.085121: step 14874, loss 0.198938, acc 0.921875
2017-03-02T17:51:41.155133: step 14875, loss 0.136935, acc 0.96875
2017-03-02T17:51:41.231524: step 14876, loss 0.235599, acc 0.890625
2017-03-02T17:51:41.311462: step 14877, loss 0.250788, acc 0.890625
2017-03-02T17:51:41.383569: step 14878, loss 0.101996, acc 0.9375
2017-03-02T17:51:41.448542: step 14879, loss 0.255491, acc 0.875
2017-03-02T17:51:41.513595: step 14880, loss 0.0468455, acc 1
2017-03-02T17:51:41.586850: step 14881, loss 0.110817, acc 0.96875
2017-03-02T17:51:41.658422: step 14882, loss 0.221589, acc 0.9375
2017-03-02T17:51:41.732531: step 14883, loss 0.219589, acc 0.9375
2017-03-02T17:51:41.804244: step 14884, loss 0.354554, acc 0.84375
2017-03-02T17:51:41.875864: step 14885, loss 0.171439, acc 0.9375
2017-03-02T17:51:41.947431: step 14886, loss 0.141343, acc 0.953125
2017-03-02T17:51:42.019243: step 14887, loss 0.151973, acc 0.9375
2017-03-02T17:51:42.098799: step 14888, loss 0.158116, acc 0.9375
2017-03-02T17:51:42.168144: step 14889, loss 0.205087, acc 0.90625
2017-03-02T17:51:42.235968: step 14890, loss 0.217519, acc 0.875
2017-03-02T17:51:42.306831: step 14891, loss 0.195777, acc 0.921875
2017-03-02T17:51:42.382910: step 14892, loss 0.21456, acc 0.890625
2017-03-02T17:51:42.453451: step 14893, loss 0.0883389, acc 0.953125
2017-03-02T17:51:42.525912: step 14894, loss 0.0965839, acc 0.96875
2017-03-02T17:51:42.596939: step 14895, loss 0.275891, acc 0.90625
2017-03-02T17:51:42.663547: step 14896, loss 0.0296736, acc 1
2017-03-02T17:51:42.738880: step 14897, loss 0.212808, acc 0.90625
2017-03-02T17:51:42.811565: step 14898, loss 0.110694, acc 0.9375
2017-03-02T17:51:42.883120: step 14899, loss 0.238127, acc 0.875
2017-03-02T17:51:42.946756: step 14900, loss 0.0798593, acc 0.984375

Evaluation:
2017-03-02T17:51:42.983412: step 14900, loss 1.78127, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-14900

2017-03-02T17:51:43.456093: step 14901, loss 0.238247, acc 0.890625
2017-03-02T17:51:43.530238: step 14902, loss 0.14741, acc 0.890625
2017-03-02T17:51:43.605724: step 14903, loss 0.164703, acc 0.9375
2017-03-02T17:51:43.690815: step 14904, loss 0.157956, acc 0.9375
2017-03-02T17:51:43.762422: step 14905, loss 0.117932, acc 0.953125
2017-03-02T17:51:43.849834: step 14906, loss 0.110863, acc 0.953125
2017-03-02T17:51:43.925747: step 14907, loss 0.122084, acc 0.953125
2017-03-02T17:51:44.009030: step 14908, loss 0.154406, acc 0.9375
2017-03-02T17:51:44.080247: step 14909, loss 0.0563778, acc 0.984375
2017-03-02T17:51:44.158581: step 14910, loss 0.128137, acc 0.921875
2017-03-02T17:51:44.228660: step 14911, loss 0.136247, acc 0.953125
2017-03-02T17:51:44.307476: step 14912, loss 0.353806, acc 0.875
2017-03-02T17:51:44.380649: step 14913, loss 0.159386, acc 0.96875
2017-03-02T17:51:44.453412: step 14914, loss 0.173515, acc 0.9375
2017-03-02T17:51:44.528216: step 14915, loss 0.0565337, acc 0.96875
2017-03-02T17:51:44.602563: step 14916, loss 0.0969449, acc 0.953125
2017-03-02T17:51:44.680333: step 14917, loss 0.118402, acc 0.9375
2017-03-02T17:51:44.752909: step 14918, loss 0.241185, acc 0.9375
2017-03-02T17:51:44.825117: step 14919, loss 0.137713, acc 0.953125
2017-03-02T17:51:44.893561: step 14920, loss 0.231014, acc 0.875
2017-03-02T17:51:44.959482: step 14921, loss 0.0787572, acc 0.96875
2017-03-02T17:51:45.027979: step 14922, loss 0.326593, acc 0.84375
2017-03-02T17:51:45.101344: step 14923, loss 0.250599, acc 0.890625
2017-03-02T17:51:45.174395: step 14924, loss 0.212404, acc 0.890625
2017-03-02T17:51:45.250006: step 14925, loss 0.106844, acc 0.96875
2017-03-02T17:51:45.321422: step 14926, loss 0.317365, acc 0.875
2017-03-02T17:51:45.391713: step 14927, loss 0.227997, acc 0.890625
2017-03-02T17:51:45.464219: step 14928, loss 0.150133, acc 0.96875
2017-03-02T17:51:45.547769: step 14929, loss 0.163863, acc 0.921875
2017-03-02T17:51:45.617208: step 14930, loss 0.103499, acc 0.9375
2017-03-02T17:51:45.689032: step 14931, loss 0.159608, acc 0.953125
2017-03-02T17:51:45.760440: step 14932, loss 0.114738, acc 0.953125
2017-03-02T17:51:45.835025: step 14933, loss 0.103033, acc 0.96875
2017-03-02T17:51:45.907094: step 14934, loss 0.121802, acc 0.953125
2017-03-02T17:51:45.979393: step 14935, loss 0.137914, acc 0.9375
2017-03-02T17:51:46.053649: step 14936, loss 0.220771, acc 0.875
2017-03-02T17:51:46.124243: step 14937, loss 0.156634, acc 0.90625
2017-03-02T17:51:46.199962: step 14938, loss 0.122719, acc 0.921875
2017-03-02T17:51:46.269709: step 14939, loss 0.139544, acc 0.921875
2017-03-02T17:51:46.337286: step 14940, loss 0.20918, acc 0.875
2017-03-02T17:51:46.427538: step 14941, loss 0.212075, acc 0.890625
2017-03-02T17:51:46.502283: step 14942, loss 0.0674044, acc 0.96875
2017-03-02T17:51:46.577004: step 14943, loss 0.140586, acc 0.9375
2017-03-02T17:51:46.649882: step 14944, loss 0.199925, acc 0.921875
2017-03-02T17:51:46.723816: step 14945, loss 0.0828698, acc 0.9375
2017-03-02T17:51:46.794617: step 14946, loss 0.197778, acc 0.9375
2017-03-02T17:51:46.865785: step 14947, loss 0.0964571, acc 0.96875
2017-03-02T17:51:46.935568: step 14948, loss 0.338011, acc 0.890625
2017-03-02T17:51:47.007388: step 14949, loss 0.141129, acc 0.921875
2017-03-02T17:51:47.081547: step 14950, loss 0.164068, acc 0.90625
2017-03-02T17:51:47.168670: step 14951, loss 0.133709, acc 0.953125
2017-03-02T17:51:47.236645: step 14952, loss 0.0954941, acc 0.953125
2017-03-02T17:51:47.318883: step 14953, loss 0.222136, acc 0.90625
2017-03-02T17:51:47.400119: step 14954, loss 0.252024, acc 0.890625
2017-03-02T17:51:47.479443: step 14955, loss 0.150392, acc 0.953125
2017-03-02T17:51:47.557958: step 14956, loss 0.217309, acc 0.890625
2017-03-02T17:51:47.633433: step 14957, loss 0.113781, acc 0.953125
2017-03-02T17:51:47.703763: step 14958, loss 0.126831, acc 0.953125
2017-03-02T17:51:47.776040: step 14959, loss 0.165136, acc 0.90625
2017-03-02T17:51:47.850215: step 14960, loss 0.100069, acc 0.953125
2017-03-02T17:51:47.924987: step 14961, loss 0.167315, acc 0.953125
2017-03-02T17:51:47.998238: step 14962, loss 0.186764, acc 0.90625
2017-03-02T17:51:48.081933: step 14963, loss 0.23163, acc 0.890625
2017-03-02T17:51:48.153730: step 14964, loss 0.238563, acc 0.875
2017-03-02T17:51:48.227209: step 14965, loss 0.167274, acc 0.9375
2017-03-02T17:51:48.296411: step 14966, loss 0.103677, acc 0.953125
2017-03-02T17:51:48.371400: step 14967, loss 0.0836703, acc 0.96875
2017-03-02T17:51:48.439771: step 14968, loss 0.116525, acc 0.96875
2017-03-02T17:51:48.510905: step 14969, loss 0.167213, acc 0.890625
2017-03-02T17:51:48.581730: step 14970, loss 0.120939, acc 0.953125
2017-03-02T17:51:48.668649: step 14971, loss 0.125344, acc 0.9375
2017-03-02T17:51:48.740146: step 14972, loss 0.0875792, acc 0.96875
2017-03-02T17:51:48.819335: step 14973, loss 0.118625, acc 0.9375
2017-03-02T17:51:48.894669: step 14974, loss 0.136185, acc 0.921875
2017-03-02T17:51:48.968446: step 14975, loss 0.058504, acc 0.96875
2017-03-02T17:51:49.042860: step 14976, loss 0.162568, acc 0.921875
2017-03-02T17:51:49.116539: step 14977, loss 0.108784, acc 0.9375
2017-03-02T17:51:49.189774: step 14978, loss 0.14378, acc 0.921875
2017-03-02T17:51:49.262583: step 14979, loss 0.0914575, acc 0.953125
2017-03-02T17:51:49.335969: step 14980, loss 0.149198, acc 0.921875
2017-03-02T17:51:49.414480: step 14981, loss 0.173329, acc 0.90625
2017-03-02T17:51:49.502356: step 14982, loss 0.216752, acc 0.90625
2017-03-02T17:51:49.574995: step 14983, loss 0.10323, acc 0.96875
2017-03-02T17:51:49.648570: step 14984, loss 0.197488, acc 0.9375
2017-03-02T17:51:49.729817: step 14985, loss 0.143475, acc 0.9375
2017-03-02T17:51:49.798998: step 14986, loss 0.206095, acc 0.953125
2017-03-02T17:51:49.869340: step 14987, loss 0.183711, acc 0.921875
2017-03-02T17:51:49.948634: step 14988, loss 0.11853, acc 0.9375
2017-03-02T17:51:50.019494: step 14989, loss 0.230015, acc 0.921875
2017-03-02T17:51:50.097290: step 14990, loss 0.215007, acc 0.90625
2017-03-02T17:51:50.166953: step 14991, loss 0.184995, acc 0.921875
2017-03-02T17:51:50.239349: step 14992, loss 0.15649, acc 0.9375
2017-03-02T17:51:50.310829: step 14993, loss 0.153027, acc 0.9375
2017-03-02T17:51:50.381706: step 14994, loss 0.255133, acc 0.875
2017-03-02T17:51:50.466243: step 14995, loss 0.137549, acc 0.953125
2017-03-02T17:51:50.541505: step 14996, loss 0.173198, acc 0.953125
2017-03-02T17:51:50.627846: step 14997, loss 0.2774, acc 0.890625
2017-03-02T17:51:50.708388: step 14998, loss 0.127522, acc 0.9375
2017-03-02T17:51:50.784935: step 14999, loss 0.10068, acc 0.953125
2017-03-02T17:51:50.852709: step 15000, loss 0.158586, acc 0.921875

Evaluation:
2017-03-02T17:51:50.886735: step 15000, loss 1.88331, acc 0.659697

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15000

2017-03-02T17:51:51.371877: step 15001, loss 0.117443, acc 0.953125
2017-03-02T17:51:51.451182: step 15002, loss 0.223728, acc 0.921875
2017-03-02T17:51:51.524145: step 15003, loss 0.127984, acc 0.953125
2017-03-02T17:51:51.602302: step 15004, loss 0.255718, acc 0.890625
2017-03-02T17:51:51.678931: step 15005, loss 0.178669, acc 0.921875
2017-03-02T17:51:51.750963: step 15006, loss 0.193985, acc 0.953125
2017-03-02T17:51:51.814465: step 15007, loss 0.204936, acc 0.9375
2017-03-02T17:51:51.884645: step 15008, loss 0.145667, acc 0.9375
2017-03-02T17:51:51.953597: step 15009, loss 0.203605, acc 0.921875
2017-03-02T17:51:52.032132: step 15010, loss 0.149002, acc 0.9375
2017-03-02T17:51:52.105460: step 15011, loss 0.190722, acc 0.9375
2017-03-02T17:51:52.181061: step 15012, loss 0.142339, acc 0.9375
2017-03-02T17:51:52.253995: step 15013, loss 0.122734, acc 0.9375
2017-03-02T17:51:52.328868: step 15014, loss 0.144048, acc 0.9375
2017-03-02T17:51:52.402827: step 15015, loss 0.166696, acc 0.96875
2017-03-02T17:51:52.468488: step 15016, loss 0.150177, acc 0.9375
2017-03-02T17:51:52.534869: step 15017, loss 0.08814, acc 0.953125
2017-03-02T17:51:52.609189: step 15018, loss 0.142058, acc 0.921875
2017-03-02T17:51:52.684297: step 15019, loss 0.098196, acc 0.953125
2017-03-02T17:51:52.755243: step 15020, loss 0.249094, acc 0.921875
2017-03-02T17:51:52.833594: step 15021, loss 0.181954, acc 0.90625
2017-03-02T17:51:52.918659: step 15022, loss 0.203696, acc 0.9375
2017-03-02T17:51:52.999331: step 15023, loss 0.203289, acc 0.890625
2017-03-02T17:51:53.081006: step 15024, loss 0.21338, acc 0.90625
2017-03-02T17:51:53.158321: step 15025, loss 0.173, acc 0.90625
2017-03-02T17:51:53.231646: step 15026, loss 0.10811, acc 0.953125
2017-03-02T17:51:53.309181: step 15027, loss 0.0861692, acc 0.953125
2017-03-02T17:51:53.382607: step 15028, loss 0.111853, acc 0.921875
2017-03-02T17:51:53.455613: step 15029, loss 0.121319, acc 0.96875
2017-03-02T17:51:53.516848: step 15030, loss 0.133704, acc 0.953125
2017-03-02T17:51:53.595239: step 15031, loss 0.0785997, acc 0.96875
2017-03-02T17:51:53.668094: step 15032, loss 0.173233, acc 0.90625
2017-03-02T17:51:53.744206: step 15033, loss 0.263271, acc 0.890625
2017-03-02T17:51:53.812478: step 15034, loss 0.20301, acc 0.921875
2017-03-02T17:51:53.894729: step 15035, loss 0.0555534, acc 0.984375
2017-03-02T17:51:53.963936: step 15036, loss 0.0765929, acc 0.96875
2017-03-02T17:51:54.039059: step 15037, loss 0.222384, acc 0.890625
2017-03-02T17:51:54.113122: step 15038, loss 0.154121, acc 0.890625
2017-03-02T17:51:54.183274: step 15039, loss 0.0990935, acc 0.953125
2017-03-02T17:51:54.258898: step 15040, loss 0.063586, acc 0.984375
2017-03-02T17:51:54.338425: step 15041, loss 0.182917, acc 0.90625
2017-03-02T17:51:54.404617: step 15042, loss 0.130603, acc 0.9375
2017-03-02T17:51:54.479523: step 15043, loss 0.213065, acc 0.90625
2017-03-02T17:51:54.545578: step 15044, loss 0.176335, acc 0.9375
2017-03-02T17:51:54.616820: step 15045, loss 0.30143, acc 0.859375
2017-03-02T17:51:54.697209: step 15046, loss 0.200177, acc 0.953125
2017-03-02T17:51:54.771827: step 15047, loss 0.166036, acc 0.921875
2017-03-02T17:51:54.846149: step 15048, loss 0.180434, acc 0.921875
2017-03-02T17:51:54.918966: step 15049, loss 0.179256, acc 0.96875
2017-03-02T17:51:54.988540: step 15050, loss 0.0657894, acc 0.984375
2017-03-02T17:51:55.060560: step 15051, loss 0.0951768, acc 0.953125
2017-03-02T17:51:55.136721: step 15052, loss 0.153205, acc 0.953125
2017-03-02T17:51:55.208186: step 15053, loss 0.134904, acc 0.921875
2017-03-02T17:51:55.276239: step 15054, loss 0.18789, acc 0.921875
2017-03-02T17:51:55.357421: step 15055, loss 0.193233, acc 0.921875
2017-03-02T17:51:55.431657: step 15056, loss 0.11809, acc 0.9375
2017-03-02T17:51:55.509699: step 15057, loss 0.112771, acc 0.9375
2017-03-02T17:51:55.583007: step 15058, loss 0.116772, acc 0.9375
2017-03-02T17:51:55.657754: step 15059, loss 0.0234516, acc 1
2017-03-02T17:51:55.724937: step 15060, loss 0.0734843, acc 0.984375
2017-03-02T17:51:55.797147: step 15061, loss 0.053502, acc 0.984375
2017-03-02T17:51:55.880542: step 15062, loss 0.0913635, acc 0.953125
2017-03-02T17:51:55.958792: step 15063, loss 0.183147, acc 0.921875
2017-03-02T17:51:56.029183: step 15064, loss 0.133413, acc 0.953125
2017-03-02T17:51:56.103170: step 15065, loss 0.191057, acc 0.90625
2017-03-02T17:51:56.177087: step 15066, loss 0.13647, acc 0.96875
2017-03-02T17:51:56.252788: step 15067, loss 0.0714388, acc 0.96875
2017-03-02T17:51:56.322958: step 15068, loss 0.294034, acc 0.875
2017-03-02T17:51:56.397153: step 15069, loss 0.168912, acc 0.953125
2017-03-02T17:51:56.467028: step 15070, loss 0.188931, acc 0.90625
2017-03-02T17:51:56.545217: step 15071, loss 0.297205, acc 0.875
2017-03-02T17:51:56.613758: step 15072, loss 0.155312, acc 0.9375
2017-03-02T17:51:56.683903: step 15073, loss 0.14678, acc 0.9375
2017-03-02T17:51:56.759722: step 15074, loss 0.149293, acc 0.9375
2017-03-02T17:51:56.830591: step 15075, loss 0.053089, acc 0.984375
2017-03-02T17:51:56.899568: step 15076, loss 0.145617, acc 0.9375
2017-03-02T17:51:56.970646: step 15077, loss 0.204604, acc 0.90625
2017-03-02T17:51:57.048360: step 15078, loss 0.199828, acc 0.890625
2017-03-02T17:51:57.123846: step 15079, loss 0.0703061, acc 0.984375
2017-03-02T17:51:57.202010: step 15080, loss 0.0500948, acc 0.984375
2017-03-02T17:51:57.265988: step 15081, loss 0.12677, acc 0.921875
2017-03-02T17:51:57.330100: step 15082, loss 0.201996, acc 0.890625
2017-03-02T17:51:57.404210: step 15083, loss 0.177202, acc 0.921875
2017-03-02T17:51:57.474079: step 15084, loss 0.248217, acc 0.921875
2017-03-02T17:51:57.546782: step 15085, loss 0.193094, acc 0.90625
2017-03-02T17:51:57.640342: step 15086, loss 0.136781, acc 0.96875
2017-03-02T17:51:57.715775: step 15087, loss 0.132185, acc 0.921875
2017-03-02T17:51:57.794429: step 15088, loss 0.175041, acc 0.890625
2017-03-02T17:51:57.877724: step 15089, loss 0.190208, acc 0.921875
2017-03-02T17:51:57.952922: step 15090, loss 0.207819, acc 0.875
2017-03-02T17:51:58.017511: step 15091, loss 0.070118, acc 0.984375
2017-03-02T17:51:58.082935: step 15092, loss 0.360045, acc 0.75
2017-03-02T17:51:58.161255: step 15093, loss 0.164139, acc 0.921875
2017-03-02T17:51:58.243544: step 15094, loss 0.122184, acc 0.9375
2017-03-02T17:51:58.309344: step 15095, loss 0.185437, acc 0.9375
2017-03-02T17:51:58.383743: step 15096, loss 0.0784637, acc 0.96875
2017-03-02T17:51:58.456885: step 15097, loss 0.182881, acc 0.921875
2017-03-02T17:51:58.531103: step 15098, loss 0.124603, acc 0.921875
2017-03-02T17:51:58.598815: step 15099, loss 0.210804, acc 0.9375
2017-03-02T17:51:58.667688: step 15100, loss 0.0415336, acc 1

Evaluation:
2017-03-02T17:51:58.702287: step 15100, loss 1.80519, acc 0.652487

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15100

2017-03-02T17:51:59.181001: step 15101, loss 0.156816, acc 0.9375
2017-03-02T17:51:59.261282: step 15102, loss 0.16539, acc 0.9375
2017-03-02T17:51:59.332070: step 15103, loss 0.0637392, acc 0.96875
2017-03-02T17:51:59.403028: step 15104, loss 0.121666, acc 0.953125
2017-03-02T17:51:59.476489: step 15105, loss 0.13136, acc 0.953125
2017-03-02T17:51:59.551825: step 15106, loss 0.128268, acc 0.953125
2017-03-02T17:51:59.632159: step 15107, loss 0.0867959, acc 0.953125
2017-03-02T17:51:59.720015: step 15108, loss 0.114451, acc 0.96875
2017-03-02T17:51:59.879369: step 15109, loss 0.122544, acc 0.96875
2017-03-02T17:51:59.955812: step 15110, loss 0.0857176, acc 0.953125
2017-03-02T17:52:00.025803: step 15111, loss 0.225729, acc 0.9375
2017-03-02T17:52:00.092201: step 15112, loss 0.188902, acc 0.9375
2017-03-02T17:52:00.175932: step 15113, loss 0.140362, acc 0.9375
2017-03-02T17:52:00.254336: step 15114, loss 0.234682, acc 0.875
2017-03-02T17:52:00.327229: step 15115, loss 0.292799, acc 0.84375
2017-03-02T17:52:00.403596: step 15116, loss 0.289196, acc 0.875
2017-03-02T17:52:00.474936: step 15117, loss 0.0770386, acc 0.984375
2017-03-02T17:52:00.565939: step 15118, loss 0.117918, acc 0.9375
2017-03-02T17:52:00.642372: step 15119, loss 0.177659, acc 0.90625
2017-03-02T17:52:00.713531: step 15120, loss 0.124118, acc 0.9375
2017-03-02T17:52:00.785369: step 15121, loss 0.239565, acc 0.890625
2017-03-02T17:52:00.857504: step 15122, loss 0.0795095, acc 0.96875
2017-03-02T17:52:00.925525: step 15123, loss 0.21135, acc 0.90625
2017-03-02T17:52:01.002463: step 15124, loss 0.202907, acc 0.9375
2017-03-02T17:52:01.075488: step 15125, loss 0.144862, acc 0.921875
2017-03-02T17:52:01.147318: step 15126, loss 0.285754, acc 0.859375
2017-03-02T17:52:01.220743: step 15127, loss 0.173742, acc 0.921875
2017-03-02T17:52:01.291762: step 15128, loss 0.17969, acc 0.875
2017-03-02T17:52:01.360782: step 15129, loss 0.187106, acc 0.9375
2017-03-02T17:52:01.440497: step 15130, loss 0.2161, acc 0.921875
2017-03-02T17:52:01.521621: step 15131, loss 0.0962855, acc 0.96875
2017-03-02T17:52:01.595306: step 15132, loss 0.0913471, acc 0.96875
2017-03-02T17:52:01.670173: step 15133, loss 0.111753, acc 0.96875
2017-03-02T17:52:01.745272: step 15134, loss 0.19351, acc 0.921875
2017-03-02T17:52:01.826184: step 15135, loss 0.174711, acc 0.90625
2017-03-02T17:52:01.901842: step 15136, loss 0.138007, acc 0.953125
2017-03-02T17:52:01.982191: step 15137, loss 0.258508, acc 0.890625
2017-03-02T17:52:02.052823: step 15138, loss 0.129077, acc 0.9375
2017-03-02T17:52:02.120465: step 15139, loss 0.113497, acc 0.96875
2017-03-02T17:52:02.190321: step 15140, loss 0.133729, acc 0.921875
2017-03-02T17:52:02.267386: step 15141, loss 0.190771, acc 0.90625
2017-03-02T17:52:02.340853: step 15142, loss 0.0935329, acc 0.9375
2017-03-02T17:52:02.416021: step 15143, loss 0.0943208, acc 0.953125
2017-03-02T17:52:02.489148: step 15144, loss 0.199548, acc 0.921875
2017-03-02T17:52:02.562589: step 15145, loss 0.173649, acc 0.90625
2017-03-02T17:52:02.634147: step 15146, loss 0.175836, acc 0.9375
2017-03-02T17:52:02.710302: step 15147, loss 0.188381, acc 0.921875
2017-03-02T17:52:02.779025: step 15148, loss 0.28074, acc 0.890625
2017-03-02T17:52:02.851347: step 15149, loss 0.165817, acc 0.96875
2017-03-02T17:52:02.934146: step 15150, loss 0.115188, acc 0.96875
2017-03-02T17:52:03.009205: step 15151, loss 0.323787, acc 0.875
2017-03-02T17:52:03.087198: step 15152, loss 0.187622, acc 0.90625
2017-03-02T17:52:03.161436: step 15153, loss 0.17495, acc 0.9375
2017-03-02T17:52:03.247368: step 15154, loss 0.206409, acc 0.90625
2017-03-02T17:52:03.318765: step 15155, loss 0.115127, acc 0.921875
2017-03-02T17:52:03.391745: step 15156, loss 0.116774, acc 0.9375
2017-03-02T17:52:03.457423: step 15157, loss 0.219977, acc 0.921875
2017-03-02T17:52:03.525036: step 15158, loss 0.159087, acc 0.953125
2017-03-02T17:52:03.597523: step 15159, loss 0.122686, acc 0.96875
2017-03-02T17:52:03.671942: step 15160, loss 0.205016, acc 0.921875
2017-03-02T17:52:03.748208: step 15161, loss 0.158874, acc 0.90625
2017-03-02T17:52:03.822004: step 15162, loss 0.0799223, acc 0.96875
2017-03-02T17:52:03.894817: step 15163, loss 0.147404, acc 0.921875
2017-03-02T17:52:03.958076: step 15164, loss 0.0991475, acc 0.9375
2017-03-02T17:52:04.033017: step 15165, loss 0.138563, acc 0.984375
2017-03-02T17:52:04.111617: step 15166, loss 0.12013, acc 0.9375
2017-03-02T17:52:04.178650: step 15167, loss 0.253796, acc 0.890625
2017-03-02T17:52:04.250683: step 15168, loss 0.0907131, acc 0.984375
2017-03-02T17:52:04.323095: step 15169, loss 0.163752, acc 0.9375
2017-03-02T17:52:04.397046: step 15170, loss 0.0864558, acc 0.953125
2017-03-02T17:52:04.481493: step 15171, loss 0.195576, acc 0.921875
2017-03-02T17:52:04.563183: step 15172, loss 0.205613, acc 0.90625
2017-03-02T17:52:04.638395: step 15173, loss 0.160492, acc 0.953125
2017-03-02T17:52:04.708669: step 15174, loss 0.128692, acc 0.9375
2017-03-02T17:52:04.782540: step 15175, loss 0.148539, acc 0.953125
2017-03-02T17:52:04.861016: step 15176, loss 0.150742, acc 0.90625
2017-03-02T17:52:04.938377: step 15177, loss 0.216073, acc 0.90625
2017-03-02T17:52:05.016629: step 15178, loss 0.0729198, acc 0.953125
2017-03-02T17:52:05.092414: step 15179, loss 0.140673, acc 0.9375
2017-03-02T17:52:05.165125: step 15180, loss 0.0669687, acc 0.96875
2017-03-02T17:52:05.240129: step 15181, loss 0.19542, acc 0.890625
2017-03-02T17:52:05.318923: step 15182, loss 0.0878093, acc 0.984375
2017-03-02T17:52:05.404659: step 15183, loss 0.162518, acc 0.921875
2017-03-02T17:52:05.486227: step 15184, loss 0.162874, acc 0.921875
2017-03-02T17:52:05.558391: step 15185, loss 0.264532, acc 0.859375
2017-03-02T17:52:05.631453: step 15186, loss 0.251348, acc 0.890625
2017-03-02T17:52:05.705541: step 15187, loss 0.147568, acc 0.9375
2017-03-02T17:52:05.795447: step 15188, loss 0.153065, acc 0.96875
2017-03-02T17:52:05.870055: step 15189, loss 0.120944, acc 0.9375
2017-03-02T17:52:05.945823: step 15190, loss 0.353648, acc 0.84375
2017-03-02T17:52:06.019443: step 15191, loss 0.260355, acc 0.890625
2017-03-02T17:52:06.103380: step 15192, loss 0.122894, acc 0.953125
2017-03-02T17:52:06.179634: step 15193, loss 0.122765, acc 0.984375
2017-03-02T17:52:06.248377: step 15194, loss 0.184932, acc 0.90625
2017-03-02T17:52:06.316791: step 15195, loss 0.0535911, acc 0.984375
2017-03-02T17:52:06.391076: step 15196, loss 0.21577, acc 0.890625
2017-03-02T17:52:06.472501: step 15197, loss 0.148911, acc 0.9375
2017-03-02T17:52:06.544597: step 15198, loss 0.175423, acc 0.90625
2017-03-02T17:52:06.621823: step 15199, loss 0.160543, acc 0.9375
2017-03-02T17:52:06.693099: step 15200, loss 0.117238, acc 0.9375

Evaluation:
2017-03-02T17:52:06.726941: step 15200, loss 1.85681, acc 0.650324

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15200

2017-03-02T17:52:07.178563: step 15201, loss 0.165264, acc 0.921875
2017-03-02T17:52:07.251031: step 15202, loss 0.247711, acc 0.890625
2017-03-02T17:52:07.334558: step 15203, loss 0.141568, acc 0.9375
2017-03-02T17:52:07.404480: step 15204, loss 0.256637, acc 0.828125
2017-03-02T17:52:07.478726: step 15205, loss 0.0998288, acc 0.96875
2017-03-02T17:52:07.553972: step 15206, loss 0.205406, acc 0.890625
2017-03-02T17:52:07.625017: step 15207, loss 0.167069, acc 0.9375
2017-03-02T17:52:07.701835: step 15208, loss 0.138893, acc 0.9375
2017-03-02T17:52:07.774167: step 15209, loss 0.0970903, acc 0.96875
2017-03-02T17:52:07.852294: step 15210, loss 0.185085, acc 0.90625
2017-03-02T17:52:07.926741: step 15211, loss 0.132268, acc 0.953125
2017-03-02T17:52:08.019942: step 15212, loss 0.0661625, acc 0.96875
2017-03-02T17:52:08.099526: step 15213, loss 0.222816, acc 0.921875
2017-03-02T17:52:08.176456: step 15214, loss 0.166655, acc 0.953125
2017-03-02T17:52:08.252066: step 15215, loss 0.143249, acc 0.9375
2017-03-02T17:52:08.327514: step 15216, loss 0.179406, acc 0.9375
2017-03-02T17:52:08.402870: step 15217, loss 0.168352, acc 0.921875
2017-03-02T17:52:08.477883: step 15218, loss 0.128667, acc 0.953125
2017-03-02T17:52:08.550254: step 15219, loss 0.122283, acc 0.9375
2017-03-02T17:52:08.625597: step 15220, loss 0.241942, acc 0.875
2017-03-02T17:52:08.717606: step 15221, loss 0.195836, acc 0.953125
2017-03-02T17:52:08.789631: step 15222, loss 0.0829677, acc 0.984375
2017-03-02T17:52:08.863453: step 15223, loss 0.161878, acc 0.90625
2017-03-02T17:52:08.934113: step 15224, loss 0.265044, acc 0.90625
2017-03-02T17:52:09.022799: step 15225, loss 0.0560117, acc 0.96875
2017-03-02T17:52:09.084829: step 15226, loss 0.206009, acc 0.90625
2017-03-02T17:52:09.161667: step 15227, loss 0.0926144, acc 0.96875
2017-03-02T17:52:09.235793: step 15228, loss 0.302378, acc 0.84375
2017-03-02T17:52:09.313653: step 15229, loss 0.241378, acc 0.921875
2017-03-02T17:52:09.388340: step 15230, loss 0.121591, acc 0.9375
2017-03-02T17:52:09.471289: step 15231, loss 0.197363, acc 0.890625
2017-03-02T17:52:09.548552: step 15232, loss 0.150628, acc 0.953125
2017-03-02T17:52:09.621862: step 15233, loss 0.228097, acc 0.921875
2017-03-02T17:52:09.690912: step 15234, loss 0.145625, acc 0.9375
2017-03-02T17:52:09.758531: step 15235, loss 0.118862, acc 0.953125
2017-03-02T17:52:09.834323: step 15236, loss 0.171829, acc 0.921875
2017-03-02T17:52:09.914462: step 15237, loss 0.108929, acc 0.9375
2017-03-02T17:52:09.988800: step 15238, loss 0.0633474, acc 0.984375
2017-03-02T17:52:10.060413: step 15239, loss 0.206172, acc 0.90625
2017-03-02T17:52:10.135673: step 15240, loss 0.137149, acc 0.921875
2017-03-02T17:52:10.218321: step 15241, loss 0.208146, acc 0.9375
2017-03-02T17:52:10.293997: step 15242, loss 0.177329, acc 0.921875
2017-03-02T17:52:10.361477: step 15243, loss 0.155289, acc 0.921875
2017-03-02T17:52:10.427162: step 15244, loss 0.128187, acc 0.953125
2017-03-02T17:52:10.494717: step 15245, loss 0.148801, acc 0.96875
2017-03-02T17:52:10.567090: step 15246, loss 0.174204, acc 0.90625
2017-03-02T17:52:10.639552: step 15247, loss 0.0931049, acc 0.96875
2017-03-02T17:52:10.720231: step 15248, loss 0.267118, acc 0.90625
2017-03-02T17:52:10.796935: step 15249, loss 0.171978, acc 0.9375
2017-03-02T17:52:10.870317: step 15250, loss 0.10751, acc 0.953125
2017-03-02T17:52:10.939686: step 15251, loss 0.0732954, acc 0.984375
2017-03-02T17:52:11.011963: step 15252, loss 0.163904, acc 0.9375
2017-03-02T17:52:11.083548: step 15253, loss 0.172591, acc 0.921875
2017-03-02T17:52:11.155195: step 15254, loss 0.0875335, acc 0.953125
2017-03-02T17:52:11.227821: step 15255, loss 0.150195, acc 0.9375
2017-03-02T17:52:11.303027: step 15256, loss 0.320409, acc 0.84375
2017-03-02T17:52:11.377596: step 15257, loss 0.149753, acc 0.90625
2017-03-02T17:52:11.450259: step 15258, loss 0.127541, acc 0.9375
2017-03-02T17:52:11.523089: step 15259, loss 0.0656751, acc 0.953125
2017-03-02T17:52:11.597073: step 15260, loss 0.0857095, acc 0.953125
2017-03-02T17:52:11.667103: step 15261, loss 0.209873, acc 0.890625
2017-03-02T17:52:11.741719: step 15262, loss 0.210334, acc 0.90625
2017-03-02T17:52:11.821379: step 15263, loss 0.19333, acc 0.875
2017-03-02T17:52:11.905155: step 15264, loss 0.202407, acc 0.9375
2017-03-02T17:52:11.979127: step 15265, loss 0.234773, acc 0.875
2017-03-02T17:52:12.060320: step 15266, loss 0.163313, acc 0.90625
2017-03-02T17:52:12.136611: step 15267, loss 0.209837, acc 0.90625
2017-03-02T17:52:12.212346: step 15268, loss 0.137612, acc 0.9375
2017-03-02T17:52:12.286999: step 15269, loss 0.0832788, acc 0.96875
2017-03-02T17:52:12.366663: step 15270, loss 0.197572, acc 0.90625
2017-03-02T17:52:12.443743: step 15271, loss 0.157889, acc 0.921875
2017-03-02T17:52:12.512498: step 15272, loss 0.0700003, acc 0.953125
2017-03-02T17:52:12.587290: step 15273, loss 0.150946, acc 0.921875
2017-03-02T17:52:12.666957: step 15274, loss 0.268775, acc 0.84375
2017-03-02T17:52:12.739938: step 15275, loss 0.100297, acc 0.953125
2017-03-02T17:52:12.818780: step 15276, loss 0.197294, acc 0.9375
2017-03-02T17:52:12.893633: step 15277, loss 0.185348, acc 0.890625
2017-03-02T17:52:12.965241: step 15278, loss 0.164886, acc 0.953125
2017-03-02T17:52:13.039405: step 15279, loss 0.184648, acc 0.921875
2017-03-02T17:52:13.102883: step 15280, loss 0.0724391, acc 0.96875
2017-03-02T17:52:13.172398: step 15281, loss 0.218743, acc 0.921875
2017-03-02T17:52:13.246832: step 15282, loss 0.279468, acc 0.875
2017-03-02T17:52:13.313979: step 15283, loss 0.112958, acc 0.9375
2017-03-02T17:52:13.386506: step 15284, loss 0.212523, acc 0.921875
2017-03-02T17:52:13.458576: step 15285, loss 0.0974846, acc 0.953125
2017-03-02T17:52:13.529858: step 15286, loss 0.194644, acc 0.921875
2017-03-02T17:52:13.601997: step 15287, loss 0.25419, acc 0.890625
2017-03-02T17:52:13.670982: step 15288, loss 0.147403, acc 1
2017-03-02T17:52:13.745261: step 15289, loss 0.065406, acc 1
2017-03-02T17:52:13.824587: step 15290, loss 0.0843519, acc 0.953125
2017-03-02T17:52:13.896644: step 15291, loss 0.226189, acc 0.90625
2017-03-02T17:52:13.976142: step 15292, loss 0.093749, acc 0.96875
2017-03-02T17:52:14.050569: step 15293, loss 0.1143, acc 0.953125
2017-03-02T17:52:14.128512: step 15294, loss 0.129939, acc 0.921875
2017-03-02T17:52:14.209649: step 15295, loss 0.114409, acc 0.96875
2017-03-02T17:52:14.284625: step 15296, loss 0.159265, acc 0.921875
2017-03-02T17:52:14.358646: step 15297, loss 0.10667, acc 0.953125
2017-03-02T17:52:14.429918: step 15298, loss 0.135001, acc 0.9375
2017-03-02T17:52:14.510848: step 15299, loss 0.122994, acc 0.953125
2017-03-02T17:52:14.580329: step 15300, loss 0.0758888, acc 0.984375

Evaluation:
2017-03-02T17:52:14.613557: step 15300, loss 1.8596, acc 0.648882

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15300

2017-03-02T17:52:15.064981: step 15301, loss 0.0878078, acc 0.953125
2017-03-02T17:52:15.138872: step 15302, loss 0.106017, acc 0.953125
2017-03-02T17:52:15.206995: step 15303, loss 0.206003, acc 0.921875
2017-03-02T17:52:15.277390: step 15304, loss 0.0690615, acc 0.984375
2017-03-02T17:52:15.349181: step 15305, loss 0.268508, acc 0.90625
2017-03-02T17:52:15.420154: step 15306, loss 0.163354, acc 0.9375
2017-03-02T17:52:15.495745: step 15307, loss 0.166144, acc 0.9375
2017-03-02T17:52:15.576246: step 15308, loss 0.215029, acc 0.90625
2017-03-02T17:52:15.646185: step 15309, loss 0.10322, acc 0.953125
2017-03-02T17:52:15.723752: step 15310, loss 0.108696, acc 0.9375
2017-03-02T17:52:15.796372: step 15311, loss 0.115616, acc 0.953125
2017-03-02T17:52:15.869514: step 15312, loss 0.117644, acc 0.953125
2017-03-02T17:52:15.941647: step 15313, loss 0.240377, acc 0.875
2017-03-02T17:52:16.015739: step 15314, loss 0.204806, acc 0.90625
2017-03-02T17:52:16.086626: step 15315, loss 0.120869, acc 0.953125
2017-03-02T17:52:16.159930: step 15316, loss 0.159297, acc 0.9375
2017-03-02T17:52:16.236314: step 15317, loss 0.131714, acc 0.953125
2017-03-02T17:52:16.312745: step 15318, loss 0.144991, acc 0.921875
2017-03-02T17:52:16.382977: step 15319, loss 0.326442, acc 0.828125
2017-03-02T17:52:16.456635: step 15320, loss 0.071892, acc 0.984375
2017-03-02T17:52:16.531613: step 15321, loss 0.0966035, acc 0.96875
2017-03-02T17:52:16.603936: step 15322, loss 0.162487, acc 0.90625
2017-03-02T17:52:16.673388: step 15323, loss 0.199527, acc 0.921875
2017-03-02T17:52:16.748776: step 15324, loss 0.212949, acc 0.890625
2017-03-02T17:52:16.824193: step 15325, loss 0.11762, acc 0.953125
2017-03-02T17:52:16.900487: step 15326, loss 0.104441, acc 0.9375
2017-03-02T17:52:16.973259: step 15327, loss 0.163544, acc 0.9375
2017-03-02T17:52:17.046662: step 15328, loss 0.144376, acc 0.921875
2017-03-02T17:52:17.121545: step 15329, loss 0.298409, acc 0.859375
2017-03-02T17:52:17.202968: step 15330, loss 0.207087, acc 0.90625
2017-03-02T17:52:17.274857: step 15331, loss 0.284064, acc 0.875
2017-03-02T17:52:17.342075: step 15332, loss 0.208461, acc 0.9375
2017-03-02T17:52:17.426602: step 15333, loss 0.146987, acc 0.921875
2017-03-02T17:52:17.501776: step 15334, loss 0.189371, acc 0.90625
2017-03-02T17:52:17.576962: step 15335, loss 0.0725665, acc 0.953125
2017-03-02T17:52:17.644775: step 15336, loss 0.166045, acc 0.921875
2017-03-02T17:52:17.727910: step 15337, loss 0.235625, acc 0.9375
2017-03-02T17:52:17.803253: step 15338, loss 0.130661, acc 0.90625
2017-03-02T17:52:17.880245: step 15339, loss 0.212883, acc 0.9375
2017-03-02T17:52:17.968149: step 15340, loss 0.14074, acc 0.921875
2017-03-02T17:52:18.035917: step 15341, loss 0.224027, acc 0.9375
2017-03-02T17:52:18.110783: step 15342, loss 0.193357, acc 0.90625
2017-03-02T17:52:18.187805: step 15343, loss 0.103753, acc 0.984375
2017-03-02T17:52:18.262361: step 15344, loss 0.109414, acc 0.953125
2017-03-02T17:52:18.333857: step 15345, loss 0.100832, acc 0.96875
2017-03-02T17:52:18.403812: step 15346, loss 0.1492, acc 0.953125
2017-03-02T17:52:18.481838: step 15347, loss 0.0294804, acc 1
2017-03-02T17:52:18.561223: step 15348, loss 0.109977, acc 0.953125
2017-03-02T17:52:18.632983: step 15349, loss 0.180991, acc 0.921875
2017-03-02T17:52:18.725497: step 15350, loss 0.154851, acc 0.90625
2017-03-02T17:52:18.834743: step 15351, loss 0.175991, acc 0.921875
2017-03-02T17:52:18.916368: step 15352, loss 0.15993, acc 0.9375
2017-03-02T17:52:18.992316: step 15353, loss 0.202397, acc 0.875
2017-03-02T17:52:19.064210: step 15354, loss 0.24523, acc 0.90625
2017-03-02T17:52:19.132697: step 15355, loss 0.210071, acc 0.90625
2017-03-02T17:52:19.200456: step 15356, loss 0.158385, acc 0.953125
2017-03-02T17:52:19.277407: step 15357, loss 0.19898, acc 0.875
2017-03-02T17:52:19.347214: step 15358, loss 0.173377, acc 0.921875
2017-03-02T17:52:19.415257: step 15359, loss 0.170505, acc 0.953125
2017-03-02T17:52:19.500612: step 15360, loss 0.192576, acc 0.875
2017-03-02T17:52:19.576005: step 15361, loss 0.251836, acc 0.890625
2017-03-02T17:52:19.652375: step 15362, loss 0.0952065, acc 0.96875
2017-03-02T17:52:19.735908: step 15363, loss 0.183794, acc 0.890625
2017-03-02T17:52:19.813331: step 15364, loss 0.173418, acc 0.9375
2017-03-02T17:52:19.879332: step 15365, loss 0.158521, acc 0.921875
2017-03-02T17:52:19.953277: step 15366, loss 0.116078, acc 0.953125
2017-03-02T17:52:20.022531: step 15367, loss 0.0959251, acc 0.96875
2017-03-02T17:52:20.099240: step 15368, loss 0.136443, acc 0.953125
2017-03-02T17:52:20.178374: step 15369, loss 0.084715, acc 0.953125
2017-03-02T17:52:20.257081: step 15370, loss 0.154632, acc 0.90625
2017-03-02T17:52:20.334262: step 15371, loss 0.149759, acc 0.9375
2017-03-02T17:52:20.406371: step 15372, loss 0.0832022, acc 0.96875
2017-03-02T17:52:20.497923: step 15373, loss 0.130637, acc 0.96875
2017-03-02T17:52:20.573526: step 15374, loss 0.174308, acc 0.9375
2017-03-02T17:52:20.646571: step 15375, loss 0.0815023, acc 0.953125
2017-03-02T17:52:20.716997: step 15376, loss 0.149808, acc 0.9375
2017-03-02T17:52:20.787258: step 15377, loss 0.130853, acc 0.921875
2017-03-02T17:52:20.858944: step 15378, loss 0.153891, acc 0.953125
2017-03-02T17:52:20.945601: step 15379, loss 0.089039, acc 0.984375
2017-03-02T17:52:21.015335: step 15380, loss 0.113912, acc 0.96875
2017-03-02T17:52:21.083746: step 15381, loss 0.0789515, acc 0.953125
2017-03-02T17:52:21.158545: step 15382, loss 0.10256, acc 0.953125
2017-03-02T17:52:21.234021: step 15383, loss 0.16101, acc 0.90625
2017-03-02T17:52:21.300035: step 15384, loss 0.184207, acc 0.921875
2017-03-02T17:52:21.371404: step 15385, loss 0.109837, acc 0.984375
2017-03-02T17:52:21.437812: step 15386, loss 0.166744, acc 0.921875
2017-03-02T17:52:21.504699: step 15387, loss 0.187576, acc 0.90625
2017-03-02T17:52:21.582894: step 15388, loss 0.12911, acc 0.921875
2017-03-02T17:52:21.661892: step 15389, loss 0.059329, acc 1
2017-03-02T17:52:21.736772: step 15390, loss 0.203721, acc 0.921875
2017-03-02T17:52:21.815394: step 15391, loss 0.126019, acc 0.9375
2017-03-02T17:52:21.891807: step 15392, loss 0.108673, acc 0.9375
2017-03-02T17:52:21.964664: step 15393, loss 0.24503, acc 0.890625
2017-03-02T17:52:22.045819: step 15394, loss 0.238903, acc 0.921875
2017-03-02T17:52:22.117955: step 15395, loss 0.0866637, acc 0.96875
2017-03-02T17:52:22.189973: step 15396, loss 0.116613, acc 0.953125
2017-03-02T17:52:22.270670: step 15397, loss 0.227819, acc 0.890625
2017-03-02T17:52:22.356980: step 15398, loss 0.164503, acc 0.90625
2017-03-02T17:52:22.435489: step 15399, loss 0.176447, acc 0.890625
2017-03-02T17:52:22.506332: step 15400, loss 0.043388, acc 1

Evaluation:
2017-03-02T17:52:22.537414: step 15400, loss 1.92734, acc 0.65465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15400

2017-03-02T17:52:22.977825: step 15401, loss 0.277376, acc 0.859375
2017-03-02T17:52:23.048047: step 15402, loss 0.15759, acc 0.9375
2017-03-02T17:52:23.116224: step 15403, loss 0.142167, acc 0.921875
2017-03-02T17:52:23.196697: step 15404, loss 0.146922, acc 0.9375
2017-03-02T17:52:23.270371: step 15405, loss 0.112969, acc 0.953125
2017-03-02T17:52:23.343918: step 15406, loss 0.0815085, acc 0.984375
2017-03-02T17:52:23.405455: step 15407, loss 0.144917, acc 0.9375
2017-03-02T17:52:23.474420: step 15408, loss 0.163161, acc 0.921875
2017-03-02T17:52:23.548018: step 15409, loss 0.347937, acc 0.890625
2017-03-02T17:52:23.623994: step 15410, loss 0.097816, acc 0.9375
2017-03-02T17:52:23.695047: step 15411, loss 0.145064, acc 0.9375
2017-03-02T17:52:23.781107: step 15412, loss 0.189684, acc 0.875
2017-03-02T17:52:23.858769: step 15413, loss 0.236021, acc 0.90625
2017-03-02T17:52:23.926782: step 15414, loss 0.214892, acc 0.859375
2017-03-02T17:52:24.001793: step 15415, loss 0.16687, acc 0.921875
2017-03-02T17:52:24.076366: step 15416, loss 0.17393, acc 0.921875
2017-03-02T17:52:24.153130: step 15417, loss 0.158876, acc 0.9375
2017-03-02T17:52:24.222034: step 15418, loss 0.220251, acc 0.890625
2017-03-02T17:52:24.300281: step 15419, loss 0.198168, acc 0.890625
2017-03-02T17:52:24.377445: step 15420, loss 0.14382, acc 0.953125
2017-03-02T17:52:24.454745: step 15421, loss 0.360327, acc 0.875
2017-03-02T17:52:24.535372: step 15422, loss 0.216871, acc 0.875
2017-03-02T17:52:24.615497: step 15423, loss 0.12497, acc 0.921875
2017-03-02T17:52:24.688653: step 15424, loss 0.158418, acc 0.9375
2017-03-02T17:52:24.771542: step 15425, loss 0.148452, acc 0.890625
2017-03-02T17:52:24.844523: step 15426, loss 0.20694, acc 0.90625
2017-03-02T17:52:24.917612: step 15427, loss 0.181922, acc 0.9375
2017-03-02T17:52:25.000734: step 15428, loss 0.111564, acc 0.921875
2017-03-02T17:52:25.075126: step 15429, loss 0.104407, acc 0.953125
2017-03-02T17:52:25.148564: step 15430, loss 0.126264, acc 0.9375
2017-03-02T17:52:25.225397: step 15431, loss 0.195057, acc 0.890625
2017-03-02T17:52:25.301154: step 15432, loss 0.0603863, acc 0.984375
2017-03-02T17:52:25.373169: step 15433, loss 0.142252, acc 0.953125
2017-03-02T17:52:25.457653: step 15434, loss 0.185604, acc 0.921875
2017-03-02T17:52:25.528958: step 15435, loss 0.145839, acc 0.9375
2017-03-02T17:52:25.610477: step 15436, loss 0.0944655, acc 0.9375
2017-03-02T17:52:25.682978: step 15437, loss 0.146465, acc 0.953125
2017-03-02T17:52:25.761284: step 15438, loss 0.193241, acc 0.875
2017-03-02T17:52:25.834194: step 15439, loss 0.153364, acc 0.96875
2017-03-02T17:52:25.913551: step 15440, loss 0.156086, acc 0.921875
2017-03-02T17:52:25.986599: step 15441, loss 0.155218, acc 0.921875
2017-03-02T17:52:26.061492: step 15442, loss 0.106993, acc 0.96875
2017-03-02T17:52:26.139372: step 15443, loss 0.116969, acc 0.96875
2017-03-02T17:52:26.211896: step 15444, loss 0.426302, acc 0.84375
2017-03-02T17:52:26.281289: step 15445, loss 0.0716444, acc 0.96875
2017-03-02T17:52:26.354854: step 15446, loss 0.270712, acc 0.90625
2017-03-02T17:52:26.431721: step 15447, loss 0.297073, acc 0.875
2017-03-02T17:52:26.510127: step 15448, loss 0.120802, acc 0.953125
2017-03-02T17:52:26.592530: step 15449, loss 0.172772, acc 0.90625
2017-03-02T17:52:26.667414: step 15450, loss 0.0535778, acc 1
2017-03-02T17:52:26.741333: step 15451, loss 0.171092, acc 0.96875
2017-03-02T17:52:26.813373: step 15452, loss 0.240084, acc 0.921875
2017-03-02T17:52:26.881968: step 15453, loss 0.195285, acc 0.90625
2017-03-02T17:52:26.968945: step 15454, loss 0.172176, acc 0.921875
2017-03-02T17:52:27.042362: step 15455, loss 0.0943409, acc 0.984375
2017-03-02T17:52:27.110710: step 15456, loss 0.216476, acc 0.9375
2017-03-02T17:52:27.184368: step 15457, loss 0.0881015, acc 0.96875
2017-03-02T17:52:27.257022: step 15458, loss 0.0497617, acc 0.96875
2017-03-02T17:52:27.334535: step 15459, loss 0.0869284, acc 0.953125
2017-03-02T17:52:27.410585: step 15460, loss 0.218372, acc 0.890625
2017-03-02T17:52:27.488436: step 15461, loss 0.0955846, acc 0.953125
2017-03-02T17:52:27.560588: step 15462, loss 0.160298, acc 0.90625
2017-03-02T17:52:27.635919: step 15463, loss 0.152903, acc 0.953125
2017-03-02T17:52:27.711646: step 15464, loss 0.20446, acc 0.921875
2017-03-02T17:52:27.784772: step 15465, loss 0.238004, acc 0.90625
2017-03-02T17:52:27.864772: step 15466, loss 0.138036, acc 0.921875
2017-03-02T17:52:27.940646: step 15467, loss 0.125965, acc 0.9375
2017-03-02T17:52:28.028897: step 15468, loss 0.221457, acc 0.90625
2017-03-02T17:52:28.101590: step 15469, loss 0.119726, acc 0.953125
2017-03-02T17:52:28.181420: step 15470, loss 0.0738433, acc 0.984375
2017-03-02T17:52:28.250683: step 15471, loss 0.146114, acc 0.921875
2017-03-02T17:52:28.322426: step 15472, loss 0.262788, acc 0.921875
2017-03-02T17:52:28.397825: step 15473, loss 0.0583955, acc 0.984375
2017-03-02T17:52:28.467914: step 15474, loss 0.333652, acc 0.859375
2017-03-02T17:52:28.541996: step 15475, loss 0.0807177, acc 0.96875
2017-03-02T17:52:28.646513: step 15476, loss 0.0688604, acc 1
2017-03-02T17:52:28.720185: step 15477, loss 0.296862, acc 0.875
2017-03-02T17:52:28.794120: step 15478, loss 0.145723, acc 0.921875
2017-03-02T17:52:28.876435: step 15479, loss 0.20256, acc 0.875
2017-03-02T17:52:28.947389: step 15480, loss 0.173863, acc 0.921875
2017-03-02T17:52:29.017622: step 15481, loss 0.230803, acc 0.84375
2017-03-02T17:52:29.091413: step 15482, loss 0.171397, acc 0.921875
2017-03-02T17:52:29.162836: step 15483, loss 0.168099, acc 0.9375
2017-03-02T17:52:29.227348: step 15484, loss 0.0193763, acc 1
2017-03-02T17:52:29.308943: step 15485, loss 0.144587, acc 0.953125
2017-03-02T17:52:29.381660: step 15486, loss 0.233039, acc 0.90625
2017-03-02T17:52:29.453377: step 15487, loss 0.134657, acc 0.9375
2017-03-02T17:52:29.525294: step 15488, loss 0.0884088, acc 0.953125
2017-03-02T17:52:29.601473: step 15489, loss 0.213766, acc 0.90625
2017-03-02T17:52:29.666317: step 15490, loss 0.106066, acc 0.984375
2017-03-02T17:52:29.737822: step 15491, loss 0.11579, acc 0.953125
2017-03-02T17:52:29.811522: step 15492, loss 0.0559932, acc 0.96875
2017-03-02T17:52:29.893814: step 15493, loss 0.133996, acc 0.9375
2017-03-02T17:52:29.964928: step 15494, loss 0.0764072, acc 0.953125
2017-03-02T17:52:30.040635: step 15495, loss 0.0857569, acc 0.96875
2017-03-02T17:52:30.113740: step 15496, loss 0.116375, acc 0.9375
2017-03-02T17:52:30.187348: step 15497, loss 0.0815853, acc 0.953125
2017-03-02T17:52:30.258386: step 15498, loss 0.188829, acc 0.9375
2017-03-02T17:52:30.339260: step 15499, loss 0.161492, acc 0.9375
2017-03-02T17:52:30.405324: step 15500, loss 0.204241, acc 0.90625

Evaluation:
2017-03-02T17:52:30.442678: step 15500, loss 1.8711, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15500

2017-03-02T17:52:30.956530: step 15501, loss 0.196587, acc 0.921875
2017-03-02T17:52:31.023671: step 15502, loss 0.152065, acc 0.921875
2017-03-02T17:52:31.106398: step 15503, loss 0.0850379, acc 0.953125
2017-03-02T17:52:31.195712: step 15504, loss 0.105867, acc 0.953125
2017-03-02T17:52:31.267315: step 15505, loss 0.146413, acc 0.9375
2017-03-02T17:52:31.337086: step 15506, loss 0.131535, acc 0.953125
2017-03-02T17:52:31.415072: step 15507, loss 0.181444, acc 0.875
2017-03-02T17:52:31.491312: step 15508, loss 0.114753, acc 0.96875
2017-03-02T17:52:31.563967: step 15509, loss 0.117552, acc 0.921875
2017-03-02T17:52:31.634250: step 15510, loss 0.143056, acc 0.9375
2017-03-02T17:52:31.702050: step 15511, loss 0.17073, acc 0.921875
2017-03-02T17:52:31.769654: step 15512, loss 0.0818497, acc 0.96875
2017-03-02T17:52:31.848030: step 15513, loss 0.198532, acc 0.921875
2017-03-02T17:52:31.923055: step 15514, loss 0.135757, acc 0.9375
2017-03-02T17:52:31.996112: step 15515, loss 0.134084, acc 0.9375
2017-03-02T17:52:32.071859: step 15516, loss 0.120813, acc 0.953125
2017-03-02T17:52:32.162740: step 15517, loss 0.163515, acc 0.921875
2017-03-02T17:52:32.234280: step 15518, loss 0.0946011, acc 0.984375
2017-03-02T17:52:32.315554: step 15519, loss 0.0780486, acc 0.96875
2017-03-02T17:52:32.387458: step 15520, loss 0.088887, acc 0.96875
2017-03-02T17:52:32.458765: step 15521, loss 0.115737, acc 0.953125
2017-03-02T17:52:32.529624: step 15522, loss 0.13222, acc 0.953125
2017-03-02T17:52:32.601376: step 15523, loss 0.164004, acc 0.9375
2017-03-02T17:52:32.670696: step 15524, loss 0.0499316, acc 0.96875
2017-03-02T17:52:32.742694: step 15525, loss 0.137228, acc 0.96875
2017-03-02T17:52:32.821366: step 15526, loss 0.176957, acc 0.9375
2017-03-02T17:52:32.894354: step 15527, loss 0.157504, acc 0.953125
2017-03-02T17:52:32.968352: step 15528, loss 0.0758457, acc 0.96875
2017-03-02T17:52:33.048579: step 15529, loss 0.132525, acc 0.921875
2017-03-02T17:52:33.116163: step 15530, loss 0.0538897, acc 0.984375
2017-03-02T17:52:33.186745: step 15531, loss 0.0877586, acc 0.96875
2017-03-02T17:52:33.276449: step 15532, loss 0.14487, acc 0.953125
2017-03-02T17:52:33.354662: step 15533, loss 0.219593, acc 0.90625
2017-03-02T17:52:33.427932: step 15534, loss 0.172927, acc 0.9375
2017-03-02T17:52:33.498561: step 15535, loss 0.147684, acc 0.9375
2017-03-02T17:52:33.569495: step 15536, loss 0.112811, acc 0.96875
2017-03-02T17:52:33.643136: step 15537, loss 0.281223, acc 0.828125
2017-03-02T17:52:33.712315: step 15538, loss 0.227314, acc 0.890625
2017-03-02T17:52:33.790809: step 15539, loss 0.19472, acc 0.953125
2017-03-02T17:52:33.866419: step 15540, loss 0.111302, acc 0.953125
2017-03-02T17:52:33.948197: step 15541, loss 0.139142, acc 0.953125
2017-03-02T17:52:34.025777: step 15542, loss 0.229489, acc 0.90625
2017-03-02T17:52:34.098985: step 15543, loss 0.123548, acc 0.9375
2017-03-02T17:52:34.171729: step 15544, loss 0.114097, acc 0.96875
2017-03-02T17:52:34.242900: step 15545, loss 0.0987356, acc 0.953125
2017-03-02T17:52:34.315912: step 15546, loss 0.0572989, acc 0.96875
2017-03-02T17:52:34.387182: step 15547, loss 0.146025, acc 0.9375
2017-03-02T17:52:34.459231: step 15548, loss 0.177597, acc 0.890625
2017-03-02T17:52:34.529536: step 15549, loss 0.20717, acc 0.90625
2017-03-02T17:52:34.604298: step 15550, loss 0.174267, acc 0.921875
2017-03-02T17:52:34.675989: step 15551, loss 0.102586, acc 0.9375
2017-03-02T17:52:34.749733: step 15552, loss 0.11313, acc 0.953125
2017-03-02T17:52:34.825266: step 15553, loss 0.121019, acc 0.953125
2017-03-02T17:52:34.907347: step 15554, loss 0.167024, acc 0.96875
2017-03-02T17:52:34.984112: step 15555, loss 0.142861, acc 0.9375
2017-03-02T17:52:35.058616: step 15556, loss 0.278026, acc 0.90625
2017-03-02T17:52:35.134307: step 15557, loss 0.102888, acc 0.953125
2017-03-02T17:52:35.201197: step 15558, loss 0.0813731, acc 0.96875
2017-03-02T17:52:35.276986: step 15559, loss 0.239696, acc 0.890625
2017-03-02T17:52:35.343409: step 15560, loss 0.19915, acc 0.9375
2017-03-02T17:52:35.422159: step 15561, loss 0.118337, acc 0.90625
2017-03-02T17:52:35.493177: step 15562, loss 0.176679, acc 0.9375
2017-03-02T17:52:35.564089: step 15563, loss 0.160322, acc 0.921875
2017-03-02T17:52:35.633688: step 15564, loss 0.12176, acc 0.953125
2017-03-02T17:52:35.708770: step 15565, loss 0.129733, acc 0.921875
2017-03-02T17:52:35.785399: step 15566, loss 0.16169, acc 0.921875
2017-03-02T17:52:35.857003: step 15567, loss 0.175004, acc 0.921875
2017-03-02T17:52:35.926803: step 15568, loss 0.0747831, acc 0.953125
2017-03-02T17:52:36.001285: step 15569, loss 0.0256766, acc 1
2017-03-02T17:52:36.080817: step 15570, loss 0.236647, acc 0.890625
2017-03-02T17:52:36.150400: step 15571, loss 0.274125, acc 0.90625
2017-03-02T17:52:36.223605: step 15572, loss 0.175393, acc 0.890625
2017-03-02T17:52:36.300799: step 15573, loss 0.254086, acc 0.875
2017-03-02T17:52:36.375547: step 15574, loss 0.21181, acc 0.90625
2017-03-02T17:52:36.450111: step 15575, loss 0.161864, acc 0.90625
2017-03-02T17:52:36.523130: step 15576, loss 0.211875, acc 0.953125
2017-03-02T17:52:36.608708: step 15577, loss 0.192822, acc 0.90625
2017-03-02T17:52:36.682304: step 15578, loss 0.157979, acc 0.890625
2017-03-02T17:52:36.750108: step 15579, loss 0.112803, acc 0.953125
2017-03-02T17:52:36.822183: step 15580, loss 0.17702, acc 0.90625
2017-03-02T17:52:36.896123: step 15581, loss 0.136766, acc 0.90625
2017-03-02T17:52:36.982519: step 15582, loss 0.170452, acc 0.921875
2017-03-02T17:52:37.053090: step 15583, loss 0.0823742, acc 0.9375
2017-03-02T17:52:37.122597: step 15584, loss 0.134412, acc 0.953125
2017-03-02T17:52:37.192793: step 15585, loss 0.173113, acc 0.953125
2017-03-02T17:52:37.261324: step 15586, loss 0.0814379, acc 0.96875
2017-03-02T17:52:37.329158: step 15587, loss 0.148141, acc 0.9375
2017-03-02T17:52:37.405514: step 15588, loss 0.148641, acc 0.953125
2017-03-02T17:52:37.479486: step 15589, loss 0.189745, acc 0.921875
2017-03-02T17:52:37.567836: step 15590, loss 0.101362, acc 0.953125
2017-03-02T17:52:37.646634: step 15591, loss 0.100649, acc 0.984375
2017-03-02T17:52:37.725581: step 15592, loss 0.164597, acc 0.921875
2017-03-02T17:52:37.797662: step 15593, loss 0.169685, acc 0.921875
2017-03-02T17:52:37.865718: step 15594, loss 0.303421, acc 0.84375
2017-03-02T17:52:37.937587: step 15595, loss 0.061263, acc 0.96875
2017-03-02T17:52:38.002953: step 15596, loss 0.18138, acc 0.953125
2017-03-02T17:52:38.072728: step 15597, loss 0.0855674, acc 0.953125
2017-03-02T17:52:38.144398: step 15598, loss 0.130901, acc 0.921875
2017-03-02T17:52:38.221783: step 15599, loss 0.359256, acc 0.828125
2017-03-02T17:52:38.297128: step 15600, loss 0.249646, acc 0.890625

Evaluation:
2017-03-02T17:52:38.334234: step 15600, loss 1.88637, acc 0.653208

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15600

2017-03-02T17:52:38.811079: step 15601, loss 0.0949528, acc 0.984375
2017-03-02T17:52:38.876483: step 15602, loss 0.0922177, acc 0.953125
2017-03-02T17:52:38.957599: step 15603, loss 0.291144, acc 0.890625
2017-03-02T17:52:39.032420: step 15604, loss 0.131666, acc 0.953125
2017-03-02T17:52:39.107424: step 15605, loss 0.141376, acc 0.96875
2017-03-02T17:52:39.196449: step 15606, loss 0.0998837, acc 0.953125
2017-03-02T17:52:39.268617: step 15607, loss 0.204846, acc 0.890625
2017-03-02T17:52:39.334477: step 15608, loss 0.111058, acc 0.96875
2017-03-02T17:52:39.403546: step 15609, loss 0.209497, acc 0.890625
2017-03-02T17:52:39.475067: step 15610, loss 0.200617, acc 0.90625
2017-03-02T17:52:39.546580: step 15611, loss 0.164193, acc 0.90625
2017-03-02T17:52:39.617723: step 15612, loss 0.230641, acc 0.90625
2017-03-02T17:52:39.697257: step 15613, loss 0.177627, acc 0.90625
2017-03-02T17:52:39.764046: step 15614, loss 0.152555, acc 0.921875
2017-03-02T17:52:39.832699: step 15615, loss 0.155299, acc 0.9375
2017-03-02T17:52:39.904050: step 15616, loss 0.254516, acc 0.921875
2017-03-02T17:52:39.978467: step 15617, loss 0.0985346, acc 0.96875
2017-03-02T17:52:40.049142: step 15618, loss 0.166789, acc 0.90625
2017-03-02T17:52:40.123376: step 15619, loss 0.197215, acc 0.90625
2017-03-02T17:52:40.198335: step 15620, loss 0.0917243, acc 0.953125
2017-03-02T17:52:40.268616: step 15621, loss 0.114336, acc 0.953125
2017-03-02T17:52:40.340035: step 15622, loss 0.293657, acc 0.859375
2017-03-02T17:52:40.413011: step 15623, loss 0.218802, acc 0.875
2017-03-02T17:52:40.481352: step 15624, loss 0.170923, acc 0.921875
2017-03-02T17:52:40.565016: step 15625, loss 0.0593169, acc 1
2017-03-02T17:52:40.630375: step 15626, loss 0.132158, acc 0.953125
2017-03-02T17:52:40.694462: step 15627, loss 0.159013, acc 0.9375
2017-03-02T17:52:40.756212: step 15628, loss 0.323825, acc 0.84375
2017-03-02T17:52:40.828401: step 15629, loss 0.0970104, acc 1
2017-03-02T17:52:40.899631: step 15630, loss 0.310322, acc 0.859375
2017-03-02T17:52:40.972621: step 15631, loss 0.231432, acc 0.859375
2017-03-02T17:52:41.052150: step 15632, loss 0.0948509, acc 0.9375
2017-03-02T17:52:41.138583: step 15633, loss 0.11806, acc 0.953125
2017-03-02T17:52:41.212893: step 15634, loss 0.212447, acc 0.90625
2017-03-02T17:52:41.303184: step 15635, loss 0.142178, acc 0.921875
2017-03-02T17:52:41.372950: step 15636, loss 0.192986, acc 0.90625
2017-03-02T17:52:41.445272: step 15637, loss 0.207114, acc 0.890625
2017-03-02T17:52:41.526125: step 15638, loss 0.189219, acc 0.953125
2017-03-02T17:52:41.629063: step 15639, loss 0.0839064, acc 0.96875
2017-03-02T17:52:41.708084: step 15640, loss 0.182744, acc 0.921875
2017-03-02T17:52:41.788224: step 15641, loss 0.170446, acc 0.9375
2017-03-02T17:52:41.868119: step 15642, loss 0.109315, acc 0.953125
2017-03-02T17:52:41.943152: step 15643, loss 0.22078, acc 0.90625
2017-03-02T17:52:42.013486: step 15644, loss 0.112535, acc 0.921875
2017-03-02T17:52:42.091661: step 15645, loss 0.20216, acc 0.90625
2017-03-02T17:52:42.168543: step 15646, loss 0.105556, acc 0.9375
2017-03-02T17:52:42.243024: step 15647, loss 0.235716, acc 0.875
2017-03-02T17:52:42.324552: step 15648, loss 0.202461, acc 0.890625
2017-03-02T17:52:42.402105: step 15649, loss 0.192612, acc 0.9375
2017-03-02T17:52:42.473657: step 15650, loss 0.191797, acc 0.921875
2017-03-02T17:52:42.544963: step 15651, loss 0.0843564, acc 0.96875
2017-03-02T17:52:42.617549: step 15652, loss 0.170841, acc 0.90625
2017-03-02T17:52:42.693871: step 15653, loss 0.216699, acc 0.890625
2017-03-02T17:52:42.755751: step 15654, loss 0.194012, acc 0.90625
2017-03-02T17:52:42.811808: step 15655, loss 0.115118, acc 0.953125
2017-03-02T17:52:42.879960: step 15656, loss 0.46822, acc 0.828125
2017-03-02T17:52:42.959069: step 15657, loss 0.277501, acc 0.875
2017-03-02T17:52:43.040490: step 15658, loss 0.0725757, acc 0.984375
2017-03-02T17:52:43.111339: step 15659, loss 0.186262, acc 0.921875
2017-03-02T17:52:43.183319: step 15660, loss 0.237485, acc 0.90625
2017-03-02T17:52:43.259546: step 15661, loss 0.0905784, acc 0.953125
2017-03-02T17:52:43.334100: step 15662, loss 0.116062, acc 0.984375
2017-03-02T17:52:43.402867: step 15663, loss 0.21944, acc 0.890625
2017-03-02T17:52:43.476427: step 15664, loss 0.244948, acc 0.875
2017-03-02T17:52:43.546025: step 15665, loss 0.236967, acc 0.890625
2017-03-02T17:52:43.615905: step 15666, loss 0.152128, acc 0.921875
2017-03-02T17:52:43.686457: step 15667, loss 0.173117, acc 0.9375
2017-03-02T17:52:43.756061: step 15668, loss 0.221252, acc 0.875
2017-03-02T17:52:43.834137: step 15669, loss 0.109888, acc 0.96875
2017-03-02T17:52:43.910244: step 15670, loss 0.222054, acc 0.890625
2017-03-02T17:52:43.983839: step 15671, loss 0.0447565, acc 1
2017-03-02T17:52:44.060943: step 15672, loss 0.180292, acc 0.921875
2017-03-02T17:52:44.134776: step 15673, loss 0.226572, acc 0.9375
2017-03-02T17:52:44.210790: step 15674, loss 0.280033, acc 0.90625
2017-03-02T17:52:44.290751: step 15675, loss 0.212729, acc 0.921875
2017-03-02T17:52:44.370677: step 15676, loss 0.163175, acc 0.921875
2017-03-02T17:52:44.446686: step 15677, loss 0.210228, acc 0.90625
2017-03-02T17:52:44.522338: step 15678, loss 0.0886399, acc 0.96875
2017-03-02T17:52:44.597611: step 15679, loss 0.284988, acc 0.875
2017-03-02T17:52:44.668878: step 15680, loss 0.500363, acc 0.75
2017-03-02T17:52:44.745402: step 15681, loss 0.0501248, acc 1
2017-03-02T17:52:44.811132: step 15682, loss 0.266699, acc 0.875
2017-03-02T17:52:44.874412: step 15683, loss 0.202321, acc 0.921875
2017-03-02T17:52:44.951223: step 15684, loss 0.121589, acc 0.921875
2017-03-02T17:52:45.021834: step 15685, loss 0.0964273, acc 0.96875
2017-03-02T17:52:45.092064: step 15686, loss 0.0841834, acc 0.984375
2017-03-02T17:52:45.165379: step 15687, loss 0.13159, acc 0.96875
2017-03-02T17:52:45.236733: step 15688, loss 0.0930975, acc 0.96875
2017-03-02T17:52:45.307411: step 15689, loss 0.0993599, acc 0.984375
2017-03-02T17:52:45.375552: step 15690, loss 0.171217, acc 0.921875
2017-03-02T17:52:45.452772: step 15691, loss 0.261055, acc 0.875
2017-03-02T17:52:45.519266: step 15692, loss 0.142764, acc 0.953125
2017-03-02T17:52:45.592574: step 15693, loss 0.0787114, acc 0.953125
2017-03-02T17:52:45.664045: step 15694, loss 0.107961, acc 0.953125
2017-03-02T17:52:45.737059: step 15695, loss 0.092773, acc 0.9375
2017-03-02T17:52:45.826287: step 15696, loss 0.142629, acc 0.9375
2017-03-02T17:52:45.898362: step 15697, loss 0.103877, acc 0.953125
2017-03-02T17:52:45.969627: step 15698, loss 0.253432, acc 0.890625
2017-03-02T17:52:46.038340: step 15699, loss 0.0561434, acc 0.984375
2017-03-02T17:52:46.109007: step 15700, loss 0.0843232, acc 0.984375

Evaluation:
2017-03-02T17:52:46.135373: step 15700, loss 1.92883, acc 0.666907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15700

2017-03-02T17:52:46.589401: step 15701, loss 0.180809, acc 0.9375
2017-03-02T17:52:46.662890: step 15702, loss 0.101695, acc 0.984375
2017-03-02T17:52:46.736252: step 15703, loss 0.213663, acc 0.90625
2017-03-02T17:52:46.812540: step 15704, loss 0.164191, acc 0.921875
2017-03-02T17:52:46.882843: step 15705, loss 0.146796, acc 0.9375
2017-03-02T17:52:46.957384: step 15706, loss 0.101855, acc 0.96875
2017-03-02T17:52:47.033379: step 15707, loss 0.293014, acc 0.859375
2017-03-02T17:52:47.107427: step 15708, loss 0.156622, acc 0.921875
2017-03-02T17:52:47.179349: step 15709, loss 0.197561, acc 0.890625
2017-03-02T17:52:47.257151: step 15710, loss 0.247595, acc 0.859375
2017-03-02T17:52:47.330942: step 15711, loss 0.205276, acc 0.921875
2017-03-02T17:52:47.409329: step 15712, loss 0.108009, acc 0.953125
2017-03-02T17:52:47.481806: step 15713, loss 0.147366, acc 0.9375
2017-03-02T17:52:47.564402: step 15714, loss 0.203995, acc 0.890625
2017-03-02T17:52:47.636221: step 15715, loss 0.162935, acc 0.96875
2017-03-02T17:52:47.715071: step 15716, loss 0.114356, acc 0.9375
2017-03-02T17:52:47.789387: step 15717, loss 0.184624, acc 0.9375
2017-03-02T17:52:47.862450: step 15718, loss 0.228153, acc 0.890625
2017-03-02T17:52:47.941845: step 15719, loss 0.0932008, acc 0.96875
2017-03-02T17:52:48.023822: step 15720, loss 0.240138, acc 0.921875
2017-03-02T17:52:48.098930: step 15721, loss 0.112035, acc 0.953125
2017-03-02T17:52:48.170034: step 15722, loss 0.193682, acc 0.96875
2017-03-02T17:52:48.243076: step 15723, loss 0.188599, acc 0.90625
2017-03-02T17:52:48.315736: step 15724, loss 0.222678, acc 0.921875
2017-03-02T17:52:48.397112: step 15725, loss 0.26349, acc 0.890625
2017-03-02T17:52:48.473499: step 15726, loss 0.125048, acc 0.96875
2017-03-02T17:52:48.550564: step 15727, loss 0.119411, acc 0.96875
2017-03-02T17:52:48.622144: step 15728, loss 0.119788, acc 0.96875
2017-03-02T17:52:48.691511: step 15729, loss 0.153392, acc 0.921875
2017-03-02T17:52:48.762343: step 15730, loss 0.193813, acc 0.90625
2017-03-02T17:52:48.844673: step 15731, loss 0.178207, acc 0.953125
2017-03-02T17:52:48.917470: step 15732, loss 0.144291, acc 0.953125
2017-03-02T17:52:48.988345: step 15733, loss 0.208788, acc 0.90625
2017-03-02T17:52:49.058760: step 15734, loss 0.0614104, acc 0.984375
2017-03-02T17:52:49.131911: step 15735, loss 0.12167, acc 0.96875
2017-03-02T17:52:49.205581: step 15736, loss 0.164755, acc 0.90625
2017-03-02T17:52:49.275486: step 15737, loss 0.117269, acc 0.953125
2017-03-02T17:52:49.352910: step 15738, loss 0.139372, acc 0.9375
2017-03-02T17:52:49.423866: step 15739, loss 0.121748, acc 0.953125
2017-03-02T17:52:49.497569: step 15740, loss 0.268451, acc 0.890625
2017-03-02T17:52:49.568368: step 15741, loss 0.130556, acc 0.9375
2017-03-02T17:52:49.634921: step 15742, loss 0.121712, acc 0.953125
2017-03-02T17:52:49.709292: step 15743, loss 0.105948, acc 0.96875
2017-03-02T17:52:49.781901: step 15744, loss 0.192077, acc 0.921875
2017-03-02T17:52:49.864113: step 15745, loss 0.16872, acc 0.921875
2017-03-02T17:52:49.937410: step 15746, loss 0.116404, acc 0.96875
2017-03-02T17:52:50.011970: step 15747, loss 0.179276, acc 0.890625
2017-03-02T17:52:50.087574: step 15748, loss 0.0887191, acc 0.96875
2017-03-02T17:52:50.155591: step 15749, loss 0.144292, acc 0.9375
2017-03-02T17:52:50.226196: step 15750, loss 0.139047, acc 0.9375
2017-03-02T17:52:50.295686: step 15751, loss 0.135919, acc 0.921875
2017-03-02T17:52:50.372406: step 15752, loss 0.109854, acc 0.96875
2017-03-02T17:52:50.447897: step 15753, loss 0.049611, acc 0.984375
2017-03-02T17:52:50.522298: step 15754, loss 0.117625, acc 0.953125
2017-03-02T17:52:50.613017: step 15755, loss 0.116035, acc 0.953125
2017-03-02T17:52:50.683840: step 15756, loss 0.188381, acc 0.921875
2017-03-02T17:52:50.753848: step 15757, loss 0.100383, acc 0.953125
2017-03-02T17:52:50.829137: step 15758, loss 0.167065, acc 0.921875
2017-03-02T17:52:50.906587: step 15759, loss 0.161582, acc 0.9375
2017-03-02T17:52:50.983853: step 15760, loss 0.157151, acc 0.953125
2017-03-02T17:52:51.052117: step 15761, loss 0.155856, acc 0.9375
2017-03-02T17:52:51.125659: step 15762, loss 0.202022, acc 0.890625
2017-03-02T17:52:51.197278: step 15763, loss 0.158501, acc 0.921875
2017-03-02T17:52:51.270579: step 15764, loss 0.214024, acc 0.890625
2017-03-02T17:52:51.353829: step 15765, loss 0.106287, acc 0.953125
2017-03-02T17:52:51.417825: step 15766, loss 0.141942, acc 0.921875
2017-03-02T17:52:51.494730: step 15767, loss 0.28758, acc 0.890625
2017-03-02T17:52:51.566156: step 15768, loss 0.208022, acc 0.875
2017-03-02T17:52:51.634057: step 15769, loss 0.0688649, acc 0.984375
2017-03-02T17:52:51.702541: step 15770, loss 0.136502, acc 0.953125
2017-03-02T17:52:51.774498: step 15771, loss 0.215337, acc 0.90625
2017-03-02T17:52:51.848982: step 15772, loss 0.0916388, acc 0.96875
2017-03-02T17:52:51.920769: step 15773, loss 0.145142, acc 0.921875
2017-03-02T17:52:51.991861: step 15774, loss 0.100142, acc 0.953125
2017-03-02T17:52:52.065009: step 15775, loss 0.215128, acc 0.9375
2017-03-02T17:52:52.134022: step 15776, loss 0.29031, acc 0.921875
2017-03-02T17:52:52.205746: step 15777, loss 0.206588, acc 0.890625
2017-03-02T17:52:52.278012: step 15778, loss 0.136341, acc 0.953125
2017-03-02T17:52:52.356319: step 15779, loss 0.27951, acc 0.859375
2017-03-02T17:52:52.427259: step 15780, loss 0.21551, acc 0.890625
2017-03-02T17:52:52.502162: step 15781, loss 0.0856936, acc 0.953125
2017-03-02T17:52:52.570692: step 15782, loss 0.221158, acc 0.875
2017-03-02T17:52:52.650076: step 15783, loss 0.220778, acc 0.90625
2017-03-02T17:52:52.722489: step 15784, loss 0.171615, acc 0.90625
2017-03-02T17:52:52.792622: step 15785, loss 0.232184, acc 0.9375
2017-03-02T17:52:52.861511: step 15786, loss 0.165269, acc 0.90625
2017-03-02T17:52:52.934335: step 15787, loss 0.268485, acc 0.859375
2017-03-02T17:52:53.020586: step 15788, loss 0.220063, acc 0.90625
2017-03-02T17:52:53.089028: step 15789, loss 0.207036, acc 0.90625
2017-03-02T17:52:53.159212: step 15790, loss 0.178284, acc 0.9375
2017-03-02T17:52:53.228445: step 15791, loss 0.194943, acc 0.953125
2017-03-02T17:52:53.303388: step 15792, loss 0.0801893, acc 0.96875
2017-03-02T17:52:53.377194: step 15793, loss 0.184295, acc 0.890625
2017-03-02T17:52:53.453175: step 15794, loss 0.279079, acc 0.9375
2017-03-02T17:52:53.520953: step 15795, loss 0.215546, acc 0.90625
2017-03-02T17:52:53.592425: step 15796, loss 0.185215, acc 0.890625
2017-03-02T17:52:53.667691: step 15797, loss 0.112757, acc 0.953125
2017-03-02T17:52:53.741397: step 15798, loss 0.289212, acc 0.875
2017-03-02T17:52:53.805543: step 15799, loss 0.0460443, acc 0.984375
2017-03-02T17:52:53.877596: step 15800, loss 0.173032, acc 0.953125

Evaluation:
2017-03-02T17:52:53.912934: step 15800, loss 1.91163, acc 0.668349

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15800

2017-03-02T17:52:54.363902: step 15801, loss 0.161739, acc 0.9375
2017-03-02T17:52:54.434763: step 15802, loss 0.261751, acc 0.859375
2017-03-02T17:52:54.509074: step 15803, loss 0.128294, acc 0.9375
2017-03-02T17:52:54.589923: step 15804, loss 0.135805, acc 0.9375
2017-03-02T17:52:54.667703: step 15805, loss 0.253926, acc 0.84375
2017-03-02T17:52:54.732041: step 15806, loss 0.164881, acc 0.9375
2017-03-02T17:52:54.796070: step 15807, loss 0.172009, acc 0.921875
2017-03-02T17:52:54.862662: step 15808, loss 0.0768826, acc 0.96875
2017-03-02T17:52:54.933896: step 15809, loss 0.23562, acc 0.90625
2017-03-02T17:52:55.004909: step 15810, loss 0.0883059, acc 0.984375
2017-03-02T17:52:55.075339: step 15811, loss 0.0781741, acc 0.96875
2017-03-02T17:52:55.135458: step 15812, loss 0.134099, acc 0.921875
2017-03-02T17:52:55.223431: step 15813, loss 0.0856539, acc 0.96875
2017-03-02T17:52:55.288601: step 15814, loss 0.0889443, acc 0.96875
2017-03-02T17:52:55.369693: step 15815, loss 0.157883, acc 0.921875
2017-03-02T17:52:55.443574: step 15816, loss 0.134394, acc 0.9375
2017-03-02T17:52:55.520683: step 15817, loss 0.175887, acc 0.953125
2017-03-02T17:52:55.591336: step 15818, loss 0.0497604, acc 0.984375
2017-03-02T17:52:55.660794: step 15819, loss 0.125259, acc 0.9375
2017-03-02T17:52:55.736473: step 15820, loss 0.148268, acc 0.921875
2017-03-02T17:52:55.804945: step 15821, loss 0.177726, acc 0.921875
2017-03-02T17:52:55.882740: step 15822, loss 0.217088, acc 0.9375
2017-03-02T17:52:55.957457: step 15823, loss 0.118858, acc 0.96875
2017-03-02T17:52:56.031234: step 15824, loss 0.106736, acc 0.96875
2017-03-02T17:52:56.134325: step 15825, loss 0.149711, acc 0.9375
2017-03-02T17:52:56.204034: step 15826, loss 0.0439851, acc 1
2017-03-02T17:52:56.275793: step 15827, loss 0.108916, acc 0.921875
2017-03-02T17:52:56.347067: step 15828, loss 0.156742, acc 0.9375
2017-03-02T17:52:56.419476: step 15829, loss 0.106065, acc 0.984375
2017-03-02T17:52:56.490015: step 15830, loss 0.109588, acc 0.9375
2017-03-02T17:52:56.562348: step 15831, loss 0.213331, acc 0.90625
2017-03-02T17:52:56.630963: step 15832, loss 0.0999942, acc 0.9375
2017-03-02T17:52:56.719266: step 15833, loss 0.227759, acc 0.90625
2017-03-02T17:52:56.794539: step 15834, loss 0.195778, acc 0.90625
2017-03-02T17:52:56.886681: step 15835, loss 0.126599, acc 0.90625
2017-03-02T17:52:56.958153: step 15836, loss 0.171629, acc 0.921875
2017-03-02T17:52:57.035580: step 15837, loss 0.0744923, acc 0.953125
2017-03-02T17:52:57.107738: step 15838, loss 0.203968, acc 0.921875
2017-03-02T17:52:57.169010: step 15839, loss 0.198098, acc 0.890625
2017-03-02T17:52:57.251101: step 15840, loss 0.166969, acc 0.9375
2017-03-02T17:52:57.332443: step 15841, loss 0.17787, acc 0.953125
2017-03-02T17:52:57.405296: step 15842, loss 0.130282, acc 0.9375
2017-03-02T17:52:57.478153: step 15843, loss 0.167325, acc 0.921875
2017-03-02T17:52:57.555403: step 15844, loss 0.100926, acc 0.953125
2017-03-02T17:52:57.629924: step 15845, loss 0.0588573, acc 0.984375
2017-03-02T17:52:57.700706: step 15846, loss 0.148427, acc 0.921875
2017-03-02T17:52:57.774826: step 15847, loss 0.161517, acc 0.90625
2017-03-02T17:52:57.846336: step 15848, loss 0.285869, acc 0.890625
2017-03-02T17:52:57.922800: step 15849, loss 0.0825731, acc 0.953125
2017-03-02T17:52:57.999855: step 15850, loss 0.158991, acc 0.953125
2017-03-02T17:52:58.073224: step 15851, loss 0.224764, acc 0.875
2017-03-02T17:52:58.154110: step 15852, loss 0.174277, acc 0.921875
2017-03-02T17:52:58.228567: step 15853, loss 0.296132, acc 0.875
2017-03-02T17:52:58.296842: step 15854, loss 0.232185, acc 0.90625
2017-03-02T17:52:58.374541: step 15855, loss 0.122222, acc 0.9375
2017-03-02T17:52:58.458251: step 15856, loss 0.179774, acc 0.90625
2017-03-02T17:52:58.523948: step 15857, loss 0.0347167, acc 1
2017-03-02T17:52:58.590506: step 15858, loss 0.235256, acc 0.875
2017-03-02T17:52:58.664562: step 15859, loss 0.256639, acc 0.875
2017-03-02T17:52:58.736599: step 15860, loss 0.19592, acc 0.9375
2017-03-02T17:52:58.807172: step 15861, loss 0.166984, acc 0.921875
2017-03-02T17:52:58.888410: step 15862, loss 0.0533442, acc 0.953125
2017-03-02T17:52:58.959760: step 15863, loss 0.0859474, acc 0.96875
2017-03-02T17:52:59.033401: step 15864, loss 0.152452, acc 0.9375
2017-03-02T17:52:59.110824: step 15865, loss 0.218893, acc 0.90625
2017-03-02T17:52:59.175445: step 15866, loss 0.17036, acc 0.96875
2017-03-02T17:52:59.247282: step 15867, loss 0.0927999, acc 0.921875
2017-03-02T17:52:59.323555: step 15868, loss 0.159867, acc 0.90625
2017-03-02T17:52:59.397202: step 15869, loss 0.200369, acc 0.890625
2017-03-02T17:52:59.471118: step 15870, loss 0.238801, acc 0.859375
2017-03-02T17:52:59.535098: step 15871, loss 0.20992, acc 0.875
2017-03-02T17:52:59.605495: step 15872, loss 0.100972, acc 0.9375
2017-03-02T17:52:59.678961: step 15873, loss 0.227173, acc 0.90625
2017-03-02T17:52:59.757941: step 15874, loss 0.229483, acc 0.90625
2017-03-02T17:52:59.845342: step 15875, loss 0.320618, acc 0.875
2017-03-02T17:52:59.913706: step 15876, loss 0.0935703, acc 1
2017-03-02T17:53:00.001598: step 15877, loss 0.108039, acc 0.96875
2017-03-02T17:53:00.091842: step 15878, loss 0.180452, acc 0.90625
2017-03-02T17:53:00.178144: step 15879, loss 0.129079, acc 0.9375
2017-03-02T17:53:00.258053: step 15880, loss 0.127923, acc 0.953125
2017-03-02T17:53:00.336290: step 15881, loss 0.118795, acc 0.953125
2017-03-02T17:53:00.408838: step 15882, loss 0.250453, acc 0.875
2017-03-02T17:53:00.487841: step 15883, loss 0.136612, acc 0.90625
2017-03-02T17:53:00.568165: step 15884, loss 0.118191, acc 0.96875
2017-03-02T17:53:00.637293: step 15885, loss 0.214607, acc 0.9375
2017-03-02T17:53:00.711635: step 15886, loss 0.118709, acc 0.9375
2017-03-02T17:53:00.785111: step 15887, loss 0.118134, acc 0.9375
2017-03-02T17:53:00.860750: step 15888, loss 0.0829388, acc 0.96875
2017-03-02T17:53:00.946257: step 15889, loss 0.093649, acc 0.953125
2017-03-02T17:53:01.018478: step 15890, loss 0.178578, acc 0.90625
2017-03-02T17:53:01.089003: step 15891, loss 0.216645, acc 0.9375
2017-03-02T17:53:01.187210: step 15892, loss 0.10939, acc 0.953125
2017-03-02T17:53:01.254663: step 15893, loss 0.0457886, acc 0.984375
2017-03-02T17:53:01.322377: step 15894, loss 0.154325, acc 0.90625
2017-03-02T17:53:01.401298: step 15895, loss 0.21909, acc 0.921875
2017-03-02T17:53:01.482850: step 15896, loss 0.145364, acc 0.921875
2017-03-02T17:53:01.558207: step 15897, loss 0.0999411, acc 0.96875
2017-03-02T17:53:01.635961: step 15898, loss 0.181433, acc 0.921875
2017-03-02T17:53:01.714884: step 15899, loss 0.115717, acc 0.96875
2017-03-02T17:53:01.787099: step 15900, loss 0.16067, acc 0.90625

Evaluation:
2017-03-02T17:53:01.818728: step 15900, loss 1.96817, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-15900

2017-03-02T17:53:02.332084: step 15901, loss 0.282453, acc 0.9375
2017-03-02T17:53:02.408831: step 15902, loss 0.200072, acc 0.90625
2017-03-02T17:53:02.480615: step 15903, loss 0.166654, acc 0.921875
2017-03-02T17:53:02.562383: step 15904, loss 0.12624, acc 0.953125
2017-03-02T17:53:02.632149: step 15905, loss 0.154275, acc 0.953125
2017-03-02T17:53:02.710476: step 15906, loss 0.164263, acc 0.9375
2017-03-02T17:53:02.788573: step 15907, loss 0.115213, acc 0.9375
2017-03-02T17:53:02.865153: step 15908, loss 0.173562, acc 0.9375
2017-03-02T17:53:02.941248: step 15909, loss 0.217097, acc 0.890625
2017-03-02T17:53:03.016447: step 15910, loss 0.191911, acc 0.859375
2017-03-02T17:53:03.090643: step 15911, loss 0.217251, acc 0.921875
2017-03-02T17:53:03.171808: step 15912, loss 0.102149, acc 0.9375
2017-03-02T17:53:03.246541: step 15913, loss 0.101929, acc 0.9375
2017-03-02T17:53:03.317995: step 15914, loss 0.216317, acc 0.875
2017-03-02T17:53:03.387766: step 15915, loss 0.109091, acc 0.953125
2017-03-02T17:53:03.458639: step 15916, loss 0.141826, acc 0.953125
2017-03-02T17:53:03.534367: step 15917, loss 0.0893529, acc 0.96875
2017-03-02T17:53:03.613647: step 15918, loss 0.10971, acc 0.9375
2017-03-02T17:53:03.689580: step 15919, loss 0.188407, acc 0.890625
2017-03-02T17:53:03.764901: step 15920, loss 0.111332, acc 0.953125
2017-03-02T17:53:03.836667: step 15921, loss 0.101693, acc 0.9375
2017-03-02T17:53:03.900691: step 15922, loss 0.204452, acc 0.90625
2017-03-02T17:53:03.973796: step 15923, loss 0.241485, acc 0.890625
2017-03-02T17:53:04.045971: step 15924, loss 0.16303, acc 0.890625
2017-03-02T17:53:04.116792: step 15925, loss 0.24542, acc 0.921875
2017-03-02T17:53:04.193574: step 15926, loss 0.105424, acc 0.9375
2017-03-02T17:53:04.264907: step 15927, loss 0.15645, acc 0.921875
2017-03-02T17:53:04.346021: step 15928, loss 0.116812, acc 0.953125
2017-03-02T17:53:04.421165: step 15929, loss 0.185246, acc 0.90625
2017-03-02T17:53:04.490739: step 15930, loss 0.1003, acc 0.96875
2017-03-02T17:53:04.555090: step 15931, loss 0.180855, acc 0.921875
2017-03-02T17:53:04.626995: step 15932, loss 0.226907, acc 0.875
2017-03-02T17:53:04.693717: step 15933, loss 0.171102, acc 0.9375
2017-03-02T17:53:04.767866: step 15934, loss 0.112805, acc 0.953125
2017-03-02T17:53:04.853437: step 15935, loss 0.0752052, acc 0.96875
2017-03-02T17:53:04.926213: step 15936, loss 0.166915, acc 0.9375
2017-03-02T17:53:04.996334: step 15937, loss 0.172302, acc 0.953125
2017-03-02T17:53:05.067525: step 15938, loss 0.0991856, acc 0.96875
2017-03-02T17:53:05.141200: step 15939, loss 0.177335, acc 0.9375
2017-03-02T17:53:05.213475: step 15940, loss 0.143075, acc 0.921875
2017-03-02T17:53:05.290461: step 15941, loss 0.112804, acc 0.953125
2017-03-02T17:53:05.401738: step 15942, loss 0.0971136, acc 0.96875
2017-03-02T17:53:05.466648: step 15943, loss 0.176265, acc 0.90625
2017-03-02T17:53:05.552218: step 15944, loss 0.0997207, acc 0.984375
2017-03-02T17:53:05.621848: step 15945, loss 0.103556, acc 0.953125
2017-03-02T17:53:05.692477: step 15946, loss 0.161196, acc 0.9375
2017-03-02T17:53:05.767025: step 15947, loss 0.237241, acc 0.90625
2017-03-02T17:53:05.836132: step 15948, loss 0.155132, acc 0.9375
2017-03-02T17:53:05.909173: step 15949, loss 0.244301, acc 0.875
2017-03-02T17:53:05.981498: step 15950, loss 0.211708, acc 0.9375
2017-03-02T17:53:06.057014: step 15951, loss 0.256066, acc 0.90625
2017-03-02T17:53:06.125864: step 15952, loss 0.154905, acc 0.9375
2017-03-02T17:53:06.201637: step 15953, loss 0.0969543, acc 0.96875
2017-03-02T17:53:06.278506: step 15954, loss 0.126729, acc 0.953125
2017-03-02T17:53:06.359194: step 15955, loss 0.232433, acc 0.890625
2017-03-02T17:53:06.435921: step 15956, loss 0.223082, acc 0.875
2017-03-02T17:53:06.509193: step 15957, loss 0.170056, acc 0.890625
2017-03-02T17:53:06.581622: step 15958, loss 0.160443, acc 0.9375
2017-03-02T17:53:06.658339: step 15959, loss 0.172046, acc 0.890625
2017-03-02T17:53:06.733333: step 15960, loss 0.181073, acc 0.9375
2017-03-02T17:53:06.800574: step 15961, loss 0.158202, acc 0.9375
2017-03-02T17:53:06.876997: step 15962, loss 0.245357, acc 0.90625
2017-03-02T17:53:06.951466: step 15963, loss 0.151235, acc 0.90625
2017-03-02T17:53:07.023156: step 15964, loss 0.18664, acc 0.890625
2017-03-02T17:53:07.097415: step 15965, loss 0.254379, acc 0.859375
2017-03-02T17:53:07.177756: step 15966, loss 0.119408, acc 0.953125
2017-03-02T17:53:07.251107: step 15967, loss 0.0925708, acc 0.953125
2017-03-02T17:53:07.325037: step 15968, loss 0.122837, acc 0.96875
2017-03-02T17:53:07.397762: step 15969, loss 0.329399, acc 0.859375
2017-03-02T17:53:07.468762: step 15970, loss 0.0649604, acc 0.984375
2017-03-02T17:53:07.535425: step 15971, loss 0.196966, acc 0.890625
2017-03-02T17:53:07.605767: step 15972, loss 0.106454, acc 0.953125
2017-03-02T17:53:07.680293: step 15973, loss 0.0964745, acc 0.953125
2017-03-02T17:53:07.760834: step 15974, loss 0.132932, acc 0.921875
2017-03-02T17:53:07.830643: step 15975, loss 0.185261, acc 0.953125
2017-03-02T17:53:07.912635: step 15976, loss 0.108434, acc 0.96875
2017-03-02T17:53:07.981446: step 15977, loss 0.142815, acc 0.90625
2017-03-02T17:53:08.055109: step 15978, loss 0.11518, acc 0.9375
2017-03-02T17:53:08.125657: step 15979, loss 0.143912, acc 0.953125
2017-03-02T17:53:08.193796: step 15980, loss 0.161676, acc 0.953125
2017-03-02T17:53:08.267923: step 15981, loss 0.103675, acc 0.96875
2017-03-02T17:53:08.340145: step 15982, loss 0.0527637, acc 0.96875
2017-03-02T17:53:08.421728: step 15983, loss 0.244294, acc 0.890625
2017-03-02T17:53:08.494207: step 15984, loss 0.0696136, acc 0.953125
2017-03-02T17:53:08.575691: step 15985, loss 0.128942, acc 0.921875
2017-03-02T17:53:08.648020: step 15986, loss 0.126114, acc 0.921875
2017-03-02T17:53:08.718004: step 15987, loss 0.116861, acc 0.96875
2017-03-02T17:53:08.787736: step 15988, loss 0.0584949, acc 0.96875
2017-03-02T17:53:08.858314: step 15989, loss 0.188645, acc 0.921875
2017-03-02T17:53:08.931028: step 15990, loss 0.101172, acc 0.96875
2017-03-02T17:53:09.004026: step 15991, loss 0.350228, acc 0.875
2017-03-02T17:53:09.082293: step 15992, loss 0.0870849, acc 0.9375
2017-03-02T17:53:09.151215: step 15993, loss 0.126004, acc 0.953125
2017-03-02T17:53:09.228103: step 15994, loss 0.144674, acc 0.953125
2017-03-02T17:53:09.304511: step 15995, loss 0.141291, acc 0.953125
2017-03-02T17:53:09.377779: step 15996, loss 0.1091, acc 0.953125
2017-03-02T17:53:09.453523: step 15997, loss 0.0617549, acc 0.984375
2017-03-02T17:53:09.523345: step 15998, loss 0.250455, acc 0.890625
2017-03-02T17:53:09.588321: step 15999, loss 0.175495, acc 0.921875
2017-03-02T17:53:09.655831: step 16000, loss 0.163216, acc 0.90625

Evaluation:
2017-03-02T17:53:09.693232: step 16000, loss 1.97304, acc 0.663302

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16000

2017-03-02T17:53:10.154476: step 16001, loss 0.291383, acc 0.890625
2017-03-02T17:53:10.220415: step 16002, loss 0.194606, acc 0.90625
2017-03-02T17:53:10.289933: step 16003, loss 0.250327, acc 0.890625
2017-03-02T17:53:10.363156: step 16004, loss 0.169383, acc 0.90625
2017-03-02T17:53:10.451997: step 16005, loss 0.127966, acc 0.9375
2017-03-02T17:53:10.544078: step 16006, loss 0.206569, acc 0.953125
2017-03-02T17:53:10.625850: step 16007, loss 0.0452665, acc 0.984375
2017-03-02T17:53:10.699574: step 16008, loss 0.0740266, acc 0.9375
2017-03-02T17:53:10.772656: step 16009, loss 0.266124, acc 0.890625
2017-03-02T17:53:10.848427: step 16010, loss 0.246787, acc 0.90625
2017-03-02T17:53:10.917741: step 16011, loss 0.164829, acc 0.890625
2017-03-02T17:53:10.993434: step 16012, loss 0.2113, acc 0.9375
2017-03-02T17:53:11.068990: step 16013, loss 0.244349, acc 0.890625
2017-03-02T17:53:11.146723: step 16014, loss 0.0925945, acc 0.984375
2017-03-02T17:53:11.221239: step 16015, loss 0.158371, acc 0.921875
2017-03-02T17:53:11.294742: step 16016, loss 0.284089, acc 0.90625
2017-03-02T17:53:11.369550: step 16017, loss 0.227717, acc 0.9375
2017-03-02T17:53:11.449977: step 16018, loss 0.130257, acc 0.9375
2017-03-02T17:53:11.523684: step 16019, loss 0.183658, acc 0.9375
2017-03-02T17:53:11.596917: step 16020, loss 0.105516, acc 0.953125
2017-03-02T17:53:11.672081: step 16021, loss 0.149796, acc 0.9375
2017-03-02T17:53:11.745345: step 16022, loss 0.163872, acc 0.9375
2017-03-02T17:53:11.815976: step 16023, loss 0.14438, acc 0.953125
2017-03-02T17:53:11.894656: step 16024, loss 0.200488, acc 0.921875
2017-03-02T17:53:11.969045: step 16025, loss 0.148503, acc 0.96875
2017-03-02T17:53:12.045627: step 16026, loss 0.175411, acc 0.984375
2017-03-02T17:53:12.123418: step 16027, loss 0.266461, acc 0.890625
2017-03-02T17:53:12.199530: step 16028, loss 0.235008, acc 0.84375
2017-03-02T17:53:12.271949: step 16029, loss 0.0840916, acc 0.984375
2017-03-02T17:53:12.340921: step 16030, loss 0.129377, acc 0.96875
2017-03-02T17:53:12.412954: step 16031, loss 0.199072, acc 0.890625
2017-03-02T17:53:12.488932: step 16032, loss 0.160467, acc 0.9375
2017-03-02T17:53:12.567556: step 16033, loss 0.0955652, acc 0.96875
2017-03-02T17:53:12.639516: step 16034, loss 0.107239, acc 0.953125
2017-03-02T17:53:12.715887: step 16035, loss 0.195011, acc 0.921875
2017-03-02T17:53:12.789018: step 16036, loss 0.1361, acc 0.921875
2017-03-02T17:53:12.860881: step 16037, loss 0.283416, acc 0.84375
2017-03-02T17:53:12.929664: step 16038, loss 0.050815, acc 0.984375
2017-03-02T17:53:12.998002: step 16039, loss 0.20614, acc 0.890625
2017-03-02T17:53:13.069878: step 16040, loss 0.1152, acc 0.953125
2017-03-02T17:53:13.143700: step 16041, loss 0.157137, acc 0.9375
2017-03-02T17:53:13.217526: step 16042, loss 0.142446, acc 0.921875
2017-03-02T17:53:13.290724: step 16043, loss 0.0714688, acc 0.96875
2017-03-02T17:53:13.373393: step 16044, loss 0.349421, acc 0.796875
2017-03-02T17:53:13.445807: step 16045, loss 0.126425, acc 0.921875
2017-03-02T17:53:13.518093: step 16046, loss 0.192074, acc 0.90625
2017-03-02T17:53:13.585073: step 16047, loss 0.117437, acc 0.9375
2017-03-02T17:53:13.651409: step 16048, loss 0.151245, acc 0.921875
2017-03-02T17:53:13.721784: step 16049, loss 0.277433, acc 0.859375
2017-03-02T17:53:13.790915: step 16050, loss 0.0836698, acc 0.984375
2017-03-02T17:53:13.860832: step 16051, loss 0.0759161, acc 0.96875
2017-03-02T17:53:13.935406: step 16052, loss 0.164564, acc 0.90625
2017-03-02T17:53:14.015494: step 16053, loss 0.147401, acc 0.921875
2017-03-02T17:53:14.087071: step 16054, loss 0.20955, acc 0.875
2017-03-02T17:53:14.160665: step 16055, loss 0.277189, acc 0.90625
2017-03-02T17:53:14.245574: step 16056, loss 0.130637, acc 0.9375
2017-03-02T17:53:14.315959: step 16057, loss 0.149322, acc 0.953125
2017-03-02T17:53:14.379163: step 16058, loss 0.0915439, acc 0.96875
2017-03-02T17:53:14.449482: step 16059, loss 0.0925145, acc 0.953125
2017-03-02T17:53:14.524026: step 16060, loss 0.243094, acc 0.890625
2017-03-02T17:53:14.598903: step 16061, loss 0.227937, acc 0.921875
2017-03-02T17:53:14.675737: step 16062, loss 0.132556, acc 0.9375
2017-03-02T17:53:14.775780: step 16063, loss 0.209785, acc 0.921875
2017-03-02T17:53:14.852024: step 16064, loss 0.199717, acc 0.875
2017-03-02T17:53:14.917804: step 16065, loss 0.227143, acc 0.90625
2017-03-02T17:53:14.993155: step 16066, loss 0.205693, acc 0.890625
2017-03-02T17:53:15.066509: step 16067, loss 0.160107, acc 0.9375
2017-03-02T17:53:15.136679: step 16068, loss 0.111715, acc 0.9375
2017-03-02T17:53:15.210749: step 16069, loss 0.186908, acc 0.90625
2017-03-02T17:53:15.296965: step 16070, loss 0.0864186, acc 0.953125
2017-03-02T17:53:15.375184: step 16071, loss 0.193237, acc 0.9375
2017-03-02T17:53:15.447969: step 16072, loss 0.230057, acc 1
2017-03-02T17:53:15.521054: step 16073, loss 0.109754, acc 0.9375
2017-03-02T17:53:15.594899: step 16074, loss 0.173132, acc 0.9375
2017-03-02T17:53:15.682007: step 16075, loss 0.302789, acc 0.84375
2017-03-02T17:53:15.749555: step 16076, loss 0.107028, acc 0.953125
2017-03-02T17:53:15.823660: step 16077, loss 0.111886, acc 0.953125
2017-03-02T17:53:15.895860: step 16078, loss 0.139218, acc 0.953125
2017-03-02T17:53:15.968429: step 16079, loss 0.084374, acc 0.984375
2017-03-02T17:53:16.038920: step 16080, loss 0.139535, acc 0.921875
2017-03-02T17:53:16.115576: step 16081, loss 0.104976, acc 0.96875
2017-03-02T17:53:16.198124: step 16082, loss 0.15555, acc 0.90625
2017-03-02T17:53:16.273963: step 16083, loss 0.165221, acc 0.90625
2017-03-02T17:53:16.354708: step 16084, loss 0.0708541, acc 0.96875
2017-03-02T17:53:16.429058: step 16085, loss 0.196848, acc 0.890625
2017-03-02T17:53:16.496637: step 16086, loss 0.238208, acc 0.90625
2017-03-02T17:53:16.579299: step 16087, loss 0.108728, acc 0.953125
2017-03-02T17:53:16.655657: step 16088, loss 0.171909, acc 0.9375
2017-03-02T17:53:16.726378: step 16089, loss 0.149773, acc 0.953125
2017-03-02T17:53:16.806605: step 16090, loss 0.12141, acc 0.921875
2017-03-02T17:53:16.879225: step 16091, loss 0.163846, acc 0.9375
2017-03-02T17:53:16.950054: step 16092, loss 0.162556, acc 0.9375
2017-03-02T17:53:17.023107: step 16093, loss 0.16827, acc 0.921875
2017-03-02T17:53:17.095248: step 16094, loss 0.073918, acc 0.984375
2017-03-02T17:53:17.168824: step 16095, loss 0.102584, acc 0.953125
2017-03-02T17:53:17.239047: step 16096, loss 0.124513, acc 0.96875
2017-03-02T17:53:17.311159: step 16097, loss 0.163278, acc 0.921875
2017-03-02T17:53:17.384097: step 16098, loss 0.100976, acc 0.96875
2017-03-02T17:53:17.454939: step 16099, loss 0.240066, acc 0.90625
2017-03-02T17:53:17.527264: step 16100, loss 0.182803, acc 0.890625

Evaluation:
2017-03-02T17:53:17.561048: step 16100, loss 1.95275, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16100

2017-03-02T17:53:18.025015: step 16101, loss 0.0984684, acc 0.96875
2017-03-02T17:53:18.108779: step 16102, loss 0.0686851, acc 0.96875
2017-03-02T17:53:18.184126: step 16103, loss 0.20888, acc 0.9375
2017-03-02T17:53:18.257298: step 16104, loss 0.210322, acc 0.9375
2017-03-02T17:53:18.329821: step 16105, loss 0.198335, acc 0.90625
2017-03-02T17:53:18.405128: step 16106, loss 0.188871, acc 0.921875
2017-03-02T17:53:18.463839: step 16107, loss 0.225411, acc 0.90625
2017-03-02T17:53:18.533496: step 16108, loss 0.219554, acc 0.890625
2017-03-02T17:53:18.604398: step 16109, loss 0.1601, acc 0.921875
2017-03-02T17:53:18.677629: step 16110, loss 0.12633, acc 0.953125
2017-03-02T17:53:18.749869: step 16111, loss 0.0699059, acc 0.984375
2017-03-02T17:53:18.822247: step 16112, loss 0.0682567, acc 0.984375
2017-03-02T17:53:18.896893: step 16113, loss 0.173327, acc 0.90625
2017-03-02T17:53:18.956976: step 16114, loss 0.14617, acc 0.9375
2017-03-02T17:53:19.045343: step 16115, loss 0.112411, acc 0.984375
2017-03-02T17:53:19.115161: step 16116, loss 0.0565449, acc 0.984375
2017-03-02T17:53:19.182007: step 16117, loss 0.186334, acc 0.9375
2017-03-02T17:53:19.250223: step 16118, loss 0.260861, acc 0.890625
2017-03-02T17:53:19.324887: step 16119, loss 0.152428, acc 0.921875
2017-03-02T17:53:19.396093: step 16120, loss 0.0701861, acc 0.96875
2017-03-02T17:53:19.468119: step 16121, loss 0.16249, acc 0.90625
2017-03-02T17:53:19.539690: step 16122, loss 0.139262, acc 0.921875
2017-03-02T17:53:19.616631: step 16123, loss 0.196633, acc 0.9375
2017-03-02T17:53:19.676889: step 16124, loss 0.119076, acc 0.9375
2017-03-02T17:53:19.751996: step 16125, loss 0.130138, acc 0.953125
2017-03-02T17:53:19.823844: step 16126, loss 0.144091, acc 0.921875
2017-03-02T17:53:19.901971: step 16127, loss 0.169889, acc 0.921875
2017-03-02T17:53:19.972553: step 16128, loss 0.391077, acc 0.90625
2017-03-02T17:53:20.069607: step 16129, loss 0.126716, acc 0.9375
2017-03-02T17:53:20.141398: step 16130, loss 0.152547, acc 0.90625
2017-03-02T17:53:20.212385: step 16131, loss 0.164851, acc 0.953125
2017-03-02T17:53:20.289514: step 16132, loss 0.136034, acc 0.953125
2017-03-02T17:53:20.365626: step 16133, loss 0.300658, acc 0.890625
2017-03-02T17:53:20.441395: step 16134, loss 0.131653, acc 0.9375
2017-03-02T17:53:20.515784: step 16135, loss 0.131244, acc 0.953125
2017-03-02T17:53:20.585446: step 16136, loss 0.140449, acc 0.96875
2017-03-02T17:53:20.665783: step 16137, loss 0.0569267, acc 0.984375
2017-03-02T17:53:20.746069: step 16138, loss 0.109177, acc 0.9375
2017-03-02T17:53:20.828156: step 16139, loss 0.226309, acc 0.890625
2017-03-02T17:53:20.899584: step 16140, loss 0.107754, acc 0.921875
2017-03-02T17:53:20.974150: step 16141, loss 0.138447, acc 0.921875
2017-03-02T17:53:21.041157: step 16142, loss 0.0657881, acc 0.984375
2017-03-02T17:53:21.118276: step 16143, loss 0.355825, acc 0.859375
2017-03-02T17:53:21.194737: step 16144, loss 0.111124, acc 0.921875
2017-03-02T17:53:21.268246: step 16145, loss 0.0848648, acc 0.953125
2017-03-02T17:53:21.345805: step 16146, loss 0.0789063, acc 0.984375
2017-03-02T17:53:21.415874: step 16147, loss 0.313952, acc 0.875
2017-03-02T17:53:21.494387: step 16148, loss 0.10249, acc 0.953125
2017-03-02T17:53:21.570927: step 16149, loss 0.238335, acc 0.875
2017-03-02T17:53:21.643765: step 16150, loss 0.100865, acc 0.953125
2017-03-02T17:53:21.731840: step 16151, loss 0.163337, acc 0.96875
2017-03-02T17:53:21.809637: step 16152, loss 0.0269901, acc 1
2017-03-02T17:53:21.882652: step 16153, loss 0.236882, acc 0.9375
2017-03-02T17:53:21.960934: step 16154, loss 0.107358, acc 0.953125
2017-03-02T17:53:22.038784: step 16155, loss 0.18665, acc 0.890625
2017-03-02T17:53:22.107876: step 16156, loss 0.129953, acc 0.9375
2017-03-02T17:53:22.180479: step 16157, loss 0.101424, acc 0.921875
2017-03-02T17:53:22.253779: step 16158, loss 0.260423, acc 0.890625
2017-03-02T17:53:22.332869: step 16159, loss 0.213464, acc 0.90625
2017-03-02T17:53:22.404857: step 16160, loss 0.141716, acc 0.9375
2017-03-02T17:53:22.482113: step 16161, loss 0.133619, acc 0.953125
2017-03-02T17:53:22.554327: step 16162, loss 0.141303, acc 0.953125
2017-03-02T17:53:22.628581: step 16163, loss 0.135981, acc 0.953125
2017-03-02T17:53:22.696989: step 16164, loss 0.194371, acc 0.90625
2017-03-02T17:53:22.766809: step 16165, loss 0.142336, acc 0.953125
2017-03-02T17:53:22.839999: step 16166, loss 0.273821, acc 0.90625
2017-03-02T17:53:22.911402: step 16167, loss 0.11129, acc 0.953125
2017-03-02T17:53:22.985971: step 16168, loss 0.136256, acc 0.953125
2017-03-02T17:53:23.053134: step 16169, loss 0.196681, acc 0.90625
2017-03-02T17:53:23.128436: step 16170, loss 0.12879, acc 0.921875
2017-03-02T17:53:23.200344: step 16171, loss 0.277835, acc 0.875
2017-03-02T17:53:23.278540: step 16172, loss 0.20565, acc 0.921875
2017-03-02T17:53:23.344249: step 16173, loss 0.0975183, acc 0.9375
2017-03-02T17:53:23.408651: step 16174, loss 0.080836, acc 0.96875
2017-03-02T17:53:23.477236: step 16175, loss 0.11335, acc 0.953125
2017-03-02T17:53:23.548811: step 16176, loss 0.134351, acc 0.921875
2017-03-02T17:53:23.633215: step 16177, loss 0.184147, acc 0.90625
2017-03-02T17:53:23.710966: step 16178, loss 0.124261, acc 0.9375
2017-03-02T17:53:23.793437: step 16179, loss 0.21373, acc 0.921875
2017-03-02T17:53:23.868646: step 16180, loss 0.160695, acc 0.9375
2017-03-02T17:53:23.941490: step 16181, loss 0.14541, acc 0.921875
2017-03-02T17:53:24.018710: step 16182, loss 0.107021, acc 0.96875
2017-03-02T17:53:24.088594: step 16183, loss 0.118636, acc 0.953125
2017-03-02T17:53:24.154701: step 16184, loss 0.154461, acc 0.921875
2017-03-02T17:53:24.230460: step 16185, loss 0.193722, acc 0.921875
2017-03-02T17:53:24.306220: step 16186, loss 0.202924, acc 0.890625
2017-03-02T17:53:24.380105: step 16187, loss 0.274311, acc 0.90625
2017-03-02T17:53:24.467563: step 16188, loss 0.207575, acc 0.90625
2017-03-02T17:53:24.542538: step 16189, loss 0.0992455, acc 0.96875
2017-03-02T17:53:24.619780: step 16190, loss 0.189396, acc 0.9375
2017-03-02T17:53:24.692188: step 16191, loss 0.151491, acc 0.953125
2017-03-02T17:53:24.769007: step 16192, loss 0.109604, acc 0.96875
2017-03-02T17:53:24.838982: step 16193, loss 0.148914, acc 0.90625
2017-03-02T17:53:24.909337: step 16194, loss 0.139039, acc 0.953125
2017-03-02T17:53:24.978575: step 16195, loss 0.241623, acc 0.90625
2017-03-02T17:53:25.050216: step 16196, loss 0.1704, acc 0.9375
2017-03-02T17:53:25.125154: step 16197, loss 0.22398, acc 0.890625
2017-03-02T17:53:25.210959: step 16198, loss 0.172984, acc 0.90625
2017-03-02T17:53:25.288954: step 16199, loss 0.0923184, acc 0.96875
2017-03-02T17:53:25.367106: step 16200, loss 0.212817, acc 0.90625

Evaluation:
2017-03-02T17:53:25.393031: step 16200, loss 2.00941, acc 0.665465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16200

2017-03-02T17:53:25.889887: step 16201, loss 0.1425, acc 0.984375
2017-03-02T17:53:25.963631: step 16202, loss 0.0629374, acc 0.96875
2017-03-02T17:53:26.045092: step 16203, loss 0.0755456, acc 0.984375
2017-03-02T17:53:26.113530: step 16204, loss 0.137442, acc 0.9375
2017-03-02T17:53:26.186349: step 16205, loss 0.185353, acc 0.859375
2017-03-02T17:53:26.256646: step 16206, loss 0.0263979, acc 0.984375
2017-03-02T17:53:26.333499: step 16207, loss 0.130926, acc 0.9375
2017-03-02T17:53:26.408905: step 16208, loss 0.126344, acc 0.953125
2017-03-02T17:53:26.485219: step 16209, loss 0.174144, acc 0.921875
2017-03-02T17:53:26.559428: step 16210, loss 0.173591, acc 0.921875
2017-03-02T17:53:26.633279: step 16211, loss 0.156402, acc 0.921875
2017-03-02T17:53:26.706149: step 16212, loss 0.0907259, acc 0.953125
2017-03-02T17:53:26.783204: step 16213, loss 0.160751, acc 0.921875
2017-03-02T17:53:26.863419: step 16214, loss 0.132653, acc 0.90625
2017-03-02T17:53:26.934877: step 16215, loss 0.108263, acc 0.953125
2017-03-02T17:53:27.008865: step 16216, loss 0.239451, acc 0.9375
2017-03-02T17:53:27.078986: step 16217, loss 0.183754, acc 0.9375
2017-03-02T17:53:27.149916: step 16218, loss 0.165019, acc 0.90625
2017-03-02T17:53:27.231831: step 16219, loss 0.121801, acc 0.96875
2017-03-02T17:53:27.311880: step 16220, loss 0.266107, acc 0.890625
2017-03-02T17:53:27.386348: step 16221, loss 0.174107, acc 0.921875
2017-03-02T17:53:27.459550: step 16222, loss 0.110069, acc 0.953125
2017-03-02T17:53:27.536012: step 16223, loss 0.134604, acc 0.953125
2017-03-02T17:53:27.606383: step 16224, loss 0.112943, acc 0.9375
2017-03-02T17:53:27.677612: step 16225, loss 0.195286, acc 0.921875
2017-03-02T17:53:27.750196: step 16226, loss 0.229604, acc 0.90625
2017-03-02T17:53:27.825810: step 16227, loss 0.095409, acc 0.96875
2017-03-02T17:53:27.908063: step 16228, loss 0.168431, acc 0.953125
2017-03-02T17:53:27.978700: step 16229, loss 0.151232, acc 0.96875
2017-03-02T17:53:28.053952: step 16230, loss 0.15724, acc 0.921875
2017-03-02T17:53:28.135535: step 16231, loss 0.182407, acc 0.90625
2017-03-02T17:53:28.208810: step 16232, loss 0.16175, acc 0.9375
2017-03-02T17:53:28.282784: step 16233, loss 0.119318, acc 0.90625
2017-03-02T17:53:28.350379: step 16234, loss 0.191775, acc 0.953125
2017-03-02T17:53:28.422447: step 16235, loss 0.322271, acc 0.90625
2017-03-02T17:53:28.505239: step 16236, loss 0.12504, acc 0.953125
2017-03-02T17:53:28.576229: step 16237, loss 0.182412, acc 0.9375
2017-03-02T17:53:28.652901: step 16238, loss 0.190737, acc 0.921875
2017-03-02T17:53:28.734841: step 16239, loss 0.049166, acc 0.984375
2017-03-02T17:53:28.810889: step 16240, loss 0.158011, acc 0.921875
2017-03-02T17:53:28.879029: step 16241, loss 0.18603, acc 0.90625
2017-03-02T17:53:28.956499: step 16242, loss 0.109496, acc 0.9375
2017-03-02T17:53:29.025379: step 16243, loss 0.145226, acc 0.921875
2017-03-02T17:53:29.097087: step 16244, loss 0.0917769, acc 0.953125
2017-03-02T17:53:29.165391: step 16245, loss 0.166519, acc 0.9375
2017-03-02T17:53:29.238128: step 16246, loss 0.199368, acc 0.921875
2017-03-02T17:53:29.311513: step 16247, loss 0.0582933, acc 0.984375
2017-03-02T17:53:29.379001: step 16248, loss 0.298946, acc 0.859375
2017-03-02T17:53:29.459025: step 16249, loss 0.182252, acc 0.890625
2017-03-02T17:53:29.528584: step 16250, loss 0.166561, acc 0.9375
2017-03-02T17:53:29.594009: step 16251, loss 0.0958963, acc 0.9375
2017-03-02T17:53:29.664076: step 16252, loss 0.116984, acc 0.953125
2017-03-02T17:53:29.731354: step 16253, loss 0.233729, acc 0.875
2017-03-02T17:53:29.802918: step 16254, loss 0.270178, acc 0.90625
2017-03-02T17:53:29.883344: step 16255, loss 0.199568, acc 0.890625
2017-03-02T17:53:29.965446: step 16256, loss 0.119474, acc 0.953125
2017-03-02T17:53:30.054304: step 16257, loss 0.187629, acc 0.9375
2017-03-02T17:53:30.129226: step 16258, loss 0.227908, acc 0.921875
2017-03-02T17:53:30.199813: step 16259, loss 0.223537, acc 0.875
2017-03-02T17:53:30.272796: step 16260, loss 0.165105, acc 0.9375
2017-03-02T17:53:30.344383: step 16261, loss 0.402759, acc 0.875
2017-03-02T17:53:30.418209: step 16262, loss 0.202003, acc 0.921875
2017-03-02T17:53:30.490907: step 16263, loss 0.21054, acc 0.921875
2017-03-02T17:53:30.563220: step 16264, loss 0.0790989, acc 0.96875
2017-03-02T17:53:30.650298: step 16265, loss 0.170691, acc 0.90625
2017-03-02T17:53:30.723399: step 16266, loss 0.293633, acc 0.90625
2017-03-02T17:53:30.801465: step 16267, loss 0.166593, acc 0.953125
2017-03-02T17:53:30.872112: step 16268, loss 0.0107421, acc 1
2017-03-02T17:53:30.950041: step 16269, loss 0.0396872, acc 0.984375
2017-03-02T17:53:31.023738: step 16270, loss 0.108366, acc 0.953125
2017-03-02T17:53:31.092005: step 16271, loss 0.118736, acc 0.953125
2017-03-02T17:53:31.164603: step 16272, loss 0.195751, acc 0.921875
2017-03-02T17:53:31.233524: step 16273, loss 0.239265, acc 0.9375
2017-03-02T17:53:31.311105: step 16274, loss 0.20448, acc 0.890625
2017-03-02T17:53:31.387104: step 16275, loss 0.163237, acc 0.921875
2017-03-02T17:53:31.464610: step 16276, loss 0.083692, acc 0.984375
2017-03-02T17:53:31.537361: step 16277, loss 0.126231, acc 0.953125
2017-03-02T17:53:31.611267: step 16278, loss 0.195538, acc 0.90625
2017-03-02T17:53:31.685721: step 16279, loss 0.151464, acc 0.953125
2017-03-02T17:53:31.760086: step 16280, loss 0.142587, acc 0.96875
2017-03-02T17:53:31.829255: step 16281, loss 0.223334, acc 0.90625
2017-03-02T17:53:31.902526: step 16282, loss 0.196418, acc 0.875
2017-03-02T17:53:31.972779: step 16283, loss 0.189518, acc 0.90625
2017-03-02T17:53:32.040753: step 16284, loss 0.147718, acc 0.90625
2017-03-02T17:53:32.115412: step 16285, loss 0.118887, acc 0.953125
2017-03-02T17:53:32.214806: step 16286, loss 0.123898, acc 0.921875
2017-03-02T17:53:32.285329: step 16287, loss 0.110054, acc 0.953125
2017-03-02T17:53:32.361104: step 16288, loss 0.124319, acc 0.9375
2017-03-02T17:53:32.433374: step 16289, loss 0.103761, acc 0.9375
2017-03-02T17:53:32.491437: step 16290, loss 0.360466, acc 0.9375
2017-03-02T17:53:32.559846: step 16291, loss 0.101054, acc 0.96875
2017-03-02T17:53:32.631267: step 16292, loss 0.0928004, acc 0.96875
2017-03-02T17:53:32.703123: step 16293, loss 0.182827, acc 0.953125
2017-03-02T17:53:32.782179: step 16294, loss 0.148484, acc 0.984375
2017-03-02T17:53:32.852402: step 16295, loss 0.135603, acc 0.921875
2017-03-02T17:53:32.933087: step 16296, loss 0.0886584, acc 0.96875
2017-03-02T17:53:33.003951: step 16297, loss 0.146883, acc 0.921875
2017-03-02T17:53:33.074529: step 16298, loss 0.0528333, acc 0.984375
2017-03-02T17:53:33.153233: step 16299, loss 0.200629, acc 0.9375
2017-03-02T17:53:33.219617: step 16300, loss 0.0535821, acc 1

Evaluation:
2017-03-02T17:53:33.255843: step 16300, loss 1.99322, acc 0.660418

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16300

2017-03-02T17:53:33.724517: step 16301, loss 0.140216, acc 0.9375
2017-03-02T17:53:33.802016: step 16302, loss 0.13316, acc 0.9375
2017-03-02T17:53:33.873411: step 16303, loss 0.193651, acc 0.921875
2017-03-02T17:53:33.942258: step 16304, loss 0.160255, acc 0.921875
2017-03-02T17:53:34.013461: step 16305, loss 0.070865, acc 0.984375
2017-03-02T17:53:34.082154: step 16306, loss 0.0375787, acc 0.984375
2017-03-02T17:53:34.156032: step 16307, loss 0.157209, acc 0.953125
2017-03-02T17:53:34.229230: step 16308, loss 0.233672, acc 0.875
2017-03-02T17:53:34.310220: step 16309, loss 0.137836, acc 0.96875
2017-03-02T17:53:34.377262: step 16310, loss 0.0645185, acc 0.96875
2017-03-02T17:53:34.450793: step 16311, loss 0.153916, acc 0.921875
2017-03-02T17:53:34.516438: step 16312, loss 0.131993, acc 0.96875
2017-03-02T17:53:34.588727: step 16313, loss 0.132904, acc 0.921875
2017-03-02T17:53:34.676127: step 16314, loss 0.227794, acc 0.890625
2017-03-02T17:53:34.760074: step 16315, loss 0.100198, acc 0.96875
2017-03-02T17:53:34.838132: step 16316, loss 0.232752, acc 0.875
2017-03-02T17:53:34.924646: step 16317, loss 0.109831, acc 0.953125
2017-03-02T17:53:35.005184: step 16318, loss 0.140934, acc 0.953125
2017-03-02T17:53:35.074695: step 16319, loss 0.171687, acc 0.9375
2017-03-02T17:53:35.150833: step 16320, loss 0.123149, acc 0.90625
2017-03-02T17:53:35.216620: step 16321, loss 0.200248, acc 0.921875
2017-03-02T17:53:35.284558: step 16322, loss 0.176026, acc 0.890625
2017-03-02T17:53:35.359962: step 16323, loss 0.0514845, acc 0.984375
2017-03-02T17:53:35.437442: step 16324, loss 0.227013, acc 0.90625
2017-03-02T17:53:35.505964: step 16325, loss 0.223207, acc 0.890625
2017-03-02T17:53:35.588299: step 16326, loss 0.119687, acc 0.953125
2017-03-02T17:53:35.654272: step 16327, loss 0.181827, acc 0.921875
2017-03-02T17:53:35.726212: step 16328, loss 0.0841892, acc 0.953125
2017-03-02T17:53:35.815469: step 16329, loss 0.13659, acc 0.953125
2017-03-02T17:53:35.885716: step 16330, loss 0.0565759, acc 0.984375
2017-03-02T17:53:35.950228: step 16331, loss 0.212183, acc 0.90625
2017-03-02T17:53:36.023030: step 16332, loss 0.0753919, acc 0.96875
2017-03-02T17:53:36.100804: step 16333, loss 0.225602, acc 0.921875
2017-03-02T17:53:36.175556: step 16334, loss 0.200885, acc 0.921875
2017-03-02T17:53:36.248021: step 16335, loss 0.142406, acc 0.9375
2017-03-02T17:53:36.321119: step 16336, loss 0.0874219, acc 0.953125
2017-03-02T17:53:36.392910: step 16337, loss 0.175729, acc 0.90625
2017-03-02T17:53:36.466289: step 16338, loss 0.140042, acc 0.9375
2017-03-02T17:53:36.546983: step 16339, loss 0.0945502, acc 0.953125
2017-03-02T17:53:36.629104: step 16340, loss 0.134716, acc 0.921875
2017-03-02T17:53:36.705783: step 16341, loss 0.158183, acc 0.90625
2017-03-02T17:53:36.789685: step 16342, loss 0.115831, acc 0.9375
2017-03-02T17:53:36.860687: step 16343, loss 0.159714, acc 0.953125
2017-03-02T17:53:36.938398: step 16344, loss 0.193978, acc 0.953125
2017-03-02T17:53:37.011816: step 16345, loss 0.100875, acc 0.953125
2017-03-02T17:53:37.087081: step 16346, loss 0.225386, acc 0.875
2017-03-02T17:53:37.166065: step 16347, loss 0.141363, acc 0.953125
2017-03-02T17:53:37.251614: step 16348, loss 0.265375, acc 0.890625
2017-03-02T17:53:37.324448: step 16349, loss 0.131293, acc 0.9375
2017-03-02T17:53:37.399027: step 16350, loss 0.279, acc 0.890625
2017-03-02T17:53:37.473354: step 16351, loss 0.149067, acc 0.96875
2017-03-02T17:53:37.559329: step 16352, loss 0.209646, acc 0.90625
2017-03-02T17:53:37.635626: step 16353, loss 0.0665504, acc 0.96875
2017-03-02T17:53:37.709675: step 16354, loss 0.187404, acc 0.890625
2017-03-02T17:53:37.783521: step 16355, loss 0.198203, acc 0.90625
2017-03-02T17:53:37.854430: step 16356, loss 0.157748, acc 0.921875
2017-03-02T17:53:37.932173: step 16357, loss 0.154059, acc 0.953125
2017-03-02T17:53:38.000599: step 16358, loss 0.144715, acc 0.921875
2017-03-02T17:53:38.070102: step 16359, loss 0.0946438, acc 0.9375
2017-03-02T17:53:38.145981: step 16360, loss 0.183256, acc 0.921875
2017-03-02T17:53:38.220670: step 16361, loss 0.156981, acc 0.953125
2017-03-02T17:53:38.314797: step 16362, loss 0.252495, acc 0.890625
2017-03-02T17:53:38.393330: step 16363, loss 0.157111, acc 0.921875
2017-03-02T17:53:38.464976: step 16364, loss 0.198725, acc 0.953125
2017-03-02T17:53:38.535667: step 16365, loss 0.07323, acc 0.984375
2017-03-02T17:53:38.615781: step 16366, loss 0.118623, acc 0.9375
2017-03-02T17:53:38.683785: step 16367, loss 0.164065, acc 0.9375
2017-03-02T17:53:38.748170: step 16368, loss 0.225912, acc 0.90625
2017-03-02T17:53:38.822078: step 16369, loss 0.0632376, acc 0.96875
2017-03-02T17:53:38.920375: step 16370, loss 0.232413, acc 0.890625
2017-03-02T17:53:38.998699: step 16371, loss 0.259843, acc 0.953125
2017-03-02T17:53:39.074234: step 16372, loss 0.0817852, acc 0.96875
2017-03-02T17:53:39.148309: step 16373, loss 0.0996085, acc 0.9375
2017-03-02T17:53:39.221143: step 16374, loss 0.13759, acc 0.9375
2017-03-02T17:53:39.294604: step 16375, loss 0.139616, acc 0.9375
2017-03-02T17:53:39.376782: step 16376, loss 0.247854, acc 0.875
2017-03-02T17:53:39.446895: step 16377, loss 0.170258, acc 0.90625
2017-03-02T17:53:39.518908: step 16378, loss 0.244612, acc 0.875
2017-03-02T17:53:39.600958: step 16379, loss 0.170774, acc 0.9375
2017-03-02T17:53:39.676655: step 16380, loss 0.148073, acc 0.90625
2017-03-02T17:53:39.749320: step 16381, loss 0.191693, acc 0.921875
2017-03-02T17:53:39.823863: step 16382, loss 0.191586, acc 0.90625
2017-03-02T17:53:39.891647: step 16383, loss 0.153795, acc 0.921875
2017-03-02T17:53:39.965615: step 16384, loss 0.130162, acc 0.953125
2017-03-02T17:53:40.036771: step 16385, loss 0.156262, acc 0.90625
2017-03-02T17:53:40.103872: step 16386, loss 0.125289, acc 0.9375
2017-03-02T17:53:40.169931: step 16387, loss 0.126011, acc 0.953125
2017-03-02T17:53:40.243951: step 16388, loss 0.157897, acc 0.921875
2017-03-02T17:53:40.313333: step 16389, loss 0.0965717, acc 0.96875
2017-03-02T17:53:40.388307: step 16390, loss 0.210084, acc 0.90625
2017-03-02T17:53:40.458326: step 16391, loss 0.227385, acc 0.953125
2017-03-02T17:53:40.530534: step 16392, loss 0.284382, acc 0.890625
2017-03-02T17:53:40.606217: step 16393, loss 0.190687, acc 0.90625
2017-03-02T17:53:40.686790: step 16394, loss 0.18125, acc 0.9375
2017-03-02T17:53:40.761966: step 16395, loss 0.134308, acc 0.90625
2017-03-02T17:53:40.828993: step 16396, loss 0.232109, acc 0.890625
2017-03-02T17:53:40.905101: step 16397, loss 0.193783, acc 0.90625
2017-03-02T17:53:40.978198: step 16398, loss 0.0740106, acc 0.984375
2017-03-02T17:53:41.049030: step 16399, loss 0.251817, acc 0.859375
2017-03-02T17:53:41.124821: step 16400, loss 0.139147, acc 0.921875

Evaluation:
2017-03-02T17:53:41.158525: step 16400, loss 1.9543, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16400

2017-03-02T17:53:41.613408: step 16401, loss 0.14161, acc 0.953125
2017-03-02T17:53:41.690553: step 16402, loss 0.133688, acc 0.96875
2017-03-02T17:53:41.765286: step 16403, loss 0.150657, acc 0.90625
2017-03-02T17:53:41.843322: step 16404, loss 0.178016, acc 0.921875
2017-03-02T17:53:41.919375: step 16405, loss 0.0947622, acc 0.96875
2017-03-02T17:53:41.993812: step 16406, loss 0.176916, acc 0.921875
2017-03-02T17:53:42.073898: step 16407, loss 0.365482, acc 0.84375
2017-03-02T17:53:42.145058: step 16408, loss 0.130289, acc 0.96875
2017-03-02T17:53:42.211265: step 16409, loss 0.272328, acc 0.8125
2017-03-02T17:53:42.286720: step 16410, loss 0.239475, acc 0.921875
2017-03-02T17:53:42.367149: step 16411, loss 0.262264, acc 0.90625
2017-03-02T17:53:42.447576: step 16412, loss 0.13735, acc 0.96875
2017-03-02T17:53:42.521953: step 16413, loss 0.0596273, acc 0.984375
2017-03-02T17:53:42.597044: step 16414, loss 0.178621, acc 0.90625
2017-03-02T17:53:42.669677: step 16415, loss 0.16988, acc 0.921875
2017-03-02T17:53:42.741541: step 16416, loss 0.105784, acc 0.96875
2017-03-02T17:53:42.816615: step 16417, loss 0.177007, acc 0.9375
2017-03-02T17:53:42.888426: step 16418, loss 0.199056, acc 0.890625
2017-03-02T17:53:42.957146: step 16419, loss 0.145375, acc 0.953125
2017-03-02T17:53:43.033903: step 16420, loss 0.0977291, acc 0.984375
2017-03-02T17:53:43.108276: step 16421, loss 0.179718, acc 0.921875
2017-03-02T17:53:43.176771: step 16422, loss 0.236501, acc 0.859375
2017-03-02T17:53:43.250668: step 16423, loss 0.213593, acc 0.890625
2017-03-02T17:53:43.336457: step 16424, loss 0.0982374, acc 0.953125
2017-03-02T17:53:43.408779: step 16425, loss 0.208212, acc 0.90625
2017-03-02T17:53:43.488321: step 16426, loss 0.150928, acc 0.921875
2017-03-02T17:53:43.565542: step 16427, loss 0.102795, acc 0.9375
2017-03-02T17:53:43.632389: step 16428, loss 0.133317, acc 0.921875
2017-03-02T17:53:43.708028: step 16429, loss 0.236899, acc 0.921875
2017-03-02T17:53:43.779592: step 16430, loss 0.115552, acc 0.9375
2017-03-02T17:53:43.858591: step 16431, loss 0.0585734, acc 0.96875
2017-03-02T17:53:43.941177: step 16432, loss 0.0714191, acc 0.96875
2017-03-02T17:53:44.010452: step 16433, loss 0.199429, acc 0.90625
2017-03-02T17:53:44.083510: step 16434, loss 0.1669, acc 0.921875
2017-03-02T17:53:44.154558: step 16435, loss 0.0444901, acc 1
2017-03-02T17:53:44.227081: step 16436, loss 0.17015, acc 0.9375
2017-03-02T17:53:44.296012: step 16437, loss 0.278231, acc 0.859375
2017-03-02T17:53:44.371197: step 16438, loss 0.218772, acc 0.875
2017-03-02T17:53:44.451964: step 16439, loss 0.0970328, acc 0.953125
2017-03-02T17:53:44.529779: step 16440, loss 0.136611, acc 0.9375
2017-03-02T17:53:44.606820: step 16441, loss 0.190904, acc 0.9375
2017-03-02T17:53:44.682791: step 16442, loss 0.140445, acc 0.953125
2017-03-02T17:53:44.752707: step 16443, loss 0.0954482, acc 0.9375
2017-03-02T17:53:44.826727: step 16444, loss 0.143438, acc 0.921875
2017-03-02T17:53:44.900934: step 16445, loss 0.309884, acc 0.875
2017-03-02T17:53:44.969071: step 16446, loss 0.103563, acc 0.953125
2017-03-02T17:53:45.042009: step 16447, loss 0.168838, acc 0.921875
2017-03-02T17:53:45.115433: step 16448, loss 0.284714, acc 0.875
2017-03-02T17:53:45.183446: step 16449, loss 0.157527, acc 0.90625
2017-03-02T17:53:45.255570: step 16450, loss 0.234865, acc 0.90625
2017-03-02T17:53:45.331541: step 16451, loss 0.079271, acc 0.96875
2017-03-02T17:53:45.415107: step 16452, loss 0.123283, acc 0.953125
2017-03-02T17:53:45.490710: step 16453, loss 0.229377, acc 0.921875
2017-03-02T17:53:45.578418: step 16454, loss 0.310714, acc 0.875
2017-03-02T17:53:45.660736: step 16455, loss 0.170095, acc 0.921875
2017-03-02T17:53:45.733814: step 16456, loss 0.226946, acc 0.890625
2017-03-02T17:53:45.807995: step 16457, loss 0.13667, acc 0.9375
2017-03-02T17:53:45.882428: step 16458, loss 0.255755, acc 0.890625
2017-03-02T17:53:45.956789: step 16459, loss 0.247471, acc 0.9375
2017-03-02T17:53:46.034221: step 16460, loss 0.118892, acc 0.9375
2017-03-02T17:53:46.105905: step 16461, loss 0.19908, acc 0.921875
2017-03-02T17:53:46.179162: step 16462, loss 0.10491, acc 0.953125
2017-03-02T17:53:46.254439: step 16463, loss 0.124946, acc 0.9375
2017-03-02T17:53:46.320386: step 16464, loss 3.85617e-05, acc 1
2017-03-02T17:53:46.390772: step 16465, loss 0.119413, acc 0.9375
2017-03-02T17:53:46.472990: step 16466, loss 0.23542, acc 0.90625
2017-03-02T17:53:46.537453: step 16467, loss 0.0924394, acc 0.953125
2017-03-02T17:53:46.618280: step 16468, loss 0.237858, acc 0.890625
2017-03-02T17:53:46.690514: step 16469, loss 0.147774, acc 0.9375
2017-03-02T17:53:46.763729: step 16470, loss 0.0573734, acc 0.96875
2017-03-02T17:53:46.836457: step 16471, loss 0.167014, acc 0.921875
2017-03-02T17:53:46.906803: step 16472, loss 0.0866743, acc 0.96875
2017-03-02T17:53:46.974695: step 16473, loss 0.108282, acc 0.9375
2017-03-02T17:53:47.045274: step 16474, loss 0.115888, acc 0.96875
2017-03-02T17:53:47.118170: step 16475, loss 0.21714, acc 0.859375
2017-03-02T17:53:47.198619: step 16476, loss 0.0435483, acc 0.984375
2017-03-02T17:53:47.271037: step 16477, loss 0.165764, acc 0.953125
2017-03-02T17:53:47.346682: step 16478, loss 0.0806445, acc 0.953125
2017-03-02T17:53:47.424415: step 16479, loss 0.139983, acc 0.9375
2017-03-02T17:53:47.502776: step 16480, loss 0.101116, acc 0.96875
2017-03-02T17:53:47.574191: step 16481, loss 0.149171, acc 0.90625
2017-03-02T17:53:47.650159: step 16482, loss 0.122733, acc 0.953125
2017-03-02T17:53:47.720908: step 16483, loss 0.121608, acc 0.96875
2017-03-02T17:53:47.788362: step 16484, loss 0.0847117, acc 0.96875
2017-03-02T17:53:47.860736: step 16485, loss 0.151468, acc 0.9375
2017-03-02T17:53:47.933062: step 16486, loss 0.275468, acc 0.90625
2017-03-02T17:53:48.005964: step 16487, loss 0.0961664, acc 0.953125
2017-03-02T17:53:48.080140: step 16488, loss 0.204305, acc 0.875
2017-03-02T17:53:48.156479: step 16489, loss 0.142465, acc 0.90625
2017-03-02T17:53:48.236761: step 16490, loss 0.101982, acc 0.953125
2017-03-02T17:53:48.310217: step 16491, loss 0.0610679, acc 0.984375
2017-03-02T17:53:48.384189: step 16492, loss 0.152519, acc 0.921875
2017-03-02T17:53:48.448528: step 16493, loss 0.141994, acc 0.9375
2017-03-02T17:53:48.515013: step 16494, loss 0.209518, acc 0.890625
2017-03-02T17:53:48.589080: step 16495, loss 0.223044, acc 0.90625
2017-03-02T17:53:48.669203: step 16496, loss 0.097008, acc 0.984375
2017-03-02T17:53:48.740188: step 16497, loss 0.128915, acc 0.921875
2017-03-02T17:53:48.810431: step 16498, loss 0.201978, acc 0.921875
2017-03-02T17:53:48.884571: step 16499, loss 0.204238, acc 0.875
2017-03-02T17:53:48.956717: step 16500, loss 0.179534, acc 0.9375

Evaluation:
2017-03-02T17:53:48.983770: step 16500, loss 1.95475, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16500

2017-03-02T17:53:49.443508: step 16501, loss 0.0662939, acc 0.984375
2017-03-02T17:53:49.516126: step 16502, loss 0.082838, acc 0.953125
2017-03-02T17:53:49.585991: step 16503, loss 0.170307, acc 0.921875
2017-03-02T17:53:49.670323: step 16504, loss 0.100556, acc 0.953125
2017-03-02T17:53:49.742967: step 16505, loss 0.189352, acc 0.921875
2017-03-02T17:53:49.814109: step 16506, loss 0.0999683, acc 0.953125
2017-03-02T17:53:49.888379: step 16507, loss 0.142849, acc 0.9375
2017-03-02T17:53:49.964248: step 16508, loss 0.102176, acc 0.9375
2017-03-02T17:53:50.038313: step 16509, loss 0.191192, acc 0.921875
2017-03-02T17:53:50.118983: step 16510, loss 0.145576, acc 0.90625
2017-03-02T17:53:50.193016: step 16511, loss 0.164989, acc 0.921875
2017-03-02T17:53:50.265418: step 16512, loss 0.120768, acc 0.96875
2017-03-02T17:53:50.339013: step 16513, loss 0.219889, acc 0.921875
2017-03-02T17:53:50.417809: step 16514, loss 0.231699, acc 0.90625
2017-03-02T17:53:50.486042: step 16515, loss 0.105831, acc 0.9375
2017-03-02T17:53:50.565237: step 16516, loss 0.168903, acc 0.90625
2017-03-02T17:53:50.637177: step 16517, loss 0.172659, acc 0.921875
2017-03-02T17:53:50.722350: step 16518, loss 0.160112, acc 0.921875
2017-03-02T17:53:50.798898: step 16519, loss 0.13435, acc 0.90625
2017-03-02T17:53:50.874586: step 16520, loss 0.122342, acc 0.953125
2017-03-02T17:53:50.957891: step 16521, loss 0.201597, acc 0.9375
2017-03-02T17:53:51.027350: step 16522, loss 0.159597, acc 0.953125
2017-03-02T17:53:51.105441: step 16523, loss 0.149779, acc 0.90625
2017-03-02T17:53:51.176009: step 16524, loss 0.165011, acc 0.921875
2017-03-02T17:53:51.248038: step 16525, loss 0.168675, acc 0.953125
2017-03-02T17:53:51.317418: step 16526, loss 0.220685, acc 0.890625
2017-03-02T17:53:51.381501: step 16527, loss 0.164838, acc 0.9375
2017-03-02T17:53:51.456037: step 16528, loss 0.119566, acc 0.984375
2017-03-02T17:53:51.532737: step 16529, loss 0.185175, acc 0.921875
2017-03-02T17:53:51.608856: step 16530, loss 0.0806978, acc 0.953125
2017-03-02T17:53:51.678242: step 16531, loss 0.26239, acc 0.921875
2017-03-02T17:53:51.753213: step 16532, loss 0.0984862, acc 0.984375
2017-03-02T17:53:51.828282: step 16533, loss 0.254796, acc 0.890625
2017-03-02T17:53:51.896971: step 16534, loss 0.0921689, acc 0.96875
2017-03-02T17:53:51.965158: step 16535, loss 0.127316, acc 0.9375
2017-03-02T17:53:52.036514: step 16536, loss 0.13838, acc 0.9375
2017-03-02T17:53:52.109392: step 16537, loss 0.155517, acc 0.90625
2017-03-02T17:53:52.189056: step 16538, loss 0.22134, acc 0.90625
2017-03-02T17:53:52.265899: step 16539, loss 0.25464, acc 0.875
2017-03-02T17:53:52.340996: step 16540, loss 0.193762, acc 0.890625
2017-03-02T17:53:52.411603: step 16541, loss 0.0901585, acc 0.953125
2017-03-02T17:53:52.487997: step 16542, loss 0.189542, acc 0.921875
2017-03-02T17:53:52.561834: step 16543, loss 0.203775, acc 0.921875
2017-03-02T17:53:52.637608: step 16544, loss 0.173894, acc 0.9375
2017-03-02T17:53:52.708174: step 16545, loss 0.146271, acc 0.953125
2017-03-02T17:53:52.789036: step 16546, loss 0.131006, acc 0.921875
2017-03-02T17:53:52.858691: step 16547, loss 0.161506, acc 0.9375
2017-03-02T17:53:52.939931: step 16548, loss 0.0601199, acc 0.984375
2017-03-02T17:53:53.043849: step 16549, loss 0.261599, acc 0.90625
2017-03-02T17:53:53.118637: step 16550, loss 0.266575, acc 0.921875
2017-03-02T17:53:53.187924: step 16551, loss 0.265083, acc 0.890625
2017-03-02T17:53:53.260593: step 16552, loss 0.0975247, acc 0.953125
2017-03-02T17:53:53.332937: step 16553, loss 0.124731, acc 0.953125
2017-03-02T17:53:53.406888: step 16554, loss 0.140363, acc 0.953125
2017-03-02T17:53:53.481848: step 16555, loss 0.187573, acc 0.890625
2017-03-02T17:53:53.554577: step 16556, loss 0.332298, acc 0.84375
2017-03-02T17:53:53.629687: step 16557, loss 0.0566278, acc 0.984375
2017-03-02T17:53:53.702449: step 16558, loss 0.0483789, acc 1
2017-03-02T17:53:53.782797: step 16559, loss 0.189314, acc 0.890625
2017-03-02T17:53:53.851624: step 16560, loss 0.11572, acc 0.984375
2017-03-02T17:53:53.927596: step 16561, loss 0.152658, acc 0.9375
2017-03-02T17:53:53.995175: step 16562, loss 0.209618, acc 0.9375
2017-03-02T17:53:54.066159: step 16563, loss 0.393277, acc 0.875
2017-03-02T17:53:54.141851: step 16564, loss 0.212944, acc 0.859375
2017-03-02T17:53:54.219271: step 16565, loss 0.174292, acc 0.90625
2017-03-02T17:53:54.297820: step 16566, loss 0.143187, acc 0.953125
2017-03-02T17:53:54.370972: step 16567, loss 0.0930801, acc 0.96875
2017-03-02T17:53:54.448396: step 16568, loss 0.24045, acc 0.859375
2017-03-02T17:53:54.517182: step 16569, loss 0.182273, acc 0.9375
2017-03-02T17:53:54.590278: step 16570, loss 0.179663, acc 0.90625
2017-03-02T17:53:54.666915: step 16571, loss 0.256156, acc 0.921875
2017-03-02T17:53:54.751034: step 16572, loss 0.0958007, acc 0.9375
2017-03-02T17:53:54.823020: step 16573, loss 0.183918, acc 0.90625
2017-03-02T17:53:54.893891: step 16574, loss 0.242552, acc 0.90625
2017-03-02T17:53:54.983900: step 16575, loss 0.211963, acc 0.921875
2017-03-02T17:53:55.055418: step 16576, loss 0.09026, acc 0.96875
2017-03-02T17:53:55.130300: step 16577, loss 0.156412, acc 0.921875
2017-03-02T17:53:55.206497: step 16578, loss 0.124059, acc 0.921875
2017-03-02T17:53:55.276999: step 16579, loss 0.124677, acc 0.9375
2017-03-02T17:53:55.346255: step 16580, loss 0.210041, acc 0.890625
2017-03-02T17:53:55.420400: step 16581, loss 0.255655, acc 0.84375
2017-03-02T17:53:55.495197: step 16582, loss 0.13534, acc 0.921875
2017-03-02T17:53:55.567356: step 16583, loss 0.348935, acc 0.90625
2017-03-02T17:53:55.639222: step 16584, loss 0.175069, acc 0.9375
2017-03-02T17:53:55.712208: step 16585, loss 0.183154, acc 0.921875
2017-03-02T17:53:55.785060: step 16586, loss 0.117854, acc 0.9375
2017-03-02T17:53:55.855926: step 16587, loss 0.172299, acc 0.890625
2017-03-02T17:53:55.923944: step 16588, loss 0.136743, acc 0.921875
2017-03-02T17:53:55.998796: step 16589, loss 0.152847, acc 0.9375
2017-03-02T17:53:56.068425: step 16590, loss 0.109678, acc 0.953125
2017-03-02T17:53:56.141826: step 16591, loss 0.160853, acc 0.9375
2017-03-02T17:53:56.211926: step 16592, loss 0.116867, acc 0.9375
2017-03-02T17:53:56.284775: step 16593, loss 0.148984, acc 0.953125
2017-03-02T17:53:56.356199: step 16594, loss 0.104317, acc 0.984375
2017-03-02T17:53:56.428395: step 16595, loss 0.10948, acc 0.953125
2017-03-02T17:53:56.500171: step 16596, loss 0.175761, acc 0.9375
2017-03-02T17:53:56.571628: step 16597, loss 0.304748, acc 0.875
2017-03-02T17:53:56.646700: step 16598, loss 0.158029, acc 0.9375
2017-03-02T17:53:56.715483: step 16599, loss 0.162786, acc 0.90625
2017-03-02T17:53:56.785610: step 16600, loss 0.151763, acc 0.9375

Evaluation:
2017-03-02T17:53:56.823186: step 16600, loss 1.95127, acc 0.653208

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16600

2017-03-02T17:53:57.273787: step 16601, loss 0.189034, acc 0.921875
2017-03-02T17:53:57.341533: step 16602, loss 0.192653, acc 0.90625
2017-03-02T17:53:57.407963: step 16603, loss 0.246926, acc 0.890625
2017-03-02T17:53:57.482462: step 16604, loss 0.111104, acc 0.9375
2017-03-02T17:53:57.560900: step 16605, loss 0.144399, acc 0.90625
2017-03-02T17:53:57.633508: step 16606, loss 0.0986825, acc 0.953125
2017-03-02T17:53:57.705408: step 16607, loss 0.130851, acc 0.9375
2017-03-02T17:53:57.782526: step 16608, loss 0.135142, acc 0.953125
2017-03-02T17:53:57.852075: step 16609, loss 0.22202, acc 0.890625
2017-03-02T17:53:57.929685: step 16610, loss 0.112028, acc 0.96875
2017-03-02T17:53:58.000886: step 16611, loss 0.219686, acc 0.890625
2017-03-02T17:53:58.070120: step 16612, loss 0.1146, acc 0.953125
2017-03-02T17:53:58.143731: step 16613, loss 0.138536, acc 0.953125
2017-03-02T17:53:58.209296: step 16614, loss 0.243752, acc 0.875
2017-03-02T17:53:58.281070: step 16615, loss 0.119249, acc 0.953125
2017-03-02T17:53:58.359596: step 16616, loss 0.150681, acc 0.9375
2017-03-02T17:53:58.433043: step 16617, loss 0.139121, acc 0.9375
2017-03-02T17:53:58.504828: step 16618, loss 0.165681, acc 0.90625
2017-03-02T17:53:58.577624: step 16619, loss 0.0729611, acc 0.96875
2017-03-02T17:53:58.656150: step 16620, loss 0.0827826, acc 0.953125
2017-03-02T17:53:58.725762: step 16621, loss 0.0974067, acc 0.953125
2017-03-02T17:53:58.792230: step 16622, loss 0.1387, acc 0.9375
2017-03-02T17:53:58.864591: step 16623, loss 0.171439, acc 0.921875
2017-03-02T17:53:58.954279: step 16624, loss 0.231207, acc 0.90625
2017-03-02T17:53:59.030883: step 16625, loss 0.135831, acc 0.9375
2017-03-02T17:53:59.113267: step 16626, loss 0.165406, acc 0.9375
2017-03-02T17:53:59.189344: step 16627, loss 0.0866333, acc 0.96875
2017-03-02T17:53:59.264224: step 16628, loss 0.0579597, acc 0.96875
2017-03-02T17:53:59.334597: step 16629, loss 0.151543, acc 0.953125
2017-03-02T17:53:59.400448: step 16630, loss 0.111284, acc 0.9375
2017-03-02T17:53:59.466689: step 16631, loss 0.100025, acc 0.96875
2017-03-02T17:53:59.537852: step 16632, loss 0.208438, acc 0.890625
2017-03-02T17:53:59.621361: step 16633, loss 0.10003, acc 0.953125
2017-03-02T17:53:59.693261: step 16634, loss 0.133055, acc 0.96875
2017-03-02T17:53:59.763803: step 16635, loss 0.170183, acc 0.90625
2017-03-02T17:53:59.830059: step 16636, loss 0.0792186, acc 0.984375
2017-03-02T17:53:59.898333: step 16637, loss 0.172566, acc 0.953125
2017-03-02T17:53:59.968608: step 16638, loss 0.177333, acc 0.953125
2017-03-02T17:54:00.042744: step 16639, loss 0.11823, acc 0.96875
2017-03-02T17:54:00.117129: step 16640, loss 0.187741, acc 0.953125
2017-03-02T17:54:00.183344: step 16641, loss 0.206291, acc 0.90625
2017-03-02T17:54:00.256633: step 16642, loss 0.235203, acc 0.90625
2017-03-02T17:54:00.328360: step 16643, loss 0.146695, acc 0.953125
2017-03-02T17:54:00.401819: step 16644, loss 0.0825247, acc 0.96875
2017-03-02T17:54:00.473139: step 16645, loss 0.220426, acc 0.921875
2017-03-02T17:54:00.547890: step 16646, loss 0.178876, acc 0.9375
2017-03-02T17:54:00.631586: step 16647, loss 0.184853, acc 0.90625
2017-03-02T17:54:00.697266: step 16648, loss 0.131584, acc 0.953125
2017-03-02T17:54:00.750260: step 16649, loss 0.275672, acc 0.90625
2017-03-02T17:54:00.819798: step 16650, loss 0.171641, acc 0.921875
2017-03-02T17:54:00.892707: step 16651, loss 0.114695, acc 0.96875
2017-03-02T17:54:00.970575: step 16652, loss 0.164579, acc 0.953125
2017-03-02T17:54:01.061640: step 16653, loss 0.139088, acc 0.9375
2017-03-02T17:54:01.135413: step 16654, loss 0.202944, acc 0.953125
2017-03-02T17:54:01.214324: step 16655, loss 0.0886787, acc 0.96875
2017-03-02T17:54:01.286751: step 16656, loss 0.131915, acc 0.9375
2017-03-02T17:54:01.361284: step 16657, loss 0.197921, acc 0.9375
2017-03-02T17:54:01.427932: step 16658, loss 0.279028, acc 0.875
2017-03-02T17:54:01.495725: step 16659, loss 0.182296, acc 0.90625
2017-03-02T17:54:01.564090: step 16660, loss 0.000161247, acc 1
2017-03-02T17:54:01.640472: step 16661, loss 0.0996412, acc 0.953125
2017-03-02T17:54:01.711498: step 16662, loss 0.188102, acc 0.90625
2017-03-02T17:54:01.785675: step 16663, loss 0.186894, acc 0.90625
2017-03-02T17:54:01.852231: step 16664, loss 0.0537382, acc 0.96875
2017-03-02T17:54:01.926166: step 16665, loss 0.0858409, acc 0.953125
2017-03-02T17:54:01.996347: step 16666, loss 0.17897, acc 0.90625
2017-03-02T17:54:02.085008: step 16667, loss 0.259259, acc 0.859375
2017-03-02T17:54:02.152308: step 16668, loss 0.100875, acc 0.953125
2017-03-02T17:54:02.230182: step 16669, loss 0.117896, acc 0.921875
2017-03-02T17:54:02.306075: step 16670, loss 0.207334, acc 0.953125
2017-03-02T17:54:02.381839: step 16671, loss 0.190309, acc 0.921875
2017-03-02T17:54:02.460175: step 16672, loss 0.247125, acc 0.90625
2017-03-02T17:54:02.534058: step 16673, loss 0.175687, acc 0.921875
2017-03-02T17:54:02.608054: step 16674, loss 0.142757, acc 0.953125
2017-03-02T17:54:02.681563: step 16675, loss 0.206551, acc 0.921875
2017-03-02T17:54:02.753966: step 16676, loss 0.113208, acc 0.9375
2017-03-02T17:54:02.825311: step 16677, loss 0.189472, acc 0.90625
2017-03-02T17:54:02.900539: step 16678, loss 0.0751331, acc 0.984375
2017-03-02T17:54:02.970617: step 16679, loss 0.134489, acc 0.953125
2017-03-02T17:54:03.044751: step 16680, loss 0.126165, acc 0.953125
2017-03-02T17:54:03.122713: step 16681, loss 0.192235, acc 0.921875
2017-03-02T17:54:03.196646: step 16682, loss 0.0833253, acc 0.984375
2017-03-02T17:54:03.274094: step 16683, loss 0.128743, acc 0.96875
2017-03-02T17:54:03.341296: step 16684, loss 0.194862, acc 0.875
2017-03-02T17:54:03.416482: step 16685, loss 0.172688, acc 0.921875
2017-03-02T17:54:03.492538: step 16686, loss 0.0512851, acc 1
2017-03-02T17:54:03.556944: step 16687, loss 0.142359, acc 0.90625
2017-03-02T17:54:03.622521: step 16688, loss 0.0563446, acc 0.984375
2017-03-02T17:54:03.698725: step 16689, loss 0.206795, acc 0.921875
2017-03-02T17:54:03.784325: step 16690, loss 0.152515, acc 0.9375
2017-03-02T17:54:03.857673: step 16691, loss 0.208898, acc 0.90625
2017-03-02T17:54:03.929656: step 16692, loss 0.152531, acc 0.953125
2017-03-02T17:54:04.007810: step 16693, loss 0.179326, acc 0.921875
2017-03-02T17:54:04.091309: step 16694, loss 0.0778374, acc 0.984375
2017-03-02T17:54:04.172633: step 16695, loss 0.101511, acc 0.9375
2017-03-02T17:54:04.242609: step 16696, loss 0.115903, acc 0.953125
2017-03-02T17:54:04.312211: step 16697, loss 0.153678, acc 0.921875
2017-03-02T17:54:04.386579: step 16698, loss 0.0924499, acc 0.953125
2017-03-02T17:54:04.466307: step 16699, loss 0.174249, acc 0.9375
2017-03-02T17:54:04.540488: step 16700, loss 0.109305, acc 0.921875

Evaluation:
2017-03-02T17:54:04.577782: step 16700, loss 2.08235, acc 0.66907

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16700

2017-03-02T17:54:05.077660: step 16701, loss 0.171763, acc 0.9375
2017-03-02T17:54:05.167137: step 16702, loss 0.136661, acc 0.9375
2017-03-02T17:54:05.238637: step 16703, loss 0.203036, acc 0.921875
2017-03-02T17:54:05.330365: step 16704, loss 0.124449, acc 0.90625
2017-03-02T17:54:05.408180: step 16705, loss 0.10332, acc 0.953125
2017-03-02T17:54:05.482594: step 16706, loss 0.322641, acc 0.859375
2017-03-02T17:54:05.557675: step 16707, loss 0.179091, acc 0.90625
2017-03-02T17:54:05.624304: step 16708, loss 0.104696, acc 0.9375
2017-03-02T17:54:05.695550: step 16709, loss 0.264628, acc 0.875
2017-03-02T17:54:05.780003: step 16710, loss 0.12901, acc 0.9375
2017-03-02T17:54:05.854346: step 16711, loss 0.129143, acc 0.9375
2017-03-02T17:54:05.922193: step 16712, loss 0.119967, acc 0.921875
2017-03-02T17:54:06.002464: step 16713, loss 0.0957549, acc 0.96875
2017-03-02T17:54:06.075918: step 16714, loss 0.105342, acc 0.953125
2017-03-02T17:54:06.154372: step 16715, loss 0.126957, acc 0.9375
2017-03-02T17:54:06.227248: step 16716, loss 0.168973, acc 0.921875
2017-03-02T17:54:06.299592: step 16717, loss 0.20255, acc 0.90625
2017-03-02T17:54:06.365676: step 16718, loss 0.0564823, acc 0.96875
2017-03-02T17:54:06.435590: step 16719, loss 0.248595, acc 0.921875
2017-03-02T17:54:06.513014: step 16720, loss 0.104967, acc 0.96875
2017-03-02T17:54:06.590980: step 16721, loss 0.160795, acc 0.9375
2017-03-02T17:54:06.665196: step 16722, loss 0.106057, acc 0.953125
2017-03-02T17:54:06.738130: step 16723, loss 0.188372, acc 0.921875
2017-03-02T17:54:06.818392: step 16724, loss 0.0950637, acc 0.953125
2017-03-02T17:54:06.889524: step 16725, loss 0.174244, acc 0.9375
2017-03-02T17:54:06.957692: step 16726, loss 0.116814, acc 0.953125
2017-03-02T17:54:07.025542: step 16727, loss 0.188545, acc 0.90625
2017-03-02T17:54:07.094090: step 16728, loss 0.181239, acc 0.921875
2017-03-02T17:54:07.170536: step 16729, loss 0.290738, acc 0.890625
2017-03-02T17:54:07.243873: step 16730, loss 0.120942, acc 0.953125
2017-03-02T17:54:07.316675: step 16731, loss 0.210376, acc 0.90625
2017-03-02T17:54:07.392493: step 16732, loss 0.275775, acc 0.90625
2017-03-02T17:54:07.466957: step 16733, loss 0.0927529, acc 0.953125
2017-03-02T17:54:07.542718: step 16734, loss 0.153066, acc 0.9375
2017-03-02T17:54:07.618512: step 16735, loss 0.243726, acc 0.859375
2017-03-02T17:54:07.686989: step 16736, loss 0.15247, acc 0.921875
2017-03-02T17:54:07.757178: step 16737, loss 0.165188, acc 0.90625
2017-03-02T17:54:07.836204: step 16738, loss 0.136919, acc 0.90625
2017-03-02T17:54:07.910699: step 16739, loss 0.167322, acc 0.90625
2017-03-02T17:54:07.986955: step 16740, loss 0.138711, acc 0.921875
2017-03-02T17:54:08.065798: step 16741, loss 0.11332, acc 0.96875
2017-03-02T17:54:08.139582: step 16742, loss 0.170037, acc 0.953125
2017-03-02T17:54:08.219292: step 16743, loss 0.0367845, acc 0.984375
2017-03-02T17:54:08.304522: step 16744, loss 0.0994755, acc 0.984375
2017-03-02T17:54:08.378268: step 16745, loss 0.144698, acc 0.921875
2017-03-02T17:54:08.435717: step 16746, loss 0.25684, acc 0.875
2017-03-02T17:54:08.505193: step 16747, loss 0.133654, acc 0.9375
2017-03-02T17:54:08.567867: step 16748, loss 0.0835998, acc 0.984375
2017-03-02T17:54:08.643585: step 16749, loss 0.0699713, acc 0.96875
2017-03-02T17:54:08.730008: step 16750, loss 0.200358, acc 0.890625
2017-03-02T17:54:08.803764: step 16751, loss 0.153721, acc 0.9375
2017-03-02T17:54:08.873930: step 16752, loss 0.109797, acc 0.953125
2017-03-02T17:54:08.943875: step 16753, loss 0.218601, acc 0.859375
2017-03-02T17:54:09.015191: step 16754, loss 0.0676331, acc 0.953125
2017-03-02T17:54:09.088679: step 16755, loss 0.258907, acc 0.90625
2017-03-02T17:54:09.166050: step 16756, loss 0.232813, acc 0.90625
2017-03-02T17:54:09.235824: step 16757, loss 0.126472, acc 0.953125
2017-03-02T17:54:09.307866: step 16758, loss 0.208924, acc 0.875
2017-03-02T17:54:09.385891: step 16759, loss 0.108575, acc 0.953125
2017-03-02T17:54:09.459443: step 16760, loss 0.121562, acc 0.921875
2017-03-02T17:54:09.531873: step 16761, loss 0.288953, acc 0.875
2017-03-02T17:54:09.603914: step 16762, loss 0.109018, acc 0.953125
2017-03-02T17:54:09.686393: step 16763, loss 0.226481, acc 0.875
2017-03-02T17:54:09.752209: step 16764, loss 0.161507, acc 0.9375
2017-03-02T17:54:09.821185: step 16765, loss 0.227541, acc 0.890625
2017-03-02T17:54:09.892779: step 16766, loss 0.0965071, acc 0.953125
2017-03-02T17:54:09.965426: step 16767, loss 0.165408, acc 0.953125
2017-03-02T17:54:10.044796: step 16768, loss 0.162741, acc 0.890625
2017-03-02T17:54:10.121846: step 16769, loss 0.164472, acc 0.921875
2017-03-02T17:54:10.192443: step 16770, loss 0.20878, acc 0.90625
2017-03-02T17:54:10.265055: step 16771, loss 0.113635, acc 0.953125
2017-03-02T17:54:10.338075: step 16772, loss 0.209509, acc 0.875
2017-03-02T17:54:10.409278: step 16773, loss 0.121734, acc 0.96875
2017-03-02T17:54:10.479616: step 16774, loss 0.25075, acc 0.9375
2017-03-02T17:54:10.546026: step 16775, loss 0.207106, acc 0.859375
2017-03-02T17:54:10.623879: step 16776, loss 0.193768, acc 0.921875
2017-03-02T17:54:10.704876: step 16777, loss 0.127686, acc 0.984375
2017-03-02T17:54:10.783377: step 16778, loss 0.168406, acc 0.9375
2017-03-02T17:54:10.855458: step 16779, loss 0.133666, acc 0.921875
2017-03-02T17:54:10.929000: step 16780, loss 0.138634, acc 0.9375
2017-03-02T17:54:11.001871: step 16781, loss 0.109804, acc 0.96875
2017-03-02T17:54:11.073100: step 16782, loss 0.218325, acc 0.890625
2017-03-02T17:54:11.141990: step 16783, loss 0.0419527, acc 1
2017-03-02T17:54:11.212518: step 16784, loss 0.119927, acc 0.96875
2017-03-02T17:54:11.284654: step 16785, loss 0.0431885, acc 0.984375
2017-03-02T17:54:11.357850: step 16786, loss 0.38104, acc 0.890625
2017-03-02T17:54:11.433174: step 16787, loss 0.15519, acc 0.9375
2017-03-02T17:54:11.505267: step 16788, loss 0.136393, acc 0.90625
2017-03-02T17:54:11.574023: step 16789, loss 0.204361, acc 0.90625
2017-03-02T17:54:11.650715: step 16790, loss 0.196353, acc 0.890625
2017-03-02T17:54:11.724723: step 16791, loss 0.189513, acc 0.921875
2017-03-02T17:54:11.795178: step 16792, loss 0.195275, acc 0.921875
2017-03-02T17:54:11.862009: step 16793, loss 0.197517, acc 0.90625
2017-03-02T17:54:11.930322: step 16794, loss 0.214784, acc 0.90625
2017-03-02T17:54:12.011516: step 16795, loss 0.0816789, acc 0.96875
2017-03-02T17:54:12.086438: step 16796, loss 0.0901823, acc 0.96875
2017-03-02T17:54:12.172797: step 16797, loss 0.144599, acc 0.921875
2017-03-02T17:54:12.247134: step 16798, loss 0.0716968, acc 0.953125
2017-03-02T17:54:12.318090: step 16799, loss 0.149304, acc 0.953125
2017-03-02T17:54:12.389994: step 16800, loss 0.181251, acc 0.921875

Evaluation:
2017-03-02T17:54:12.422042: step 16800, loss 2.00178, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16800

2017-03-02T17:54:12.886830: step 16801, loss 0.142657, acc 0.953125
2017-03-02T17:54:12.963683: step 16802, loss 0.10203, acc 0.96875
2017-03-02T17:54:13.034757: step 16803, loss 0.152253, acc 0.9375
2017-03-02T17:54:13.110048: step 16804, loss 0.093488, acc 0.953125
2017-03-02T17:54:13.183207: step 16805, loss 0.20105, acc 0.890625
2017-03-02T17:54:13.252358: step 16806, loss 0.123798, acc 0.953125
2017-03-02T17:54:13.320440: step 16807, loss 0.156706, acc 0.921875
2017-03-02T17:54:13.403102: step 16808, loss 0.105584, acc 0.96875
2017-03-02T17:54:13.472135: step 16809, loss 0.224407, acc 0.90625
2017-03-02T17:54:13.558483: step 16810, loss 0.0905742, acc 0.96875
2017-03-02T17:54:13.636552: step 16811, loss 0.113576, acc 0.953125
2017-03-02T17:54:13.716824: step 16812, loss 0.031414, acc 0.984375
2017-03-02T17:54:13.801422: step 16813, loss 0.374841, acc 0.890625
2017-03-02T17:54:13.884590: step 16814, loss 0.0278069, acc 1
2017-03-02T17:54:13.958399: step 16815, loss 0.227881, acc 0.90625
2017-03-02T17:54:14.033383: step 16816, loss 0.0798772, acc 0.984375
2017-03-02T17:54:14.108681: step 16817, loss 0.198867, acc 0.90625
2017-03-02T17:54:14.181854: step 16818, loss 0.173198, acc 0.921875
2017-03-02T17:54:14.264033: step 16819, loss 0.159994, acc 0.90625
2017-03-02T17:54:14.339968: step 16820, loss 0.129724, acc 0.953125
2017-03-02T17:54:14.414646: step 16821, loss 0.128102, acc 0.921875
2017-03-02T17:54:14.490029: step 16822, loss 0.230771, acc 0.90625
2017-03-02T17:54:14.563971: step 16823, loss 0.120012, acc 0.953125
2017-03-02T17:54:14.634355: step 16824, loss 0.158649, acc 0.921875
2017-03-02T17:54:14.707910: step 16825, loss 0.173028, acc 0.953125
2017-03-02T17:54:14.793412: step 16826, loss 0.163333, acc 0.953125
2017-03-02T17:54:14.867384: step 16827, loss 0.230136, acc 0.890625
2017-03-02T17:54:14.937021: step 16828, loss 0.257409, acc 0.890625
2017-03-02T17:54:15.006899: step 16829, loss 0.196109, acc 0.90625
2017-03-02T17:54:15.075511: step 16830, loss 0.187251, acc 0.921875
2017-03-02T17:54:15.143994: step 16831, loss 0.114435, acc 0.9375
2017-03-02T17:54:15.218819: step 16832, loss 0.11285, acc 0.921875
2017-03-02T17:54:15.286709: step 16833, loss 0.128911, acc 0.953125
2017-03-02T17:54:15.345722: step 16834, loss 0.228444, acc 0.921875
2017-03-02T17:54:15.415768: step 16835, loss 0.250406, acc 0.890625
2017-03-02T17:54:15.497119: step 16836, loss 0.210218, acc 0.90625
2017-03-02T17:54:15.571038: step 16837, loss 0.193618, acc 0.875
2017-03-02T17:54:15.640228: step 16838, loss 0.25408, acc 0.859375
2017-03-02T17:54:15.715665: step 16839, loss 0.113504, acc 0.953125
2017-03-02T17:54:15.783517: step 16840, loss 0.102968, acc 0.953125
2017-03-02T17:54:15.855513: step 16841, loss 0.122103, acc 0.9375
2017-03-02T17:54:15.928591: step 16842, loss 0.189871, acc 0.90625
2017-03-02T17:54:16.010285: step 16843, loss 0.192847, acc 0.890625
2017-03-02T17:54:16.081310: step 16844, loss 0.0408938, acc 1
2017-03-02T17:54:16.154163: step 16845, loss 0.0764457, acc 0.96875
2017-03-02T17:54:16.234736: step 16846, loss 0.226492, acc 0.875
2017-03-02T17:54:16.309597: step 16847, loss 0.0778295, acc 0.96875
2017-03-02T17:54:16.379120: step 16848, loss 0.21733, acc 0.890625
2017-03-02T17:54:16.454623: step 16849, loss 0.118219, acc 0.9375
2017-03-02T17:54:16.528485: step 16850, loss 0.157827, acc 0.9375
2017-03-02T17:54:16.602489: step 16851, loss 0.222596, acc 0.859375
2017-03-02T17:54:16.671321: step 16852, loss 0.0952031, acc 0.953125
2017-03-02T17:54:16.737224: step 16853, loss 0.0735046, acc 0.953125
2017-03-02T17:54:16.816516: step 16854, loss 0.153414, acc 0.953125
2017-03-02T17:54:16.895280: step 16855, loss 0.155159, acc 0.921875
2017-03-02T17:54:16.958202: step 16856, loss 0.0413178, acc 1
2017-03-02T17:54:17.034440: step 16857, loss 0.166361, acc 0.921875
2017-03-02T17:54:17.107009: step 16858, loss 0.195595, acc 0.890625
2017-03-02T17:54:17.180626: step 16859, loss 0.169964, acc 0.9375
2017-03-02T17:54:17.260841: step 16860, loss 0.0855936, acc 0.96875
2017-03-02T17:54:17.338418: step 16861, loss 0.126229, acc 0.953125
2017-03-02T17:54:17.434217: step 16862, loss 0.172568, acc 0.953125
2017-03-02T17:54:17.510469: step 16863, loss 0.141952, acc 0.921875
2017-03-02T17:54:17.584471: step 16864, loss 0.114304, acc 0.9375
2017-03-02T17:54:17.659297: step 16865, loss 0.0742289, acc 0.96875
2017-03-02T17:54:17.739653: step 16866, loss 0.191789, acc 0.921875
2017-03-02T17:54:17.815062: step 16867, loss 0.219701, acc 0.90625
2017-03-02T17:54:17.892326: step 16868, loss 0.140973, acc 0.921875
2017-03-02T17:54:17.980693: step 16869, loss 0.174149, acc 0.90625
2017-03-02T17:54:18.051313: step 16870, loss 0.075535, acc 0.953125
2017-03-02T17:54:18.115423: step 16871, loss 0.0982115, acc 0.953125
2017-03-02T17:54:18.184145: step 16872, loss 0.126869, acc 0.953125
2017-03-02T17:54:18.263171: step 16873, loss 0.289681, acc 0.9375
2017-03-02T17:54:18.336312: step 16874, loss 0.164266, acc 0.921875
2017-03-02T17:54:18.407426: step 16875, loss 0.112554, acc 0.9375
2017-03-02T17:54:18.499664: step 16876, loss 0.121791, acc 0.953125
2017-03-02T17:54:18.577824: step 16877, loss 0.139464, acc 0.9375
2017-03-02T17:54:18.650236: step 16878, loss 0.119319, acc 0.96875
2017-03-02T17:54:18.728629: step 16879, loss 0.0449953, acc 0.984375
2017-03-02T17:54:18.798735: step 16880, loss 0.0505663, acc 1
2017-03-02T17:54:18.871438: step 16881, loss 0.304154, acc 0.875
2017-03-02T17:54:18.941369: step 16882, loss 0.162181, acc 0.9375
2017-03-02T17:54:19.015768: step 16883, loss 0.146982, acc 0.953125
2017-03-02T17:54:19.092405: step 16884, loss 0.115982, acc 0.9375
2017-03-02T17:54:19.170899: step 16885, loss 0.0834279, acc 0.953125
2017-03-02T17:54:19.240585: step 16886, loss 0.190372, acc 0.953125
2017-03-02T17:54:19.312644: step 16887, loss 0.113293, acc 0.9375
2017-03-02T17:54:19.388155: step 16888, loss 0.0784905, acc 0.984375
2017-03-02T17:54:19.463169: step 16889, loss 0.0965324, acc 0.9375
2017-03-02T17:54:19.532516: step 16890, loss 0.127327, acc 0.921875
2017-03-02T17:54:19.615321: step 16891, loss 0.224579, acc 0.90625
2017-03-02T17:54:19.698381: step 16892, loss 0.259037, acc 0.90625
2017-03-02T17:54:19.770577: step 16893, loss 0.0880846, acc 0.96875
2017-03-02T17:54:19.843362: step 16894, loss 0.117923, acc 0.9375
2017-03-02T17:54:19.917586: step 16895, loss 0.0713837, acc 0.96875
2017-03-02T17:54:19.995562: step 16896, loss 0.177395, acc 0.90625
2017-03-02T17:54:20.080002: step 16897, loss 0.117615, acc 0.9375
2017-03-02T17:54:20.162472: step 16898, loss 0.15295, acc 0.890625
2017-03-02T17:54:20.235227: step 16899, loss 0.0823052, acc 0.96875
2017-03-02T17:54:20.311205: step 16900, loss 0.207086, acc 0.953125

Evaluation:
2017-03-02T17:54:20.354029: step 16900, loss 2.04222, acc 0.648162

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-16900

2017-03-02T17:54:20.808585: step 16901, loss 0.194798, acc 0.890625
2017-03-02T17:54:20.884969: step 16902, loss 0.180801, acc 0.921875
2017-03-02T17:54:20.961165: step 16903, loss 0.0959785, acc 0.9375
2017-03-02T17:54:21.031423: step 16904, loss 0.130633, acc 0.90625
2017-03-02T17:54:21.113976: step 16905, loss 0.205849, acc 0.890625
2017-03-02T17:54:21.186149: step 16906, loss 0.15609, acc 0.953125
2017-03-02T17:54:21.257954: step 16907, loss 0.19164, acc 0.921875
2017-03-02T17:54:21.332326: step 16908, loss 0.10304, acc 0.96875
2017-03-02T17:54:21.419067: step 16909, loss 0.135122, acc 0.953125
2017-03-02T17:54:21.490418: step 16910, loss 0.124282, acc 0.953125
2017-03-02T17:54:21.557177: step 16911, loss 0.218351, acc 0.953125
2017-03-02T17:54:21.623713: step 16912, loss 0.146406, acc 0.96875
2017-03-02T17:54:21.702428: step 16913, loss 0.245464, acc 0.890625
2017-03-02T17:54:21.773946: step 16914, loss 0.142205, acc 0.921875
2017-03-02T17:54:21.845145: step 16915, loss 0.123012, acc 0.921875
2017-03-02T17:54:21.917397: step 16916, loss 0.134465, acc 0.9375
2017-03-02T17:54:21.989407: step 16917, loss 0.129431, acc 0.921875
2017-03-02T17:54:22.065055: step 16918, loss 0.148722, acc 0.96875
2017-03-02T17:54:22.138689: step 16919, loss 0.0972436, acc 0.9375
2017-03-02T17:54:22.217712: step 16920, loss 0.219867, acc 0.890625
2017-03-02T17:54:22.292762: step 16921, loss 0.190333, acc 0.9375
2017-03-02T17:54:22.364082: step 16922, loss 0.134898, acc 0.96875
2017-03-02T17:54:22.424345: step 16923, loss 0.139067, acc 0.9375
2017-03-02T17:54:22.492783: step 16924, loss 0.0730711, acc 0.953125
2017-03-02T17:54:22.567161: step 16925, loss 0.0904408, acc 0.9375
2017-03-02T17:54:22.640021: step 16926, loss 0.162855, acc 0.890625
2017-03-02T17:54:22.711462: step 16927, loss 0.208187, acc 0.921875
2017-03-02T17:54:22.783093: step 16928, loss 0.175802, acc 0.890625
2017-03-02T17:54:22.857583: step 16929, loss 0.127973, acc 0.9375
2017-03-02T17:54:22.925238: step 16930, loss 0.220493, acc 0.890625
2017-03-02T17:54:23.004420: step 16931, loss 0.100619, acc 0.953125
2017-03-02T17:54:23.080524: step 16932, loss 0.158424, acc 0.9375
2017-03-02T17:54:23.153890: step 16933, loss 0.0703822, acc 0.984375
2017-03-02T17:54:23.232345: step 16934, loss 0.21112, acc 0.90625
2017-03-02T17:54:23.306994: step 16935, loss 0.182547, acc 0.96875
2017-03-02T17:54:23.382625: step 16936, loss 0.102262, acc 0.953125
2017-03-02T17:54:23.460974: step 16937, loss 0.191624, acc 0.90625
2017-03-02T17:54:23.539068: step 16938, loss 0.175619, acc 0.953125
2017-03-02T17:54:23.616743: step 16939, loss 0.122343, acc 0.9375
2017-03-02T17:54:23.685448: step 16940, loss 0.104293, acc 0.984375
2017-03-02T17:54:23.755395: step 16941, loss 0.25338, acc 0.84375
2017-03-02T17:54:23.833634: step 16942, loss 0.156953, acc 0.953125
2017-03-02T17:54:23.908597: step 16943, loss 0.179731, acc 0.9375
2017-03-02T17:54:23.982620: step 16944, loss 0.2087, acc 0.890625
2017-03-02T17:54:24.056100: step 16945, loss 0.116345, acc 0.9375
2017-03-02T17:54:24.130390: step 16946, loss 0.112466, acc 0.953125
2017-03-02T17:54:24.203822: step 16947, loss 0.174928, acc 0.9375
2017-03-02T17:54:24.270572: step 16948, loss 0.139059, acc 0.953125
2017-03-02T17:54:24.341477: step 16949, loss 0.0987643, acc 0.953125
2017-03-02T17:54:24.406364: step 16950, loss 0.104466, acc 0.953125
2017-03-02T17:54:24.476015: step 16951, loss 0.100457, acc 0.953125
2017-03-02T17:54:24.563026: step 16952, loss 0.121707, acc 0.953125
2017-03-02T17:54:24.632615: step 16953, loss 0.214037, acc 0.9375
2017-03-02T17:54:24.703569: step 16954, loss 0.210113, acc 0.90625
2017-03-02T17:54:24.775355: step 16955, loss 0.0475281, acc 0.984375
2017-03-02T17:54:24.847494: step 16956, loss 0.292866, acc 0.90625
2017-03-02T17:54:24.921986: step 16957, loss 0.0935208, acc 0.96875
2017-03-02T17:54:24.992653: step 16958, loss 0.200174, acc 0.921875
2017-03-02T17:54:25.056536: step 16959, loss 0.242773, acc 0.90625
2017-03-02T17:54:25.141972: step 16960, loss 0.138668, acc 0.921875
2017-03-02T17:54:25.218435: step 16961, loss 0.125436, acc 0.9375
2017-03-02T17:54:25.294181: step 16962, loss 0.197443, acc 0.890625
2017-03-02T17:54:25.368005: step 16963, loss 0.143391, acc 0.921875
2017-03-02T17:54:25.457087: step 16964, loss 0.17952, acc 0.90625
2017-03-02T17:54:25.536666: step 16965, loss 0.254693, acc 0.859375
2017-03-02T17:54:25.614073: step 16966, loss 0.208157, acc 0.921875
2017-03-02T17:54:25.686794: step 16967, loss 0.142512, acc 0.921875
2017-03-02T17:54:25.758792: step 16968, loss 0.192703, acc 0.921875
2017-03-02T17:54:25.830424: step 16969, loss 0.108923, acc 0.96875
2017-03-02T17:54:25.901962: step 16970, loss 0.132549, acc 0.90625
2017-03-02T17:54:25.971825: step 16971, loss 0.141415, acc 0.9375
2017-03-02T17:54:26.046352: step 16972, loss 0.274812, acc 0.921875
2017-03-02T17:54:26.114042: step 16973, loss 0.149147, acc 0.90625
2017-03-02T17:54:26.189266: step 16974, loss 0.0945648, acc 0.953125
2017-03-02T17:54:26.261247: step 16975, loss 0.150138, acc 0.9375
2017-03-02T17:54:26.331932: step 16976, loss 0.157552, acc 0.953125
2017-03-02T17:54:26.400317: step 16977, loss 0.157248, acc 0.953125
2017-03-02T17:54:26.476886: step 16978, loss 0.0636512, acc 0.984375
2017-03-02T17:54:26.548152: step 16979, loss 0.0816393, acc 0.984375
2017-03-02T17:54:26.622656: step 16980, loss 0.197182, acc 0.921875
2017-03-02T17:54:26.710895: step 16981, loss 0.18654, acc 0.9375
2017-03-02T17:54:26.790240: step 16982, loss 0.228556, acc 0.90625
2017-03-02T17:54:26.868094: step 16983, loss 0.103798, acc 0.96875
2017-03-02T17:54:26.943438: step 16984, loss 0.209274, acc 0.9375
2017-03-02T17:54:27.020463: step 16985, loss 0.174404, acc 0.9375
2017-03-02T17:54:27.088224: step 16986, loss 0.0949082, acc 0.9375
2017-03-02T17:54:27.158187: step 16987, loss 0.232934, acc 0.953125
2017-03-02T17:54:27.232293: step 16988, loss 0.240916, acc 0.9375
2017-03-02T17:54:27.308202: step 16989, loss 0.196023, acc 0.921875
2017-03-02T17:54:27.380539: step 16990, loss 0.0765743, acc 0.96875
2017-03-02T17:54:27.452018: step 16991, loss 0.286595, acc 0.90625
2017-03-02T17:54:27.526633: step 16992, loss 0.065399, acc 0.953125
2017-03-02T17:54:27.607611: step 16993, loss 0.162746, acc 0.875
2017-03-02T17:54:27.681328: step 16994, loss 0.110076, acc 0.953125
2017-03-02T17:54:27.753566: step 16995, loss 0.179771, acc 0.921875
2017-03-02T17:54:27.827467: step 16996, loss 0.0997963, acc 0.9375
2017-03-02T17:54:27.903590: step 16997, loss 0.237238, acc 0.9375
2017-03-02T17:54:27.976166: step 16998, loss 0.114405, acc 0.9375
2017-03-02T17:54:28.048580: step 16999, loss 0.0851409, acc 0.96875
2017-03-02T17:54:28.135224: step 17000, loss 0.0727443, acc 0.953125

Evaluation:
2017-03-02T17:54:28.173275: step 17000, loss 2.07463, acc 0.662581

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17000

2017-03-02T17:54:28.625154: step 17001, loss 0.172046, acc 0.921875
2017-03-02T17:54:28.695377: step 17002, loss 0.276716, acc 0.890625
2017-03-02T17:54:28.768610: step 17003, loss 0.0970392, acc 0.953125
2017-03-02T17:54:28.841440: step 17004, loss 0.147308, acc 0.953125
2017-03-02T17:54:28.923779: step 17005, loss 0.178217, acc 0.921875
2017-03-02T17:54:28.994685: step 17006, loss 0.102885, acc 0.953125
2017-03-02T17:54:29.080813: step 17007, loss 0.21607, acc 0.9375
2017-03-02T17:54:29.146528: step 17008, loss 0.141947, acc 0.921875
2017-03-02T17:54:29.218182: step 17009, loss 0.131833, acc 0.9375
2017-03-02T17:54:29.307572: step 17010, loss 0.194472, acc 0.9375
2017-03-02T17:54:29.385345: step 17011, loss 0.232077, acc 0.875
2017-03-02T17:54:29.458919: step 17012, loss 0.306772, acc 0.921875
2017-03-02T17:54:29.529754: step 17013, loss 0.378349, acc 0.859375
2017-03-02T17:54:29.592891: step 17014, loss 0.215566, acc 0.90625
2017-03-02T17:54:29.666710: step 17015, loss 0.178668, acc 0.921875
2017-03-02T17:54:29.745799: step 17016, loss 0.127709, acc 0.921875
2017-03-02T17:54:29.816030: step 17017, loss 0.272391, acc 0.890625
2017-03-02T17:54:29.887389: step 17018, loss 0.119871, acc 0.921875
2017-03-02T17:54:29.970317: step 17019, loss 0.137114, acc 0.9375
2017-03-02T17:54:30.052682: step 17020, loss 0.110612, acc 0.953125
2017-03-02T17:54:30.160761: step 17021, loss 0.182856, acc 0.953125
2017-03-02T17:54:30.233873: step 17022, loss 0.189747, acc 0.921875
2017-03-02T17:54:30.310005: step 17023, loss 0.148292, acc 0.921875
2017-03-02T17:54:30.380112: step 17024, loss 0.145858, acc 0.96875
2017-03-02T17:54:30.451414: step 17025, loss 0.0967462, acc 0.953125
2017-03-02T17:54:30.517151: step 17026, loss 0.104756, acc 0.9375
2017-03-02T17:54:30.582389: step 17027, loss 0.317434, acc 0.875
2017-03-02T17:54:30.655516: step 17028, loss 0.158868, acc 0.9375
2017-03-02T17:54:30.730088: step 17029, loss 0.0648495, acc 0.96875
2017-03-02T17:54:30.801150: step 17030, loss 0.18561, acc 0.875
2017-03-02T17:54:30.872212: step 17031, loss 0.153096, acc 0.921875
2017-03-02T17:54:30.957839: step 17032, loss 0.204551, acc 0.890625
2017-03-02T17:54:31.032240: step 17033, loss 0.135082, acc 0.96875
2017-03-02T17:54:31.111627: step 17034, loss 0.0849367, acc 0.96875
2017-03-02T17:54:31.189405: step 17035, loss 0.215312, acc 0.90625
2017-03-02T17:54:31.263908: step 17036, loss 0.138401, acc 0.953125
2017-03-02T17:54:31.337259: step 17037, loss 0.209967, acc 0.90625
2017-03-02T17:54:31.426882: step 17038, loss 0.169431, acc 0.96875
2017-03-02T17:54:31.500919: step 17039, loss 0.228892, acc 0.90625
2017-03-02T17:54:31.584950: step 17040, loss 0.209059, acc 0.890625
2017-03-02T17:54:31.658635: step 17041, loss 0.10766, acc 0.953125
2017-03-02T17:54:31.734465: step 17042, loss 0.0827911, acc 0.96875
2017-03-02T17:54:31.804962: step 17043, loss 0.153054, acc 0.90625
2017-03-02T17:54:31.877459: step 17044, loss 0.0841163, acc 0.984375
2017-03-02T17:54:31.946198: step 17045, loss 0.326204, acc 0.859375
2017-03-02T17:54:32.019819: step 17046, loss 0.151572, acc 0.921875
2017-03-02T17:54:32.094185: step 17047, loss 0.178529, acc 0.9375
2017-03-02T17:54:32.188393: step 17048, loss 0.151154, acc 0.9375
2017-03-02T17:54:32.261064: step 17049, loss 0.140046, acc 0.96875
2017-03-02T17:54:32.332925: step 17050, loss 0.22514, acc 0.890625
2017-03-02T17:54:32.404476: step 17051, loss 0.214083, acc 0.90625
2017-03-02T17:54:32.480674: step 17052, loss 0.017482, acc 1
2017-03-02T17:54:32.541867: step 17053, loss 0.143783, acc 0.90625
2017-03-02T17:54:32.612161: step 17054, loss 0.147345, acc 0.921875
2017-03-02T17:54:32.682282: step 17055, loss 0.155668, acc 0.9375
2017-03-02T17:54:32.754832: step 17056, loss 0.178598, acc 0.953125
2017-03-02T17:54:32.845440: step 17057, loss 0.0976064, acc 0.984375
2017-03-02T17:54:32.919539: step 17058, loss 0.16324, acc 0.9375
2017-03-02T17:54:32.992929: step 17059, loss 0.147337, acc 0.921875
2017-03-02T17:54:33.066420: step 17060, loss 0.0783627, acc 0.953125
2017-03-02T17:54:33.140445: step 17061, loss 0.274089, acc 0.875
2017-03-02T17:54:33.219959: step 17062, loss 0.138544, acc 0.9375
2017-03-02T17:54:33.290452: step 17063, loss 0.101469, acc 0.96875
2017-03-02T17:54:33.355783: step 17064, loss 0.16234, acc 0.9375
2017-03-02T17:54:33.434164: step 17065, loss 0.145973, acc 0.953125
2017-03-02T17:54:33.506812: step 17066, loss 0.0704201, acc 0.984375
2017-03-02T17:54:33.579557: step 17067, loss 0.109793, acc 0.96875
2017-03-02T17:54:33.653925: step 17068, loss 0.0436589, acc 0.984375
2017-03-02T17:54:33.730085: step 17069, loss 0.129842, acc 0.9375
2017-03-02T17:54:33.801282: step 17070, loss 0.161558, acc 0.921875
2017-03-02T17:54:33.872068: step 17071, loss 0.182905, acc 0.90625
2017-03-02T17:54:33.941928: step 17072, loss 0.11795, acc 0.96875
2017-03-02T17:54:34.009398: step 17073, loss 0.0926454, acc 0.953125
2017-03-02T17:54:34.084180: step 17074, loss 0.130558, acc 0.9375
2017-03-02T17:54:34.158429: step 17075, loss 0.234648, acc 0.890625
2017-03-02T17:54:34.243547: step 17076, loss 0.205897, acc 0.875
2017-03-02T17:54:34.319861: step 17077, loss 0.0597678, acc 0.984375
2017-03-02T17:54:34.395180: step 17078, loss 0.194622, acc 0.9375
2017-03-02T17:54:34.471992: step 17079, loss 0.342679, acc 0.859375
2017-03-02T17:54:34.551136: step 17080, loss 0.0960656, acc 0.96875
2017-03-02T17:54:34.622295: step 17081, loss 0.10037, acc 0.9375
2017-03-02T17:54:34.699614: step 17082, loss 0.100002, acc 0.96875
2017-03-02T17:54:34.771915: step 17083, loss 0.235457, acc 0.90625
2017-03-02T17:54:34.844118: step 17084, loss 0.109616, acc 0.9375
2017-03-02T17:54:34.913430: step 17085, loss 0.210457, acc 0.90625
2017-03-02T17:54:34.985468: step 17086, loss 0.126492, acc 0.9375
2017-03-02T17:54:35.059122: step 17087, loss 0.0815078, acc 0.96875
2017-03-02T17:54:35.126597: step 17088, loss 0.108901, acc 0.96875
2017-03-02T17:54:35.203068: step 17089, loss 0.277239, acc 0.875
2017-03-02T17:54:35.280523: step 17090, loss 0.154147, acc 0.9375
2017-03-02T17:54:35.346029: step 17091, loss 0.311122, acc 0.84375
2017-03-02T17:54:35.417300: step 17092, loss 0.10659, acc 0.984375
2017-03-02T17:54:35.484078: step 17093, loss 0.10222, acc 0.953125
2017-03-02T17:54:35.558138: step 17094, loss 0.0881153, acc 0.96875
2017-03-02T17:54:35.633546: step 17095, loss 0.102548, acc 0.953125
2017-03-02T17:54:35.710730: step 17096, loss 0.253779, acc 0.890625
2017-03-02T17:54:35.781344: step 17097, loss 0.157285, acc 0.9375
2017-03-02T17:54:35.854667: step 17098, loss 0.158076, acc 0.9375
2017-03-02T17:54:35.930714: step 17099, loss 0.125332, acc 0.9375
2017-03-02T17:54:35.997672: step 17100, loss 0.192265, acc 0.90625

Evaluation:
2017-03-02T17:54:36.024825: step 17100, loss 1.99592, acc 0.643836

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17100

2017-03-02T17:54:36.473898: step 17101, loss 0.132001, acc 0.90625
2017-03-02T17:54:36.548675: step 17102, loss 0.194779, acc 0.890625
2017-03-02T17:54:36.626584: step 17103, loss 0.136017, acc 0.921875
2017-03-02T17:54:36.704892: step 17104, loss 0.0981457, acc 0.96875
2017-03-02T17:54:36.774301: step 17105, loss 0.199804, acc 0.921875
2017-03-02T17:54:36.837396: step 17106, loss 0.15109, acc 0.921875
2017-03-02T17:54:36.925111: step 17107, loss 0.177663, acc 0.90625
2017-03-02T17:54:37.002727: step 17108, loss 0.0923661, acc 0.96875
2017-03-02T17:54:37.076105: step 17109, loss 0.142507, acc 0.96875
2017-03-02T17:54:37.148395: step 17110, loss 0.201551, acc 0.90625
2017-03-02T17:54:37.220059: step 17111, loss 0.12901, acc 0.921875
2017-03-02T17:54:37.297251: step 17112, loss 0.216089, acc 0.90625
2017-03-02T17:54:37.373881: step 17113, loss 0.14974, acc 0.921875
2017-03-02T17:54:37.439337: step 17114, loss 0.068694, acc 0.96875
2017-03-02T17:54:37.509634: step 17115, loss 0.138515, acc 0.90625
2017-03-02T17:54:37.577743: step 17116, loss 0.120997, acc 0.953125
2017-03-02T17:54:37.654418: step 17117, loss 0.0840515, acc 0.953125
2017-03-02T17:54:37.725168: step 17118, loss 0.113135, acc 0.953125
2017-03-02T17:54:37.806152: step 17119, loss 0.129064, acc 0.90625
2017-03-02T17:54:37.891998: step 17120, loss 0.216338, acc 0.90625
2017-03-02T17:54:37.964644: step 17121, loss 0.0745721, acc 0.984375
2017-03-02T17:54:38.037210: step 17122, loss 0.0927059, acc 0.9375
2017-03-02T17:54:38.103857: step 17123, loss 0.102006, acc 0.96875
2017-03-02T17:54:38.173144: step 17124, loss 0.0845812, acc 0.96875
2017-03-02T17:54:38.248560: step 17125, loss 0.282141, acc 0.9375
2017-03-02T17:54:38.336819: step 17126, loss 0.162887, acc 0.953125
2017-03-02T17:54:38.407910: step 17127, loss 0.0995495, acc 0.953125
2017-03-02T17:54:38.478408: step 17128, loss 0.123315, acc 0.96875
2017-03-02T17:54:38.555083: step 17129, loss 0.110083, acc 0.9375
2017-03-02T17:54:38.643143: step 17130, loss 0.128524, acc 0.921875
2017-03-02T17:54:38.720815: step 17131, loss 0.175768, acc 0.921875
2017-03-02T17:54:38.790018: step 17132, loss 0.224536, acc 0.921875
2017-03-02T17:54:38.860006: step 17133, loss 0.145574, acc 0.9375
2017-03-02T17:54:38.933624: step 17134, loss 0.123936, acc 0.9375
2017-03-02T17:54:38.997490: step 17135, loss 0.186859, acc 0.90625
2017-03-02T17:54:39.080648: step 17136, loss 0.0411306, acc 1
2017-03-02T17:54:39.166415: step 17137, loss 0.060459, acc 0.96875
2017-03-02T17:54:39.256219: step 17138, loss 0.213361, acc 0.890625
2017-03-02T17:54:39.331082: step 17139, loss 0.125618, acc 0.953125
2017-03-02T17:54:39.402739: step 17140, loss 0.191606, acc 0.890625
2017-03-02T17:54:39.468069: step 17141, loss 0.198281, acc 0.921875
2017-03-02T17:54:39.535162: step 17142, loss 0.249748, acc 0.921875
2017-03-02T17:54:39.605430: step 17143, loss 0.140726, acc 0.953125
2017-03-02T17:54:39.678226: step 17144, loss 0.147185, acc 0.921875
2017-03-02T17:54:39.748649: step 17145, loss 0.134776, acc 0.96875
2017-03-02T17:54:39.825641: step 17146, loss 0.134691, acc 0.953125
2017-03-02T17:54:39.895598: step 17147, loss 0.296936, acc 0.875
2017-03-02T17:54:39.976404: step 17148, loss 0.1196, acc 0.953125
2017-03-02T17:54:40.056619: step 17149, loss 0.332655, acc 0.875
2017-03-02T17:54:40.132373: step 17150, loss 0.126672, acc 0.921875
2017-03-02T17:54:40.200478: step 17151, loss 0.108954, acc 0.96875
2017-03-02T17:54:40.268316: step 17152, loss 0.0706928, acc 0.96875
2017-03-02T17:54:40.358197: step 17153, loss 0.128996, acc 0.9375
2017-03-02T17:54:40.437870: step 17154, loss 0.20923, acc 0.9375
2017-03-02T17:54:40.518138: step 17155, loss 0.111047, acc 0.96875
2017-03-02T17:54:40.592748: step 17156, loss 0.319517, acc 0.84375
2017-03-02T17:54:40.664408: step 17157, loss 0.129324, acc 0.921875
2017-03-02T17:54:40.738170: step 17158, loss 0.234164, acc 0.890625
2017-03-02T17:54:40.808982: step 17159, loss 0.236944, acc 0.90625
2017-03-02T17:54:40.883328: step 17160, loss 0.0988408, acc 0.96875
2017-03-02T17:54:40.960520: step 17161, loss 0.124455, acc 0.921875
2017-03-02T17:54:41.037169: step 17162, loss 0.081398, acc 0.984375
2017-03-02T17:54:41.112679: step 17163, loss 0.124922, acc 0.953125
2017-03-02T17:54:41.190056: step 17164, loss 0.123026, acc 0.921875
2017-03-02T17:54:41.266916: step 17165, loss 0.0757003, acc 0.96875
2017-03-02T17:54:41.341863: step 17166, loss 0.113768, acc 0.953125
2017-03-02T17:54:41.415020: step 17167, loss 0.18378, acc 0.890625
2017-03-02T17:54:41.499990: step 17168, loss 0.184075, acc 0.890625
2017-03-02T17:54:41.570525: step 17169, loss 0.0968835, acc 0.9375
2017-03-02T17:54:41.643360: step 17170, loss 0.162461, acc 0.953125
2017-03-02T17:54:41.713840: step 17171, loss 0.177554, acc 0.921875
2017-03-02T17:54:41.789294: step 17172, loss 0.202577, acc 0.859375
2017-03-02T17:54:41.861788: step 17173, loss 0.183233, acc 0.90625
2017-03-02T17:54:41.935729: step 17174, loss 0.171252, acc 0.890625
2017-03-02T17:54:42.019763: step 17175, loss 0.17785, acc 0.921875
2017-03-02T17:54:42.092560: step 17176, loss 0.18035, acc 0.953125
2017-03-02T17:54:42.159574: step 17177, loss 0.183608, acc 0.90625
2017-03-02T17:54:42.233525: step 17178, loss 0.205918, acc 0.90625
2017-03-02T17:54:42.301982: step 17179, loss 0.234885, acc 0.90625
2017-03-02T17:54:42.368935: step 17180, loss 0.123259, acc 0.96875
2017-03-02T17:54:42.446476: step 17181, loss 0.0590727, acc 0.96875
2017-03-02T17:54:42.522905: step 17182, loss 0.128189, acc 0.953125
2017-03-02T17:54:42.599680: step 17183, loss 0.135331, acc 0.96875
2017-03-02T17:54:42.668001: step 17184, loss 0.0832778, acc 0.96875
2017-03-02T17:54:42.745690: step 17185, loss 0.154602, acc 0.921875
2017-03-02T17:54:42.821775: step 17186, loss 0.0796485, acc 0.984375
2017-03-02T17:54:42.909105: step 17187, loss 0.0955182, acc 0.96875
2017-03-02T17:54:42.979141: step 17188, loss 0.241576, acc 0.90625
2017-03-02T17:54:43.049443: step 17189, loss 0.195109, acc 0.90625
2017-03-02T17:54:43.132913: step 17190, loss 0.234992, acc 0.875
2017-03-02T17:54:43.219279: step 17191, loss 0.224225, acc 0.9375
2017-03-02T17:54:43.291753: step 17192, loss 0.213307, acc 0.921875
2017-03-02T17:54:43.362337: step 17193, loss 0.15715, acc 0.921875
2017-03-02T17:54:43.435252: step 17194, loss 0.159443, acc 0.921875
2017-03-02T17:54:43.508366: step 17195, loss 0.198904, acc 0.921875
2017-03-02T17:54:43.577791: step 17196, loss 0.261385, acc 0.890625
2017-03-02T17:54:43.656049: step 17197, loss 0.284079, acc 0.90625
2017-03-02T17:54:43.728448: step 17198, loss 0.120966, acc 0.96875
2017-03-02T17:54:43.823677: step 17199, loss 0.143154, acc 0.921875
2017-03-02T17:54:43.887373: step 17200, loss 0.155898, acc 0.921875

Evaluation:
2017-03-02T17:54:43.923177: step 17200, loss 1.98718, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17200

2017-03-02T17:54:44.369624: step 17201, loss 0.0974921, acc 0.96875
2017-03-02T17:54:44.445710: step 17202, loss 0.171798, acc 0.921875
2017-03-02T17:54:44.519401: step 17203, loss 0.231957, acc 0.890625
2017-03-02T17:54:44.593943: step 17204, loss 0.206402, acc 0.90625
2017-03-02T17:54:44.668436: step 17205, loss 0.0419905, acc 1
2017-03-02T17:54:44.742901: step 17206, loss 0.198875, acc 0.921875
2017-03-02T17:54:44.819466: step 17207, loss 0.164743, acc 0.90625
2017-03-02T17:54:44.895227: step 17208, loss 0.153812, acc 0.90625
2017-03-02T17:54:44.973815: step 17209, loss 0.17441, acc 0.921875
2017-03-02T17:54:45.041243: step 17210, loss 0.0597552, acc 0.96875
2017-03-02T17:54:45.118491: step 17211, loss 0.133878, acc 0.9375
2017-03-02T17:54:45.192634: step 17212, loss 0.172245, acc 0.9375
2017-03-02T17:54:45.269025: step 17213, loss 0.183003, acc 0.9375
2017-03-02T17:54:45.343899: step 17214, loss 0.0651635, acc 0.96875
2017-03-02T17:54:45.415188: step 17215, loss 0.124939, acc 0.953125
2017-03-02T17:54:45.496088: step 17216, loss 0.116174, acc 0.9375
2017-03-02T17:54:45.567376: step 17217, loss 0.174979, acc 0.921875
2017-03-02T17:54:45.641089: step 17218, loss 0.200852, acc 0.9375
2017-03-02T17:54:45.710587: step 17219, loss 0.0583503, acc 0.96875
2017-03-02T17:54:45.775492: step 17220, loss 0.165606, acc 0.921875
2017-03-02T17:54:45.841717: step 17221, loss 0.178419, acc 0.9375
2017-03-02T17:54:45.913709: step 17222, loss 0.12016, acc 0.9375
2017-03-02T17:54:45.987937: step 17223, loss 0.101794, acc 0.984375
2017-03-02T17:54:46.060501: step 17224, loss 0.139308, acc 0.9375
2017-03-02T17:54:46.137144: step 17225, loss 0.317258, acc 0.875
2017-03-02T17:54:46.218056: step 17226, loss 0.173374, acc 0.921875
2017-03-02T17:54:46.291802: step 17227, loss 0.119469, acc 0.9375
2017-03-02T17:54:46.362885: step 17228, loss 0.258087, acc 0.875
2017-03-02T17:54:46.432516: step 17229, loss 0.124963, acc 0.9375
2017-03-02T17:54:46.503773: step 17230, loss 0.102532, acc 0.9375
2017-03-02T17:54:46.568237: step 17231, loss 0.11305, acc 0.953125
2017-03-02T17:54:46.656123: step 17232, loss 0.205978, acc 0.96875
2017-03-02T17:54:46.732071: step 17233, loss 0.158604, acc 0.921875
2017-03-02T17:54:46.810898: step 17234, loss 0.162752, acc 0.921875
2017-03-02T17:54:46.888375: step 17235, loss 0.151093, acc 0.9375
2017-03-02T17:54:46.966902: step 17236, loss 0.0667297, acc 0.953125
2017-03-02T17:54:47.042755: step 17237, loss 0.0870882, acc 0.953125
2017-03-02T17:54:47.109004: step 17238, loss 0.24975, acc 0.90625
2017-03-02T17:54:47.178178: step 17239, loss 0.0888545, acc 0.96875
2017-03-02T17:54:47.255784: step 17240, loss 0.147668, acc 0.921875
2017-03-02T17:54:47.342175: step 17241, loss 0.154787, acc 0.9375
2017-03-02T17:54:47.411051: step 17242, loss 0.128675, acc 0.953125
2017-03-02T17:54:47.482029: step 17243, loss 0.360335, acc 0.859375
2017-03-02T17:54:47.581777: step 17244, loss 0.101289, acc 0.9375
2017-03-02T17:54:47.650668: step 17245, loss 0.157604, acc 0.96875
2017-03-02T17:54:47.721784: step 17246, loss 0.248522, acc 0.921875
2017-03-02T17:54:47.790646: step 17247, loss 0.128467, acc 0.953125
2017-03-02T17:54:47.845948: step 17248, loss 0.00994353, acc 1
2017-03-02T17:54:47.919492: step 17249, loss 0.182938, acc 0.9375
2017-03-02T17:54:47.992901: step 17250, loss 0.110501, acc 0.9375
2017-03-02T17:54:48.068583: step 17251, loss 0.114415, acc 0.953125
2017-03-02T17:54:48.138925: step 17252, loss 0.150035, acc 0.9375
2017-03-02T17:54:48.210001: step 17253, loss 0.144786, acc 0.9375
2017-03-02T17:54:48.287833: step 17254, loss 0.191776, acc 0.90625
2017-03-02T17:54:48.370364: step 17255, loss 0.152648, acc 0.9375
2017-03-02T17:54:48.446991: step 17256, loss 0.161456, acc 0.921875
2017-03-02T17:54:48.522993: step 17257, loss 0.158232, acc 0.921875
2017-03-02T17:54:48.592529: step 17258, loss 0.237319, acc 0.875
2017-03-02T17:54:48.662485: step 17259, loss 0.158757, acc 0.953125
2017-03-02T17:54:48.733515: step 17260, loss 0.108848, acc 0.9375
2017-03-02T17:54:48.806778: step 17261, loss 0.0509681, acc 0.984375
2017-03-02T17:54:48.882058: step 17262, loss 0.0829701, acc 0.984375
2017-03-02T17:54:48.953100: step 17263, loss 0.108837, acc 0.96875
2017-03-02T17:54:49.025305: step 17264, loss 0.127251, acc 0.96875
2017-03-02T17:54:49.090147: step 17265, loss 0.0700923, acc 0.984375
2017-03-02T17:54:49.156270: step 17266, loss 0.292489, acc 0.875
2017-03-02T17:54:49.233847: step 17267, loss 0.112164, acc 0.9375
2017-03-02T17:54:49.304782: step 17268, loss 0.154299, acc 0.921875
2017-03-02T17:54:49.380078: step 17269, loss 0.176017, acc 0.953125
2017-03-02T17:54:49.451419: step 17270, loss 0.227177, acc 0.90625
2017-03-02T17:54:49.520717: step 17271, loss 0.188656, acc 0.9375
2017-03-02T17:54:49.589654: step 17272, loss 0.0894756, acc 0.96875
2017-03-02T17:54:49.662045: step 17273, loss 0.0582064, acc 1
2017-03-02T17:54:49.743626: step 17274, loss 0.105192, acc 0.953125
2017-03-02T17:54:49.814705: step 17275, loss 0.154492, acc 0.953125
2017-03-02T17:54:49.882535: step 17276, loss 0.0955283, acc 0.9375
2017-03-02T17:54:49.962073: step 17277, loss 0.279766, acc 0.90625
2017-03-02T17:54:50.033525: step 17278, loss 0.0591312, acc 0.984375
2017-03-02T17:54:50.109025: step 17279, loss 0.0966053, acc 0.953125
2017-03-02T17:54:50.180068: step 17280, loss 0.120144, acc 0.921875
2017-03-02T17:54:50.253151: step 17281, loss 0.122044, acc 0.9375
2017-03-02T17:54:50.328978: step 17282, loss 0.110145, acc 0.953125
2017-03-02T17:54:50.405571: step 17283, loss 0.0881121, acc 0.953125
2017-03-02T17:54:50.480428: step 17284, loss 0.228347, acc 0.90625
2017-03-02T17:54:50.551141: step 17285, loss 0.184671, acc 0.9375
2017-03-02T17:54:50.632116: step 17286, loss 0.0775065, acc 0.984375
2017-03-02T17:54:50.723166: step 17287, loss 0.0613606, acc 0.984375
2017-03-02T17:54:50.801698: step 17288, loss 0.195116, acc 0.9375
2017-03-02T17:54:50.878623: step 17289, loss 0.155296, acc 0.9375
2017-03-02T17:54:50.955095: step 17290, loss 0.175684, acc 0.890625
2017-03-02T17:54:51.027794: step 17291, loss 0.198604, acc 0.953125
2017-03-02T17:54:51.100877: step 17292, loss 0.161929, acc 0.9375
2017-03-02T17:54:51.174896: step 17293, loss 0.117735, acc 0.921875
2017-03-02T17:54:51.243626: step 17294, loss 0.165469, acc 0.9375
2017-03-02T17:54:51.311579: step 17295, loss 0.166868, acc 0.921875
2017-03-02T17:54:51.388310: step 17296, loss 0.152146, acc 0.921875
2017-03-02T17:54:51.460351: step 17297, loss 0.29042, acc 0.890625
2017-03-02T17:54:51.531892: step 17298, loss 0.128853, acc 0.9375
2017-03-02T17:54:51.610912: step 17299, loss 0.177351, acc 0.9375
2017-03-02T17:54:51.679465: step 17300, loss 0.107698, acc 0.96875

Evaluation:
2017-03-02T17:54:51.712975: step 17300, loss 2.05637, acc 0.656813

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17300

2017-03-02T17:54:52.166484: step 17301, loss 0.152447, acc 0.9375
2017-03-02T17:54:52.237940: step 17302, loss 0.266278, acc 0.875
2017-03-02T17:54:52.316339: step 17303, loss 0.106365, acc 0.96875
2017-03-02T17:54:52.384025: step 17304, loss 0.174156, acc 0.875
2017-03-02T17:54:52.458989: step 17305, loss 0.238986, acc 0.921875
2017-03-02T17:54:52.536379: step 17306, loss 0.225074, acc 0.859375
2017-03-02T17:54:52.603097: step 17307, loss 0.297133, acc 0.84375
2017-03-02T17:54:52.667534: step 17308, loss 0.12651, acc 0.9375
2017-03-02T17:54:52.734871: step 17309, loss 0.213154, acc 0.875
2017-03-02T17:54:52.813899: step 17310, loss 0.111426, acc 0.96875
2017-03-02T17:54:52.893626: step 17311, loss 0.103941, acc 0.953125
2017-03-02T17:54:52.976544: step 17312, loss 0.173179, acc 0.953125
2017-03-02T17:54:53.056963: step 17313, loss 0.124182, acc 0.953125
2017-03-02T17:54:53.130530: step 17314, loss 0.251219, acc 0.875
2017-03-02T17:54:53.198622: step 17315, loss 0.0753781, acc 0.984375
2017-03-02T17:54:53.272763: step 17316, loss 0.145521, acc 0.9375
2017-03-02T17:54:53.343997: step 17317, loss 0.189944, acc 0.921875
2017-03-02T17:54:53.413347: step 17318, loss 0.151726, acc 0.9375
2017-03-02T17:54:53.494660: step 17319, loss 0.148155, acc 0.9375
2017-03-02T17:54:53.570836: step 17320, loss 0.250484, acc 0.859375
2017-03-02T17:54:53.646496: step 17321, loss 0.089715, acc 0.984375
2017-03-02T17:54:53.722139: step 17322, loss 0.0759246, acc 0.984375
2017-03-02T17:54:53.790088: step 17323, loss 0.244101, acc 0.90625
2017-03-02T17:54:53.864596: step 17324, loss 0.168009, acc 0.921875
2017-03-02T17:54:53.943964: step 17325, loss 0.0915217, acc 0.96875
2017-03-02T17:54:54.024427: step 17326, loss 0.0931971, acc 0.953125
2017-03-02T17:54:54.095142: step 17327, loss 0.229754, acc 0.90625
2017-03-02T17:54:54.170015: step 17328, loss 0.169418, acc 0.921875
2017-03-02T17:54:54.245715: step 17329, loss 0.185982, acc 0.90625
2017-03-02T17:54:54.316567: step 17330, loss 0.204968, acc 0.90625
2017-03-02T17:54:54.384050: step 17331, loss 0.11677, acc 0.953125
2017-03-02T17:54:54.458713: step 17332, loss 0.0794048, acc 0.984375
2017-03-02T17:54:54.537225: step 17333, loss 0.165906, acc 0.953125
2017-03-02T17:54:54.607218: step 17334, loss 0.203897, acc 0.890625
2017-03-02T17:54:54.683143: step 17335, loss 0.163197, acc 0.921875
2017-03-02T17:54:54.751844: step 17336, loss 0.0966278, acc 0.96875
2017-03-02T17:54:54.828303: step 17337, loss 0.124042, acc 0.953125
2017-03-02T17:54:54.894867: step 17338, loss 0.166748, acc 0.921875
2017-03-02T17:54:54.960948: step 17339, loss 0.1408, acc 0.921875
2017-03-02T17:54:55.030720: step 17340, loss 0.114004, acc 0.96875
2017-03-02T17:54:55.105124: step 17341, loss 0.0694362, acc 0.984375
2017-03-02T17:54:55.183059: step 17342, loss 0.158405, acc 0.9375
2017-03-02T17:54:55.250599: step 17343, loss 0.166659, acc 0.921875
2017-03-02T17:54:55.330251: step 17344, loss 0.163744, acc 0.90625
2017-03-02T17:54:55.393116: step 17345, loss 0.184827, acc 0.90625
2017-03-02T17:54:55.469480: step 17346, loss 0.211454, acc 0.921875
2017-03-02T17:54:55.539737: step 17347, loss 0.192608, acc 0.90625
2017-03-02T17:54:55.613696: step 17348, loss 0.143341, acc 0.9375
2017-03-02T17:54:55.685873: step 17349, loss 0.220663, acc 0.9375
2017-03-02T17:54:55.758070: step 17350, loss 0.12823, acc 0.9375
2017-03-02T17:54:55.830345: step 17351, loss 0.119602, acc 0.921875
2017-03-02T17:54:55.903161: step 17352, loss 0.150282, acc 0.9375
2017-03-02T17:54:55.976473: step 17353, loss 0.123112, acc 0.9375
2017-03-02T17:54:56.057007: step 17354, loss 0.0779715, acc 0.96875
2017-03-02T17:54:56.128576: step 17355, loss 0.181021, acc 0.9375
2017-03-02T17:54:56.199033: step 17356, loss 0.24036, acc 0.890625
2017-03-02T17:54:56.273526: step 17357, loss 0.195017, acc 0.921875
2017-03-02T17:54:56.345366: step 17358, loss 0.130018, acc 0.96875
2017-03-02T17:54:56.420682: step 17359, loss 0.157825, acc 0.96875
2017-03-02T17:54:56.506014: step 17360, loss 0.163158, acc 0.9375
2017-03-02T17:54:56.572027: step 17361, loss 0.152497, acc 0.90625
2017-03-02T17:54:56.645946: step 17362, loss 0.0647593, acc 0.984375
2017-03-02T17:54:56.726019: step 17363, loss 0.302546, acc 0.828125
2017-03-02T17:54:56.790025: step 17364, loss 0.094185, acc 0.9375
2017-03-02T17:54:56.880706: step 17365, loss 0.110693, acc 0.96875
2017-03-02T17:54:56.955632: step 17366, loss 0.161159, acc 0.921875
2017-03-02T17:54:57.031260: step 17367, loss 0.0718584, acc 0.96875
2017-03-02T17:54:57.105700: step 17368, loss 0.198309, acc 0.921875
2017-03-02T17:54:57.186330: step 17369, loss 0.0761986, acc 0.984375
2017-03-02T17:54:57.259529: step 17370, loss 0.206134, acc 0.890625
2017-03-02T17:54:57.330100: step 17371, loss 0.0666559, acc 0.96875
2017-03-02T17:54:57.406685: step 17372, loss 0.146029, acc 0.921875
2017-03-02T17:54:57.481621: step 17373, loss 0.153075, acc 0.9375
2017-03-02T17:54:57.577471: step 17374, loss 0.171652, acc 0.90625
2017-03-02T17:54:57.653778: step 17375, loss 0.114339, acc 0.9375
2017-03-02T17:54:57.728744: step 17376, loss 0.195588, acc 0.921875
2017-03-02T17:54:57.800867: step 17377, loss 0.194472, acc 0.90625
2017-03-02T17:54:57.879921: step 17378, loss 0.085972, acc 0.953125
2017-03-02T17:54:57.955236: step 17379, loss 0.198696, acc 0.875
2017-03-02T17:54:58.028128: step 17380, loss 0.159588, acc 0.9375
2017-03-02T17:54:58.108087: step 17381, loss 0.147215, acc 0.9375
2017-03-02T17:54:58.186936: step 17382, loss 0.187616, acc 0.90625
2017-03-02T17:54:58.254161: step 17383, loss 0.130055, acc 0.921875
2017-03-02T17:54:58.342001: step 17384, loss 0.149962, acc 0.9375
2017-03-02T17:54:58.417806: step 17385, loss 0.204247, acc 0.90625
2017-03-02T17:54:58.494223: step 17386, loss 0.0866423, acc 0.96875
2017-03-02T17:54:58.564132: step 17387, loss 0.202175, acc 0.90625
2017-03-02T17:54:58.632939: step 17388, loss 0.0350856, acc 1
2017-03-02T17:54:58.712036: step 17389, loss 0.144766, acc 0.921875
2017-03-02T17:54:58.785310: step 17390, loss 0.142053, acc 0.9375
2017-03-02T17:54:58.854007: step 17391, loss 0.115675, acc 0.953125
2017-03-02T17:54:58.938208: step 17392, loss 0.0611839, acc 0.96875
2017-03-02T17:54:59.008421: step 17393, loss 0.238903, acc 0.859375
2017-03-02T17:54:59.082127: step 17394, loss 0.110405, acc 0.953125
2017-03-02T17:54:59.152880: step 17395, loss 0.188948, acc 0.90625
2017-03-02T17:54:59.227157: step 17396, loss 0.0776215, acc 0.96875
2017-03-02T17:54:59.297326: step 17397, loss 0.13456, acc 0.9375
2017-03-02T17:54:59.371880: step 17398, loss 0.262933, acc 0.890625
2017-03-02T17:54:59.446488: step 17399, loss 0.158986, acc 0.90625
2017-03-02T17:54:59.513167: step 17400, loss 0.174187, acc 0.9375

Evaluation:
2017-03-02T17:54:59.541501: step 17400, loss 2.09589, acc 0.657534

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17400

2017-03-02T17:54:59.981357: step 17401, loss 0.18684, acc 0.921875
2017-03-02T17:55:00.053713: step 17402, loss 0.364718, acc 0.890625
2017-03-02T17:55:00.127982: step 17403, loss 0.271821, acc 0.875
2017-03-02T17:55:00.208761: step 17404, loss 0.214365, acc 0.9375
2017-03-02T17:55:00.276585: step 17405, loss 0.255033, acc 0.890625
2017-03-02T17:55:00.346258: step 17406, loss 0.161351, acc 0.9375
2017-03-02T17:55:00.419848: step 17407, loss 0.0958299, acc 0.9375
2017-03-02T17:55:00.489190: step 17408, loss 0.137479, acc 0.9375
2017-03-02T17:55:00.561829: step 17409, loss 0.081132, acc 0.953125
2017-03-02T17:55:00.639256: step 17410, loss 0.18481, acc 0.890625
2017-03-02T17:55:00.712821: step 17411, loss 0.0883105, acc 0.96875
2017-03-02T17:55:00.786406: step 17412, loss 0.0989892, acc 0.96875
2017-03-02T17:55:00.859710: step 17413, loss 0.143042, acc 0.9375
2017-03-02T17:55:00.933354: step 17414, loss 0.071753, acc 0.96875
2017-03-02T17:55:01.002614: step 17415, loss 0.133183, acc 0.9375
2017-03-02T17:55:01.080215: step 17416, loss 0.251441, acc 0.90625
2017-03-02T17:55:01.155028: step 17417, loss 0.11692, acc 0.9375
2017-03-02T17:55:01.231774: step 17418, loss 0.158525, acc 0.921875
2017-03-02T17:55:01.308674: step 17419, loss 0.131834, acc 0.9375
2017-03-02T17:55:01.390259: step 17420, loss 0.105083, acc 0.953125
2017-03-02T17:55:01.460686: step 17421, loss 0.197138, acc 0.90625
2017-03-02T17:55:01.538910: step 17422, loss 0.26476, acc 0.859375
2017-03-02T17:55:01.606240: step 17423, loss 0.19102, acc 0.921875
2017-03-02T17:55:01.677945: step 17424, loss 0.25394, acc 0.875
2017-03-02T17:55:01.747003: step 17425, loss 0.260672, acc 0.90625
2017-03-02T17:55:01.819094: step 17426, loss 0.23477, acc 0.90625
2017-03-02T17:55:01.889422: step 17427, loss 0.143928, acc 0.9375
2017-03-02T17:55:01.960835: step 17428, loss 0.123392, acc 0.9375
2017-03-02T17:55:02.024281: step 17429, loss 0.116921, acc 0.921875
2017-03-02T17:55:02.095050: step 17430, loss 0.304032, acc 0.859375
2017-03-02T17:55:02.166033: step 17431, loss 0.338412, acc 0.859375
2017-03-02T17:55:02.239387: step 17432, loss 0.0798618, acc 0.984375
2017-03-02T17:55:02.309650: step 17433, loss 0.150066, acc 0.921875
2017-03-02T17:55:02.376271: step 17434, loss 0.0712659, acc 0.9375
2017-03-02T17:55:02.450887: step 17435, loss 0.178546, acc 0.90625
2017-03-02T17:55:02.533089: step 17436, loss 0.234739, acc 0.90625
2017-03-02T17:55:02.607178: step 17437, loss 0.1761, acc 0.90625
2017-03-02T17:55:02.676247: step 17438, loss 0.133657, acc 0.953125
2017-03-02T17:55:02.755864: step 17439, loss 0.118415, acc 0.953125
2017-03-02T17:55:02.836361: step 17440, loss 0.0902742, acc 0.953125
2017-03-02T17:55:02.911997: step 17441, loss 0.25584, acc 0.875
2017-03-02T17:55:02.982570: step 17442, loss 0.12037, acc 0.953125
2017-03-02T17:55:03.046829: step 17443, loss 0.0984395, acc 0.9375
2017-03-02T17:55:03.111728: step 17444, loss 0.000721543, acc 1
2017-03-02T17:55:03.180661: step 17445, loss 0.161869, acc 0.90625
2017-03-02T17:55:03.253356: step 17446, loss 0.100818, acc 0.96875
2017-03-02T17:55:03.326106: step 17447, loss 0.154461, acc 0.921875
2017-03-02T17:55:03.399538: step 17448, loss 0.261154, acc 0.890625
2017-03-02T17:55:03.480454: step 17449, loss 0.154174, acc 0.953125
2017-03-02T17:55:03.558312: step 17450, loss 0.245748, acc 0.859375
2017-03-02T17:55:03.635782: step 17451, loss 0.109832, acc 0.90625
2017-03-02T17:55:03.707076: step 17452, loss 0.198663, acc 0.90625
2017-03-02T17:55:03.775073: step 17453, loss 0.0554573, acc 1
2017-03-02T17:55:03.846339: step 17454, loss 0.159173, acc 0.9375
2017-03-02T17:55:03.923485: step 17455, loss 0.106832, acc 0.9375
2017-03-02T17:55:04.000986: step 17456, loss 0.292829, acc 0.859375
2017-03-02T17:55:04.082962: step 17457, loss 0.0925343, acc 0.9375
2017-03-02T17:55:04.155915: step 17458, loss 0.165964, acc 0.90625
2017-03-02T17:55:04.229377: step 17459, loss 0.130988, acc 0.921875
2017-03-02T17:55:04.304429: step 17460, loss 0.148062, acc 0.90625
2017-03-02T17:55:04.376610: step 17461, loss 0.113576, acc 0.984375
2017-03-02T17:55:04.444044: step 17462, loss 0.169816, acc 0.96875
2017-03-02T17:55:04.511125: step 17463, loss 0.233799, acc 0.90625
2017-03-02T17:55:04.596611: step 17464, loss 0.165776, acc 0.921875
2017-03-02T17:55:04.673152: step 17465, loss 0.141905, acc 0.921875
2017-03-02T17:55:04.747415: step 17466, loss 0.195093, acc 0.890625
2017-03-02T17:55:04.817160: step 17467, loss 0.144345, acc 0.9375
2017-03-02T17:55:04.888421: step 17468, loss 0.164336, acc 0.953125
2017-03-02T17:55:04.954785: step 17469, loss 0.209387, acc 0.890625
2017-03-02T17:55:05.026294: step 17470, loss 0.261775, acc 0.890625
2017-03-02T17:55:05.092672: step 17471, loss 0.153367, acc 0.9375
2017-03-02T17:55:05.158536: step 17472, loss 0.144326, acc 0.90625
2017-03-02T17:55:05.232616: step 17473, loss 0.196625, acc 0.859375
2017-03-02T17:55:05.298309: step 17474, loss 0.121012, acc 0.953125
2017-03-02T17:55:05.372426: step 17475, loss 0.156144, acc 0.921875
2017-03-02T17:55:05.447404: step 17476, loss 0.175441, acc 0.953125
2017-03-02T17:55:05.515862: step 17477, loss 0.175326, acc 0.9375
2017-03-02T17:55:05.611264: step 17478, loss 0.147771, acc 0.921875
2017-03-02T17:55:05.684745: step 17479, loss 0.0738836, acc 0.953125
2017-03-02T17:55:05.755120: step 17480, loss 0.225179, acc 0.890625
2017-03-02T17:55:05.822231: step 17481, loss 0.12071, acc 0.9375
2017-03-02T17:55:05.887456: step 17482, loss 0.125694, acc 0.953125
2017-03-02T17:55:05.963419: step 17483, loss 0.134726, acc 0.953125
2017-03-02T17:55:06.042396: step 17484, loss 0.146556, acc 0.9375
2017-03-02T17:55:06.117259: step 17485, loss 0.17215, acc 0.9375
2017-03-02T17:55:06.190302: step 17486, loss 0.115972, acc 0.96875
2017-03-02T17:55:06.260385: step 17487, loss 0.165852, acc 0.90625
2017-03-02T17:55:06.339172: step 17488, loss 0.185806, acc 0.9375
2017-03-02T17:55:06.417024: step 17489, loss 0.100078, acc 0.921875
2017-03-02T17:55:06.496251: step 17490, loss 0.198556, acc 0.921875
2017-03-02T17:55:06.566155: step 17491, loss 0.0787118, acc 0.96875
2017-03-02T17:55:06.647410: step 17492, loss 0.153694, acc 0.9375
2017-03-02T17:55:06.722051: step 17493, loss 0.106703, acc 0.953125
2017-03-02T17:55:06.789136: step 17494, loss 0.0608059, acc 0.96875
2017-03-02T17:55:06.861788: step 17495, loss 0.14994, acc 0.90625
2017-03-02T17:55:06.933264: step 17496, loss 0.1482, acc 0.921875
2017-03-02T17:55:07.003788: step 17497, loss 0.165532, acc 0.9375
2017-03-02T17:55:07.073246: step 17498, loss 0.20048, acc 0.90625
2017-03-02T17:55:07.148000: step 17499, loss 0.139147, acc 0.9375
2017-03-02T17:55:07.216072: step 17500, loss 0.13334, acc 0.96875

Evaluation:
2017-03-02T17:55:07.248699: step 17500, loss 2.12521, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17500

2017-03-02T17:55:07.697359: step 17501, loss 0.0971833, acc 0.953125
2017-03-02T17:55:07.764777: step 17502, loss 0.138649, acc 0.9375
2017-03-02T17:55:07.840693: step 17503, loss 0.264075, acc 0.875
2017-03-02T17:55:07.910614: step 17504, loss 0.140579, acc 0.9375
2017-03-02T17:55:07.978538: step 17505, loss 0.12289, acc 0.953125
2017-03-02T17:55:08.048712: step 17506, loss 0.12779, acc 0.96875
2017-03-02T17:55:08.130015: step 17507, loss 0.183515, acc 0.953125
2017-03-02T17:55:08.203634: step 17508, loss 0.130412, acc 0.921875
2017-03-02T17:55:08.278570: step 17509, loss 0.0805068, acc 0.96875
2017-03-02T17:55:08.348697: step 17510, loss 0.186426, acc 0.890625
2017-03-02T17:55:08.419523: step 17511, loss 0.139755, acc 0.953125
2017-03-02T17:55:08.489995: step 17512, loss 0.222851, acc 0.890625
2017-03-02T17:55:08.563935: step 17513, loss 0.184508, acc 0.890625
2017-03-02T17:55:08.633078: step 17514, loss 0.223021, acc 0.9375
2017-03-02T17:55:08.703645: step 17515, loss 0.107407, acc 0.96875
2017-03-02T17:55:08.777582: step 17516, loss 0.198086, acc 0.921875
2017-03-02T17:55:08.855789: step 17517, loss 0.284447, acc 0.875
2017-03-02T17:55:08.924748: step 17518, loss 0.207686, acc 0.890625
2017-03-02T17:55:09.002042: step 17519, loss 0.119424, acc 0.921875
2017-03-02T17:55:09.071543: step 17520, loss 0.070099, acc 0.984375
2017-03-02T17:55:09.140716: step 17521, loss 0.147613, acc 0.90625
2017-03-02T17:55:09.226034: step 17522, loss 0.281944, acc 0.875
2017-03-02T17:55:09.300145: step 17523, loss 0.160885, acc 0.96875
2017-03-02T17:55:09.368306: step 17524, loss 0.116286, acc 0.953125
2017-03-02T17:55:09.443552: step 17525, loss 0.201147, acc 0.96875
2017-03-02T17:55:09.512126: step 17526, loss 0.336079, acc 0.875
2017-03-02T17:55:09.583493: step 17527, loss 0.1125, acc 0.9375
2017-03-02T17:55:09.661831: step 17528, loss 0.149558, acc 0.9375
2017-03-02T17:55:09.742287: step 17529, loss 0.0857468, acc 0.96875
2017-03-02T17:55:09.817698: step 17530, loss 0.190598, acc 0.890625
2017-03-02T17:55:09.897174: step 17531, loss 0.179158, acc 0.9375
2017-03-02T17:55:09.965160: step 17532, loss 0.0911392, acc 0.96875
2017-03-02T17:55:10.038975: step 17533, loss 0.0642612, acc 0.984375
2017-03-02T17:55:10.111497: step 17534, loss 0.098794, acc 0.96875
2017-03-02T17:55:10.182531: step 17535, loss 0.213499, acc 0.921875
2017-03-02T17:55:10.264941: step 17536, loss 0.188281, acc 0.921875
2017-03-02T17:55:10.338100: step 17537, loss 0.105192, acc 0.953125
2017-03-02T17:55:10.412276: step 17538, loss 0.148405, acc 0.953125
2017-03-02T17:55:10.479910: step 17539, loss 0.141099, acc 0.921875
2017-03-02T17:55:10.568951: step 17540, loss 0.116492, acc 0.9375
2017-03-02T17:55:10.648397: step 17541, loss 0.171542, acc 0.9375
2017-03-02T17:55:10.720106: step 17542, loss 0.145986, acc 0.9375
2017-03-02T17:55:10.805515: step 17543, loss 0.123581, acc 0.9375
2017-03-02T17:55:10.880507: step 17544, loss 0.0841345, acc 0.984375
2017-03-02T17:55:10.949099: step 17545, loss 0.12446, acc 0.9375
2017-03-02T17:55:11.029383: step 17546, loss 0.19846, acc 0.921875
2017-03-02T17:55:11.102238: step 17547, loss 0.187375, acc 0.9375
2017-03-02T17:55:11.175056: step 17548, loss 0.117899, acc 0.953125
2017-03-02T17:55:11.257724: step 17549, loss 0.18948, acc 0.90625
2017-03-02T17:55:11.325647: step 17550, loss 0.181891, acc 0.921875
2017-03-02T17:55:11.394139: step 17551, loss 0.124729, acc 0.984375
2017-03-02T17:55:11.465616: step 17552, loss 0.0731623, acc 0.984375
2017-03-02T17:55:11.539196: step 17553, loss 0.135174, acc 0.9375
2017-03-02T17:55:11.610708: step 17554, loss 0.153721, acc 0.921875
2017-03-02T17:55:11.684132: step 17555, loss 0.126772, acc 0.953125
2017-03-02T17:55:11.760164: step 17556, loss 0.215036, acc 0.90625
2017-03-02T17:55:11.834303: step 17557, loss 0.126211, acc 0.9375
2017-03-02T17:55:11.906625: step 17558, loss 0.127466, acc 0.953125
2017-03-02T17:55:11.981238: step 17559, loss 0.191488, acc 0.90625
2017-03-02T17:55:12.080379: step 17560, loss 0.1147, acc 0.96875
2017-03-02T17:55:12.151665: step 17561, loss 0.099386, acc 0.953125
2017-03-02T17:55:12.247495: step 17562, loss 0.112509, acc 0.921875
2017-03-02T17:55:12.312944: step 17563, loss 0.164539, acc 0.953125
2017-03-02T17:55:12.397560: step 17564, loss 0.142419, acc 0.921875
2017-03-02T17:55:12.475776: step 17565, loss 0.0992688, acc 0.96875
2017-03-02T17:55:12.549082: step 17566, loss 0.129627, acc 0.90625
2017-03-02T17:55:12.628604: step 17567, loss 0.144788, acc 0.9375
2017-03-02T17:55:12.699085: step 17568, loss 0.0960266, acc 0.9375
2017-03-02T17:55:12.768173: step 17569, loss 0.326313, acc 0.859375
2017-03-02T17:55:12.846506: step 17570, loss 0.155014, acc 0.9375
2017-03-02T17:55:12.931171: step 17571, loss 0.109235, acc 0.9375
2017-03-02T17:55:13.007270: step 17572, loss 0.21678, acc 0.921875
2017-03-02T17:55:13.084516: step 17573, loss 0.153314, acc 0.9375
2017-03-02T17:55:13.157871: step 17574, loss 0.0577335, acc 0.96875
2017-03-02T17:55:13.246192: step 17575, loss 0.137919, acc 0.9375
2017-03-02T17:55:13.323372: step 17576, loss 0.241189, acc 0.90625
2017-03-02T17:55:13.391360: step 17577, loss 0.202862, acc 0.9375
2017-03-02T17:55:13.460564: step 17578, loss 0.182555, acc 0.90625
2017-03-02T17:55:13.537937: step 17579, loss 0.0473823, acc 0.96875
2017-03-02T17:55:13.611812: step 17580, loss 0.138855, acc 0.9375
2017-03-02T17:55:13.688876: step 17581, loss 0.178371, acc 0.890625
2017-03-02T17:55:13.782031: step 17582, loss 0.101245, acc 0.953125
2017-03-02T17:55:13.862816: step 17583, loss 0.0857823, acc 0.953125
2017-03-02T17:55:13.940822: step 17584, loss 0.154678, acc 0.953125
2017-03-02T17:55:14.015765: step 17585, loss 0.182101, acc 0.921875
2017-03-02T17:55:14.085112: step 17586, loss 0.232119, acc 0.890625
2017-03-02T17:55:14.159826: step 17587, loss 0.26755, acc 0.859375
2017-03-02T17:55:14.235764: step 17588, loss 0.116592, acc 0.9375
2017-03-02T17:55:14.326887: step 17589, loss 0.162465, acc 0.90625
2017-03-02T17:55:14.396457: step 17590, loss 0.0556002, acc 0.96875
2017-03-02T17:55:14.473913: step 17591, loss 0.155608, acc 0.921875
2017-03-02T17:55:14.546502: step 17592, loss 0.146392, acc 0.953125
2017-03-02T17:55:14.619748: step 17593, loss 0.147873, acc 0.921875
2017-03-02T17:55:14.690100: step 17594, loss 0.151817, acc 0.9375
2017-03-02T17:55:14.758670: step 17595, loss 0.118177, acc 0.953125
2017-03-02T17:55:14.825372: step 17596, loss 0.144208, acc 0.9375
2017-03-02T17:55:14.894846: step 17597, loss 0.079535, acc 0.953125
2017-03-02T17:55:14.979599: step 17598, loss 0.231355, acc 0.890625
2017-03-02T17:55:15.051809: step 17599, loss 0.0991244, acc 0.9375
2017-03-02T17:55:15.128524: step 17600, loss 0.151827, acc 0.96875

Evaluation:
2017-03-02T17:55:15.163241: step 17600, loss 2.09814, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17600

2017-03-02T17:55:15.606821: step 17601, loss 0.144567, acc 0.9375
2017-03-02T17:55:15.679650: step 17602, loss 0.148264, acc 0.9375
2017-03-02T17:55:15.750062: step 17603, loss 0.119862, acc 0.953125
2017-03-02T17:55:15.822799: step 17604, loss 0.368375, acc 0.875
2017-03-02T17:55:15.909008: step 17605, loss 0.220146, acc 0.875
2017-03-02T17:55:15.973298: step 17606, loss 0.138079, acc 0.9375
2017-03-02T17:55:16.043794: step 17607, loss 0.123634, acc 0.9375
2017-03-02T17:55:16.121611: step 17608, loss 0.205735, acc 0.921875
2017-03-02T17:55:16.192676: step 17609, loss 0.130671, acc 0.921875
2017-03-02T17:55:16.264833: step 17610, loss 0.132435, acc 0.953125
2017-03-02T17:55:16.338823: step 17611, loss 0.102759, acc 0.9375
2017-03-02T17:55:16.414574: step 17612, loss 0.173536, acc 0.875
2017-03-02T17:55:16.491044: step 17613, loss 0.149362, acc 0.921875
2017-03-02T17:55:16.566308: step 17614, loss 0.0570769, acc 0.96875
2017-03-02T17:55:16.640220: step 17615, loss 0.156917, acc 0.953125
2017-03-02T17:55:16.714068: step 17616, loss 0.10628, acc 0.96875
2017-03-02T17:55:16.787996: step 17617, loss 0.109936, acc 0.9375
2017-03-02T17:55:16.856392: step 17618, loss 0.0616584, acc 0.984375
2017-03-02T17:55:16.926613: step 17619, loss 0.226107, acc 0.90625
2017-03-02T17:55:17.005830: step 17620, loss 0.317688, acc 0.859375
2017-03-02T17:55:17.081623: step 17621, loss 0.0759868, acc 0.96875
2017-03-02T17:55:17.155587: step 17622, loss 0.0696912, acc 0.96875
2017-03-02T17:55:17.228339: step 17623, loss 0.196961, acc 0.921875
2017-03-02T17:55:17.299786: step 17624, loss 0.131892, acc 0.9375
2017-03-02T17:55:17.370189: step 17625, loss 0.0728775, acc 0.984375
2017-03-02T17:55:17.442100: step 17626, loss 0.142807, acc 0.953125
2017-03-02T17:55:17.518329: step 17627, loss 0.086157, acc 0.96875
2017-03-02T17:55:17.583811: step 17628, loss 0.17329, acc 0.921875
2017-03-02T17:55:17.653717: step 17629, loss 0.27188, acc 0.84375
2017-03-02T17:55:17.726981: step 17630, loss 0.0996986, acc 0.921875
2017-03-02T17:55:17.794452: step 17631, loss 0.106019, acc 0.953125
2017-03-02T17:55:17.864148: step 17632, loss 0.323902, acc 0.84375
2017-03-02T17:55:17.934286: step 17633, loss 0.0930229, acc 0.953125
2017-03-02T17:55:18.008507: step 17634, loss 0.287081, acc 0.921875
2017-03-02T17:55:18.080717: step 17635, loss 0.304595, acc 0.90625
2017-03-02T17:55:18.150933: step 17636, loss 0.209244, acc 0.90625
2017-03-02T17:55:18.219682: step 17637, loss 0.209352, acc 0.90625
2017-03-02T17:55:18.288195: step 17638, loss 0.187122, acc 0.921875
2017-03-02T17:55:18.356003: step 17639, loss 0.155742, acc 0.921875
2017-03-02T17:55:18.430813: step 17640, loss 0.141141, acc 1
2017-03-02T17:55:18.524335: step 17641, loss 0.204776, acc 0.90625
2017-03-02T17:55:18.592356: step 17642, loss 0.13936, acc 0.9375
2017-03-02T17:55:18.671793: step 17643, loss 0.0953268, acc 0.953125
2017-03-02T17:55:18.750545: step 17644, loss 0.122621, acc 0.9375
2017-03-02T17:55:18.817901: step 17645, loss 0.206249, acc 0.90625
2017-03-02T17:55:18.894448: step 17646, loss 0.171418, acc 0.953125
2017-03-02T17:55:18.960407: step 17647, loss 0.189866, acc 0.921875
2017-03-02T17:55:19.026426: step 17648, loss 0.0874426, acc 0.953125
2017-03-02T17:55:19.108354: step 17649, loss 0.0614182, acc 0.984375
2017-03-02T17:55:19.184926: step 17650, loss 0.142553, acc 0.90625
2017-03-02T17:55:19.261944: step 17651, loss 0.160217, acc 0.921875
2017-03-02T17:55:19.340537: step 17652, loss 0.16449, acc 0.921875
2017-03-02T17:55:19.425183: step 17653, loss 0.211923, acc 0.90625
2017-03-02T17:55:19.509071: step 17654, loss 0.10099, acc 0.953125
2017-03-02T17:55:19.585314: step 17655, loss 0.148064, acc 0.9375
2017-03-02T17:55:19.653138: step 17656, loss 0.114572, acc 0.953125
2017-03-02T17:55:19.722416: step 17657, loss 0.160461, acc 0.9375
2017-03-02T17:55:19.793611: step 17658, loss 0.168243, acc 0.90625
2017-03-02T17:55:19.865260: step 17659, loss 0.180745, acc 0.90625
2017-03-02T17:55:19.938167: step 17660, loss 0.0809518, acc 0.9375
2017-03-02T17:55:20.009513: step 17661, loss 0.178116, acc 0.9375
2017-03-02T17:55:20.098328: step 17662, loss 0.19743, acc 0.890625
2017-03-02T17:55:20.170474: step 17663, loss 0.123405, acc 0.9375
2017-03-02T17:55:20.246398: step 17664, loss 0.096006, acc 0.953125
2017-03-02T17:55:20.332592: step 17665, loss 0.0784151, acc 0.953125
2017-03-02T17:55:20.400906: step 17666, loss 0.120539, acc 0.953125
2017-03-02T17:55:20.467284: step 17667, loss 0.130846, acc 0.921875
2017-03-02T17:55:20.540198: step 17668, loss 0.252364, acc 0.890625
2017-03-02T17:55:20.611274: step 17669, loss 0.322541, acc 0.875
2017-03-02T17:55:20.673616: step 17670, loss 0.0830496, acc 0.96875
2017-03-02T17:55:20.742153: step 17671, loss 0.100735, acc 0.96875
2017-03-02T17:55:20.813455: step 17672, loss 0.242133, acc 0.921875
2017-03-02T17:55:20.887015: step 17673, loss 0.0891644, acc 0.96875
2017-03-02T17:55:20.964647: step 17674, loss 0.115565, acc 0.953125
2017-03-02T17:55:21.033785: step 17675, loss 0.220241, acc 0.90625
2017-03-02T17:55:21.108410: step 17676, loss 0.107884, acc 0.96875
2017-03-02T17:55:21.192032: step 17677, loss 0.136404, acc 0.953125
2017-03-02T17:55:21.266693: step 17678, loss 0.121022, acc 0.953125
2017-03-02T17:55:21.339511: step 17679, loss 0.109818, acc 0.953125
2017-03-02T17:55:21.417375: step 17680, loss 0.146029, acc 0.9375
2017-03-02T17:55:21.487606: step 17681, loss 0.0735712, acc 0.96875
2017-03-02T17:55:21.557850: step 17682, loss 0.12546, acc 0.9375
2017-03-02T17:55:21.630074: step 17683, loss 0.169518, acc 0.90625
2017-03-02T17:55:21.701756: step 17684, loss 0.134361, acc 0.921875
2017-03-02T17:55:21.769254: step 17685, loss 0.120131, acc 0.96875
2017-03-02T17:55:21.839179: step 17686, loss 0.171005, acc 0.9375
2017-03-02T17:55:21.914054: step 17687, loss 0.125104, acc 0.9375
2017-03-02T17:55:21.992944: step 17688, loss 0.189042, acc 0.953125
2017-03-02T17:55:22.068090: step 17689, loss 0.153524, acc 0.9375
2017-03-02T17:55:22.145298: step 17690, loss 0.218053, acc 0.9375
2017-03-02T17:55:22.227343: step 17691, loss 0.157357, acc 0.953125
2017-03-02T17:55:22.302629: step 17692, loss 0.0890161, acc 0.953125
2017-03-02T17:55:22.379121: step 17693, loss 0.116132, acc 0.953125
2017-03-02T17:55:22.452210: step 17694, loss 0.218296, acc 0.921875
2017-03-02T17:55:22.522118: step 17695, loss 0.102492, acc 0.953125
2017-03-02T17:55:22.591655: step 17696, loss 0.10523, acc 0.9375
2017-03-02T17:55:22.674038: step 17697, loss 0.0965836, acc 0.953125
2017-03-02T17:55:22.746183: step 17698, loss 0.129, acc 0.9375
2017-03-02T17:55:22.815527: step 17699, loss 0.115378, acc 0.953125
2017-03-02T17:55:22.893557: step 17700, loss 0.155, acc 0.953125

Evaluation:
2017-03-02T17:55:22.930905: step 17700, loss 2.11546, acc 0.64672

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17700

2017-03-02T17:55:23.387310: step 17701, loss 0.147059, acc 0.9375
2017-03-02T17:55:23.458563: step 17702, loss 0.100062, acc 0.96875
2017-03-02T17:55:23.538131: step 17703, loss 0.213375, acc 0.9375
2017-03-02T17:55:23.613827: step 17704, loss 0.10563, acc 0.9375
2017-03-02T17:55:23.691889: step 17705, loss 0.174993, acc 0.9375
2017-03-02T17:55:23.761136: step 17706, loss 0.146207, acc 0.90625
2017-03-02T17:55:23.833946: step 17707, loss 0.288386, acc 0.921875
2017-03-02T17:55:23.903590: step 17708, loss 0.196408, acc 0.890625
2017-03-02T17:55:23.978462: step 17709, loss 0.34788, acc 0.828125
2017-03-02T17:55:24.053106: step 17710, loss 0.153639, acc 0.921875
2017-03-02T17:55:24.125326: step 17711, loss 0.0692924, acc 0.96875
2017-03-02T17:55:24.192004: step 17712, loss 0.148073, acc 0.9375
2017-03-02T17:55:24.272678: step 17713, loss 0.207094, acc 0.921875
2017-03-02T17:55:24.342131: step 17714, loss 0.16004, acc 0.953125
2017-03-02T17:55:24.424113: step 17715, loss 0.091847, acc 0.953125
2017-03-02T17:55:24.497675: step 17716, loss 0.107657, acc 0.953125
2017-03-02T17:55:24.564549: step 17717, loss 0.0816609, acc 0.96875
2017-03-02T17:55:24.629474: step 17718, loss 0.126319, acc 0.921875
2017-03-02T17:55:24.697018: step 17719, loss 0.223842, acc 0.890625
2017-03-02T17:55:24.771110: step 17720, loss 0.128079, acc 0.921875
2017-03-02T17:55:24.844554: step 17721, loss 0.11914, acc 0.953125
2017-03-02T17:55:24.920080: step 17722, loss 0.0981373, acc 0.921875
2017-03-02T17:55:24.994324: step 17723, loss 0.113193, acc 0.9375
2017-03-02T17:55:25.064467: step 17724, loss 0.14132, acc 0.953125
2017-03-02T17:55:25.150078: step 17725, loss 0.0863768, acc 0.96875
2017-03-02T17:55:25.221775: step 17726, loss 0.13646, acc 0.953125
2017-03-02T17:55:25.293027: step 17727, loss 0.171377, acc 0.90625
2017-03-02T17:55:25.363995: step 17728, loss 0.117872, acc 0.9375
2017-03-02T17:55:25.436462: step 17729, loss 0.141164, acc 0.953125
2017-03-02T17:55:25.522246: step 17730, loss 0.0557101, acc 0.984375
2017-03-02T17:55:25.597537: step 17731, loss 0.301832, acc 0.875
2017-03-02T17:55:25.669333: step 17732, loss 0.216799, acc 0.921875
2017-03-02T17:55:25.745308: step 17733, loss 0.23315, acc 0.875
2017-03-02T17:55:25.817764: step 17734, loss 0.15984, acc 0.921875
2017-03-02T17:55:25.894569: step 17735, loss 0.117548, acc 0.921875
2017-03-02T17:55:25.961079: step 17736, loss 0.194226, acc 0.90625
2017-03-02T17:55:26.022845: step 17737, loss 0.109807, acc 0.953125
2017-03-02T17:55:26.098156: step 17738, loss 0.210335, acc 0.890625
2017-03-02T17:55:26.166503: step 17739, loss 0.165558, acc 0.921875
2017-03-02T17:55:26.239543: step 17740, loss 0.0932861, acc 0.96875
2017-03-02T17:55:26.313799: step 17741, loss 0.0402065, acc 0.984375
2017-03-02T17:55:26.394476: step 17742, loss 0.0907039, acc 0.96875
2017-03-02T17:55:26.465266: step 17743, loss 0.140807, acc 0.9375
2017-03-02T17:55:26.529338: step 17744, loss 0.0714875, acc 0.953125
2017-03-02T17:55:26.598194: step 17745, loss 0.122648, acc 0.921875
2017-03-02T17:55:26.661930: step 17746, loss 0.158493, acc 0.953125
2017-03-02T17:55:26.735137: step 17747, loss 0.211531, acc 0.90625
2017-03-02T17:55:26.817456: step 17748, loss 0.160627, acc 0.9375
2017-03-02T17:55:26.899350: step 17749, loss 0.176813, acc 0.890625
2017-03-02T17:55:26.971990: step 17750, loss 0.0956978, acc 0.953125
2017-03-02T17:55:27.050371: step 17751, loss 0.240062, acc 0.890625
2017-03-02T17:55:27.124718: step 17752, loss 0.092491, acc 0.9375
2017-03-02T17:55:27.198086: step 17753, loss 0.163654, acc 0.90625
2017-03-02T17:55:27.273664: step 17754, loss 0.155253, acc 0.9375
2017-03-02T17:55:27.345631: step 17755, loss 0.0918932, acc 0.9375
2017-03-02T17:55:27.416400: step 17756, loss 0.103525, acc 0.96875
2017-03-02T17:55:27.491176: step 17757, loss 0.0655483, acc 0.984375
2017-03-02T17:55:27.564243: step 17758, loss 0.227363, acc 0.875
2017-03-02T17:55:27.639424: step 17759, loss 0.354305, acc 0.875
2017-03-02T17:55:27.714725: step 17760, loss 0.143092, acc 0.9375
2017-03-02T17:55:27.786237: step 17761, loss 0.23211, acc 0.875
2017-03-02T17:55:27.868002: step 17762, loss 0.189271, acc 0.9375
2017-03-02T17:55:27.938857: step 17763, loss 0.12509, acc 0.9375
2017-03-02T17:55:28.013280: step 17764, loss 0.0666196, acc 0.96875
2017-03-02T17:55:28.081675: step 17765, loss 0.351271, acc 0.859375
2017-03-02T17:55:28.153039: step 17766, loss 0.208341, acc 0.890625
2017-03-02T17:55:28.224094: step 17767, loss 0.151147, acc 0.96875
2017-03-02T17:55:28.295568: step 17768, loss 0.213067, acc 0.890625
2017-03-02T17:55:28.369580: step 17769, loss 0.0913898, acc 0.984375
2017-03-02T17:55:28.450295: step 17770, loss 0.289744, acc 0.859375
2017-03-02T17:55:28.519044: step 17771, loss 0.0855308, acc 0.953125
2017-03-02T17:55:28.597719: step 17772, loss 0.101054, acc 0.953125
2017-03-02T17:55:28.665194: step 17773, loss 0.205407, acc 0.9375
2017-03-02T17:55:28.740921: step 17774, loss 0.187325, acc 0.9375
2017-03-02T17:55:28.808322: step 17775, loss 0.316214, acc 0.875
2017-03-02T17:55:28.882627: step 17776, loss 0.159477, acc 0.9375
2017-03-02T17:55:28.951126: step 17777, loss 0.109841, acc 0.96875
2017-03-02T17:55:29.021802: step 17778, loss 0.103455, acc 0.953125
2017-03-02T17:55:29.091790: step 17779, loss 0.226736, acc 0.921875
2017-03-02T17:55:29.165139: step 17780, loss 0.280628, acc 0.921875
2017-03-02T17:55:29.257469: step 17781, loss 0.254436, acc 0.890625
2017-03-02T17:55:29.339122: step 17782, loss 0.190472, acc 0.9375
2017-03-02T17:55:29.409096: step 17783, loss 0.125551, acc 0.953125
2017-03-02T17:55:29.486748: step 17784, loss 0.19398, acc 0.875
2017-03-02T17:55:29.570148: step 17785, loss 0.188001, acc 0.90625
2017-03-02T17:55:29.650005: step 17786, loss 0.0751868, acc 0.984375
2017-03-02T17:55:29.724548: step 17787, loss 0.240639, acc 0.875
2017-03-02T17:55:29.795479: step 17788, loss 0.144035, acc 0.921875
2017-03-02T17:55:29.864665: step 17789, loss 0.189709, acc 0.90625
2017-03-02T17:55:29.939396: step 17790, loss 0.21464, acc 0.90625
2017-03-02T17:55:30.020945: step 17791, loss 0.251567, acc 0.90625
2017-03-02T17:55:30.090660: step 17792, loss 0.143152, acc 0.953125
2017-03-02T17:55:30.161776: step 17793, loss 0.0579047, acc 0.984375
2017-03-02T17:55:30.237628: step 17794, loss 0.28257, acc 0.9375
2017-03-02T17:55:30.321789: step 17795, loss 0.14549, acc 0.953125
2017-03-02T17:55:30.399782: step 17796, loss 0.156308, acc 0.921875
2017-03-02T17:55:30.481943: step 17797, loss 0.0829756, acc 0.953125
2017-03-02T17:55:30.555522: step 17798, loss 0.135641, acc 0.953125
2017-03-02T17:55:30.629828: step 17799, loss 0.177543, acc 0.921875
2017-03-02T17:55:30.707849: step 17800, loss 0.161034, acc 0.90625

Evaluation:
2017-03-02T17:55:30.730122: step 17800, loss 2.06659, acc 0.66186

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17800

2017-03-02T17:55:31.171660: step 17801, loss 0.357682, acc 0.859375
2017-03-02T17:55:31.251023: step 17802, loss 0.163456, acc 0.921875
2017-03-02T17:55:31.322033: step 17803, loss 0.301003, acc 0.84375
2017-03-02T17:55:31.402394: step 17804, loss 0.112229, acc 0.9375
2017-03-02T17:55:31.484200: step 17805, loss 0.118049, acc 0.953125
2017-03-02T17:55:31.556454: step 17806, loss 0.30992, acc 0.890625
2017-03-02T17:55:31.623558: step 17807, loss 0.151814, acc 0.90625
2017-03-02T17:55:31.698530: step 17808, loss 0.137496, acc 0.9375
2017-03-02T17:55:31.787041: step 17809, loss 0.10506, acc 0.96875
2017-03-02T17:55:31.861239: step 17810, loss 0.110057, acc 0.953125
2017-03-02T17:55:31.938274: step 17811, loss 0.153591, acc 0.9375
2017-03-02T17:55:32.014785: step 17812, loss 0.0317638, acc 1
2017-03-02T17:55:32.094864: step 17813, loss 0.154285, acc 0.9375
2017-03-02T17:55:32.177247: step 17814, loss 0.18788, acc 0.9375
2017-03-02T17:55:32.247180: step 17815, loss 0.156704, acc 0.921875
2017-03-02T17:55:32.319682: step 17816, loss 0.0819871, acc 0.953125
2017-03-02T17:55:32.397399: step 17817, loss 0.139167, acc 0.9375
2017-03-02T17:55:32.472631: step 17818, loss 0.151804, acc 0.953125
2017-03-02T17:55:32.549600: step 17819, loss 0.101091, acc 0.9375
2017-03-02T17:55:32.619953: step 17820, loss 0.108345, acc 0.953125
2017-03-02T17:55:32.692801: step 17821, loss 0.34955, acc 0.859375
2017-03-02T17:55:32.768387: step 17822, loss 0.0857408, acc 0.953125
2017-03-02T17:55:32.836293: step 17823, loss 0.153888, acc 0.9375
2017-03-02T17:55:32.902173: step 17824, loss 0.278482, acc 0.890625
2017-03-02T17:55:32.971399: step 17825, loss 0.174463, acc 0.921875
2017-03-02T17:55:33.043398: step 17826, loss 0.122838, acc 0.921875
2017-03-02T17:55:33.119007: step 17827, loss 0.188664, acc 0.9375
2017-03-02T17:55:33.196564: step 17828, loss 0.167345, acc 0.921875
2017-03-02T17:55:33.280766: step 17829, loss 0.242347, acc 0.875
2017-03-02T17:55:33.352650: step 17830, loss 0.176252, acc 0.953125
2017-03-02T17:55:33.426494: step 17831, loss 0.0620892, acc 0.96875
2017-03-02T17:55:33.497488: step 17832, loss 0.164569, acc 0.9375
2017-03-02T17:55:33.568404: step 17833, loss 0.0521928, acc 0.984375
2017-03-02T17:55:33.641532: step 17834, loss 0.150216, acc 0.953125
2017-03-02T17:55:33.730653: step 17835, loss 0.269899, acc 0.90625
2017-03-02T17:55:33.813133: step 17836, loss 3.94867e-05, acc 1
2017-03-02T17:55:33.897071: step 17837, loss 0.0928331, acc 0.96875
2017-03-02T17:55:33.970150: step 17838, loss 0.134375, acc 0.9375
2017-03-02T17:55:34.046411: step 17839, loss 0.111622, acc 0.9375
2017-03-02T17:55:34.126970: step 17840, loss 0.093004, acc 0.96875
2017-03-02T17:55:34.195865: step 17841, loss 0.166354, acc 0.921875
2017-03-02T17:55:34.264995: step 17842, loss 0.176346, acc 0.9375
2017-03-02T17:55:34.334479: step 17843, loss 0.165123, acc 0.921875
2017-03-02T17:55:34.407209: step 17844, loss 0.214273, acc 0.90625
2017-03-02T17:55:34.482491: step 17845, loss 0.11418, acc 0.953125
2017-03-02T17:55:34.553520: step 17846, loss 0.159401, acc 0.9375
2017-03-02T17:55:34.629250: step 17847, loss 0.0831012, acc 0.9375
2017-03-02T17:55:34.700746: step 17848, loss 0.187709, acc 0.921875
2017-03-02T17:55:34.773501: step 17849, loss 0.149972, acc 0.9375
2017-03-02T17:55:34.850673: step 17850, loss 0.182169, acc 0.890625
2017-03-02T17:55:34.922109: step 17851, loss 0.0785229, acc 0.96875
2017-03-02T17:55:34.991122: step 17852, loss 0.138949, acc 0.921875
2017-03-02T17:55:35.065622: step 17853, loss 0.0918911, acc 0.96875
2017-03-02T17:55:35.138068: step 17854, loss 0.280177, acc 0.875
2017-03-02T17:55:35.211195: step 17855, loss 0.106985, acc 0.953125
2017-03-02T17:55:35.285130: step 17856, loss 0.0650734, acc 0.984375
2017-03-02T17:55:35.375825: step 17857, loss 0.172086, acc 0.890625
2017-03-02T17:55:35.456527: step 17858, loss 0.0630203, acc 0.984375
2017-03-02T17:55:35.523329: step 17859, loss 0.0765065, acc 0.96875
2017-03-02T17:55:35.597524: step 17860, loss 0.169832, acc 0.90625
2017-03-02T17:55:35.670918: step 17861, loss 0.059091, acc 0.984375
2017-03-02T17:55:35.737561: step 17862, loss 0.0721229, acc 0.96875
2017-03-02T17:55:35.818226: step 17863, loss 0.165287, acc 0.9375
2017-03-02T17:55:35.914364: step 17864, loss 0.123585, acc 0.921875
2017-03-02T17:55:35.990114: step 17865, loss 0.103038, acc 0.921875
2017-03-02T17:55:36.066046: step 17866, loss 0.120552, acc 0.953125
2017-03-02T17:55:36.143368: step 17867, loss 0.178283, acc 0.9375
2017-03-02T17:55:36.226779: step 17868, loss 0.14785, acc 0.953125
2017-03-02T17:55:36.301835: step 17869, loss 0.258028, acc 0.90625
2017-03-02T17:55:36.374576: step 17870, loss 0.0562085, acc 0.953125
2017-03-02T17:55:36.452011: step 17871, loss 0.27102, acc 0.859375
2017-03-02T17:55:36.526223: step 17872, loss 0.249635, acc 0.890625
2017-03-02T17:55:36.604038: step 17873, loss 0.153705, acc 0.953125
2017-03-02T17:55:36.673480: step 17874, loss 0.14809, acc 0.90625
2017-03-02T17:55:36.754792: step 17875, loss 0.257296, acc 0.875
2017-03-02T17:55:36.830119: step 17876, loss 0.157983, acc 0.953125
2017-03-02T17:55:36.913145: step 17877, loss 0.155256, acc 0.921875
2017-03-02T17:55:36.985962: step 17878, loss 0.190007, acc 0.921875
2017-03-02T17:55:37.056601: step 17879, loss 0.117552, acc 0.96875
2017-03-02T17:55:37.130785: step 17880, loss 0.0799586, acc 0.984375
2017-03-02T17:55:37.204967: step 17881, loss 0.0801994, acc 0.9375
2017-03-02T17:55:37.278394: step 17882, loss 0.0643046, acc 0.96875
2017-03-02T17:55:37.351357: step 17883, loss 0.18321, acc 0.921875
2017-03-02T17:55:37.425202: step 17884, loss 0.19178, acc 0.921875
2017-03-02T17:55:37.499429: step 17885, loss 0.136169, acc 0.953125
2017-03-02T17:55:37.574086: step 17886, loss 0.139946, acc 0.953125
2017-03-02T17:55:37.646311: step 17887, loss 0.118188, acc 0.96875
2017-03-02T17:55:37.713306: step 17888, loss 0.124022, acc 0.9375
2017-03-02T17:55:37.786161: step 17889, loss 0.141789, acc 0.90625
2017-03-02T17:55:37.857987: step 17890, loss 0.167916, acc 0.953125
2017-03-02T17:55:37.934811: step 17891, loss 0.229435, acc 0.890625
2017-03-02T17:55:38.006859: step 17892, loss 0.195472, acc 0.890625
2017-03-02T17:55:38.078887: step 17893, loss 0.0929384, acc 1
2017-03-02T17:55:38.146441: step 17894, loss 0.214013, acc 0.890625
2017-03-02T17:55:38.223062: step 17895, loss 0.117162, acc 0.96875
2017-03-02T17:55:38.295840: step 17896, loss 0.129608, acc 0.921875
2017-03-02T17:55:38.368932: step 17897, loss 0.236771, acc 0.890625
2017-03-02T17:55:38.445072: step 17898, loss 0.10085, acc 0.953125
2017-03-02T17:55:38.523753: step 17899, loss 0.0653978, acc 0.96875
2017-03-02T17:55:38.603987: step 17900, loss 0.219847, acc 0.921875

Evaluation:
2017-03-02T17:55:38.638533: step 17900, loss 2.17778, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-17900

2017-03-02T17:55:39.127644: step 17901, loss 0.220343, acc 0.921875
2017-03-02T17:55:39.201762: step 17902, loss 0.0965355, acc 0.9375
2017-03-02T17:55:39.281477: step 17903, loss 0.169048, acc 0.921875
2017-03-02T17:55:39.354693: step 17904, loss 0.145856, acc 0.90625
2017-03-02T17:55:39.419769: step 17905, loss 0.129701, acc 0.96875
2017-03-02T17:55:39.495926: step 17906, loss 0.121952, acc 0.9375
2017-03-02T17:55:39.574321: step 17907, loss 0.206782, acc 0.875
2017-03-02T17:55:39.652022: step 17908, loss 0.142091, acc 0.9375
2017-03-02T17:55:39.719839: step 17909, loss 0.153231, acc 0.921875
2017-03-02T17:55:39.787482: step 17910, loss 0.12833, acc 0.953125
2017-03-02T17:55:39.858559: step 17911, loss 0.150455, acc 0.9375
2017-03-02T17:55:39.941042: step 17912, loss 0.161794, acc 0.921875
2017-03-02T17:55:40.012727: step 17913, loss 0.165823, acc 0.9375
2017-03-02T17:55:40.089133: step 17914, loss 0.0635001, acc 0.984375
2017-03-02T17:55:40.163441: step 17915, loss 0.172346, acc 0.921875
2017-03-02T17:55:40.233797: step 17916, loss 0.190769, acc 0.9375
2017-03-02T17:55:40.310142: step 17917, loss 0.222022, acc 0.890625
2017-03-02T17:55:40.391549: step 17918, loss 0.12676, acc 0.953125
2017-03-02T17:55:40.461730: step 17919, loss 0.149377, acc 0.921875
2017-03-02T17:55:40.535159: step 17920, loss 0.166329, acc 0.953125
2017-03-02T17:55:40.615835: step 17921, loss 0.184638, acc 0.9375
2017-03-02T17:55:40.695313: step 17922, loss 0.139764, acc 0.9375
2017-03-02T17:55:40.772565: step 17923, loss 0.206775, acc 0.9375
2017-03-02T17:55:40.843658: step 17924, loss 0.157887, acc 0.9375
2017-03-02T17:55:40.920858: step 17925, loss 0.118425, acc 0.953125
2017-03-02T17:55:40.996188: step 17926, loss 0.0995803, acc 0.96875
2017-03-02T17:55:41.075858: step 17927, loss 0.19983, acc 0.90625
2017-03-02T17:55:41.139896: step 17928, loss 0.198409, acc 0.921875
2017-03-02T17:55:41.202814: step 17929, loss 0.1955, acc 0.921875
2017-03-02T17:55:41.282076: step 17930, loss 0.255356, acc 0.890625
2017-03-02T17:55:41.356109: step 17931, loss 0.0679061, acc 0.96875
2017-03-02T17:55:41.428198: step 17932, loss 0.198946, acc 0.890625
2017-03-02T17:55:41.503284: step 17933, loss 0.148655, acc 0.921875
2017-03-02T17:55:41.606079: step 17934, loss 0.208407, acc 0.859375
2017-03-02T17:55:41.693059: step 17935, loss 0.120191, acc 0.953125
2017-03-02T17:55:41.759455: step 17936, loss 0.146384, acc 0.921875
2017-03-02T17:55:41.833478: step 17937, loss 0.167475, acc 0.921875
2017-03-02T17:55:41.903724: step 17938, loss 0.112333, acc 0.953125
2017-03-02T17:55:41.979827: step 17939, loss 0.200615, acc 0.921875
2017-03-02T17:55:42.048592: step 17940, loss 0.0871135, acc 0.96875
2017-03-02T17:55:42.122836: step 17941, loss 0.126322, acc 0.953125
2017-03-02T17:55:42.213383: step 17942, loss 0.146278, acc 0.921875
2017-03-02T17:55:42.285161: step 17943, loss 0.0846518, acc 0.984375
2017-03-02T17:55:42.366437: step 17944, loss 0.206274, acc 0.921875
2017-03-02T17:55:42.442618: step 17945, loss 0.254143, acc 0.90625
2017-03-02T17:55:42.506637: step 17946, loss 0.296182, acc 0.875
2017-03-02T17:55:42.570046: step 17947, loss 0.14671, acc 0.9375
2017-03-02T17:55:42.630620: step 17948, loss 0.166652, acc 0.9375
2017-03-02T17:55:42.700796: step 17949, loss 0.121461, acc 0.9375
2017-03-02T17:55:42.776931: step 17950, loss 0.169624, acc 0.921875
2017-03-02T17:55:42.840314: step 17951, loss 0.0968949, acc 0.984375
2017-03-02T17:55:42.905562: step 17952, loss 0.0891746, acc 0.96875
2017-03-02T17:55:42.980916: step 17953, loss 0.251631, acc 0.90625
2017-03-02T17:55:43.052418: step 17954, loss 0.130078, acc 0.96875
2017-03-02T17:55:43.129182: step 17955, loss 0.201261, acc 0.921875
2017-03-02T17:55:43.207093: step 17956, loss 0.0580727, acc 0.96875
2017-03-02T17:55:43.279837: step 17957, loss 0.144147, acc 0.9375
2017-03-02T17:55:43.362859: step 17958, loss 0.177941, acc 0.90625
2017-03-02T17:55:43.435055: step 17959, loss 0.18933, acc 0.9375
2017-03-02T17:55:43.511875: step 17960, loss 0.171072, acc 0.90625
2017-03-02T17:55:43.587178: step 17961, loss 0.174241, acc 0.9375
2017-03-02T17:55:43.677285: step 17962, loss 0.133719, acc 0.921875
2017-03-02T17:55:43.757033: step 17963, loss 0.25979, acc 0.90625
2017-03-02T17:55:43.830425: step 17964, loss 0.117261, acc 0.9375
2017-03-02T17:55:43.900080: step 17965, loss 0.225656, acc 0.859375
2017-03-02T17:55:43.965216: step 17966, loss 0.189794, acc 0.90625
2017-03-02T17:55:44.035583: step 17967, loss 0.133164, acc 0.921875
2017-03-02T17:55:44.110261: step 17968, loss 0.168641, acc 0.921875
2017-03-02T17:55:44.169666: step 17969, loss 0.124927, acc 0.953125
2017-03-02T17:55:44.240142: step 17970, loss 0.255083, acc 0.890625
2017-03-02T17:55:44.300837: step 17971, loss 0.099967, acc 0.953125
2017-03-02T17:55:44.372072: step 17972, loss 0.172408, acc 0.921875
2017-03-02T17:55:44.437891: step 17973, loss 0.172823, acc 0.921875
2017-03-02T17:55:44.522216: step 17974, loss 0.204115, acc 0.921875
2017-03-02T17:55:44.593954: step 17975, loss 0.165034, acc 0.9375
2017-03-02T17:55:44.678309: step 17976, loss 0.140919, acc 0.9375
2017-03-02T17:55:44.755024: step 17977, loss 0.102892, acc 0.953125
2017-03-02T17:55:44.826474: step 17978, loss 0.167147, acc 0.9375
2017-03-02T17:55:44.898778: step 17979, loss 0.180405, acc 0.875
2017-03-02T17:55:44.970835: step 17980, loss 0.237887, acc 0.875
2017-03-02T17:55:45.042442: step 17981, loss 0.0424955, acc 1
2017-03-02T17:55:45.120977: step 17982, loss 0.136941, acc 0.9375
2017-03-02T17:55:45.195737: step 17983, loss 0.154165, acc 0.875
2017-03-02T17:55:45.268863: step 17984, loss 0.188653, acc 0.921875
2017-03-02T17:55:45.336671: step 17985, loss 0.208967, acc 0.90625
2017-03-02T17:55:45.402370: step 17986, loss 0.105486, acc 0.953125
2017-03-02T17:55:45.474417: step 17987, loss 0.0791502, acc 0.984375
2017-03-02T17:55:45.545449: step 17988, loss 0.183859, acc 0.90625
2017-03-02T17:55:45.614748: step 17989, loss 0.148074, acc 0.9375
2017-03-02T17:55:45.694517: step 17990, loss 0.163364, acc 0.921875
2017-03-02T17:55:45.784444: step 17991, loss 0.187622, acc 0.9375
2017-03-02T17:55:45.856728: step 17992, loss 0.0442952, acc 0.984375
2017-03-02T17:55:45.943034: step 17993, loss 0.122306, acc 0.9375
2017-03-02T17:55:46.012469: step 17994, loss 0.252406, acc 0.859375
2017-03-02T17:55:46.084731: step 17995, loss 0.144996, acc 0.921875
2017-03-02T17:55:46.157718: step 17996, loss 0.157299, acc 0.921875
2017-03-02T17:55:46.227033: step 17997, loss 0.107678, acc 0.984375
2017-03-02T17:55:46.301813: step 17998, loss 0.0668458, acc 0.984375
2017-03-02T17:55:46.373831: step 17999, loss 0.115005, acc 0.953125
2017-03-02T17:55:46.444872: step 18000, loss 0.209527, acc 0.890625

Evaluation:
2017-03-02T17:55:46.482207: step 18000, loss 2.16493, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18000

2017-03-02T17:55:46.958352: step 18001, loss 0.13177, acc 0.9375
2017-03-02T17:55:47.032186: step 18002, loss 0.115695, acc 0.9375
2017-03-02T17:55:47.105897: step 18003, loss 0.205846, acc 0.953125
2017-03-02T17:55:47.181037: step 18004, loss 0.101209, acc 0.9375
2017-03-02T17:55:47.251505: step 18005, loss 0.100069, acc 0.984375
2017-03-02T17:55:47.316610: step 18006, loss 0.241711, acc 0.90625
2017-03-02T17:55:47.395052: step 18007, loss 0.151542, acc 0.90625
2017-03-02T17:55:47.464174: step 18008, loss 0.18886, acc 0.921875
2017-03-02T17:55:47.539268: step 18009, loss 0.152853, acc 0.9375
2017-03-02T17:55:47.609265: step 18010, loss 0.0706307, acc 0.96875
2017-03-02T17:55:47.681458: step 18011, loss 0.162017, acc 0.921875
2017-03-02T17:55:47.754819: step 18012, loss 0.127499, acc 0.921875
2017-03-02T17:55:47.826078: step 18013, loss 0.101137, acc 0.96875
2017-03-02T17:55:47.902880: step 18014, loss 0.0755797, acc 0.953125
2017-03-02T17:55:47.972877: step 18015, loss 0.153645, acc 0.9375
2017-03-02T17:55:48.039010: step 18016, loss 0.273634, acc 0.890625
2017-03-02T17:55:48.114762: step 18017, loss 0.164882, acc 0.921875
2017-03-02T17:55:48.201967: step 18018, loss 0.230254, acc 0.921875
2017-03-02T17:55:48.270150: step 18019, loss 0.103962, acc 0.96875
2017-03-02T17:55:48.341581: step 18020, loss 0.172505, acc 0.9375
2017-03-02T17:55:48.411673: step 18021, loss 0.241798, acc 0.90625
2017-03-02T17:55:48.482841: step 18022, loss 0.237214, acc 0.90625
2017-03-02T17:55:48.555908: step 18023, loss 0.210794, acc 0.921875
2017-03-02T17:55:48.623664: step 18024, loss 0.201859, acc 0.890625
2017-03-02T17:55:48.691614: step 18025, loss 0.0787001, acc 0.9375
2017-03-02T17:55:48.761097: step 18026, loss 0.182612, acc 0.90625
2017-03-02T17:55:48.833436: step 18027, loss 0.163632, acc 0.953125
2017-03-02T17:55:48.907294: step 18028, loss 0.209839, acc 0.875
2017-03-02T17:55:48.976371: step 18029, loss 0.145423, acc 0.953125
2017-03-02T17:55:49.066362: step 18030, loss 0.210261, acc 0.921875
2017-03-02T17:55:49.139310: step 18031, loss 0.22745, acc 0.90625
2017-03-02T17:55:49.204719: step 18032, loss 0.000371122, acc 1
2017-03-02T17:55:49.281846: step 18033, loss 0.188896, acc 0.921875
2017-03-02T17:55:49.352821: step 18034, loss 0.125608, acc 0.953125
2017-03-02T17:55:49.423410: step 18035, loss 0.193387, acc 0.921875
2017-03-02T17:55:49.491815: step 18036, loss 0.160724, acc 0.9375
2017-03-02T17:55:49.568959: step 18037, loss 0.0739216, acc 0.96875
2017-03-02T17:55:49.639749: step 18038, loss 0.093403, acc 0.96875
2017-03-02T17:55:49.722769: step 18039, loss 0.120411, acc 0.9375
2017-03-02T17:55:49.795712: step 18040, loss 0.0985323, acc 0.953125
2017-03-02T17:55:49.875605: step 18041, loss 0.146446, acc 0.9375
2017-03-02T17:55:49.949051: step 18042, loss 0.0664454, acc 1
2017-03-02T17:55:50.028757: step 18043, loss 0.0863321, acc 0.953125
2017-03-02T17:55:50.096588: step 18044, loss 0.249582, acc 0.921875
2017-03-02T17:55:50.165908: step 18045, loss 0.209634, acc 0.921875
2017-03-02T17:55:50.231221: step 18046, loss 0.0987713, acc 0.96875
2017-03-02T17:55:50.306753: step 18047, loss 0.0853452, acc 0.96875
2017-03-02T17:55:50.384194: step 18048, loss 0.226556, acc 0.90625
2017-03-02T17:55:50.462517: step 18049, loss 0.147412, acc 0.9375
2017-03-02T17:55:50.536135: step 18050, loss 0.1174, acc 0.953125
2017-03-02T17:55:50.611139: step 18051, loss 0.113148, acc 0.9375
2017-03-02T17:55:50.683029: step 18052, loss 0.102062, acc 0.953125
2017-03-02T17:55:50.763647: step 18053, loss 0.187327, acc 0.90625
2017-03-02T17:55:50.838456: step 18054, loss 0.202389, acc 0.875
2017-03-02T17:55:50.907215: step 18055, loss 0.0824878, acc 0.953125
2017-03-02T17:55:50.988831: step 18056, loss 0.153426, acc 0.921875
2017-03-02T17:55:51.061981: step 18057, loss 0.176197, acc 0.921875
2017-03-02T17:55:51.135746: step 18058, loss 0.160338, acc 0.9375
2017-03-02T17:55:51.200614: step 18059, loss 0.0901489, acc 0.921875
2017-03-02T17:55:51.271346: step 18060, loss 0.145856, acc 0.9375
2017-03-02T17:55:51.350708: step 18061, loss 0.17333, acc 0.921875
2017-03-02T17:55:51.428434: step 18062, loss 0.0769216, acc 0.984375
2017-03-02T17:55:51.498976: step 18063, loss 0.158423, acc 0.921875
2017-03-02T17:55:51.560817: step 18064, loss 0.109034, acc 0.9375
2017-03-02T17:55:51.635515: step 18065, loss 0.144155, acc 0.96875
2017-03-02T17:55:51.708785: step 18066, loss 0.208344, acc 0.90625
2017-03-02T17:55:51.785567: step 18067, loss 0.153347, acc 0.953125
2017-03-02T17:55:51.857073: step 18068, loss 0.207356, acc 0.890625
2017-03-02T17:55:51.927946: step 18069, loss 0.0752445, acc 0.96875
2017-03-02T17:55:52.004295: step 18070, loss 0.159513, acc 0.921875
2017-03-02T17:55:52.081850: step 18071, loss 0.103268, acc 0.953125
2017-03-02T17:55:52.155105: step 18072, loss 0.157779, acc 0.90625
2017-03-02T17:55:52.232460: step 18073, loss 0.0834216, acc 0.953125
2017-03-02T17:55:52.293478: step 18074, loss 0.215957, acc 0.875
2017-03-02T17:55:52.365270: step 18075, loss 0.0852198, acc 0.953125
2017-03-02T17:55:52.437653: step 18076, loss 0.107922, acc 0.953125
2017-03-02T17:55:52.532112: step 18077, loss 0.184382, acc 0.890625
2017-03-02T17:55:52.622060: step 18078, loss 0.200521, acc 0.90625
2017-03-02T17:55:52.703868: step 18079, loss 0.182406, acc 0.890625
2017-03-02T17:55:52.785330: step 18080, loss 0.229532, acc 0.875
2017-03-02T17:55:52.853024: step 18081, loss 0.0490946, acc 1
2017-03-02T17:55:52.925071: step 18082, loss 0.115132, acc 0.953125
2017-03-02T17:55:52.993762: step 18083, loss 0.143709, acc 0.9375
2017-03-02T17:55:53.068756: step 18084, loss 0.119284, acc 0.953125
2017-03-02T17:55:53.142023: step 18085, loss 0.205123, acc 0.9375
2017-03-02T17:55:53.215265: step 18086, loss 0.271751, acc 0.890625
2017-03-02T17:55:53.288594: step 18087, loss 0.171391, acc 0.921875
2017-03-02T17:55:53.365946: step 18088, loss 0.296318, acc 0.875
2017-03-02T17:55:53.443616: step 18089, loss 0.127, acc 0.96875
2017-03-02T17:55:53.521344: step 18090, loss 0.198352, acc 0.90625
2017-03-02T17:55:53.598808: step 18091, loss 0.0992615, acc 0.953125
2017-03-02T17:55:53.667343: step 18092, loss 0.243117, acc 0.875
2017-03-02T17:55:53.737538: step 18093, loss 0.166295, acc 0.921875
2017-03-02T17:55:53.816463: step 18094, loss 0.224739, acc 0.875
2017-03-02T17:55:53.901430: step 18095, loss 0.143361, acc 0.9375
2017-03-02T17:55:53.972841: step 18096, loss 0.126738, acc 0.921875
2017-03-02T17:55:54.044049: step 18097, loss 0.144852, acc 0.96875
2017-03-02T17:55:54.119891: step 18098, loss 0.251704, acc 0.84375
2017-03-02T17:55:54.191620: step 18099, loss 0.0716637, acc 0.96875
2017-03-02T17:55:54.258320: step 18100, loss 0.150992, acc 0.9375

Evaluation:
2017-03-02T17:55:54.284895: step 18100, loss 2.17829, acc 0.650324

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18100

2017-03-02T17:55:54.724181: step 18101, loss 0.103221, acc 0.984375
2017-03-02T17:55:54.796176: step 18102, loss 0.0969251, acc 0.9375
2017-03-02T17:55:54.873941: step 18103, loss 0.189957, acc 0.90625
2017-03-02T17:55:54.939877: step 18104, loss 0.169983, acc 0.90625
2017-03-02T17:55:55.016151: step 18105, loss 0.0836995, acc 0.96875
2017-03-02T17:55:55.087731: step 18106, loss 0.134824, acc 0.953125
2017-03-02T17:55:55.161100: step 18107, loss 0.271486, acc 0.875
2017-03-02T17:55:55.263663: step 18108, loss 0.110242, acc 0.9375
2017-03-02T17:55:55.368106: step 18109, loss 0.169843, acc 0.90625
2017-03-02T17:55:55.436947: step 18110, loss 0.17393, acc 0.921875
2017-03-02T17:55:55.510156: step 18111, loss 0.213458, acc 0.90625
2017-03-02T17:55:55.584104: step 18112, loss 0.184094, acc 0.90625
2017-03-02T17:55:55.651523: step 18113, loss 0.25284, acc 0.90625
2017-03-02T17:55:55.718827: step 18114, loss 0.136836, acc 0.9375
2017-03-02T17:55:55.797343: step 18115, loss 0.194689, acc 0.90625
2017-03-02T17:55:55.868020: step 18116, loss 0.166766, acc 0.953125
2017-03-02T17:55:55.939792: step 18117, loss 0.139637, acc 0.90625
2017-03-02T17:55:56.030679: step 18118, loss 0.150434, acc 0.953125
2017-03-02T17:55:56.115427: step 18119, loss 0.133904, acc 0.9375
2017-03-02T17:55:56.198229: step 18120, loss 0.113819, acc 0.953125
2017-03-02T17:55:56.267890: step 18121, loss 0.121797, acc 0.921875
2017-03-02T17:55:56.335402: step 18122, loss 0.0946592, acc 0.953125
2017-03-02T17:55:56.431204: step 18123, loss 0.143465, acc 0.9375
2017-03-02T17:55:56.533634: step 18124, loss 0.31053, acc 0.875
2017-03-02T17:55:56.610622: step 18125, loss 0.161251, acc 0.9375
2017-03-02T17:55:56.693986: step 18126, loss 0.10106, acc 0.96875
2017-03-02T17:55:56.768104: step 18127, loss 0.1367, acc 0.953125
2017-03-02T17:55:56.839382: step 18128, loss 0.124961, acc 0.9375
2017-03-02T17:55:56.916666: step 18129, loss 0.110886, acc 0.96875
2017-03-02T17:55:56.991951: step 18130, loss 0.182752, acc 0.90625
2017-03-02T17:55:57.076539: step 18131, loss 0.111381, acc 0.953125
2017-03-02T17:55:57.146815: step 18132, loss 0.205151, acc 0.921875
2017-03-02T17:55:57.229275: step 18133, loss 0.126807, acc 0.953125
2017-03-02T17:55:57.313663: step 18134, loss 0.162253, acc 0.921875
2017-03-02T17:55:57.388440: step 18135, loss 0.0781971, acc 0.96875
2017-03-02T17:55:57.464737: step 18136, loss 0.192835, acc 0.890625
2017-03-02T17:55:57.529001: step 18137, loss 0.150428, acc 0.953125
2017-03-02T17:55:57.601505: step 18138, loss 0.123445, acc 0.9375
2017-03-02T17:55:57.672820: step 18139, loss 0.19286, acc 0.890625
2017-03-02T17:55:57.745373: step 18140, loss 0.112615, acc 0.9375
2017-03-02T17:55:57.811948: step 18141, loss 0.184864, acc 0.90625
2017-03-02T17:55:57.889707: step 18142, loss 0.117937, acc 0.953125
2017-03-02T17:55:57.962579: step 18143, loss 0.10863, acc 0.9375
2017-03-02T17:55:58.034348: step 18144, loss 0.19907, acc 0.921875
2017-03-02T17:55:58.103818: step 18145, loss 0.0889415, acc 0.953125
2017-03-02T17:55:58.173546: step 18146, loss 0.0882131, acc 0.96875
2017-03-02T17:55:58.242415: step 18147, loss 0.124362, acc 0.96875
2017-03-02T17:55:58.308941: step 18148, loss 0.0350511, acc 0.984375
2017-03-02T17:55:58.387168: step 18149, loss 0.185317, acc 0.90625
2017-03-02T17:55:58.456533: step 18150, loss 0.154065, acc 0.9375
2017-03-02T17:55:58.523883: step 18151, loss 0.348152, acc 0.828125
2017-03-02T17:55:58.603916: step 18152, loss 0.25724, acc 0.90625
2017-03-02T17:55:58.676366: step 18153, loss 0.181658, acc 0.890625
2017-03-02T17:55:58.747046: step 18154, loss 0.201095, acc 0.890625
2017-03-02T17:55:58.825893: step 18155, loss 0.120705, acc 0.953125
2017-03-02T17:55:58.904541: step 18156, loss 0.258704, acc 0.875
2017-03-02T17:55:58.976034: step 18157, loss 0.230388, acc 0.96875
2017-03-02T17:55:59.046997: step 18158, loss 0.0512929, acc 0.984375
2017-03-02T17:55:59.112644: step 18159, loss 0.21366, acc 0.921875
2017-03-02T17:55:59.177395: step 18160, loss 0.182237, acc 0.890625
2017-03-02T17:55:59.248760: step 18161, loss 0.10851, acc 0.921875
2017-03-02T17:55:59.321912: step 18162, loss 0.0964424, acc 0.953125
2017-03-02T17:55:59.389245: step 18163, loss 0.140606, acc 0.96875
2017-03-02T17:55:59.463106: step 18164, loss 0.280359, acc 0.875
2017-03-02T17:55:59.540037: step 18165, loss 0.0753018, acc 0.96875
2017-03-02T17:55:59.614135: step 18166, loss 0.254753, acc 0.890625
2017-03-02T17:55:59.690569: step 18167, loss 0.1013, acc 0.96875
2017-03-02T17:55:59.764085: step 18168, loss 0.190756, acc 0.9375
2017-03-02T17:55:59.843553: step 18169, loss 0.140797, acc 0.953125
2017-03-02T17:55:59.914593: step 18170, loss 0.158812, acc 0.921875
2017-03-02T17:55:59.991750: step 18171, loss 0.215407, acc 0.890625
2017-03-02T17:56:00.066107: step 18172, loss 0.0848085, acc 0.953125
2017-03-02T17:56:00.139009: step 18173, loss 0.135989, acc 0.953125
2017-03-02T17:56:00.212334: step 18174, loss 0.120044, acc 0.921875
2017-03-02T17:56:00.281847: step 18175, loss 0.151633, acc 0.953125
2017-03-02T17:56:00.352916: step 18176, loss 0.13632, acc 0.953125
2017-03-02T17:56:00.423883: step 18177, loss 0.113195, acc 0.953125
2017-03-02T17:56:00.492750: step 18178, loss 0.109227, acc 0.96875
2017-03-02T17:56:00.563435: step 18179, loss 0.213567, acc 0.921875
2017-03-02T17:56:00.663067: step 18180, loss 0.135384, acc 0.953125
2017-03-02T17:56:00.730824: step 18181, loss 0.184649, acc 0.90625
2017-03-02T17:56:00.799426: step 18182, loss 0.190685, acc 0.9375
2017-03-02T17:56:00.873903: step 18183, loss 0.103661, acc 0.953125
2017-03-02T17:56:00.947192: step 18184, loss 0.173993, acc 0.9375
2017-03-02T17:56:01.016233: step 18185, loss 0.0709783, acc 0.96875
2017-03-02T17:56:01.103880: step 18186, loss 0.173534, acc 0.921875
2017-03-02T17:56:01.181893: step 18187, loss 0.0397622, acc 0.984375
2017-03-02T17:56:01.252789: step 18188, loss 0.158921, acc 0.953125
2017-03-02T17:56:01.335355: step 18189, loss 0.159381, acc 0.953125
2017-03-02T17:56:01.409819: step 18190, loss 0.0976509, acc 0.96875
2017-03-02T17:56:01.483632: step 18191, loss 0.0919274, acc 0.953125
2017-03-02T17:56:01.553929: step 18192, loss 0.264087, acc 0.90625
2017-03-02T17:56:01.626352: step 18193, loss 0.0737028, acc 0.96875
2017-03-02T17:56:01.696804: step 18194, loss 0.225733, acc 0.921875
2017-03-02T17:56:01.769651: step 18195, loss 0.23158, acc 0.875
2017-03-02T17:56:01.846622: step 18196, loss 0.202034, acc 0.921875
2017-03-02T17:56:01.922225: step 18197, loss 0.148908, acc 0.953125
2017-03-02T17:56:02.018480: step 18198, loss 0.107401, acc 0.96875
2017-03-02T17:56:02.086515: step 18199, loss 0.139463, acc 0.9375
2017-03-02T17:56:02.163004: step 18200, loss 0.19181, acc 0.90625

Evaluation:
2017-03-02T17:56:02.202822: step 18200, loss 2.19726, acc 0.650324

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18200

2017-03-02T17:56:02.647295: step 18201, loss 0.159633, acc 0.953125
2017-03-02T17:56:02.719859: step 18202, loss 0.0703015, acc 0.953125
2017-03-02T17:56:02.794220: step 18203, loss 0.150306, acc 0.9375
2017-03-02T17:56:02.865011: step 18204, loss 0.170714, acc 0.90625
2017-03-02T17:56:02.942697: step 18205, loss 0.149635, acc 0.9375
2017-03-02T17:56:03.019280: step 18206, loss 0.163117, acc 0.9375
2017-03-02T17:56:03.092084: step 18207, loss 0.186671, acc 0.90625
2017-03-02T17:56:03.180538: step 18208, loss 0.13157, acc 0.96875
2017-03-02T17:56:03.246334: step 18209, loss 0.206912, acc 0.921875
2017-03-02T17:56:03.315892: step 18210, loss 0.235273, acc 0.90625
2017-03-02T17:56:03.387918: step 18211, loss 0.0925305, acc 0.96875
2017-03-02T17:56:03.461393: step 18212, loss 0.247105, acc 0.90625
2017-03-02T17:56:03.545051: step 18213, loss 0.125807, acc 0.9375
2017-03-02T17:56:03.621487: step 18214, loss 0.145275, acc 0.9375
2017-03-02T17:56:03.710143: step 18215, loss 0.215857, acc 0.953125
2017-03-02T17:56:03.788741: step 18216, loss 0.180567, acc 0.953125
2017-03-02T17:56:03.863188: step 18217, loss 0.167399, acc 0.9375
2017-03-02T17:56:03.933358: step 18218, loss 0.234171, acc 0.890625
2017-03-02T17:56:04.000993: step 18219, loss 0.174006, acc 0.90625
2017-03-02T17:56:04.078475: step 18220, loss 0.192108, acc 0.921875
2017-03-02T17:56:04.147564: step 18221, loss 0.127714, acc 0.96875
2017-03-02T17:56:04.231994: step 18222, loss 0.202662, acc 0.890625
2017-03-02T17:56:04.303209: step 18223, loss 0.107325, acc 0.96875
2017-03-02T17:56:04.373492: step 18224, loss 0.128515, acc 0.953125
2017-03-02T17:56:04.447575: step 18225, loss 0.226763, acc 0.90625
2017-03-02T17:56:04.518281: step 18226, loss 0.0648622, acc 1
2017-03-02T17:56:04.594764: step 18227, loss 0.249505, acc 0.9375
2017-03-02T17:56:04.661225: step 18228, loss 0.110407, acc 1
2017-03-02T17:56:04.736223: step 18229, loss 0.115407, acc 0.953125
2017-03-02T17:56:04.820911: step 18230, loss 0.0995096, acc 0.96875
2017-03-02T17:56:04.892908: step 18231, loss 0.172368, acc 0.921875
2017-03-02T17:56:04.963478: step 18232, loss 0.147649, acc 0.921875
2017-03-02T17:56:05.040465: step 18233, loss 0.0701706, acc 0.984375
2017-03-02T17:56:05.116165: step 18234, loss 0.133828, acc 0.96875
2017-03-02T17:56:05.188475: step 18235, loss 0.0413658, acc 1
2017-03-02T17:56:05.262779: step 18236, loss 0.186138, acc 0.90625
2017-03-02T17:56:05.351906: step 18237, loss 0.171859, acc 0.921875
2017-03-02T17:56:05.428653: step 18238, loss 0.137168, acc 0.90625
2017-03-02T17:56:05.510862: step 18239, loss 0.133963, acc 0.921875
2017-03-02T17:56:05.578823: step 18240, loss 0.0826323, acc 0.96875
2017-03-02T17:56:05.658287: step 18241, loss 0.111525, acc 0.984375
2017-03-02T17:56:05.736798: step 18242, loss 0.229823, acc 0.921875
2017-03-02T17:56:05.801992: step 18243, loss 0.232047, acc 0.875
2017-03-02T17:56:05.879519: step 18244, loss 0.123376, acc 0.953125
2017-03-02T17:56:05.950483: step 18245, loss 0.185627, acc 0.90625
2017-03-02T17:56:06.021306: step 18246, loss 0.121158, acc 0.9375
2017-03-02T17:56:06.090055: step 18247, loss 0.188477, acc 0.921875
2017-03-02T17:56:06.165002: step 18248, loss 0.0824295, acc 0.9375
2017-03-02T17:56:06.248847: step 18249, loss 0.190234, acc 0.953125
2017-03-02T17:56:06.329122: step 18250, loss 0.136951, acc 0.953125
2017-03-02T17:56:06.404564: step 18251, loss 0.196291, acc 0.90625
2017-03-02T17:56:06.478909: step 18252, loss 0.0953866, acc 0.96875
2017-03-02T17:56:06.546039: step 18253, loss 0.123425, acc 0.9375
2017-03-02T17:56:06.618431: step 18254, loss 0.255635, acc 0.84375
2017-03-02T17:56:06.685371: step 18255, loss 0.139948, acc 0.953125
2017-03-02T17:56:06.752470: step 18256, loss 0.119068, acc 0.953125
2017-03-02T17:56:06.824828: step 18257, loss 0.151505, acc 0.90625
2017-03-02T17:56:06.891660: step 18258, loss 0.285235, acc 0.84375
2017-03-02T17:56:06.961314: step 18259, loss 0.111746, acc 0.9375
2017-03-02T17:56:07.033961: step 18260, loss 0.152518, acc 0.921875
2017-03-02T17:56:07.114934: step 18261, loss 0.127338, acc 0.953125
2017-03-02T17:56:07.193186: step 18262, loss 0.121172, acc 0.96875
2017-03-02T17:56:07.264778: step 18263, loss 0.200994, acc 0.9375
2017-03-02T17:56:07.331897: step 18264, loss 0.141593, acc 0.953125
2017-03-02T17:56:07.401684: step 18265, loss 0.105437, acc 0.953125
2017-03-02T17:56:07.470549: step 18266, loss 0.213594, acc 0.890625
2017-03-02T17:56:07.537043: step 18267, loss 0.10856, acc 0.953125
2017-03-02T17:56:07.612007: step 18268, loss 0.155697, acc 0.921875
2017-03-02T17:56:07.684236: step 18269, loss 0.1036, acc 0.953125
2017-03-02T17:56:07.756777: step 18270, loss 0.105297, acc 0.9375
2017-03-02T17:56:07.833096: step 18271, loss 0.126268, acc 0.953125
2017-03-02T17:56:07.904622: step 18272, loss 0.231721, acc 0.875
2017-03-02T17:56:07.978842: step 18273, loss 0.124393, acc 0.953125
2017-03-02T17:56:08.048796: step 18274, loss 0.156614, acc 0.9375
2017-03-02T17:56:08.119697: step 18275, loss 0.138208, acc 0.9375
2017-03-02T17:56:08.200462: step 18276, loss 0.236235, acc 0.875
2017-03-02T17:56:08.273278: step 18277, loss 0.118366, acc 0.953125
2017-03-02T17:56:08.346012: step 18278, loss 0.0605192, acc 0.96875
2017-03-02T17:56:08.409690: step 18279, loss 0.121305, acc 0.921875
2017-03-02T17:56:08.478509: step 18280, loss 0.126129, acc 0.953125
2017-03-02T17:56:08.560176: step 18281, loss 0.0583086, acc 0.96875
2017-03-02T17:56:08.631599: step 18282, loss 0.0957445, acc 0.96875
2017-03-02T17:56:08.701918: step 18283, loss 0.0933026, acc 0.953125
2017-03-02T17:56:08.773735: step 18284, loss 0.148332, acc 0.9375
2017-03-02T17:56:08.847171: step 18285, loss 0.159177, acc 0.921875
2017-03-02T17:56:08.924373: step 18286, loss 0.245095, acc 0.859375
2017-03-02T17:56:09.003959: step 18287, loss 0.294911, acc 0.90625
2017-03-02T17:56:09.078881: step 18288, loss 0.107595, acc 0.96875
2017-03-02T17:56:09.150459: step 18289, loss 0.228422, acc 0.9375
2017-03-02T17:56:09.224907: step 18290, loss 0.11144, acc 0.953125
2017-03-02T17:56:09.305159: step 18291, loss 0.244141, acc 0.921875
2017-03-02T17:56:09.379676: step 18292, loss 0.162475, acc 0.90625
2017-03-02T17:56:09.452045: step 18293, loss 0.12767, acc 0.96875
2017-03-02T17:56:09.523182: step 18294, loss 0.235741, acc 0.875
2017-03-02T17:56:09.600756: step 18295, loss 0.164422, acc 0.9375
2017-03-02T17:56:09.674464: step 18296, loss 0.301127, acc 0.875
2017-03-02T17:56:09.756438: step 18297, loss 0.0670689, acc 0.96875
2017-03-02T17:56:09.831952: step 18298, loss 0.0887578, acc 0.9375
2017-03-02T17:56:09.906134: step 18299, loss 0.0898469, acc 0.9375
2017-03-02T17:56:09.986482: step 18300, loss 0.0981051, acc 0.984375

Evaluation:
2017-03-02T17:56:10.016246: step 18300, loss 2.18249, acc 0.630137

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18300

2017-03-02T17:56:10.477810: step 18301, loss 0.177829, acc 0.9375
2017-03-02T17:56:10.548939: step 18302, loss 0.189827, acc 0.921875
2017-03-02T17:56:10.618835: step 18303, loss 0.0351434, acc 0.984375
2017-03-02T17:56:10.695575: step 18304, loss 0.101627, acc 0.96875
2017-03-02T17:56:10.770535: step 18305, loss 0.203922, acc 0.921875
2017-03-02T17:56:10.848846: step 18306, loss 0.333311, acc 0.78125
2017-03-02T17:56:10.923279: step 18307, loss 0.141132, acc 0.953125
2017-03-02T17:56:10.998523: step 18308, loss 0.076767, acc 0.984375
2017-03-02T17:56:11.069263: step 18309, loss 0.229038, acc 0.90625
2017-03-02T17:56:11.143155: step 18310, loss 0.290967, acc 0.9375
2017-03-02T17:56:11.219833: step 18311, loss 0.10723, acc 0.9375
2017-03-02T17:56:11.299841: step 18312, loss 0.136647, acc 0.9375
2017-03-02T17:56:11.375313: step 18313, loss 0.113383, acc 0.9375
2017-03-02T17:56:11.444289: step 18314, loss 0.129522, acc 0.96875
2017-03-02T17:56:11.515417: step 18315, loss 0.148978, acc 0.921875
2017-03-02T17:56:11.593787: step 18316, loss 0.0829741, acc 0.96875
2017-03-02T17:56:11.663994: step 18317, loss 0.155598, acc 0.953125
2017-03-02T17:56:11.747052: step 18318, loss 0.121764, acc 0.96875
2017-03-02T17:56:11.823190: step 18319, loss 0.0331951, acc 0.984375
2017-03-02T17:56:11.895637: step 18320, loss 0.226086, acc 0.921875
2017-03-02T17:56:11.967886: step 18321, loss 0.165074, acc 0.890625
2017-03-02T17:56:12.041154: step 18322, loss 0.126233, acc 0.9375
2017-03-02T17:56:12.112455: step 18323, loss 0.168551, acc 0.9375
2017-03-02T17:56:12.184532: step 18324, loss 0.104903, acc 0.953125
2017-03-02T17:56:12.264682: step 18325, loss 0.195831, acc 0.9375
2017-03-02T17:56:12.340099: step 18326, loss 0.079172, acc 0.96875
2017-03-02T17:56:12.413863: step 18327, loss 0.18574, acc 0.9375
2017-03-02T17:56:12.487974: step 18328, loss 0.0687161, acc 0.984375
2017-03-02T17:56:12.563022: step 18329, loss 0.237207, acc 0.875
2017-03-02T17:56:12.630909: step 18330, loss 0.190908, acc 0.90625
2017-03-02T17:56:12.707181: step 18331, loss 0.159863, acc 0.921875
2017-03-02T17:56:12.783985: step 18332, loss 0.12072, acc 0.9375
2017-03-02T17:56:12.862728: step 18333, loss 0.267795, acc 0.859375
2017-03-02T17:56:12.933226: step 18334, loss 0.0663003, acc 0.96875
2017-03-02T17:56:13.002973: step 18335, loss 0.0694259, acc 0.984375
2017-03-02T17:56:13.084828: step 18336, loss 0.136002, acc 0.921875
2017-03-02T17:56:13.164584: step 18337, loss 0.0542102, acc 0.984375
2017-03-02T17:56:13.238033: step 18338, loss 0.0737032, acc 1
2017-03-02T17:56:13.314305: step 18339, loss 0.300296, acc 0.875
2017-03-02T17:56:13.392471: step 18340, loss 0.369757, acc 0.859375
2017-03-02T17:56:13.474175: step 18341, loss 0.0710043, acc 0.984375
2017-03-02T17:56:13.549387: step 18342, loss 0.136161, acc 0.953125
2017-03-02T17:56:13.619809: step 18343, loss 0.269333, acc 0.921875
2017-03-02T17:56:13.697210: step 18344, loss 0.129765, acc 0.9375
2017-03-02T17:56:13.772056: step 18345, loss 0.21538, acc 0.90625
2017-03-02T17:56:13.844577: step 18346, loss 0.0683557, acc 1
2017-03-02T17:56:13.915709: step 18347, loss 0.0635444, acc 1
2017-03-02T17:56:13.996769: step 18348, loss 0.286737, acc 0.90625
2017-03-02T17:56:14.070779: step 18349, loss 0.0601184, acc 0.96875
2017-03-02T17:56:14.148402: step 18350, loss 0.178702, acc 0.921875
2017-03-02T17:56:14.218134: step 18351, loss 0.264093, acc 0.90625
2017-03-02T17:56:14.297563: step 18352, loss 0.213493, acc 0.84375
2017-03-02T17:56:14.366011: step 18353, loss 0.13591, acc 0.953125
2017-03-02T17:56:14.445123: step 18354, loss 0.250465, acc 0.90625
2017-03-02T17:56:14.522232: step 18355, loss 0.228383, acc 0.875
2017-03-02T17:56:14.599155: step 18356, loss 0.15015, acc 0.90625
2017-03-02T17:56:14.673585: step 18357, loss 0.145806, acc 0.9375
2017-03-02T17:56:14.747166: step 18358, loss 0.202426, acc 0.890625
2017-03-02T17:56:14.819842: step 18359, loss 0.159629, acc 0.890625
2017-03-02T17:56:14.892459: step 18360, loss 0.11957, acc 0.9375
2017-03-02T17:56:14.957262: step 18361, loss 0.223579, acc 0.90625
2017-03-02T17:56:15.029513: step 18362, loss 0.128265, acc 0.9375
2017-03-02T17:56:15.103981: step 18363, loss 0.273485, acc 0.875
2017-03-02T17:56:15.198411: step 18364, loss 0.0863345, acc 0.96875
2017-03-02T17:56:15.274097: step 18365, loss 0.142791, acc 0.96875
2017-03-02T17:56:15.343081: step 18366, loss 0.221236, acc 0.859375
2017-03-02T17:56:15.410090: step 18367, loss 0.159608, acc 0.953125
2017-03-02T17:56:15.481616: step 18368, loss 0.0998584, acc 0.96875
2017-03-02T17:56:15.544966: step 18369, loss 0.120827, acc 0.953125
2017-03-02T17:56:15.615941: step 18370, loss 0.180366, acc 0.90625
2017-03-02T17:56:15.686015: step 18371, loss 0.135605, acc 0.953125
2017-03-02T17:56:15.758859: step 18372, loss 0.118426, acc 0.90625
2017-03-02T17:56:15.833184: step 18373, loss 0.12055, acc 0.953125
2017-03-02T17:56:15.910254: step 18374, loss 0.109053, acc 0.96875
2017-03-02T17:56:15.989602: step 18375, loss 0.117222, acc 0.953125
2017-03-02T17:56:16.063147: step 18376, loss 0.130472, acc 0.953125
2017-03-02T17:56:16.146099: step 18377, loss 0.280295, acc 0.890625
2017-03-02T17:56:16.218366: step 18378, loss 0.221681, acc 0.90625
2017-03-02T17:56:16.293796: step 18379, loss 0.25829, acc 0.875
2017-03-02T17:56:16.359118: step 18380, loss 0.149462, acc 0.921875
2017-03-02T17:56:16.444041: step 18381, loss 0.112186, acc 0.953125
2017-03-02T17:56:16.514498: step 18382, loss 0.0723765, acc 0.96875
2017-03-02T17:56:16.582481: step 18383, loss 0.17908, acc 0.9375
2017-03-02T17:56:16.661919: step 18384, loss 0.255439, acc 0.890625
2017-03-02T17:56:16.735575: step 18385, loss 0.163888, acc 0.96875
2017-03-02T17:56:16.811844: step 18386, loss 0.135688, acc 0.90625
2017-03-02T17:56:16.886733: step 18387, loss 0.204883, acc 0.890625
2017-03-02T17:56:16.965552: step 18388, loss 0.19237, acc 0.90625
2017-03-02T17:56:17.035359: step 18389, loss 0.100258, acc 0.984375
2017-03-02T17:56:17.112579: step 18390, loss 0.251698, acc 0.90625
2017-03-02T17:56:17.194652: step 18391, loss 0.134502, acc 0.9375
2017-03-02T17:56:17.287610: step 18392, loss 0.140283, acc 0.921875
2017-03-02T17:56:17.358609: step 18393, loss 0.133046, acc 0.921875
2017-03-02T17:56:17.440695: step 18394, loss 0.104734, acc 0.953125
2017-03-02T17:56:17.522326: step 18395, loss 0.0691406, acc 0.953125
2017-03-02T17:56:17.601641: step 18396, loss 0.227678, acc 0.890625
2017-03-02T17:56:17.667983: step 18397, loss 0.189653, acc 0.921875
2017-03-02T17:56:17.742437: step 18398, loss 0.191634, acc 0.921875
2017-03-02T17:56:17.818163: step 18399, loss 0.127983, acc 0.953125
2017-03-02T17:56:17.894941: step 18400, loss 0.21141, acc 0.921875

Evaluation:
2017-03-02T17:56:17.929403: step 18400, loss 2.1462, acc 0.63951

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18400

2017-03-02T17:56:18.375440: step 18401, loss 0.110641, acc 0.953125
2017-03-02T17:56:18.443588: step 18402, loss 0.0944348, acc 0.96875
2017-03-02T17:56:18.519418: step 18403, loss 0.126768, acc 0.96875
2017-03-02T17:56:18.592375: step 18404, loss 0.19736, acc 0.921875
2017-03-02T17:56:18.664598: step 18405, loss 0.061626, acc 0.96875
2017-03-02T17:56:18.745330: step 18406, loss 0.163156, acc 0.9375
2017-03-02T17:56:18.822918: step 18407, loss 0.127759, acc 0.9375
2017-03-02T17:56:18.894442: step 18408, loss 0.0750836, acc 0.96875
2017-03-02T17:56:18.967046: step 18409, loss 0.167825, acc 0.921875
2017-03-02T17:56:19.036691: step 18410, loss 0.156972, acc 0.9375
2017-03-02T17:56:19.103900: step 18411, loss 0.191807, acc 0.90625
2017-03-02T17:56:19.179577: step 18412, loss 0.3283, acc 0.921875
2017-03-02T17:56:19.252088: step 18413, loss 0.123263, acc 0.9375
2017-03-02T17:56:19.335881: step 18414, loss 0.117187, acc 0.953125
2017-03-02T17:56:19.409129: step 18415, loss 0.154189, acc 0.9375
2017-03-02T17:56:19.482036: step 18416, loss 0.152259, acc 0.953125
2017-03-02T17:56:19.556091: step 18417, loss 0.133115, acc 0.953125
2017-03-02T17:56:19.627972: step 18418, loss 0.113628, acc 0.9375
2017-03-02T17:56:19.702642: step 18419, loss 0.17822, acc 0.921875
2017-03-02T17:56:19.775334: step 18420, loss 0.259614, acc 0.859375
2017-03-02T17:56:19.852498: step 18421, loss 0.220486, acc 0.90625
2017-03-02T17:56:19.926221: step 18422, loss 0.205755, acc 0.9375
2017-03-02T17:56:19.997904: step 18423, loss 0.179112, acc 0.90625
2017-03-02T17:56:20.067559: step 18424, loss 0.00283652, acc 1
2017-03-02T17:56:20.148674: step 18425, loss 0.171024, acc 0.953125
2017-03-02T17:56:20.223966: step 18426, loss 0.0752431, acc 0.953125
2017-03-02T17:56:20.297964: step 18427, loss 0.0771872, acc 0.953125
2017-03-02T17:56:20.367042: step 18428, loss 0.110515, acc 0.96875
2017-03-02T17:56:20.441793: step 18429, loss 0.144125, acc 0.953125
2017-03-02T17:56:20.527651: step 18430, loss 0.119092, acc 0.96875
2017-03-02T17:56:20.603741: step 18431, loss 0.137854, acc 0.9375
2017-03-02T17:56:20.670869: step 18432, loss 0.0363253, acc 1
2017-03-02T17:56:20.733158: step 18433, loss 0.0720157, acc 0.96875
2017-03-02T17:56:20.803912: step 18434, loss 0.0638988, acc 0.984375
2017-03-02T17:56:20.877038: step 18435, loss 0.125381, acc 0.921875
2017-03-02T17:56:20.953351: step 18436, loss 0.120071, acc 0.96875
2017-03-02T17:56:21.032293: step 18437, loss 0.126982, acc 0.953125
2017-03-02T17:56:21.100397: step 18438, loss 0.0686276, acc 0.96875
2017-03-02T17:56:21.168329: step 18439, loss 0.111843, acc 0.953125
2017-03-02T17:56:21.240921: step 18440, loss 0.0849658, acc 0.96875
2017-03-02T17:56:21.322944: step 18441, loss 0.179978, acc 0.9375
2017-03-02T17:56:21.395462: step 18442, loss 0.194397, acc 0.90625
2017-03-02T17:56:21.469392: step 18443, loss 0.149485, acc 0.90625
2017-03-02T17:56:21.543006: step 18444, loss 0.19247, acc 0.90625
2017-03-02T17:56:21.607325: step 18445, loss 0.161775, acc 0.921875
2017-03-02T17:56:21.677362: step 18446, loss 0.224931, acc 0.859375
2017-03-02T17:56:21.749318: step 18447, loss 0.0940932, acc 0.953125
2017-03-02T17:56:21.820577: step 18448, loss 0.135068, acc 0.9375
2017-03-02T17:56:21.891146: step 18449, loss 0.164437, acc 0.90625
2017-03-02T17:56:21.967534: step 18450, loss 0.0728436, acc 0.984375
2017-03-02T17:56:22.039623: step 18451, loss 0.131111, acc 0.9375
2017-03-02T17:56:22.109505: step 18452, loss 0.149562, acc 0.953125
2017-03-02T17:56:22.183608: step 18453, loss 0.212309, acc 0.921875
2017-03-02T17:56:22.250205: step 18454, loss 0.122484, acc 0.96875
2017-03-02T17:56:22.320090: step 18455, loss 0.186887, acc 0.953125
2017-03-02T17:56:22.402997: step 18456, loss 0.054241, acc 0.984375
2017-03-02T17:56:22.473240: step 18457, loss 0.0889195, acc 0.984375
2017-03-02T17:56:22.554040: step 18458, loss 0.229761, acc 0.859375
2017-03-02T17:56:22.627313: step 18459, loss 0.215868, acc 0.921875
2017-03-02T17:56:22.689977: step 18460, loss 0.107294, acc 0.953125
2017-03-02T17:56:22.759297: step 18461, loss 0.203006, acc 0.921875
2017-03-02T17:56:22.847733: step 18462, loss 0.114255, acc 0.921875
2017-03-02T17:56:22.927228: step 18463, loss 0.2379, acc 0.921875
2017-03-02T17:56:22.999083: step 18464, loss 0.0664093, acc 0.96875
2017-03-02T17:56:23.078015: step 18465, loss 0.0976319, acc 0.953125
2017-03-02T17:56:23.150984: step 18466, loss 0.107492, acc 0.9375
2017-03-02T17:56:23.228425: step 18467, loss 0.119415, acc 0.953125
2017-03-02T17:56:23.321080: step 18468, loss 0.0902022, acc 0.96875
2017-03-02T17:56:23.395563: step 18469, loss 0.106287, acc 0.96875
2017-03-02T17:56:23.473145: step 18470, loss 0.116598, acc 0.9375
2017-03-02T17:56:23.550040: step 18471, loss 0.218258, acc 0.90625
2017-03-02T17:56:23.643574: step 18472, loss 0.167135, acc 0.921875
2017-03-02T17:56:23.723168: step 18473, loss 0.164731, acc 0.921875
2017-03-02T17:56:23.798430: step 18474, loss 0.0836941, acc 0.96875
2017-03-02T17:56:23.862091: step 18475, loss 0.283432, acc 0.84375
2017-03-02T17:56:23.931236: step 18476, loss 0.212132, acc 0.890625
2017-03-02T17:56:24.004873: step 18477, loss 0.137429, acc 0.921875
2017-03-02T17:56:24.078501: step 18478, loss 0.195662, acc 0.90625
2017-03-02T17:56:24.153560: step 18479, loss 0.162969, acc 0.921875
2017-03-02T17:56:24.230183: step 18480, loss 0.183005, acc 0.90625
2017-03-02T17:56:24.308187: step 18481, loss 0.195265, acc 0.890625
2017-03-02T17:56:24.373239: step 18482, loss 0.255617, acc 0.90625
2017-03-02T17:56:24.444852: step 18483, loss 0.120197, acc 0.96875
2017-03-02T17:56:24.512177: step 18484, loss 0.138559, acc 0.953125
2017-03-02T17:56:24.575336: step 18485, loss 0.16849, acc 0.90625
2017-03-02T17:56:24.644063: step 18486, loss 0.172412, acc 0.921875
2017-03-02T17:56:24.722717: step 18487, loss 0.197823, acc 0.9375
2017-03-02T17:56:24.799001: step 18488, loss 0.204213, acc 0.890625
2017-03-02T17:56:24.875407: step 18489, loss 0.187208, acc 0.921875
2017-03-02T17:56:24.944675: step 18490, loss 0.161995, acc 0.921875
2017-03-02T17:56:25.017346: step 18491, loss 0.233733, acc 0.953125
2017-03-02T17:56:25.098054: step 18492, loss 0.213066, acc 0.90625
2017-03-02T17:56:25.178072: step 18493, loss 0.167631, acc 0.921875
2017-03-02T17:56:25.259891: step 18494, loss 0.106501, acc 0.921875
2017-03-02T17:56:25.327339: step 18495, loss 0.128327, acc 0.9375
2017-03-02T17:56:25.402324: step 18496, loss 0.115585, acc 0.9375
2017-03-02T17:56:25.488603: step 18497, loss 0.33615, acc 0.875
2017-03-02T17:56:25.562563: step 18498, loss 0.070668, acc 0.984375
2017-03-02T17:56:25.637777: step 18499, loss 0.135607, acc 0.9375
2017-03-02T17:56:25.708816: step 18500, loss 0.224777, acc 0.890625

Evaluation:
2017-03-02T17:56:25.744908: step 18500, loss 2.22825, acc 0.638068

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18500

2017-03-02T17:56:26.202825: step 18501, loss 0.0571244, acc 0.96875
2017-03-02T17:56:26.284721: step 18502, loss 0.136257, acc 0.953125
2017-03-02T17:56:26.364564: step 18503, loss 0.194367, acc 0.921875
2017-03-02T17:56:26.438837: step 18504, loss 0.265373, acc 0.859375
2017-03-02T17:56:26.513670: step 18505, loss 0.105709, acc 0.953125
2017-03-02T17:56:26.586473: step 18506, loss 0.140307, acc 0.9375
2017-03-02T17:56:26.658460: step 18507, loss 0.232119, acc 0.875
2017-03-02T17:56:26.733772: step 18508, loss 0.152822, acc 0.90625
2017-03-02T17:56:26.807947: step 18509, loss 0.198838, acc 0.921875
2017-03-02T17:56:26.877552: step 18510, loss 0.0727181, acc 0.96875
2017-03-02T17:56:26.958755: step 18511, loss 0.0926309, acc 0.953125
2017-03-02T17:56:27.031047: step 18512, loss 0.223705, acc 0.875
2017-03-02T17:56:27.100507: step 18513, loss 0.0658181, acc 0.96875
2017-03-02T17:56:27.175655: step 18514, loss 0.112519, acc 0.953125
2017-03-02T17:56:27.251737: step 18515, loss 0.117869, acc 0.9375
2017-03-02T17:56:27.321453: step 18516, loss 0.176653, acc 0.921875
2017-03-02T17:56:27.390779: step 18517, loss 0.236001, acc 0.9375
2017-03-02T17:56:27.464112: step 18518, loss 0.247373, acc 0.890625
2017-03-02T17:56:27.541586: step 18519, loss 0.165432, acc 0.890625
2017-03-02T17:56:27.618082: step 18520, loss 0.134335, acc 0.921875
2017-03-02T17:56:27.691209: step 18521, loss 0.0887859, acc 0.96875
2017-03-02T17:56:27.756462: step 18522, loss 0.161872, acc 0.953125
2017-03-02T17:56:27.832189: step 18523, loss 0.243434, acc 0.90625
2017-03-02T17:56:27.904106: step 18524, loss 0.149013, acc 0.953125
2017-03-02T17:56:27.970848: step 18525, loss 0.108517, acc 0.921875
2017-03-02T17:56:28.039872: step 18526, loss 0.0734567, acc 0.984375
2017-03-02T17:56:28.112186: step 18527, loss 0.119497, acc 0.953125
2017-03-02T17:56:28.183158: step 18528, loss 0.194606, acc 0.921875
2017-03-02T17:56:28.255567: step 18529, loss 0.202969, acc 0.90625
2017-03-02T17:56:28.325809: step 18530, loss 0.14398, acc 0.9375
2017-03-02T17:56:28.396655: step 18531, loss 0.200442, acc 0.90625
2017-03-02T17:56:28.464246: step 18532, loss 0.189397, acc 0.921875
2017-03-02T17:56:28.539934: step 18533, loss 0.123737, acc 0.921875
2017-03-02T17:56:28.616707: step 18534, loss 0.117691, acc 0.96875
2017-03-02T17:56:28.684715: step 18535, loss 0.210143, acc 0.90625
2017-03-02T17:56:28.765655: step 18536, loss 0.121003, acc 0.921875
2017-03-02T17:56:28.842453: step 18537, loss 0.122883, acc 0.9375
2017-03-02T17:56:28.913089: step 18538, loss 0.146329, acc 0.9375
2017-03-02T17:56:28.986264: step 18539, loss 0.104582, acc 0.953125
2017-03-02T17:56:29.057174: step 18540, loss 0.135567, acc 0.953125
2017-03-02T17:56:29.130859: step 18541, loss 0.217635, acc 0.921875
2017-03-02T17:56:29.200216: step 18542, loss 0.274644, acc 0.890625
2017-03-02T17:56:29.274487: step 18543, loss 0.105944, acc 0.96875
2017-03-02T17:56:29.347564: step 18544, loss 0.122933, acc 0.9375
2017-03-02T17:56:29.412086: step 18545, loss 0.156821, acc 0.9375
2017-03-02T17:56:29.484027: step 18546, loss 0.128295, acc 0.953125
2017-03-02T17:56:29.558438: step 18547, loss 0.1684, acc 0.90625
2017-03-02T17:56:29.633823: step 18548, loss 0.105252, acc 0.9375
2017-03-02T17:56:29.711230: step 18549, loss 0.169328, acc 0.921875
2017-03-02T17:56:29.787929: step 18550, loss 0.255474, acc 0.890625
2017-03-02T17:56:29.861439: step 18551, loss 0.183773, acc 0.90625
2017-03-02T17:56:29.933197: step 18552, loss 0.0944954, acc 0.953125
2017-03-02T17:56:30.012123: step 18553, loss 0.171895, acc 0.953125
2017-03-02T17:56:30.085446: step 18554, loss 0.0909591, acc 0.9375
2017-03-02T17:56:30.149248: step 18555, loss 0.0441709, acc 0.984375
2017-03-02T17:56:30.220946: step 18556, loss 0.175297, acc 0.9375
2017-03-02T17:56:30.294955: step 18557, loss 0.227866, acc 0.921875
2017-03-02T17:56:30.371847: step 18558, loss 0.108271, acc 0.96875
2017-03-02T17:56:30.450312: step 18559, loss 0.17855, acc 0.9375
2017-03-02T17:56:30.522056: step 18560, loss 0.094236, acc 0.953125
2017-03-02T17:56:30.586028: step 18561, loss 0.121258, acc 0.953125
2017-03-02T17:56:30.657604: step 18562, loss 0.130909, acc 0.90625
2017-03-02T17:56:30.722509: step 18563, loss 0.171868, acc 0.953125
2017-03-02T17:56:30.783594: step 18564, loss 0.119675, acc 0.953125
2017-03-02T17:56:30.860151: step 18565, loss 0.123597, acc 0.9375
2017-03-02T17:56:30.932992: step 18566, loss 0.102405, acc 0.96875
2017-03-02T17:56:30.998107: step 18567, loss 0.0480975, acc 0.984375
2017-03-02T17:56:31.074930: step 18568, loss 0.143599, acc 0.953125
2017-03-02T17:56:31.145198: step 18569, loss 0.110167, acc 0.96875
2017-03-02T17:56:31.213517: step 18570, loss 0.0875207, acc 0.984375
2017-03-02T17:56:31.279311: step 18571, loss 0.164606, acc 0.921875
2017-03-02T17:56:31.352441: step 18572, loss 0.0920007, acc 0.953125
2017-03-02T17:56:31.421936: step 18573, loss 0.209975, acc 0.890625
2017-03-02T17:56:31.488556: step 18574, loss 0.195993, acc 0.890625
2017-03-02T17:56:31.555497: step 18575, loss 0.155493, acc 0.90625
2017-03-02T17:56:31.631317: step 18576, loss 0.104357, acc 0.9375
2017-03-02T17:56:31.702290: step 18577, loss 0.130332, acc 0.9375
2017-03-02T17:56:31.796717: step 18578, loss 0.241279, acc 0.90625
2017-03-02T17:56:31.870971: step 18579, loss 0.207449, acc 0.921875
2017-03-02T17:56:31.943343: step 18580, loss 0.0429157, acc 0.984375
2017-03-02T17:56:32.013274: step 18581, loss 0.167379, acc 0.9375
2017-03-02T17:56:32.095330: step 18582, loss 0.233822, acc 0.921875
2017-03-02T17:56:32.162682: step 18583, loss 0.178131, acc 0.9375
2017-03-02T17:56:32.240148: step 18584, loss 0.223758, acc 0.921875
2017-03-02T17:56:32.311654: step 18585, loss 0.10824, acc 0.96875
2017-03-02T17:56:32.401193: step 18586, loss 0.156674, acc 0.921875
2017-03-02T17:56:32.479242: step 18587, loss 0.0736031, acc 0.96875
2017-03-02T17:56:32.550719: step 18588, loss 0.20215, acc 0.890625
2017-03-02T17:56:32.632538: step 18589, loss 0.116705, acc 0.953125
2017-03-02T17:56:32.718449: step 18590, loss 0.120936, acc 0.9375
2017-03-02T17:56:32.791886: step 18591, loss 0.136686, acc 0.921875
2017-03-02T17:56:32.862504: step 18592, loss 0.189104, acc 0.9375
2017-03-02T17:56:32.939405: step 18593, loss 0.161065, acc 0.921875
2017-03-02T17:56:33.018965: step 18594, loss 0.0758306, acc 0.96875
2017-03-02T17:56:33.096017: step 18595, loss 0.236787, acc 0.90625
2017-03-02T17:56:33.169055: step 18596, loss 0.173315, acc 0.9375
2017-03-02T17:56:33.238965: step 18597, loss 0.0901248, acc 0.9375
2017-03-02T17:56:33.309596: step 18598, loss 0.246862, acc 0.890625
2017-03-02T17:56:33.379977: step 18599, loss 0.249978, acc 0.875
2017-03-02T17:56:33.452683: step 18600, loss 0.208759, acc 0.921875

Evaluation:
2017-03-02T17:56:33.482933: step 18600, loss 2.2976, acc 0.658976

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18600

2017-03-02T17:56:33.915911: step 18601, loss 0.231945, acc 0.875
2017-03-02T17:56:33.991690: step 18602, loss 0.120086, acc 0.9375
2017-03-02T17:56:34.085244: step 18603, loss 0.202766, acc 0.90625
2017-03-02T17:56:34.173697: step 18604, loss 0.203525, acc 0.9375
2017-03-02T17:56:34.245389: step 18605, loss 0.131051, acc 0.96875
2017-03-02T17:56:34.315097: step 18606, loss 0.151119, acc 0.953125
2017-03-02T17:56:34.388814: step 18607, loss 0.0978053, acc 0.953125
2017-03-02T17:56:34.460858: step 18608, loss 0.17844, acc 0.890625
2017-03-02T17:56:34.532168: step 18609, loss 0.155488, acc 0.921875
2017-03-02T17:56:34.617504: step 18610, loss 0.169627, acc 0.921875
2017-03-02T17:56:34.694629: step 18611, loss 0.180394, acc 0.9375
2017-03-02T17:56:34.768687: step 18612, loss 0.201506, acc 0.90625
2017-03-02T17:56:34.839970: step 18613, loss 0.166124, acc 0.875
2017-03-02T17:56:34.921281: step 18614, loss 0.124351, acc 0.953125
2017-03-02T17:56:34.988137: step 18615, loss 0.115432, acc 0.953125
2017-03-02T17:56:35.066565: step 18616, loss 0.200302, acc 0.921875
2017-03-02T17:56:35.145639: step 18617, loss 0.220578, acc 0.890625
2017-03-02T17:56:35.228081: step 18618, loss 0.224775, acc 0.90625
2017-03-02T17:56:35.299317: step 18619, loss 0.271763, acc 0.890625
2017-03-02T17:56:35.372360: step 18620, loss 0.00047262, acc 1
2017-03-02T17:56:35.451386: step 18621, loss 0.244279, acc 0.890625
2017-03-02T17:56:35.525535: step 18622, loss 0.192974, acc 0.921875
2017-03-02T17:56:35.593987: step 18623, loss 0.175096, acc 0.9375
2017-03-02T17:56:35.666176: step 18624, loss 0.202098, acc 0.90625
2017-03-02T17:56:35.750794: step 18625, loss 0.136895, acc 0.9375
2017-03-02T17:56:35.826314: step 18626, loss 0.0913464, acc 0.96875
2017-03-02T17:56:35.897049: step 18627, loss 0.145537, acc 0.921875
2017-03-02T17:56:35.968229: step 18628, loss 0.0673508, acc 0.984375
2017-03-02T17:56:36.042324: step 18629, loss 0.0829511, acc 0.953125
2017-03-02T17:56:36.115196: step 18630, loss 0.0856953, acc 0.96875
2017-03-02T17:56:36.190159: step 18631, loss 0.222137, acc 0.921875
2017-03-02T17:56:36.264740: step 18632, loss 0.166175, acc 0.890625
2017-03-02T17:56:36.331424: step 18633, loss 0.276273, acc 0.90625
2017-03-02T17:56:36.409508: step 18634, loss 0.121736, acc 0.921875
2017-03-02T17:56:36.483530: step 18635, loss 0.174084, acc 0.921875
2017-03-02T17:56:36.558199: step 18636, loss 0.166224, acc 0.90625
2017-03-02T17:56:36.644695: step 18637, loss 0.0859569, acc 0.953125
2017-03-02T17:56:36.715890: step 18638, loss 0.115351, acc 0.9375
2017-03-02T17:56:36.791967: step 18639, loss 0.110675, acc 0.953125
2017-03-02T17:56:36.861880: step 18640, loss 0.121346, acc 0.9375
2017-03-02T17:56:36.931884: step 18641, loss 0.0722031, acc 0.984375
2017-03-02T17:56:37.001646: step 18642, loss 0.126768, acc 0.9375
2017-03-02T17:56:37.071393: step 18643, loss 0.0714686, acc 0.984375
2017-03-02T17:56:37.145388: step 18644, loss 0.250479, acc 0.921875
2017-03-02T17:56:37.233374: step 18645, loss 0.220248, acc 0.921875
2017-03-02T17:56:37.341849: step 18646, loss 0.139064, acc 0.9375
2017-03-02T17:56:37.418790: step 18647, loss 0.0841802, acc 0.984375
2017-03-02T17:56:37.499776: step 18648, loss 0.109571, acc 0.9375
2017-03-02T17:56:37.578531: step 18649, loss 0.0901705, acc 0.953125
2017-03-02T17:56:37.653616: step 18650, loss 0.115915, acc 0.9375
2017-03-02T17:56:37.726521: step 18651, loss 0.264066, acc 0.890625
2017-03-02T17:56:37.795260: step 18652, loss 0.337277, acc 0.84375
2017-03-02T17:56:37.858285: step 18653, loss 0.164052, acc 0.921875
2017-03-02T17:56:37.935550: step 18654, loss 0.0915909, acc 0.953125
2017-03-02T17:56:38.008495: step 18655, loss 0.106972, acc 0.96875
2017-03-02T17:56:38.081360: step 18656, loss 0.0795452, acc 0.984375
2017-03-02T17:56:38.152841: step 18657, loss 0.134576, acc 0.9375
2017-03-02T17:56:38.225038: step 18658, loss 0.130876, acc 0.921875
2017-03-02T17:56:38.285849: step 18659, loss 0.103908, acc 0.9375
2017-03-02T17:56:38.353670: step 18660, loss 0.264097, acc 0.921875
2017-03-02T17:56:38.434480: step 18661, loss 0.176151, acc 0.921875
2017-03-02T17:56:38.503573: step 18662, loss 0.0920651, acc 0.96875
2017-03-02T17:56:38.581101: step 18663, loss 0.119842, acc 0.96875
2017-03-02T17:56:38.654820: step 18664, loss 0.31093, acc 0.84375
2017-03-02T17:56:38.729497: step 18665, loss 0.198069, acc 0.890625
2017-03-02T17:56:38.802944: step 18666, loss 0.136705, acc 0.96875
2017-03-02T17:56:38.877551: step 18667, loss 0.136268, acc 0.9375
2017-03-02T17:56:38.947841: step 18668, loss 0.0737302, acc 0.96875
2017-03-02T17:56:39.017639: step 18669, loss 0.112208, acc 0.96875
2017-03-02T17:56:39.088415: step 18670, loss 0.145742, acc 0.953125
2017-03-02T17:56:39.160181: step 18671, loss 0.0615348, acc 0.96875
2017-03-02T17:56:39.238285: step 18672, loss 0.162801, acc 0.9375
2017-03-02T17:56:39.313335: step 18673, loss 0.206867, acc 0.890625
2017-03-02T17:56:39.403614: step 18674, loss 0.122695, acc 0.953125
2017-03-02T17:56:39.476700: step 18675, loss 0.18136, acc 0.890625
2017-03-02T17:56:39.554701: step 18676, loss 0.109766, acc 0.953125
2017-03-02T17:56:39.628905: step 18677, loss 0.19305, acc 0.875
2017-03-02T17:56:39.701917: step 18678, loss 0.0825971, acc 0.96875
2017-03-02T17:56:39.772986: step 18679, loss 0.181878, acc 0.921875
2017-03-02T17:56:39.845751: step 18680, loss 0.316843, acc 0.875
2017-03-02T17:56:39.926263: step 18681, loss 0.188575, acc 0.921875
2017-03-02T17:56:40.000635: step 18682, loss 0.0948017, acc 0.96875
2017-03-02T17:56:40.076604: step 18683, loss 0.18194, acc 0.921875
2017-03-02T17:56:40.149698: step 18684, loss 0.183659, acc 0.90625
2017-03-02T17:56:40.227318: step 18685, loss 0.149666, acc 0.9375
2017-03-02T17:56:40.294557: step 18686, loss 0.0453792, acc 0.984375
2017-03-02T17:56:40.386804: step 18687, loss 0.164485, acc 0.9375
2017-03-02T17:56:40.455359: step 18688, loss 0.131013, acc 0.953125
2017-03-02T17:56:40.523875: step 18689, loss 0.154867, acc 0.921875
2017-03-02T17:56:40.631140: step 18690, loss 0.177172, acc 0.921875
2017-03-02T17:56:40.709219: step 18691, loss 0.185906, acc 0.921875
2017-03-02T17:56:40.781081: step 18692, loss 0.129431, acc 0.953125
2017-03-02T17:56:40.855186: step 18693, loss 0.219834, acc 0.90625
2017-03-02T17:56:40.927125: step 18694, loss 0.0903041, acc 0.953125
2017-03-02T17:56:41.006197: step 18695, loss 0.167622, acc 0.90625
2017-03-02T17:56:41.070617: step 18696, loss 0.171878, acc 0.890625
2017-03-02T17:56:41.154865: step 18697, loss 0.0387685, acc 0.984375
2017-03-02T17:56:41.224061: step 18698, loss 0.188965, acc 0.90625
2017-03-02T17:56:41.303155: step 18699, loss 0.316638, acc 0.859375
2017-03-02T17:56:41.372887: step 18700, loss 0.252614, acc 0.875

Evaluation:
2017-03-02T17:56:41.399125: step 18700, loss 2.29052, acc 0.653929

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18700

2017-03-02T17:56:41.835607: step 18701, loss 0.181632, acc 0.9375
2017-03-02T17:56:41.910454: step 18702, loss 0.34774, acc 0.84375
2017-03-02T17:56:41.980623: step 18703, loss 0.167975, acc 0.921875
2017-03-02T17:56:42.053163: step 18704, loss 0.108037, acc 0.9375
2017-03-02T17:56:42.124611: step 18705, loss 0.106312, acc 0.9375
2017-03-02T17:56:42.198042: step 18706, loss 0.168888, acc 0.890625
2017-03-02T17:56:42.268632: step 18707, loss 0.10922, acc 0.953125
2017-03-02T17:56:42.357269: step 18708, loss 0.143388, acc 0.953125
2017-03-02T17:56:42.429136: step 18709, loss 0.268858, acc 0.890625
2017-03-02T17:56:42.494875: step 18710, loss 0.174632, acc 0.921875
2017-03-02T17:56:42.568295: step 18711, loss 0.132004, acc 0.953125
2017-03-02T17:56:42.644497: step 18712, loss 0.191937, acc 0.90625
2017-03-02T17:56:42.721092: step 18713, loss 0.11962, acc 0.96875
2017-03-02T17:56:42.796568: step 18714, loss 0.0832976, acc 0.96875
2017-03-02T17:56:42.869846: step 18715, loss 0.240022, acc 0.890625
2017-03-02T17:56:42.946837: step 18716, loss 0.063087, acc 0.96875
2017-03-02T17:56:43.028886: step 18717, loss 0.229869, acc 0.875
2017-03-02T17:56:43.107251: step 18718, loss 0.164647, acc 0.9375
2017-03-02T17:56:43.177186: step 18719, loss 0.143097, acc 0.9375
2017-03-02T17:56:43.262874: step 18720, loss 0.162006, acc 0.9375
2017-03-02T17:56:43.336352: step 18721, loss 0.108513, acc 0.96875
2017-03-02T17:56:43.411484: step 18722, loss 0.144646, acc 0.9375
2017-03-02T17:56:43.485308: step 18723, loss 0.183045, acc 0.9375
2017-03-02T17:56:43.555004: step 18724, loss 0.135144, acc 0.953125
2017-03-02T17:56:43.627559: step 18725, loss 0.333853, acc 0.859375
2017-03-02T17:56:43.697288: step 18726, loss 0.112417, acc 0.9375
2017-03-02T17:56:43.767700: step 18727, loss 0.187239, acc 0.890625
2017-03-02T17:56:43.835876: step 18728, loss 0.139874, acc 0.953125
2017-03-02T17:56:43.902772: step 18729, loss 0.0952621, acc 0.953125
2017-03-02T17:56:43.976823: step 18730, loss 0.184434, acc 0.921875
2017-03-02T17:56:44.047649: step 18731, loss 0.134461, acc 0.921875
2017-03-02T17:56:44.118013: step 18732, loss 0.158091, acc 0.9375
2017-03-02T17:56:44.190088: step 18733, loss 0.123204, acc 0.96875
2017-03-02T17:56:44.264939: step 18734, loss 0.0832706, acc 0.96875
2017-03-02T17:56:44.337952: step 18735, loss 0.173614, acc 0.921875
2017-03-02T17:56:44.412247: step 18736, loss 0.103388, acc 0.96875
2017-03-02T17:56:44.486843: step 18737, loss 0.24437, acc 0.90625
2017-03-02T17:56:44.558319: step 18738, loss 0.140117, acc 0.9375
2017-03-02T17:56:44.637847: step 18739, loss 0.158154, acc 0.921875
2017-03-02T17:56:44.712559: step 18740, loss 0.297922, acc 0.875
2017-03-02T17:56:44.786423: step 18741, loss 0.0519909, acc 0.984375
2017-03-02T17:56:44.862278: step 18742, loss 0.231259, acc 0.890625
2017-03-02T17:56:44.947262: step 18743, loss 0.29859, acc 0.890625
2017-03-02T17:56:45.025721: step 18744, loss 0.0964891, acc 0.953125
2017-03-02T17:56:45.098680: step 18745, loss 0.127347, acc 0.953125
2017-03-02T17:56:45.174827: step 18746, loss 0.108783, acc 0.953125
2017-03-02T17:56:45.253375: step 18747, loss 0.262252, acc 0.859375
2017-03-02T17:56:45.325724: step 18748, loss 0.133376, acc 0.9375
2017-03-02T17:56:45.395129: step 18749, loss 0.0931752, acc 0.9375
2017-03-02T17:56:45.470643: step 18750, loss 0.146781, acc 0.921875
2017-03-02T17:56:45.546489: step 18751, loss 0.146872, acc 0.953125
2017-03-02T17:56:45.625372: step 18752, loss 0.120701, acc 0.9375
2017-03-02T17:56:45.697487: step 18753, loss 0.102873, acc 0.9375
2017-03-02T17:56:45.765086: step 18754, loss 0.172433, acc 0.921875
2017-03-02T17:56:45.828460: step 18755, loss 0.146703, acc 0.921875
2017-03-02T17:56:45.897733: step 18756, loss 0.0956259, acc 0.984375
2017-03-02T17:56:45.965796: step 18757, loss 0.140242, acc 0.953125
2017-03-02T17:56:46.036228: step 18758, loss 0.171982, acc 0.9375
2017-03-02T17:56:46.108958: step 18759, loss 0.249175, acc 0.890625
2017-03-02T17:56:46.180573: step 18760, loss 0.147072, acc 0.921875
2017-03-02T17:56:46.268568: step 18761, loss 0.129254, acc 0.9375
2017-03-02T17:56:46.341586: step 18762, loss 0.319069, acc 0.859375
2017-03-02T17:56:46.411218: step 18763, loss 0.163802, acc 0.890625
2017-03-02T17:56:46.481207: step 18764, loss 0.200867, acc 0.9375
2017-03-02T17:56:46.569189: step 18765, loss 0.216442, acc 0.921875
2017-03-02T17:56:46.639001: step 18766, loss 0.196883, acc 0.90625
2017-03-02T17:56:46.711267: step 18767, loss 0.090022, acc 0.953125
2017-03-02T17:56:46.791110: step 18768, loss 0.205635, acc 0.890625
2017-03-02T17:56:46.869255: step 18769, loss 0.0683854, acc 0.96875
2017-03-02T17:56:46.938178: step 18770, loss 0.0903785, acc 0.984375
2017-03-02T17:56:47.009754: step 18771, loss 0.137554, acc 0.9375
2017-03-02T17:56:47.083285: step 18772, loss 0.114044, acc 0.9375
2017-03-02T17:56:47.153993: step 18773, loss 0.0880796, acc 0.984375
2017-03-02T17:56:47.228331: step 18774, loss 0.171974, acc 0.9375
2017-03-02T17:56:47.298361: step 18775, loss 0.152417, acc 0.921875
2017-03-02T17:56:47.369162: step 18776, loss 0.117651, acc 0.953125
2017-03-02T17:56:47.442728: step 18777, loss 0.25945, acc 0.921875
2017-03-02T17:56:47.526840: step 18778, loss 0.261498, acc 0.921875
2017-03-02T17:56:47.599479: step 18779, loss 0.29108, acc 0.90625
2017-03-02T17:56:47.669898: step 18780, loss 0.0821044, acc 0.96875
2017-03-02T17:56:47.741586: step 18781, loss 0.146726, acc 0.921875
2017-03-02T17:56:47.820513: step 18782, loss 0.0858032, acc 0.96875
2017-03-02T17:56:47.894660: step 18783, loss 0.202529, acc 0.9375
2017-03-02T17:56:47.968437: step 18784, loss 0.0972805, acc 0.953125
2017-03-02T17:56:48.037199: step 18785, loss 0.129537, acc 0.9375
2017-03-02T17:56:48.102038: step 18786, loss 0.187856, acc 0.921875
2017-03-02T17:56:48.174620: step 18787, loss 0.229142, acc 0.90625
2017-03-02T17:56:48.250926: step 18788, loss 0.132547, acc 0.9375
2017-03-02T17:56:48.339075: step 18789, loss 0.194342, acc 0.921875
2017-03-02T17:56:48.411333: step 18790, loss 0.19955, acc 0.9375
2017-03-02T17:56:48.485753: step 18791, loss 0.198555, acc 0.921875
2017-03-02T17:56:48.557271: step 18792, loss 0.227321, acc 0.90625
2017-03-02T17:56:48.631932: step 18793, loss 0.194851, acc 0.9375
2017-03-02T17:56:48.698477: step 18794, loss 0.13722, acc 0.9375
2017-03-02T17:56:48.766810: step 18795, loss 0.134215, acc 0.921875
2017-03-02T17:56:48.836443: step 18796, loss 0.0836241, acc 0.96875
2017-03-02T17:56:48.916409: step 18797, loss 0.187782, acc 0.921875
2017-03-02T17:56:48.989077: step 18798, loss 0.129711, acc 0.953125
2017-03-02T17:56:49.072954: step 18799, loss 0.273582, acc 0.875
2017-03-02T17:56:49.148381: step 18800, loss 0.214054, acc 0.9375

Evaluation:
2017-03-02T17:56:49.185686: step 18800, loss 2.23228, acc 0.656092

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18800

2017-03-02T17:56:49.678825: step 18801, loss 0.0489984, acc 0.984375
2017-03-02T17:56:49.752779: step 18802, loss 0.214872, acc 0.890625
2017-03-02T17:56:49.830817: step 18803, loss 0.0555763, acc 1
2017-03-02T17:56:49.908693: step 18804, loss 0.134704, acc 0.9375
2017-03-02T17:56:49.990908: step 18805, loss 0.187182, acc 0.953125
2017-03-02T17:56:50.060083: step 18806, loss 0.166877, acc 0.9375
2017-03-02T17:56:50.162391: step 18807, loss 0.191392, acc 0.921875
2017-03-02T17:56:50.260626: step 18808, loss 0.0815616, acc 0.96875
2017-03-02T17:56:50.337554: step 18809, loss 0.142031, acc 0.953125
2017-03-02T17:56:50.428413: step 18810, loss 0.214135, acc 0.859375
2017-03-02T17:56:50.503416: step 18811, loss 0.137049, acc 0.921875
2017-03-02T17:56:50.584521: step 18812, loss 0.14368, acc 0.9375
2017-03-02T17:56:50.662638: step 18813, loss 0.133416, acc 0.9375
2017-03-02T17:56:50.731493: step 18814, loss 0.288876, acc 0.859375
2017-03-02T17:56:50.800671: step 18815, loss 0.126379, acc 0.953125
2017-03-02T17:56:50.867193: step 18816, loss 0.0178151, acc 1
2017-03-02T17:56:50.953241: step 18817, loss 0.164828, acc 0.9375
2017-03-02T17:56:51.033944: step 18818, loss 0.0648114, acc 1
2017-03-02T17:56:51.107067: step 18819, loss 0.129406, acc 0.9375
2017-03-02T17:56:51.180462: step 18820, loss 0.129934, acc 0.9375
2017-03-02T17:56:51.243243: step 18821, loss 0.193115, acc 0.890625
2017-03-02T17:56:51.316175: step 18822, loss 0.152654, acc 0.9375
2017-03-02T17:56:51.390639: step 18823, loss 0.164201, acc 0.921875
2017-03-02T17:56:51.461213: step 18824, loss 0.129858, acc 0.953125
2017-03-02T17:56:51.527095: step 18825, loss 0.206988, acc 0.9375
2017-03-02T17:56:51.604906: step 18826, loss 0.0709515, acc 0.96875
2017-03-02T17:56:51.676867: step 18827, loss 0.122168, acc 0.953125
2017-03-02T17:56:51.751954: step 18828, loss 0.0867575, acc 0.953125
2017-03-02T17:56:51.856812: step 18829, loss 0.193812, acc 0.90625
2017-03-02T17:56:51.932373: step 18830, loss 0.111928, acc 0.953125
2017-03-02T17:56:52.005970: step 18831, loss 0.136521, acc 0.9375
2017-03-02T17:56:52.077570: step 18832, loss 0.193881, acc 0.875
2017-03-02T17:56:52.144879: step 18833, loss 0.0985045, acc 0.953125
2017-03-02T17:56:52.216486: step 18834, loss 0.08275, acc 1
2017-03-02T17:56:52.288718: step 18835, loss 0.113022, acc 0.953125
2017-03-02T17:56:52.369925: step 18836, loss 0.19564, acc 0.890625
2017-03-02T17:56:52.445978: step 18837, loss 0.10694, acc 0.921875
2017-03-02T17:56:52.529693: step 18838, loss 0.159827, acc 0.921875
2017-03-02T17:56:52.614607: step 18839, loss 0.135813, acc 0.9375
2017-03-02T17:56:52.688567: step 18840, loss 0.167582, acc 0.90625
2017-03-02T17:56:52.764031: step 18841, loss 0.166709, acc 0.96875
2017-03-02T17:56:52.829845: step 18842, loss 0.170169, acc 0.90625
2017-03-02T17:56:52.908367: step 18843, loss 0.198102, acc 0.9375
2017-03-02T17:56:52.984232: step 18844, loss 0.10933, acc 0.9375
2017-03-02T17:56:53.066973: step 18845, loss 0.0895669, acc 0.953125
2017-03-02T17:56:53.142841: step 18846, loss 0.11586, acc 0.921875
2017-03-02T17:56:53.216005: step 18847, loss 0.173893, acc 0.90625
2017-03-02T17:56:53.295611: step 18848, loss 0.281405, acc 0.875
2017-03-02T17:56:53.368246: step 18849, loss 0.0717707, acc 0.96875
2017-03-02T17:56:53.446203: step 18850, loss 0.137143, acc 0.9375
2017-03-02T17:56:53.511907: step 18851, loss 0.200105, acc 0.953125
2017-03-02T17:56:53.590567: step 18852, loss 0.134257, acc 0.90625
2017-03-02T17:56:53.663016: step 18853, loss 0.205865, acc 0.921875
2017-03-02T17:56:53.738364: step 18854, loss 0.0758969, acc 0.984375
2017-03-02T17:56:53.813000: step 18855, loss 0.111548, acc 0.96875
2017-03-02T17:56:53.887157: step 18856, loss 0.0974878, acc 0.953125
2017-03-02T17:56:53.951658: step 18857, loss 0.192167, acc 0.890625
2017-03-02T17:56:54.025035: step 18858, loss 0.154383, acc 0.921875
2017-03-02T17:56:54.092353: step 18859, loss 0.165091, acc 0.921875
2017-03-02T17:56:54.165061: step 18860, loss 0.169261, acc 0.90625
2017-03-02T17:56:54.235318: step 18861, loss 0.143981, acc 0.953125
2017-03-02T17:56:54.304517: step 18862, loss 0.126893, acc 0.953125
2017-03-02T17:56:54.379675: step 18863, loss 0.132146, acc 0.953125
2017-03-02T17:56:54.455029: step 18864, loss 0.0573134, acc 0.96875
2017-03-02T17:56:54.528705: step 18865, loss 0.150738, acc 0.921875
2017-03-02T17:56:54.598826: step 18866, loss 0.194101, acc 0.859375
2017-03-02T17:56:54.678913: step 18867, loss 0.120779, acc 0.953125
2017-03-02T17:56:54.756475: step 18868, loss 0.0997695, acc 0.953125
2017-03-02T17:56:54.835891: step 18869, loss 0.0877336, acc 0.953125
2017-03-02T17:56:54.905918: step 18870, loss 0.0750095, acc 1
2017-03-02T17:56:54.975083: step 18871, loss 0.0704703, acc 0.984375
2017-03-02T17:56:55.048316: step 18872, loss 0.270258, acc 0.890625
2017-03-02T17:56:55.120736: step 18873, loss 0.11279, acc 0.921875
2017-03-02T17:56:55.195765: step 18874, loss 0.0493952, acc 0.953125
2017-03-02T17:56:55.268683: step 18875, loss 0.200961, acc 0.90625
2017-03-02T17:56:55.342825: step 18876, loss 0.0584339, acc 0.984375
2017-03-02T17:56:55.417477: step 18877, loss 0.181163, acc 0.90625
2017-03-02T17:56:55.498455: step 18878, loss 0.216094, acc 0.921875
2017-03-02T17:56:55.571736: step 18879, loss 0.154817, acc 0.9375
2017-03-02T17:56:55.640466: step 18880, loss 0.0516446, acc 0.984375
2017-03-02T17:56:55.710716: step 18881, loss 0.153363, acc 0.90625
2017-03-02T17:56:55.783646: step 18882, loss 0.117698, acc 0.953125
2017-03-02T17:56:55.858702: step 18883, loss 0.196159, acc 0.921875
2017-03-02T17:56:55.935431: step 18884, loss 0.123323, acc 0.9375
2017-03-02T17:56:56.019626: step 18885, loss 0.151072, acc 0.9375
2017-03-02T17:56:56.095281: step 18886, loss 0.180858, acc 0.9375
2017-03-02T17:56:56.172103: step 18887, loss 0.217098, acc 0.890625
2017-03-02T17:56:56.247225: step 18888, loss 0.149695, acc 0.921875
2017-03-02T17:56:56.312450: step 18889, loss 0.113899, acc 0.953125
2017-03-02T17:56:56.381325: step 18890, loss 0.124228, acc 0.9375
2017-03-02T17:56:56.456479: step 18891, loss 0.0952912, acc 0.953125
2017-03-02T17:56:56.526784: step 18892, loss 0.145959, acc 0.9375
2017-03-02T17:56:56.600836: step 18893, loss 0.223678, acc 0.921875
2017-03-02T17:56:56.669900: step 18894, loss 0.203391, acc 0.921875
2017-03-02T17:56:56.741993: step 18895, loss 0.15291, acc 0.9375
2017-03-02T17:56:56.817717: step 18896, loss 0.087765, acc 0.96875
2017-03-02T17:56:56.886446: step 18897, loss 0.300155, acc 0.828125
2017-03-02T17:56:56.954627: step 18898, loss 0.159176, acc 0.90625
2017-03-02T17:56:57.026888: step 18899, loss 0.151008, acc 0.9375
2017-03-02T17:56:57.095496: step 18900, loss 0.15454, acc 0.90625

Evaluation:
2017-03-02T17:56:57.130671: step 18900, loss 2.30448, acc 0.661139

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-18900

2017-03-02T17:56:57.573915: step 18901, loss 0.114793, acc 0.96875
2017-03-02T17:56:57.648939: step 18902, loss 0.171744, acc 0.90625
2017-03-02T17:56:57.722793: step 18903, loss 0.148661, acc 0.953125
2017-03-02T17:56:57.792869: step 18904, loss 0.158826, acc 0.921875
2017-03-02T17:56:57.866032: step 18905, loss 0.0984765, acc 0.96875
2017-03-02T17:56:57.945315: step 18906, loss 0.106229, acc 0.953125
2017-03-02T17:56:58.016753: step 18907, loss 0.154703, acc 0.953125
2017-03-02T17:56:58.095819: step 18908, loss 0.152417, acc 0.921875
2017-03-02T17:56:58.166919: step 18909, loss 0.0876262, acc 0.96875
2017-03-02T17:56:58.245767: step 18910, loss 0.190799, acc 0.90625
2017-03-02T17:56:58.317346: step 18911, loss 0.193639, acc 0.921875
2017-03-02T17:56:58.384879: step 18912, loss 0.101417, acc 0.953125
2017-03-02T17:56:58.456538: step 18913, loss 0.129712, acc 0.9375
2017-03-02T17:56:58.531447: step 18914, loss 0.184988, acc 0.921875
2017-03-02T17:56:58.602258: step 18915, loss 0.202603, acc 0.90625
2017-03-02T17:56:58.679832: step 18916, loss 0.144594, acc 0.953125
2017-03-02T17:56:58.753997: step 18917, loss 0.0716924, acc 0.984375
2017-03-02T17:56:58.822333: step 18918, loss 0.128342, acc 0.953125
2017-03-02T17:56:58.892518: step 18919, loss 0.108718, acc 0.984375
2017-03-02T17:56:58.963151: step 18920, loss 0.0668942, acc 0.96875
2017-03-02T17:56:59.029873: step 18921, loss 0.198796, acc 0.9375
2017-03-02T17:56:59.102268: step 18922, loss 0.185186, acc 0.90625
2017-03-02T17:56:59.174623: step 18923, loss 0.115236, acc 0.96875
2017-03-02T17:56:59.251105: step 18924, loss 0.0624924, acc 0.984375
2017-03-02T17:56:59.322226: step 18925, loss 0.159532, acc 0.953125
2017-03-02T17:56:59.396221: step 18926, loss 0.187505, acc 0.9375
2017-03-02T17:56:59.482645: step 18927, loss 0.234284, acc 0.921875
2017-03-02T17:56:59.554386: step 18928, loss 0.107345, acc 0.953125
2017-03-02T17:56:59.622042: step 18929, loss 0.191224, acc 0.9375
2017-03-02T17:56:59.694502: step 18930, loss 0.0883582, acc 0.953125
2017-03-02T17:56:59.763312: step 18931, loss 0.202874, acc 0.9375
2017-03-02T17:56:59.840436: step 18932, loss 0.130678, acc 0.953125
2017-03-02T17:56:59.920577: step 18933, loss 0.203872, acc 0.875
2017-03-02T17:57:00.011342: step 18934, loss 0.21067, acc 0.921875
2017-03-02T17:57:00.083287: step 18935, loss 0.0476073, acc 0.984375
2017-03-02T17:57:00.173800: step 18936, loss 0.252575, acc 0.921875
2017-03-02T17:57:00.245168: step 18937, loss 0.243109, acc 0.921875
2017-03-02T17:57:00.318493: step 18938, loss 0.0572962, acc 0.96875
2017-03-02T17:57:00.393877: step 18939, loss 0.166592, acc 0.921875
2017-03-02T17:57:00.465714: step 18940, loss 0.0895708, acc 0.96875
2017-03-02T17:57:00.532324: step 18941, loss 0.131017, acc 0.953125
2017-03-02T17:57:00.603293: step 18942, loss 0.170371, acc 0.921875
2017-03-02T17:57:00.676532: step 18943, loss 0.0771665, acc 0.953125
2017-03-02T17:57:00.753582: step 18944, loss 0.141433, acc 0.9375
2017-03-02T17:57:00.824315: step 18945, loss 0.184546, acc 0.9375
2017-03-02T17:57:00.896422: step 18946, loss 0.195281, acc 0.921875
2017-03-02T17:57:00.965977: step 18947, loss 0.110365, acc 0.9375
2017-03-02T17:57:01.046488: step 18948, loss 0.181046, acc 0.921875
2017-03-02T17:57:01.120526: step 18949, loss 0.179594, acc 0.921875
2017-03-02T17:57:01.193243: step 18950, loss 0.259249, acc 0.90625
2017-03-02T17:57:01.263999: step 18951, loss 0.159147, acc 0.921875
2017-03-02T17:57:01.337039: step 18952, loss 0.153368, acc 0.921875
2017-03-02T17:57:01.414005: step 18953, loss 0.272418, acc 0.90625
2017-03-02T17:57:01.490210: step 18954, loss 0.104258, acc 0.953125
2017-03-02T17:57:01.563278: step 18955, loss 0.180475, acc 0.921875
2017-03-02T17:57:01.635712: step 18956, loss 0.359595, acc 0.8125
2017-03-02T17:57:01.708551: step 18957, loss 0.125556, acc 0.9375
2017-03-02T17:57:01.781212: step 18958, loss 0.303355, acc 0.890625
2017-03-02T17:57:01.851045: step 18959, loss 0.100478, acc 0.953125
2017-03-02T17:57:01.922757: step 18960, loss 0.0532096, acc 0.96875
2017-03-02T17:57:02.004568: step 18961, loss 0.189963, acc 0.90625
2017-03-02T17:57:02.080712: step 18962, loss 0.288252, acc 0.890625
2017-03-02T17:57:02.156502: step 18963, loss 0.143506, acc 0.96875
2017-03-02T17:57:02.228118: step 18964, loss 0.113152, acc 0.9375
2017-03-02T17:57:02.311616: step 18965, loss 0.0868867, acc 0.9375
2017-03-02T17:57:02.383180: step 18966, loss 0.160542, acc 0.9375
2017-03-02T17:57:02.455680: step 18967, loss 0.141943, acc 0.953125
2017-03-02T17:57:02.539190: step 18968, loss 0.158351, acc 0.890625
2017-03-02T17:57:02.610697: step 18969, loss 0.244868, acc 0.84375
2017-03-02T17:57:02.682825: step 18970, loss 0.137291, acc 0.953125
2017-03-02T17:57:02.754442: step 18971, loss 0.166614, acc 0.9375
2017-03-02T17:57:02.831502: step 18972, loss 0.0666478, acc 0.984375
2017-03-02T17:57:02.907315: step 18973, loss 0.175584, acc 0.921875
2017-03-02T17:57:02.981775: step 18974, loss 0.0975676, acc 0.984375
2017-03-02T17:57:03.056562: step 18975, loss 0.175139, acc 0.921875
2017-03-02T17:57:03.127608: step 18976, loss 0.164033, acc 0.921875
2017-03-02T17:57:03.197208: step 18977, loss 0.255571, acc 0.859375
2017-03-02T17:57:03.268753: step 18978, loss 0.132077, acc 0.953125
2017-03-02T17:57:03.355747: step 18979, loss 0.173697, acc 0.921875
2017-03-02T17:57:03.426064: step 18980, loss 0.182263, acc 0.921875
2017-03-02T17:57:03.504720: step 18981, loss 0.205967, acc 0.921875
2017-03-02T17:57:03.595988: step 18982, loss 0.330222, acc 0.875
2017-03-02T17:57:03.675480: step 18983, loss 0.23662, acc 0.890625
2017-03-02T17:57:03.747729: step 18984, loss 0.156742, acc 0.90625
2017-03-02T17:57:03.830190: step 18985, loss 0.119809, acc 0.921875
2017-03-02T17:57:03.901240: step 18986, loss 0.215338, acc 0.90625
2017-03-02T17:57:03.975039: step 18987, loss 0.235549, acc 0.890625
2017-03-02T17:57:04.049221: step 18988, loss 0.0766959, acc 0.984375
2017-03-02T17:57:04.132158: step 18989, loss 0.0879144, acc 0.984375
2017-03-02T17:57:04.206148: step 18990, loss 0.181298, acc 0.921875
2017-03-02T17:57:04.278300: step 18991, loss 0.188726, acc 0.921875
2017-03-02T17:57:04.348818: step 18992, loss 0.180974, acc 0.921875
2017-03-02T17:57:04.430831: step 18993, loss 0.0909197, acc 0.96875
2017-03-02T17:57:04.510330: step 18994, loss 0.135935, acc 0.9375
2017-03-02T17:57:04.585220: step 18995, loss 0.258487, acc 0.875
2017-03-02T17:57:04.655172: step 18996, loss 0.180211, acc 0.921875
2017-03-02T17:57:04.747783: step 18997, loss 0.355206, acc 0.875
2017-03-02T17:57:04.819412: step 18998, loss 0.204659, acc 0.890625
2017-03-02T17:57:04.893034: step 18999, loss 0.21025, acc 0.890625
2017-03-02T17:57:04.972140: step 19000, loss 0.135752, acc 0.953125

Evaluation:
2017-03-02T17:57:05.008988: step 19000, loss 2.21223, acc 0.643836

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19000

2017-03-02T17:57:05.470087: step 19001, loss 0.27428, acc 0.875
2017-03-02T17:57:05.544471: step 19002, loss 0.11374, acc 0.96875
2017-03-02T17:57:05.619352: step 19003, loss 0.272709, acc 0.890625
2017-03-02T17:57:05.691653: step 19004, loss 0.193361, acc 0.921875
2017-03-02T17:57:05.768990: step 19005, loss 0.139844, acc 0.9375
2017-03-02T17:57:05.839235: step 19006, loss 0.0870041, acc 0.96875
2017-03-02T17:57:05.917170: step 19007, loss 0.176067, acc 0.9375
2017-03-02T17:57:05.993449: step 19008, loss 0.167888, acc 0.921875
2017-03-02T17:57:06.066353: step 19009, loss 0.24676, acc 0.90625
2017-03-02T17:57:06.142896: step 19010, loss 0.16106, acc 0.921875
2017-03-02T17:57:06.217596: step 19011, loss 0.122719, acc 0.96875
2017-03-02T17:57:06.290322: step 19012, loss 0.510739, acc 0.75
2017-03-02T17:57:06.368324: step 19013, loss 0.0670593, acc 0.984375
2017-03-02T17:57:06.446785: step 19014, loss 0.145481, acc 0.9375
2017-03-02T17:57:06.527817: step 19015, loss 0.236915, acc 0.875
2017-03-02T17:57:06.606689: step 19016, loss 0.142659, acc 0.953125
2017-03-02T17:57:06.676612: step 19017, loss 0.0657718, acc 0.96875
2017-03-02T17:57:06.750927: step 19018, loss 0.13134, acc 0.96875
2017-03-02T17:57:06.830448: step 19019, loss 0.12121, acc 0.984375
2017-03-02T17:57:06.906098: step 19020, loss 0.172038, acc 0.90625
2017-03-02T17:57:06.979310: step 19021, loss 0.272342, acc 0.90625
2017-03-02T17:57:07.053796: step 19022, loss 0.117704, acc 0.9375
2017-03-02T17:57:07.131024: step 19023, loss 0.171312, acc 0.921875
2017-03-02T17:57:07.205018: step 19024, loss 0.0923127, acc 0.96875
2017-03-02T17:57:07.283326: step 19025, loss 0.13158, acc 0.9375
2017-03-02T17:57:07.347722: step 19026, loss 0.182109, acc 0.90625
2017-03-02T17:57:07.416682: step 19027, loss 0.169967, acc 0.921875
2017-03-02T17:57:07.493409: step 19028, loss 0.162818, acc 0.890625
2017-03-02T17:57:07.580994: step 19029, loss 0.312422, acc 0.890625
2017-03-02T17:57:07.650964: step 19030, loss 0.0982439, acc 0.953125
2017-03-02T17:57:07.721176: step 19031, loss 0.210599, acc 0.890625
2017-03-02T17:57:07.795439: step 19032, loss 0.19211, acc 0.9375
2017-03-02T17:57:07.866452: step 19033, loss 0.217133, acc 0.859375
2017-03-02T17:57:07.938881: step 19034, loss 0.160188, acc 0.9375
2017-03-02T17:57:08.011216: step 19035, loss 0.0955865, acc 0.96875
2017-03-02T17:57:08.079076: step 19036, loss 0.178552, acc 0.953125
2017-03-02T17:57:08.146519: step 19037, loss 0.153079, acc 0.921875
2017-03-02T17:57:08.217754: step 19038, loss 0.159067, acc 0.90625
2017-03-02T17:57:08.293612: step 19039, loss 0.1322, acc 0.953125
2017-03-02T17:57:08.371815: step 19040, loss 0.14304, acc 0.96875
2017-03-02T17:57:08.447123: step 19041, loss 0.090233, acc 0.9375
2017-03-02T17:57:08.540085: step 19042, loss 0.167718, acc 0.9375
2017-03-02T17:57:08.611847: step 19043, loss 0.191962, acc 0.9375
2017-03-02T17:57:08.682297: step 19044, loss 0.171177, acc 0.90625
2017-03-02T17:57:08.753593: step 19045, loss 0.166781, acc 0.921875
2017-03-02T17:57:08.824790: step 19046, loss 0.101025, acc 0.921875
2017-03-02T17:57:08.896178: step 19047, loss 0.148453, acc 0.90625
2017-03-02T17:57:08.968469: step 19048, loss 0.0990372, acc 0.96875
2017-03-02T17:57:09.039311: step 19049, loss 0.181498, acc 0.9375
2017-03-02T17:57:09.112430: step 19050, loss 0.181131, acc 0.90625
2017-03-02T17:57:09.191290: step 19051, loss 0.164019, acc 0.859375
2017-03-02T17:57:09.266249: step 19052, loss 0.177321, acc 0.921875
2017-03-02T17:57:09.340645: step 19053, loss 0.0942057, acc 0.984375
2017-03-02T17:57:09.410785: step 19054, loss 0.10947, acc 0.953125
2017-03-02T17:57:09.480677: step 19055, loss 0.355309, acc 0.859375
2017-03-02T17:57:09.568309: step 19056, loss 0.184934, acc 0.9375
2017-03-02T17:57:09.643687: step 19057, loss 0.113928, acc 0.9375
2017-03-02T17:57:09.718425: step 19058, loss 0.0485479, acc 1
2017-03-02T17:57:09.794845: step 19059, loss 0.0840722, acc 0.953125
2017-03-02T17:57:09.870943: step 19060, loss 0.18634, acc 0.9375
2017-03-02T17:57:09.951627: step 19061, loss 0.192261, acc 0.953125
2017-03-02T17:57:10.023948: step 19062, loss 0.145551, acc 0.9375
2017-03-02T17:57:10.100482: step 19063, loss 0.106679, acc 0.953125
2017-03-02T17:57:10.167866: step 19064, loss 0.0932927, acc 0.984375
2017-03-02T17:57:10.235252: step 19065, loss 0.154718, acc 0.921875
2017-03-02T17:57:10.308901: step 19066, loss 0.142499, acc 0.953125
2017-03-02T17:57:10.378449: step 19067, loss 0.121658, acc 0.953125
2017-03-02T17:57:10.453203: step 19068, loss 0.223228, acc 0.890625
2017-03-02T17:57:10.534428: step 19069, loss 0.190949, acc 0.90625
2017-03-02T17:57:10.604920: step 19070, loss 0.204573, acc 0.9375
2017-03-02T17:57:10.672926: step 19071, loss 0.0877279, acc 0.96875
2017-03-02T17:57:10.747042: step 19072, loss 0.189491, acc 0.90625
2017-03-02T17:57:10.817453: step 19073, loss 0.267565, acc 0.890625
2017-03-02T17:57:10.886724: step 19074, loss 0.14969, acc 0.921875
2017-03-02T17:57:10.952229: step 19075, loss 0.0898175, acc 0.953125
2017-03-02T17:57:11.030944: step 19076, loss 0.175539, acc 0.921875
2017-03-02T17:57:11.107599: step 19077, loss 0.230696, acc 0.890625
2017-03-02T17:57:11.176999: step 19078, loss 0.110018, acc 0.953125
2017-03-02T17:57:11.261723: step 19079, loss 0.0790639, acc 0.953125
2017-03-02T17:57:11.332688: step 19080, loss 0.190789, acc 0.9375
2017-03-02T17:57:11.404826: step 19081, loss 0.157659, acc 0.9375
2017-03-02T17:57:11.477809: step 19082, loss 0.256917, acc 0.875
2017-03-02T17:57:11.545983: step 19083, loss 0.0569492, acc 0.984375
2017-03-02T17:57:11.619707: step 19084, loss 0.0825205, acc 0.9375
2017-03-02T17:57:11.691142: step 19085, loss 0.112719, acc 0.953125
2017-03-02T17:57:11.763168: step 19086, loss 0.13472, acc 0.9375
2017-03-02T17:57:11.841754: step 19087, loss 0.117214, acc 0.953125
2017-03-02T17:57:11.912493: step 19088, loss 0.284183, acc 0.859375
2017-03-02T17:57:12.000085: step 19089, loss 0.146368, acc 0.96875
2017-03-02T17:57:12.071397: step 19090, loss 0.153752, acc 0.921875
2017-03-02T17:57:12.150479: step 19091, loss 0.122666, acc 0.9375
2017-03-02T17:57:12.214720: step 19092, loss 0.118578, acc 0.921875
2017-03-02T17:57:12.285630: step 19093, loss 0.073117, acc 0.984375
2017-03-02T17:57:12.359123: step 19094, loss 0.111144, acc 0.9375
2017-03-02T17:57:12.426794: step 19095, loss 0.23413, acc 0.875
2017-03-02T17:57:12.518799: step 19096, loss 0.194733, acc 0.890625
2017-03-02T17:57:12.595702: step 19097, loss 0.155626, acc 0.9375
2017-03-02T17:57:12.667272: step 19098, loss 0.142485, acc 0.953125
2017-03-02T17:57:12.750030: step 19099, loss 0.0799088, acc 0.953125
2017-03-02T17:57:12.822503: step 19100, loss 0.0730718, acc 0.953125

Evaluation:
2017-03-02T17:57:12.843366: step 19100, loss 2.27754, acc 0.648161

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19100

2017-03-02T17:57:13.364502: step 19101, loss 0.170483, acc 0.890625
2017-03-02T17:57:13.435706: step 19102, loss 0.100611, acc 0.953125
2017-03-02T17:57:13.508428: step 19103, loss 0.20849, acc 0.890625
2017-03-02T17:57:13.583899: step 19104, loss 0.193273, acc 0.890625
2017-03-02T17:57:13.652576: step 19105, loss 0.202194, acc 0.90625
2017-03-02T17:57:13.723038: step 19106, loss 0.139386, acc 0.9375
2017-03-02T17:57:13.797983: step 19107, loss 0.161214, acc 0.9375
2017-03-02T17:57:13.872814: step 19108, loss 0.239883, acc 0.890625
2017-03-02T17:57:13.945972: step 19109, loss 0.238056, acc 0.875
2017-03-02T17:57:14.013630: step 19110, loss 0.130388, acc 0.9375
2017-03-02T17:57:14.077526: step 19111, loss 0.113929, acc 0.9375
2017-03-02T17:57:14.147934: step 19112, loss 0.107121, acc 0.96875
2017-03-02T17:57:14.223554: step 19113, loss 0.226615, acc 0.875
2017-03-02T17:57:14.293962: step 19114, loss 0.0640699, acc 0.984375
2017-03-02T17:57:14.360971: step 19115, loss 0.0788354, acc 0.96875
2017-03-02T17:57:14.449749: step 19116, loss 0.0986285, acc 0.9375
2017-03-02T17:57:14.529536: step 19117, loss 0.270665, acc 0.921875
2017-03-02T17:57:14.609359: step 19118, loss 0.130919, acc 0.921875
2017-03-02T17:57:14.683366: step 19119, loss 0.192706, acc 0.90625
2017-03-02T17:57:14.758969: step 19120, loss 0.299476, acc 0.890625
2017-03-02T17:57:14.839290: step 19121, loss 0.171778, acc 0.875
2017-03-02T17:57:14.915277: step 19122, loss 0.0485348, acc 0.984375
2017-03-02T17:57:14.985349: step 19123, loss 0.163612, acc 0.90625
2017-03-02T17:57:15.054190: step 19124, loss 0.196023, acc 0.90625
2017-03-02T17:57:15.128120: step 19125, loss 0.0775212, acc 0.953125
2017-03-02T17:57:15.200321: step 19126, loss 0.135909, acc 0.921875
2017-03-02T17:57:15.274378: step 19127, loss 0.115629, acc 0.953125
2017-03-02T17:57:15.361824: step 19128, loss 0.0604759, acc 0.96875
2017-03-02T17:57:15.436292: step 19129, loss 0.153844, acc 0.921875
2017-03-02T17:57:15.509009: step 19130, loss 0.162989, acc 0.9375
2017-03-02T17:57:15.587607: step 19131, loss 0.125918, acc 0.921875
2017-03-02T17:57:15.653238: step 19132, loss 0.179274, acc 0.921875
2017-03-02T17:57:15.724196: step 19133, loss 0.201507, acc 0.890625
2017-03-02T17:57:15.796876: step 19134, loss 0.111744, acc 0.953125
2017-03-02T17:57:15.870518: step 19135, loss 0.266251, acc 0.90625
2017-03-02T17:57:15.943801: step 19136, loss 0.147757, acc 0.9375
2017-03-02T17:57:16.015670: step 19137, loss 0.121342, acc 0.96875
2017-03-02T17:57:16.086570: step 19138, loss 0.14698, acc 0.953125
2017-03-02T17:57:16.159586: step 19139, loss 0.098512, acc 0.953125
2017-03-02T17:57:16.231359: step 19140, loss 0.176702, acc 0.90625
2017-03-02T17:57:16.311416: step 19141, loss 0.2528, acc 0.875
2017-03-02T17:57:16.386785: step 19142, loss 0.169795, acc 0.921875
2017-03-02T17:57:16.460096: step 19143, loss 0.166801, acc 0.9375
2017-03-02T17:57:16.530155: step 19144, loss 0.150826, acc 0.921875
2017-03-02T17:57:16.591226: step 19145, loss 0.207086, acc 0.921875
2017-03-02T17:57:16.679808: step 19146, loss 0.121287, acc 0.953125
2017-03-02T17:57:16.744393: step 19147, loss 0.147933, acc 0.9375
2017-03-02T17:57:16.815598: step 19148, loss 0.0407354, acc 1
2017-03-02T17:57:16.878649: step 19149, loss 0.256212, acc 0.890625
2017-03-02T17:57:16.950536: step 19150, loss 0.233411, acc 0.875
2017-03-02T17:57:17.022736: step 19151, loss 0.137006, acc 0.953125
2017-03-02T17:57:17.103703: step 19152, loss 0.0827617, acc 0.96875
2017-03-02T17:57:17.175206: step 19153, loss 0.164953, acc 0.90625
2017-03-02T17:57:17.250119: step 19154, loss 0.215349, acc 0.90625
2017-03-02T17:57:17.323313: step 19155, loss 0.106157, acc 0.953125
2017-03-02T17:57:17.413145: step 19156, loss 0.096939, acc 0.953125
2017-03-02T17:57:17.489058: step 19157, loss 0.365564, acc 0.875
2017-03-02T17:57:17.566401: step 19158, loss 0.0690724, acc 0.984375
2017-03-02T17:57:17.634227: step 19159, loss 0.210402, acc 0.9375
2017-03-02T17:57:17.722974: step 19160, loss 0.147493, acc 0.921875
2017-03-02T17:57:17.791017: step 19161, loss 0.0303574, acc 1
2017-03-02T17:57:17.863558: step 19162, loss 0.0936617, acc 0.9375
2017-03-02T17:57:17.937904: step 19163, loss 0.194599, acc 0.90625
2017-03-02T17:57:18.005256: step 19164, loss 0.237783, acc 0.890625
2017-03-02T17:57:18.075743: step 19165, loss 0.134689, acc 0.9375
2017-03-02T17:57:18.151196: step 19166, loss 0.302607, acc 0.890625
2017-03-02T17:57:18.231304: step 19167, loss 0.239883, acc 0.90625
2017-03-02T17:57:18.306473: step 19168, loss 0.10432, acc 0.921875
2017-03-02T17:57:18.374991: step 19169, loss 0.100316, acc 0.953125
2017-03-02T17:57:18.445462: step 19170, loss 0.313171, acc 0.859375
2017-03-02T17:57:18.515593: step 19171, loss 0.10767, acc 0.953125
2017-03-02T17:57:18.615915: step 19172, loss 0.0961571, acc 0.953125
2017-03-02T17:57:18.687895: step 19173, loss 0.20841, acc 0.921875
2017-03-02T17:57:18.766319: step 19174, loss 0.0692725, acc 0.96875
2017-03-02T17:57:18.839979: step 19175, loss 0.101757, acc 0.9375
2017-03-02T17:57:18.912839: step 19176, loss 0.174876, acc 0.9375
2017-03-02T17:57:18.985558: step 19177, loss 0.228538, acc 0.9375
2017-03-02T17:57:19.060056: step 19178, loss 0.11316, acc 0.921875
2017-03-02T17:57:19.136201: step 19179, loss 0.201085, acc 0.90625
2017-03-02T17:57:19.216919: step 19180, loss 0.168054, acc 0.9375
2017-03-02T17:57:19.294223: step 19181, loss 0.195763, acc 0.90625
2017-03-02T17:57:19.367954: step 19182, loss 0.229136, acc 0.90625
2017-03-02T17:57:19.443153: step 19183, loss 0.100598, acc 0.96875
2017-03-02T17:57:19.516895: step 19184, loss 0.107877, acc 0.953125
2017-03-02T17:57:19.590997: step 19185, loss 0.167524, acc 0.890625
2017-03-02T17:57:19.663218: step 19186, loss 0.0720289, acc 0.96875
2017-03-02T17:57:19.732150: step 19187, loss 0.136537, acc 0.953125
2017-03-02T17:57:19.797004: step 19188, loss 0.0910163, acc 0.953125
2017-03-02T17:57:19.893824: step 19189, loss 0.113416, acc 0.953125
2017-03-02T17:57:19.970397: step 19190, loss 0.0882357, acc 0.96875
2017-03-02T17:57:20.042723: step 19191, loss 0.127824, acc 0.96875
2017-03-02T17:57:20.116966: step 19192, loss 0.249675, acc 0.90625
2017-03-02T17:57:20.189409: step 19193, loss 0.25914, acc 0.875
2017-03-02T17:57:20.269557: step 19194, loss 0.0928565, acc 0.953125
2017-03-02T17:57:20.341172: step 19195, loss 0.19439, acc 0.90625
2017-03-02T17:57:20.414567: step 19196, loss 0.141215, acc 0.96875
2017-03-02T17:57:20.490470: step 19197, loss 0.156722, acc 0.921875
2017-03-02T17:57:20.555473: step 19198, loss 0.127329, acc 0.9375
2017-03-02T17:57:20.628338: step 19199, loss 0.313047, acc 0.84375
2017-03-02T17:57:20.704591: step 19200, loss 0.140914, acc 0.9375

Evaluation:
2017-03-02T17:57:20.737707: step 19200, loss 2.30473, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19200

2017-03-02T17:57:21.199424: step 19201, loss 0.180328, acc 0.953125
2017-03-02T17:57:21.272988: step 19202, loss 0.127032, acc 0.9375
2017-03-02T17:57:21.343128: step 19203, loss 0.0620568, acc 0.96875
2017-03-02T17:57:21.413764: step 19204, loss 0.173072, acc 0.921875
2017-03-02T17:57:21.484245: step 19205, loss 0.140469, acc 0.9375
2017-03-02T17:57:21.558135: step 19206, loss 0.11572, acc 0.953125
2017-03-02T17:57:21.632398: step 19207, loss 0.137286, acc 0.953125
2017-03-02T17:57:21.701382: step 19208, loss 0.427532, acc 0.75
2017-03-02T17:57:21.789637: step 19209, loss 0.0599356, acc 0.984375
2017-03-02T17:57:21.855423: step 19210, loss 0.0516064, acc 0.984375
2017-03-02T17:57:21.927757: step 19211, loss 0.176451, acc 0.890625
2017-03-02T17:57:22.004687: step 19212, loss 0.0409388, acc 0.984375
2017-03-02T17:57:22.079418: step 19213, loss 0.109176, acc 0.9375
2017-03-02T17:57:22.158196: step 19214, loss 0.0777584, acc 0.9375
2017-03-02T17:57:22.233381: step 19215, loss 0.125072, acc 0.953125
2017-03-02T17:57:22.322221: step 19216, loss 0.138218, acc 0.90625
2017-03-02T17:57:22.389443: step 19217, loss 0.161026, acc 0.9375
2017-03-02T17:57:22.465398: step 19218, loss 0.198052, acc 0.890625
2017-03-02T17:57:22.536357: step 19219, loss 0.16731, acc 0.9375
2017-03-02T17:57:22.613870: step 19220, loss 0.286332, acc 0.90625
2017-03-02T17:57:22.685249: step 19221, loss 0.058393, acc 0.96875
2017-03-02T17:57:22.764406: step 19222, loss 0.239916, acc 0.859375
2017-03-02T17:57:22.839231: step 19223, loss 0.221625, acc 0.9375
2017-03-02T17:57:22.907193: step 19224, loss 0.17712, acc 0.9375
2017-03-02T17:57:22.980329: step 19225, loss 0.160487, acc 0.90625
2017-03-02T17:57:23.055324: step 19226, loss 0.202907, acc 0.875
2017-03-02T17:57:23.137322: step 19227, loss 0.122194, acc 0.921875
2017-03-02T17:57:23.218525: step 19228, loss 0.202216, acc 0.875
2017-03-02T17:57:23.288324: step 19229, loss 0.153121, acc 0.9375
2017-03-02T17:57:23.357212: step 19230, loss 0.186093, acc 0.921875
2017-03-02T17:57:23.432461: step 19231, loss 0.0920857, acc 0.9375
2017-03-02T17:57:23.504382: step 19232, loss 0.0822159, acc 0.96875
2017-03-02T17:57:23.575522: step 19233, loss 0.109831, acc 0.96875
2017-03-02T17:57:23.661732: step 19234, loss 0.126888, acc 0.96875
2017-03-02T17:57:23.730860: step 19235, loss 0.11179, acc 0.921875
2017-03-02T17:57:23.808819: step 19236, loss 0.22958, acc 0.875
2017-03-02T17:57:23.874538: step 19237, loss 0.121936, acc 0.953125
2017-03-02T17:57:23.941882: step 19238, loss 0.252203, acc 0.90625
2017-03-02T17:57:24.004812: step 19239, loss 0.153227, acc 0.90625
2017-03-02T17:57:24.069117: step 19240, loss 0.143745, acc 0.953125
2017-03-02T17:57:24.144750: step 19241, loss 0.111945, acc 0.953125
2017-03-02T17:57:24.224876: step 19242, loss 0.234718, acc 0.859375
2017-03-02T17:57:24.302567: step 19243, loss 0.284385, acc 0.875
2017-03-02T17:57:24.383416: step 19244, loss 0.128577, acc 0.96875
2017-03-02T17:57:24.448389: step 19245, loss 0.114366, acc 0.953125
2017-03-02T17:57:24.520591: step 19246, loss 0.0866925, acc 0.9375
2017-03-02T17:57:24.594875: step 19247, loss 0.207473, acc 0.90625
2017-03-02T17:57:24.666704: step 19248, loss 0.158637, acc 0.921875
2017-03-02T17:57:24.736367: step 19249, loss 0.103321, acc 0.953125
2017-03-02T17:57:24.804941: step 19250, loss 0.135396, acc 0.953125
2017-03-02T17:57:24.873743: step 19251, loss 0.0441182, acc 1
2017-03-02T17:57:24.951738: step 19252, loss 0.148497, acc 0.921875
2017-03-02T17:57:25.024845: step 19253, loss 0.184553, acc 0.921875
2017-03-02T17:57:25.098758: step 19254, loss 0.174141, acc 0.953125
2017-03-02T17:57:25.171062: step 19255, loss 0.157511, acc 0.9375
2017-03-02T17:57:25.254582: step 19256, loss 0.114323, acc 0.953125
2017-03-02T17:57:25.330081: step 19257, loss 0.152045, acc 0.90625
2017-03-02T17:57:25.398309: step 19258, loss 0.088933, acc 0.96875
2017-03-02T17:57:25.468573: step 19259, loss 0.103397, acc 0.96875
2017-03-02T17:57:25.541403: step 19260, loss 0.117114, acc 0.9375
2017-03-02T17:57:25.631149: step 19261, loss 0.0915589, acc 0.953125
2017-03-02T17:57:25.715563: step 19262, loss 0.0690128, acc 0.984375
2017-03-02T17:57:25.794240: step 19263, loss 0.0552365, acc 0.984375
2017-03-02T17:57:25.868137: step 19264, loss 0.268338, acc 0.921875
2017-03-02T17:57:25.948219: step 19265, loss 0.303238, acc 0.875
2017-03-02T17:57:26.025945: step 19266, loss 0.184351, acc 0.921875
2017-03-02T17:57:26.091369: step 19267, loss 0.173679, acc 0.890625
2017-03-02T17:57:26.166484: step 19268, loss 0.112865, acc 0.96875
2017-03-02T17:57:26.241526: step 19269, loss 0.154438, acc 0.921875
2017-03-02T17:57:26.323957: step 19270, loss 0.194327, acc 0.921875
2017-03-02T17:57:26.410002: step 19271, loss 0.207539, acc 0.875
2017-03-02T17:57:26.496188: step 19272, loss 0.195668, acc 0.921875
2017-03-02T17:57:26.578692: step 19273, loss 0.111208, acc 0.953125
2017-03-02T17:57:26.659781: step 19274, loss 0.141063, acc 0.90625
2017-03-02T17:57:26.740964: step 19275, loss 0.110126, acc 0.953125
2017-03-02T17:57:26.810338: step 19276, loss 0.237724, acc 0.890625
2017-03-02T17:57:26.875905: step 19277, loss 0.0971077, acc 0.953125
2017-03-02T17:57:26.947901: step 19278, loss 0.22774, acc 0.875
2017-03-02T17:57:27.025610: step 19279, loss 0.137498, acc 0.953125
2017-03-02T17:57:27.100460: step 19280, loss 0.111198, acc 0.953125
2017-03-02T17:57:27.173574: step 19281, loss 0.271398, acc 0.859375
2017-03-02T17:57:27.245366: step 19282, loss 0.206625, acc 0.890625
2017-03-02T17:57:27.317536: step 19283, loss 0.226095, acc 0.921875
2017-03-02T17:57:27.391953: step 19284, loss 0.0956343, acc 0.96875
2017-03-02T17:57:27.463379: step 19285, loss 0.178149, acc 0.921875
2017-03-02T17:57:27.531140: step 19286, loss 0.246649, acc 0.859375
2017-03-02T17:57:27.605165: step 19287, loss 0.110007, acc 0.953125
2017-03-02T17:57:27.678190: step 19288, loss 0.0918985, acc 0.96875
2017-03-02T17:57:27.754803: step 19289, loss 0.209096, acc 0.921875
2017-03-02T17:57:27.825368: step 19290, loss 0.231519, acc 0.890625
2017-03-02T17:57:27.903926: step 19291, loss 0.104071, acc 0.96875
2017-03-02T17:57:27.972091: step 19292, loss 0.125785, acc 0.953125
2017-03-02T17:57:28.048415: step 19293, loss 0.161971, acc 0.890625
2017-03-02T17:57:28.125902: step 19294, loss 0.0897455, acc 0.953125
2017-03-02T17:57:28.192528: step 19295, loss 0.232557, acc 0.921875
2017-03-02T17:57:28.261876: step 19296, loss 0.216002, acc 0.890625
2017-03-02T17:57:28.336192: step 19297, loss 0.0859467, acc 0.96875
2017-03-02T17:57:28.412337: step 19298, loss 0.176953, acc 0.921875
2017-03-02T17:57:28.490966: step 19299, loss 0.203697, acc 0.9375
2017-03-02T17:57:28.567012: step 19300, loss 0.12468, acc 0.9375

Evaluation:
2017-03-02T17:57:28.602043: step 19300, loss 2.32695, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19300

2017-03-02T17:57:29.042583: step 19301, loss 0.172894, acc 0.953125
2017-03-02T17:57:29.123187: step 19302, loss 0.116885, acc 0.9375
2017-03-02T17:57:29.211624: step 19303, loss 0.303328, acc 0.921875
2017-03-02T17:57:29.287047: step 19304, loss 0.0800996, acc 0.96875
2017-03-02T17:57:29.358189: step 19305, loss 0.0787122, acc 0.953125
2017-03-02T17:57:29.434221: step 19306, loss 0.102041, acc 0.953125
2017-03-02T17:57:29.509033: step 19307, loss 0.121264, acc 0.9375
2017-03-02T17:57:29.601298: step 19308, loss 0.126225, acc 0.953125
2017-03-02T17:57:29.680114: step 19309, loss 0.114599, acc 0.9375
2017-03-02T17:57:29.756406: step 19310, loss 0.155177, acc 0.90625
2017-03-02T17:57:29.827270: step 19311, loss 0.159113, acc 0.9375
2017-03-02T17:57:29.899629: step 19312, loss 0.0973835, acc 0.96875
2017-03-02T17:57:29.983910: step 19313, loss 0.178354, acc 0.90625
2017-03-02T17:57:30.055675: step 19314, loss 0.216387, acc 0.921875
2017-03-02T17:57:30.134563: step 19315, loss 0.240558, acc 0.90625
2017-03-02T17:57:30.202760: step 19316, loss 0.108459, acc 0.9375
2017-03-02T17:57:30.269829: step 19317, loss 0.176503, acc 0.9375
2017-03-02T17:57:30.346766: step 19318, loss 0.133038, acc 0.921875
2017-03-02T17:57:30.416815: step 19319, loss 0.165998, acc 0.921875
2017-03-02T17:57:30.493037: step 19320, loss 0.0772728, acc 0.96875
2017-03-02T17:57:30.574295: step 19321, loss 0.141722, acc 0.921875
2017-03-02T17:57:30.660051: step 19322, loss 0.11707, acc 0.9375
2017-03-02T17:57:30.729133: step 19323, loss 0.0241345, acc 1
2017-03-02T17:57:30.800171: step 19324, loss 0.193283, acc 0.921875
2017-03-02T17:57:30.867647: step 19325, loss 0.0868306, acc 0.953125
2017-03-02T17:57:30.938771: step 19326, loss 0.108348, acc 0.953125
2017-03-02T17:57:31.013655: step 19327, loss 0.242287, acc 0.921875
2017-03-02T17:57:31.109743: step 19328, loss 0.209703, acc 0.921875
2017-03-02T17:57:31.184670: step 19329, loss 0.141575, acc 0.90625
2017-03-02T17:57:31.260545: step 19330, loss 0.209244, acc 0.921875
2017-03-02T17:57:31.337157: step 19331, loss 0.217183, acc 0.953125
2017-03-02T17:57:31.414342: step 19332, loss 0.135092, acc 0.9375
2017-03-02T17:57:31.485383: step 19333, loss 0.204028, acc 0.921875
2017-03-02T17:57:31.557507: step 19334, loss 0.113184, acc 0.921875
2017-03-02T17:57:31.625907: step 19335, loss 0.312186, acc 0.875
2017-03-02T17:57:31.695583: step 19336, loss 0.0910898, acc 0.96875
2017-03-02T17:57:31.769311: step 19337, loss 0.0924174, acc 0.9375
2017-03-02T17:57:31.855034: step 19338, loss 0.162469, acc 0.953125
2017-03-02T17:57:31.926946: step 19339, loss 0.108502, acc 0.953125
2017-03-02T17:57:31.997750: step 19340, loss 0.101691, acc 0.953125
2017-03-02T17:57:32.068205: step 19341, loss 0.158743, acc 0.9375
2017-03-02T17:57:32.134866: step 19342, loss 0.18496, acc 0.9375
2017-03-02T17:57:32.206651: step 19343, loss 0.217891, acc 0.859375
2017-03-02T17:57:32.282587: step 19344, loss 0.0909633, acc 0.984375
2017-03-02T17:57:32.348376: step 19345, loss 0.150353, acc 0.953125
2017-03-02T17:57:32.417942: step 19346, loss 0.155133, acc 0.921875
2017-03-02T17:57:32.500410: step 19347, loss 0.104028, acc 0.96875
2017-03-02T17:57:32.591683: step 19348, loss 0.11307, acc 0.9375
2017-03-02T17:57:32.684587: step 19349, loss 0.156376, acc 0.9375
2017-03-02T17:57:32.761179: step 19350, loss 0.0599943, acc 1
2017-03-02T17:57:32.831800: step 19351, loss 0.140928, acc 0.953125
2017-03-02T17:57:32.912864: step 19352, loss 0.33706, acc 0.890625
2017-03-02T17:57:32.986252: step 19353, loss 0.0722122, acc 0.96875
2017-03-02T17:57:33.057144: step 19354, loss 0.243496, acc 0.9375
2017-03-02T17:57:33.131165: step 19355, loss 0.189217, acc 0.90625
2017-03-02T17:57:33.208967: step 19356, loss 0.193714, acc 0.9375
2017-03-02T17:57:33.280539: step 19357, loss 0.0991149, acc 0.953125
2017-03-02T17:57:33.354148: step 19358, loss 0.124745, acc 0.9375
2017-03-02T17:57:33.427586: step 19359, loss 0.187929, acc 0.96875
2017-03-02T17:57:33.500701: step 19360, loss 0.164091, acc 0.9375
2017-03-02T17:57:33.570297: step 19361, loss 0.157164, acc 0.921875
2017-03-02T17:57:33.644182: step 19362, loss 0.151484, acc 0.921875
2017-03-02T17:57:33.720710: step 19363, loss 0.197136, acc 0.90625
2017-03-02T17:57:33.789322: step 19364, loss 0.224054, acc 0.90625
2017-03-02T17:57:33.864392: step 19365, loss 0.0877287, acc 0.96875
2017-03-02T17:57:33.939403: step 19366, loss 0.148949, acc 0.921875
2017-03-02T17:57:34.017135: step 19367, loss 0.29071, acc 0.890625
2017-03-02T17:57:34.094068: step 19368, loss 0.134406, acc 0.9375
2017-03-02T17:57:34.167024: step 19369, loss 0.190351, acc 0.890625
2017-03-02T17:57:34.240640: step 19370, loss 0.0895682, acc 0.953125
2017-03-02T17:57:34.309641: step 19371, loss 0.195647, acc 0.921875
2017-03-02T17:57:34.373234: step 19372, loss 0.186875, acc 0.9375
2017-03-02T17:57:34.445077: step 19373, loss 0.0371963, acc 0.984375
2017-03-02T17:57:34.518603: step 19374, loss 0.109897, acc 0.9375
2017-03-02T17:57:34.592813: step 19375, loss 0.167178, acc 0.9375
2017-03-02T17:57:34.661900: step 19376, loss 0.14483, acc 0.921875
2017-03-02T17:57:34.737069: step 19377, loss 0.18285, acc 0.921875
2017-03-02T17:57:34.818160: step 19378, loss 0.104151, acc 0.953125
2017-03-02T17:57:34.898260: step 19379, loss 0.145487, acc 0.9375
2017-03-02T17:57:34.966304: step 19380, loss 0.144744, acc 0.953125
2017-03-02T17:57:35.033206: step 19381, loss 0.162072, acc 0.90625
2017-03-02T17:57:35.099663: step 19382, loss 0.1795, acc 0.9375
2017-03-02T17:57:35.174982: step 19383, loss 0.21052, acc 0.921875
2017-03-02T17:57:35.257433: step 19384, loss 0.187426, acc 0.90625
2017-03-02T17:57:35.337384: step 19385, loss 0.115669, acc 0.921875
2017-03-02T17:57:35.411109: step 19386, loss 0.222077, acc 0.921875
2017-03-02T17:57:35.490398: step 19387, loss 0.138757, acc 0.90625
2017-03-02T17:57:35.554898: step 19388, loss 0.302354, acc 0.890625
2017-03-02T17:57:35.631351: step 19389, loss 0.216441, acc 0.9375
2017-03-02T17:57:35.701168: step 19390, loss 0.106743, acc 0.96875
2017-03-02T17:57:35.778092: step 19391, loss 0.163291, acc 0.90625
2017-03-02T17:57:35.848878: step 19392, loss 0.133835, acc 0.953125
2017-03-02T17:57:35.937204: step 19393, loss 0.17666, acc 0.921875
2017-03-02T17:57:36.014892: step 19394, loss 0.0750785, acc 0.96875
2017-03-02T17:57:36.087168: step 19395, loss 0.293409, acc 0.859375
2017-03-02T17:57:36.168860: step 19396, loss 0.267549, acc 0.84375
2017-03-02T17:57:36.249593: step 19397, loss 0.0777358, acc 0.96875
2017-03-02T17:57:36.329730: step 19398, loss 0.303599, acc 0.90625
2017-03-02T17:57:36.403985: step 19399, loss 0.153021, acc 0.953125
2017-03-02T17:57:36.471734: step 19400, loss 0.235734, acc 0.828125

Evaluation:
2017-03-02T17:57:36.504416: step 19400, loss 2.24337, acc 0.638068

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19400

2017-03-02T17:57:36.954929: step 19401, loss 0.250741, acc 0.90625
2017-03-02T17:57:37.035957: step 19402, loss 0.297943, acc 0.875
2017-03-02T17:57:37.100973: step 19403, loss 0.216728, acc 0.890625
2017-03-02T17:57:37.160733: step 19404, loss 0.102925, acc 1
2017-03-02T17:57:37.232687: step 19405, loss 0.0828145, acc 0.96875
2017-03-02T17:57:37.296975: step 19406, loss 0.0662467, acc 0.984375
2017-03-02T17:57:37.371414: step 19407, loss 0.125137, acc 0.9375
2017-03-02T17:57:37.444628: step 19408, loss 0.11392, acc 0.921875
2017-03-02T17:57:37.510508: step 19409, loss 0.160629, acc 0.921875
2017-03-02T17:57:37.578746: step 19410, loss 0.0786191, acc 0.96875
2017-03-02T17:57:37.653976: step 19411, loss 0.123663, acc 0.984375
2017-03-02T17:57:37.732980: step 19412, loss 0.0906214, acc 0.96875
2017-03-02T17:57:37.800030: step 19413, loss 0.206709, acc 0.921875
2017-03-02T17:57:37.871747: step 19414, loss 0.181649, acc 0.9375
2017-03-02T17:57:37.941676: step 19415, loss 0.208006, acc 0.90625
2017-03-02T17:57:38.010177: step 19416, loss 0.130733, acc 0.90625
2017-03-02T17:57:38.092290: step 19417, loss 0.0887787, acc 0.984375
2017-03-02T17:57:38.164248: step 19418, loss 0.0816464, acc 0.96875
2017-03-02T17:57:38.237382: step 19419, loss 0.263783, acc 0.890625
2017-03-02T17:57:38.327952: step 19420, loss 0.313865, acc 0.859375
2017-03-02T17:57:38.406267: step 19421, loss 0.141359, acc 0.96875
2017-03-02T17:57:38.475545: step 19422, loss 0.102075, acc 0.96875
2017-03-02T17:57:38.556379: step 19423, loss 0.369776, acc 0.890625
2017-03-02T17:57:38.631961: step 19424, loss 0.105638, acc 0.9375
2017-03-02T17:57:38.691813: step 19425, loss 0.0630163, acc 0.96875
2017-03-02T17:57:38.752492: step 19426, loss 0.109441, acc 0.9375
2017-03-02T17:57:38.835951: step 19427, loss 0.0946611, acc 0.9375
2017-03-02T17:57:38.912888: step 19428, loss 0.107343, acc 0.953125
2017-03-02T17:57:38.990545: step 19429, loss 0.0631748, acc 1
2017-03-02T17:57:39.058759: step 19430, loss 0.192992, acc 0.890625
2017-03-02T17:57:39.135868: step 19431, loss 0.0757532, acc 0.96875
2017-03-02T17:57:39.207079: step 19432, loss 0.150792, acc 0.953125
2017-03-02T17:57:39.302845: step 19433, loss 0.198934, acc 0.921875
2017-03-02T17:57:39.375240: step 19434, loss 0.102239, acc 0.9375
2017-03-02T17:57:39.453995: step 19435, loss 0.29182, acc 0.859375
2017-03-02T17:57:39.523507: step 19436, loss 0.208645, acc 0.890625
2017-03-02T17:57:39.592569: step 19437, loss 0.0460663, acc 1
2017-03-02T17:57:39.661846: step 19438, loss 0.0932605, acc 0.9375
2017-03-02T17:57:39.739420: step 19439, loss 0.335563, acc 0.875
2017-03-02T17:57:39.811668: step 19440, loss 0.0917051, acc 0.953125
2017-03-02T17:57:39.888959: step 19441, loss 0.0596845, acc 0.984375
2017-03-02T17:57:39.957847: step 19442, loss 0.11506, acc 0.9375
2017-03-02T17:57:40.022996: step 19443, loss 0.136535, acc 0.953125
2017-03-02T17:57:40.095014: step 19444, loss 0.190796, acc 0.9375
2017-03-02T17:57:40.168545: step 19445, loss 0.142442, acc 0.90625
2017-03-02T17:57:40.242412: step 19446, loss 0.192581, acc 0.90625
2017-03-02T17:57:40.321188: step 19447, loss 0.0870637, acc 0.96875
2017-03-02T17:57:40.391665: step 19448, loss 0.106, acc 0.9375
2017-03-02T17:57:40.463903: step 19449, loss 0.158532, acc 0.9375
2017-03-02T17:57:40.539676: step 19450, loss 0.213622, acc 0.90625
2017-03-02T17:57:40.611183: step 19451, loss 0.0868284, acc 0.96875
2017-03-02T17:57:40.685168: step 19452, loss 0.140128, acc 0.9375
2017-03-02T17:57:40.769181: step 19453, loss 0.0833464, acc 0.953125
2017-03-02T17:57:40.839633: step 19454, loss 0.13829, acc 0.9375
2017-03-02T17:57:40.905462: step 19455, loss 0.201453, acc 0.953125
2017-03-02T17:57:40.978893: step 19456, loss 0.101615, acc 0.953125
2017-03-02T17:57:41.054145: step 19457, loss 0.114261, acc 0.96875
2017-03-02T17:57:41.128047: step 19458, loss 0.115194, acc 0.9375
2017-03-02T17:57:41.205777: step 19459, loss 0.181674, acc 0.921875
2017-03-02T17:57:41.277652: step 19460, loss 0.163741, acc 0.953125
2017-03-02T17:57:41.347812: step 19461, loss 0.21378, acc 0.921875
2017-03-02T17:57:41.422755: step 19462, loss 0.063694, acc 0.96875
2017-03-02T17:57:41.492293: step 19463, loss 0.172319, acc 0.921875
2017-03-02T17:57:41.567027: step 19464, loss 0.146525, acc 0.953125
2017-03-02T17:57:41.634049: step 19465, loss 0.177455, acc 0.90625
2017-03-02T17:57:41.704190: step 19466, loss 0.136308, acc 0.953125
2017-03-02T17:57:41.776207: step 19467, loss 0.0997769, acc 0.953125
2017-03-02T17:57:41.853121: step 19468, loss 0.113559, acc 0.9375
2017-03-02T17:57:41.927086: step 19469, loss 0.159698, acc 0.921875
2017-03-02T17:57:41.998174: step 19470, loss 0.284588, acc 0.90625
2017-03-02T17:57:42.066188: step 19471, loss 0.120924, acc 0.96875
2017-03-02T17:57:42.138135: step 19472, loss 0.0948236, acc 0.953125
2017-03-02T17:57:42.209943: step 19473, loss 0.208063, acc 0.90625
2017-03-02T17:57:42.283250: step 19474, loss 0.0540522, acc 0.984375
2017-03-02T17:57:42.360016: step 19475, loss 0.199098, acc 0.90625
2017-03-02T17:57:42.436405: step 19476, loss 0.208876, acc 0.90625
2017-03-02T17:57:42.520115: step 19477, loss 0.139809, acc 0.9375
2017-03-02T17:57:42.591886: step 19478, loss 0.155174, acc 0.9375
2017-03-02T17:57:42.663759: step 19479, loss 0.17785, acc 0.921875
2017-03-02T17:57:42.737135: step 19480, loss 0.074022, acc 0.953125
2017-03-02T17:57:42.812114: step 19481, loss 0.330264, acc 0.859375
2017-03-02T17:57:42.882694: step 19482, loss 0.116778, acc 0.953125
2017-03-02T17:57:42.954218: step 19483, loss 0.0988496, acc 0.953125
2017-03-02T17:57:43.025969: step 19484, loss 0.323648, acc 0.84375
2017-03-02T17:57:43.107327: step 19485, loss 0.206343, acc 0.90625
2017-03-02T17:57:43.180867: step 19486, loss 0.107227, acc 0.96875
2017-03-02T17:57:43.254220: step 19487, loss 0.128058, acc 0.921875
2017-03-02T17:57:43.321116: step 19488, loss 0.137055, acc 0.921875
2017-03-02T17:57:43.403724: step 19489, loss 0.081852, acc 0.96875
2017-03-02T17:57:43.472659: step 19490, loss 0.193989, acc 0.90625
2017-03-02T17:57:43.542135: step 19491, loss 0.0876607, acc 0.96875
2017-03-02T17:57:43.612222: step 19492, loss 0.0722031, acc 0.953125
2017-03-02T17:57:43.682435: step 19493, loss 0.285044, acc 0.875
2017-03-02T17:57:43.755740: step 19494, loss 0.21996, acc 0.890625
2017-03-02T17:57:43.842029: step 19495, loss 0.103032, acc 0.9375
2017-03-02T17:57:43.914803: step 19496, loss 0.0948363, acc 0.953125
2017-03-02T17:57:43.982659: step 19497, loss 0.134937, acc 0.953125
2017-03-02T17:57:44.051986: step 19498, loss 0.160116, acc 0.9375
2017-03-02T17:57:44.129571: step 19499, loss 0.143317, acc 0.9375
2017-03-02T17:57:44.200004: step 19500, loss 0.117179, acc 0.953125

Evaluation:
2017-03-02T17:57:44.230856: step 19500, loss 2.34857, acc 0.64744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19500

2017-03-02T17:57:46.176346: step 19501, loss 0.114985, acc 0.953125
2017-03-02T17:57:46.245746: step 19502, loss 0.0849505, acc 0.96875
2017-03-02T17:57:46.323973: step 19503, loss 0.198061, acc 0.90625
2017-03-02T17:57:46.397112: step 19504, loss 0.0971121, acc 0.953125
2017-03-02T17:57:46.479153: step 19505, loss 0.20935, acc 0.859375
2017-03-02T17:57:46.560454: step 19506, loss 0.156807, acc 0.9375
2017-03-02T17:57:46.638936: step 19507, loss 0.12463, acc 0.953125
2017-03-02T17:57:46.714137: step 19508, loss 0.230054, acc 0.921875
2017-03-02T17:57:46.783811: step 19509, loss 0.0983709, acc 0.984375
2017-03-02T17:57:46.849945: step 19510, loss 0.175361, acc 0.921875
2017-03-02T17:57:46.926298: step 19511, loss 0.0643619, acc 0.96875
2017-03-02T17:57:47.011857: step 19512, loss 0.184249, acc 0.90625
2017-03-02T17:57:47.072704: step 19513, loss 0.0644268, acc 0.96875
2017-03-02T17:57:47.147584: step 19514, loss 0.109613, acc 0.96875
2017-03-02T17:57:47.221825: step 19515, loss 0.0974464, acc 0.953125
2017-03-02T17:57:47.305004: step 19516, loss 0.179308, acc 0.9375
2017-03-02T17:57:47.377564: step 19517, loss 0.107223, acc 0.984375
2017-03-02T17:57:47.450816: step 19518, loss 0.0440789, acc 0.984375
2017-03-02T17:57:47.525643: step 19519, loss 0.207017, acc 0.90625
2017-03-02T17:57:47.600689: step 19520, loss 0.178708, acc 0.921875
2017-03-02T17:57:47.671427: step 19521, loss 0.171981, acc 0.90625
2017-03-02T17:57:47.742617: step 19522, loss 0.196605, acc 0.890625
2017-03-02T17:57:47.813216: step 19523, loss 0.304857, acc 0.875
2017-03-02T17:57:47.884754: step 19524, loss 0.118097, acc 0.90625
2017-03-02T17:57:47.969576: step 19525, loss 0.164415, acc 0.90625
2017-03-02T17:57:48.036759: step 19526, loss 0.0991009, acc 0.96875
2017-03-02T17:57:48.111749: step 19527, loss 0.220897, acc 0.875
2017-03-02T17:57:48.193716: step 19528, loss 0.11278, acc 0.96875
2017-03-02T17:57:48.267509: step 19529, loss 0.138526, acc 0.921875
2017-03-02T17:57:48.339944: step 19530, loss 0.218034, acc 0.921875
2017-03-02T17:57:48.424849: step 19531, loss 0.102172, acc 0.953125
2017-03-02T17:57:48.489002: step 19532, loss 0.242419, acc 0.890625
2017-03-02T17:57:48.560303: step 19533, loss 0.11282, acc 0.9375
2017-03-02T17:57:48.631174: step 19534, loss 0.0664819, acc 0.96875
2017-03-02T17:57:48.702572: step 19535, loss 0.250811, acc 0.921875
2017-03-02T17:57:48.771426: step 19536, loss 0.196874, acc 0.921875
2017-03-02T17:57:48.837873: step 19537, loss 0.0970948, acc 0.96875
2017-03-02T17:57:48.911753: step 19538, loss 0.0523784, acc 0.953125
2017-03-02T17:57:49.003883: step 19539, loss 0.165705, acc 0.921875
2017-03-02T17:57:49.090959: step 19540, loss 0.144944, acc 0.96875
2017-03-02T17:57:49.187343: step 19541, loss 0.178027, acc 0.90625
2017-03-02T17:57:49.265655: step 19542, loss 0.167431, acc 0.96875
2017-03-02T17:57:49.341082: step 19543, loss 0.182349, acc 0.921875
2017-03-02T17:57:49.410402: step 19544, loss 0.0759896, acc 0.953125
2017-03-02T17:57:49.481680: step 19545, loss 0.201334, acc 0.90625
2017-03-02T17:57:49.554223: step 19546, loss 0.160744, acc 0.890625
2017-03-02T17:57:49.648767: step 19547, loss 0.0975093, acc 0.96875
2017-03-02T17:57:49.720947: step 19548, loss 0.0871153, acc 0.96875
2017-03-02T17:57:49.798255: step 19549, loss 0.193936, acc 0.921875
2017-03-02T17:57:49.872157: step 19550, loss 0.227184, acc 0.890625
2017-03-02T17:57:49.951898: step 19551, loss 0.231131, acc 0.90625
2017-03-02T17:57:50.028661: step 19552, loss 0.0539676, acc 1
2017-03-02T17:57:50.097169: step 19553, loss 0.0944454, acc 0.96875
2017-03-02T17:57:50.165356: step 19554, loss 0.113133, acc 0.9375
2017-03-02T17:57:50.241724: step 19555, loss 0.156374, acc 0.9375
2017-03-02T17:57:50.315417: step 19556, loss 0.222895, acc 0.90625
2017-03-02T17:57:50.391886: step 19557, loss 0.239176, acc 0.90625
2017-03-02T17:57:50.466688: step 19558, loss 0.154059, acc 0.953125
2017-03-02T17:57:50.538192: step 19559, loss 0.151112, acc 0.953125
2017-03-02T17:57:50.615588: step 19560, loss 0.156563, acc 0.953125
2017-03-02T17:57:50.695523: step 19561, loss 0.228493, acc 0.90625
2017-03-02T17:57:50.767860: step 19562, loss 0.124307, acc 0.96875
2017-03-02T17:57:50.845408: step 19563, loss 0.260054, acc 0.875
2017-03-02T17:57:50.917529: step 19564, loss 0.160325, acc 0.953125
2017-03-02T17:57:51.001585: step 19565, loss 0.130103, acc 0.9375
2017-03-02T17:57:51.081418: step 19566, loss 0.101875, acc 0.96875
2017-03-02T17:57:51.158035: step 19567, loss 0.117418, acc 0.9375
2017-03-02T17:57:51.230307: step 19568, loss 0.278225, acc 0.84375
2017-03-02T17:57:51.307992: step 19569, loss 0.13973, acc 0.9375
2017-03-02T17:57:51.384998: step 19570, loss 0.176093, acc 0.921875
2017-03-02T17:57:51.452204: step 19571, loss 0.194024, acc 0.890625
2017-03-02T17:57:51.524844: step 19572, loss 0.083891, acc 0.96875
2017-03-02T17:57:51.594420: step 19573, loss 0.0775941, acc 0.984375
2017-03-02T17:57:51.665937: step 19574, loss 0.16095, acc 0.921875
2017-03-02T17:57:51.735273: step 19575, loss 0.124328, acc 0.984375
2017-03-02T17:57:51.811330: step 19576, loss 0.268406, acc 0.90625
2017-03-02T17:57:51.889473: step 19577, loss 0.202185, acc 0.875
2017-03-02T17:57:51.963996: step 19578, loss 0.282575, acc 0.90625
2017-03-02T17:57:52.040383: step 19579, loss 0.247723, acc 0.890625
2017-03-02T17:57:52.127618: step 19580, loss 0.219982, acc 0.90625
2017-03-02T17:57:52.201788: step 19581, loss 0.0902733, acc 0.96875
2017-03-02T17:57:52.270882: step 19582, loss 0.11494, acc 0.953125
2017-03-02T17:57:52.346844: step 19583, loss 0.293043, acc 0.859375
2017-03-02T17:57:52.438233: step 19584, loss 0.243509, acc 0.875
2017-03-02T17:57:52.513031: step 19585, loss 0.0905544, acc 0.953125
2017-03-02T17:57:52.589655: step 19586, loss 0.183461, acc 0.90625
2017-03-02T17:57:52.676993: step 19587, loss 0.327001, acc 0.84375
2017-03-02T17:57:52.747636: step 19588, loss 0.118546, acc 0.9375
2017-03-02T17:57:52.819247: step 19589, loss 0.157959, acc 0.921875
2017-03-02T17:57:52.888829: step 19590, loss 0.124638, acc 0.953125
2017-03-02T17:57:52.962588: step 19591, loss 0.149053, acc 0.96875
2017-03-02T17:57:53.036182: step 19592, loss 0.107671, acc 0.984375
2017-03-02T17:57:53.108494: step 19593, loss 0.239771, acc 0.90625
2017-03-02T17:57:53.183284: step 19594, loss 0.209573, acc 0.890625
2017-03-02T17:57:53.260353: step 19595, loss 0.0961892, acc 0.96875
2017-03-02T17:57:53.335610: step 19596, loss 0.118216, acc 0.953125
2017-03-02T17:57:53.409485: step 19597, loss 0.241281, acc 0.859375
2017-03-02T17:57:53.497980: step 19598, loss 0.164055, acc 0.9375
2017-03-02T17:57:53.571466: step 19599, loss 0.235555, acc 0.875
2017-03-02T17:57:53.637203: step 19600, loss 0.130215, acc 1

Evaluation:
2017-03-02T17:57:53.670208: step 19600, loss 2.28565, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19600

2017-03-02T17:57:54.127275: step 19601, loss 0.0620801, acc 0.96875
2017-03-02T17:57:54.212305: step 19602, loss 0.131329, acc 0.96875
2017-03-02T17:57:54.280227: step 19603, loss 0.182452, acc 0.90625
2017-03-02T17:57:54.350265: step 19604, loss 0.164278, acc 0.921875
2017-03-02T17:57:54.420499: step 19605, loss 0.17149, acc 0.9375
2017-03-02T17:57:54.497206: step 19606, loss 0.0391334, acc 1
2017-03-02T17:57:54.585083: step 19607, loss 0.280936, acc 0.921875
2017-03-02T17:57:54.656735: step 19608, loss 0.19166, acc 0.921875
2017-03-02T17:57:54.727329: step 19609, loss 0.190601, acc 0.953125
2017-03-02T17:57:54.806718: step 19610, loss 0.12775, acc 0.96875
2017-03-02T17:57:54.891339: step 19611, loss 0.146885, acc 0.9375
2017-03-02T17:57:54.958394: step 19612, loss 0.227139, acc 0.90625
2017-03-02T17:57:55.035311: step 19613, loss 0.173113, acc 0.9375
2017-03-02T17:57:55.115129: step 19614, loss 0.107461, acc 0.953125
2017-03-02T17:57:55.199467: step 19615, loss 0.368317, acc 0.8125
2017-03-02T17:57:55.268408: step 19616, loss 0.0340277, acc 1
2017-03-02T17:57:55.350076: step 19617, loss 0.161141, acc 0.9375
2017-03-02T17:57:55.423443: step 19618, loss 0.156674, acc 0.921875
2017-03-02T17:57:55.505877: step 19619, loss 0.178124, acc 0.90625
2017-03-02T17:57:55.581462: step 19620, loss 0.0607004, acc 0.984375
2017-03-02T17:57:55.645803: step 19621, loss 0.116007, acc 0.9375
2017-03-02T17:57:55.712756: step 19622, loss 0.12894, acc 0.984375
2017-03-02T17:57:55.783922: step 19623, loss 0.0809582, acc 0.953125
2017-03-02T17:57:55.856065: step 19624, loss 0.123441, acc 0.953125
2017-03-02T17:57:55.926427: step 19625, loss 0.087475, acc 0.9375
2017-03-02T17:57:56.021303: step 19626, loss 0.0513617, acc 0.96875
2017-03-02T17:57:56.096369: step 19627, loss 0.166607, acc 0.921875
2017-03-02T17:57:56.167884: step 19628, loss 0.0687757, acc 0.96875
2017-03-02T17:57:56.243730: step 19629, loss 0.0774685, acc 0.984375
2017-03-02T17:57:56.319890: step 19630, loss 0.101229, acc 0.984375
2017-03-02T17:57:56.389308: step 19631, loss 0.0894427, acc 0.96875
2017-03-02T17:57:56.463293: step 19632, loss 0.174195, acc 0.921875
2017-03-02T17:57:56.552243: step 19633, loss 0.0443723, acc 0.96875
2017-03-02T17:57:56.628238: step 19634, loss 0.0514192, acc 1
2017-03-02T17:57:56.704721: step 19635, loss 0.217502, acc 0.875
2017-03-02T17:57:56.780465: step 19636, loss 0.14687, acc 0.921875
2017-03-02T17:57:56.852836: step 19637, loss 0.21271, acc 0.921875
2017-03-02T17:57:56.914626: step 19638, loss 0.1485, acc 0.921875
2017-03-02T17:57:56.977685: step 19639, loss 0.120519, acc 0.953125
2017-03-02T17:57:57.043096: step 19640, loss 0.10952, acc 0.96875
2017-03-02T17:57:57.116613: step 19641, loss 0.216941, acc 0.9375
2017-03-02T17:57:57.194243: step 19642, loss 0.100287, acc 0.984375
2017-03-02T17:57:57.266942: step 19643, loss 0.0724215, acc 0.96875
2017-03-02T17:57:57.341535: step 19644, loss 0.179493, acc 0.921875
2017-03-02T17:57:57.417494: step 19645, loss 0.16531, acc 0.9375
2017-03-02T17:57:57.487614: step 19646, loss 0.140243, acc 0.9375
2017-03-02T17:57:57.558156: step 19647, loss 0.0791641, acc 0.953125
2017-03-02T17:57:57.649651: step 19648, loss 0.290966, acc 0.90625
2017-03-02T17:57:57.715288: step 19649, loss 0.205481, acc 0.890625
2017-03-02T17:57:57.801005: step 19650, loss 0.146053, acc 0.921875
2017-03-02T17:57:57.871559: step 19651, loss 0.0856849, acc 0.96875
2017-03-02T17:57:57.943239: step 19652, loss 0.0929781, acc 0.96875
2017-03-02T17:57:58.013068: step 19653, loss 0.176894, acc 0.90625
2017-03-02T17:57:58.085531: step 19654, loss 0.169518, acc 0.90625
2017-03-02T17:57:58.183471: step 19655, loss 0.133861, acc 0.953125
2017-03-02T17:57:58.254513: step 19656, loss 0.0723656, acc 0.96875
2017-03-02T17:57:58.326285: step 19657, loss 0.106719, acc 0.953125
2017-03-02T17:57:58.400205: step 19658, loss 0.121413, acc 0.96875
2017-03-02T17:57:58.468159: step 19659, loss 0.17665, acc 0.9375
2017-03-02T17:57:58.542855: step 19660, loss 0.313435, acc 0.859375
2017-03-02T17:57:58.613457: step 19661, loss 0.227663, acc 0.890625
2017-03-02T17:57:58.685856: step 19662, loss 0.16659, acc 0.9375
2017-03-02T17:57:58.759271: step 19663, loss 0.228825, acc 0.890625
2017-03-02T17:57:58.831192: step 19664, loss 0.165459, acc 0.9375
2017-03-02T17:57:58.903262: step 19665, loss 0.114997, acc 0.96875
2017-03-02T17:57:58.973234: step 19666, loss 0.0920381, acc 0.984375
2017-03-02T17:57:59.048226: step 19667, loss 0.131374, acc 0.9375
2017-03-02T17:57:59.121423: step 19668, loss 0.131153, acc 0.9375
2017-03-02T17:57:59.205211: step 19669, loss 0.252611, acc 0.90625
2017-03-02T17:57:59.276531: step 19670, loss 0.222332, acc 0.890625
2017-03-02T17:57:59.346245: step 19671, loss 0.183249, acc 0.921875
2017-03-02T17:57:59.418226: step 19672, loss 0.247056, acc 0.890625
2017-03-02T17:57:59.488300: step 19673, loss 0.100109, acc 0.953125
2017-03-02T17:57:59.565273: step 19674, loss 0.276886, acc 0.859375
2017-03-02T17:57:59.634328: step 19675, loss 0.11148, acc 0.9375
2017-03-02T17:57:59.706101: step 19676, loss 0.11801, acc 0.953125
2017-03-02T17:57:59.773480: step 19677, loss 0.164851, acc 0.9375
2017-03-02T17:57:59.848114: step 19678, loss 0.193995, acc 0.875
2017-03-02T17:57:59.927289: step 19679, loss 0.193165, acc 0.875
2017-03-02T17:58:00.003048: step 19680, loss 0.163554, acc 0.9375
2017-03-02T17:58:00.079521: step 19681, loss 0.242897, acc 0.890625
2017-03-02T17:58:00.159497: step 19682, loss 0.229634, acc 0.890625
2017-03-02T17:58:00.235147: step 19683, loss 0.0461291, acc 0.984375
2017-03-02T17:58:00.297609: step 19684, loss 0.154632, acc 0.921875
2017-03-02T17:58:00.367772: step 19685, loss 0.132007, acc 0.921875
2017-03-02T17:58:00.443108: step 19686, loss 0.198942, acc 0.9375
2017-03-02T17:58:00.510442: step 19687, loss 0.0618204, acc 1
2017-03-02T17:58:00.587848: step 19688, loss 0.0548997, acc 0.953125
2017-03-02T17:58:00.659486: step 19689, loss 0.142054, acc 0.9375
2017-03-02T17:58:00.729474: step 19690, loss 0.213064, acc 0.859375
2017-03-02T17:58:00.807419: step 19691, loss 0.180873, acc 0.90625
2017-03-02T17:58:00.891204: step 19692, loss 0.191967, acc 0.9375
2017-03-02T17:58:00.970676: step 19693, loss 0.158561, acc 0.921875
2017-03-02T17:58:01.040287: step 19694, loss 0.203235, acc 0.9375
2017-03-02T17:58:01.107632: step 19695, loss 0.22638, acc 0.875
2017-03-02T17:58:01.180963: step 19696, loss 0.203203, acc 0.90625
2017-03-02T17:58:01.254639: step 19697, loss 0.136749, acc 0.921875
2017-03-02T17:58:01.320639: step 19698, loss 0.138094, acc 0.96875
2017-03-02T17:58:01.402547: step 19699, loss 0.0932413, acc 0.953125
2017-03-02T17:58:01.477267: step 19700, loss 0.208586, acc 0.890625

Evaluation:
2017-03-02T17:58:01.518241: step 19700, loss 2.37788, acc 0.656092

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19700

2017-03-02T17:58:02.013410: step 19701, loss 0.0749967, acc 0.96875
2017-03-02T17:58:02.090538: step 19702, loss 0.0644609, acc 0.984375
2017-03-02T17:58:02.162744: step 19703, loss 0.134856, acc 0.921875
2017-03-02T17:58:02.236374: step 19704, loss 0.143412, acc 0.953125
2017-03-02T17:58:02.308417: step 19705, loss 0.180606, acc 0.890625
2017-03-02T17:58:02.384873: step 19706, loss 0.083903, acc 0.96875
2017-03-02T17:58:02.467112: step 19707, loss 0.0680352, acc 0.96875
2017-03-02T17:58:02.537243: step 19708, loss 0.136646, acc 0.921875
2017-03-02T17:58:02.606001: step 19709, loss 0.075746, acc 0.953125
2017-03-02T17:58:02.681020: step 19710, loss 0.159985, acc 0.921875
2017-03-02T17:58:02.766773: step 19711, loss 0.141222, acc 0.9375
2017-03-02T17:58:02.841073: step 19712, loss 0.123675, acc 0.9375
2017-03-02T17:58:02.914827: step 19713, loss 0.0879048, acc 0.9375
2017-03-02T17:58:02.978204: step 19714, loss 0.116525, acc 0.953125
2017-03-02T17:58:03.053695: step 19715, loss 0.121397, acc 0.953125
2017-03-02T17:58:03.139489: step 19716, loss 0.174878, acc 0.953125
2017-03-02T17:58:03.210156: step 19717, loss 0.197906, acc 0.90625
2017-03-02T17:58:03.276538: step 19718, loss 0.223753, acc 0.890625
2017-03-02T17:58:03.350940: step 19719, loss 0.18234, acc 0.890625
2017-03-02T17:58:03.434263: step 19720, loss 0.114378, acc 0.953125
2017-03-02T17:58:03.508036: step 19721, loss 0.107589, acc 0.9375
2017-03-02T17:58:03.582774: step 19722, loss 0.251834, acc 0.90625
2017-03-02T17:58:03.654388: step 19723, loss 0.242612, acc 0.921875
2017-03-02T17:58:03.723287: step 19724, loss 0.177272, acc 0.890625
2017-03-02T17:58:03.796373: step 19725, loss 0.114074, acc 0.96875
2017-03-02T17:58:03.870633: step 19726, loss 0.250658, acc 0.90625
2017-03-02T17:58:03.939182: step 19727, loss 0.0721595, acc 0.984375
2017-03-02T17:58:04.013016: step 19728, loss 0.077408, acc 0.984375
2017-03-02T17:58:04.084852: step 19729, loss 0.101727, acc 0.96875
2017-03-02T17:58:04.162547: step 19730, loss 0.0585137, acc 0.984375
2017-03-02T17:58:04.233353: step 19731, loss 0.228909, acc 0.875
2017-03-02T17:58:04.303833: step 19732, loss 0.136891, acc 0.921875
2017-03-02T17:58:04.377500: step 19733, loss 0.199866, acc 0.953125
2017-03-02T17:58:04.450237: step 19734, loss 0.220323, acc 0.875
2017-03-02T17:58:04.521202: step 19735, loss 0.107243, acc 0.953125
2017-03-02T17:58:04.587135: step 19736, loss 0.237294, acc 0.9375
2017-03-02T17:58:04.659371: step 19737, loss 0.263559, acc 0.875
2017-03-02T17:58:04.735390: step 19738, loss 0.106582, acc 0.953125
2017-03-02T17:58:04.807934: step 19739, loss 0.21426, acc 0.890625
2017-03-02T17:58:04.877654: step 19740, loss 0.186141, acc 0.9375
2017-03-02T17:58:04.963367: step 19741, loss 0.10872, acc 0.96875
2017-03-02T17:58:05.039135: step 19742, loss 0.104989, acc 0.9375
2017-03-02T17:58:05.107225: step 19743, loss 0.16016, acc 0.96875
2017-03-02T17:58:05.178633: step 19744, loss 0.222126, acc 0.875
2017-03-02T17:58:05.246106: step 19745, loss 0.157042, acc 0.9375
2017-03-02T17:58:05.311144: step 19746, loss 0.230666, acc 0.890625
2017-03-02T17:58:05.388815: step 19747, loss 0.175853, acc 0.9375
2017-03-02T17:58:05.469106: step 19748, loss 0.143593, acc 0.890625
2017-03-02T17:58:05.540218: step 19749, loss 0.162907, acc 0.9375
2017-03-02T17:58:05.610557: step 19750, loss 0.202283, acc 0.90625
2017-03-02T17:58:05.686280: step 19751, loss 0.110541, acc 0.96875
2017-03-02T17:58:05.760316: step 19752, loss 0.144198, acc 0.921875
2017-03-02T17:58:05.841511: step 19753, loss 0.145363, acc 0.921875
2017-03-02T17:58:05.908370: step 19754, loss 0.191874, acc 0.9375
2017-03-02T17:58:05.976426: step 19755, loss 0.132155, acc 0.96875
2017-03-02T17:58:06.050774: step 19756, loss 0.155657, acc 0.9375
2017-03-02T17:58:06.125093: step 19757, loss 0.292874, acc 0.890625
2017-03-02T17:58:06.197188: step 19758, loss 0.174931, acc 0.90625
2017-03-02T17:58:06.272740: step 19759, loss 0.418563, acc 0.84375
2017-03-02T17:58:06.343979: step 19760, loss 0.109704, acc 0.96875
2017-03-02T17:58:06.421352: step 19761, loss 0.0551213, acc 0.96875
2017-03-02T17:58:06.497016: step 19762, loss 0.0830302, acc 0.96875
2017-03-02T17:58:06.575772: step 19763, loss 0.124782, acc 0.9375
2017-03-02T17:58:06.645184: step 19764, loss 0.121928, acc 0.9375
2017-03-02T17:58:06.743160: step 19765, loss 0.185921, acc 0.890625
2017-03-02T17:58:06.813603: step 19766, loss 0.220043, acc 0.90625
2017-03-02T17:58:06.890701: step 19767, loss 0.21517, acc 0.890625
2017-03-02T17:58:06.970166: step 19768, loss 0.241091, acc 0.890625
2017-03-02T17:58:07.050924: step 19769, loss 0.128384, acc 0.9375
2017-03-02T17:58:07.125676: step 19770, loss 0.186559, acc 0.953125
2017-03-02T17:58:07.205597: step 19771, loss 0.207524, acc 0.921875
2017-03-02T17:58:07.275981: step 19772, loss 0.0851479, acc 0.953125
2017-03-02T17:58:07.348052: step 19773, loss 0.070756, acc 0.96875
2017-03-02T17:58:07.423460: step 19774, loss 0.232105, acc 0.90625
2017-03-02T17:58:07.499302: step 19775, loss 0.0752014, acc 0.96875
2017-03-02T17:58:07.577176: step 19776, loss 0.187936, acc 0.921875
2017-03-02T17:58:07.667577: step 19777, loss 0.152843, acc 0.9375
2017-03-02T17:58:07.749762: step 19778, loss 0.186792, acc 0.890625
2017-03-02T17:58:07.817429: step 19779, loss 0.289851, acc 0.890625
2017-03-02T17:58:07.890378: step 19780, loss 0.194151, acc 0.859375
2017-03-02T17:58:07.961268: step 19781, loss 0.0730557, acc 0.953125
2017-03-02T17:58:08.032407: step 19782, loss 0.20194, acc 0.90625
2017-03-02T17:58:08.100904: step 19783, loss 0.115631, acc 0.9375
2017-03-02T17:58:08.174143: step 19784, loss 0.165823, acc 0.953125
2017-03-02T17:58:08.243181: step 19785, loss 0.156985, acc 0.921875
2017-03-02T17:58:08.318000: step 19786, loss 0.26765, acc 0.890625
2017-03-02T17:58:08.406025: step 19787, loss 0.235214, acc 0.921875
2017-03-02T17:58:08.487070: step 19788, loss 0.1939, acc 0.921875
2017-03-02T17:58:08.563210: step 19789, loss 0.21864, acc 0.921875
2017-03-02T17:58:08.635667: step 19790, loss 0.155789, acc 0.921875
2017-03-02T17:58:08.704568: step 19791, loss 0.138405, acc 0.953125
2017-03-02T17:58:08.765637: step 19792, loss 0.240129, acc 0.890625
2017-03-02T17:58:08.835930: step 19793, loss 0.0866067, acc 0.96875
2017-03-02T17:58:08.908607: step 19794, loss 0.113825, acc 0.9375
2017-03-02T17:58:08.980622: step 19795, loss 0.0739997, acc 0.96875
2017-03-02T17:58:09.058577: step 19796, loss 0.225814, acc 0.75
2017-03-02T17:58:09.145079: step 19797, loss 0.160689, acc 0.9375
2017-03-02T17:58:09.219848: step 19798, loss 0.187425, acc 0.890625
2017-03-02T17:58:09.297261: step 19799, loss 0.0663661, acc 0.984375
2017-03-02T17:58:09.368608: step 19800, loss 0.124576, acc 0.9375

Evaluation:
2017-03-02T17:58:09.394139: step 19800, loss 2.32677, acc 0.648882

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19800

2017-03-02T17:58:09.838735: step 19801, loss 0.124623, acc 0.953125
2017-03-02T17:58:09.917334: step 19802, loss 0.134251, acc 0.921875
2017-03-02T17:58:09.995639: step 19803, loss 0.176167, acc 0.9375
2017-03-02T17:58:10.061956: step 19804, loss 0.161995, acc 0.921875
2017-03-02T17:58:10.130209: step 19805, loss 0.243494, acc 0.90625
2017-03-02T17:58:10.215196: step 19806, loss 0.178915, acc 0.921875
2017-03-02T17:58:10.288002: step 19807, loss 0.336422, acc 0.796875
2017-03-02T17:58:10.369649: step 19808, loss 0.0882794, acc 0.96875
2017-03-02T17:58:10.441894: step 19809, loss 0.0648406, acc 0.96875
2017-03-02T17:58:10.520434: step 19810, loss 0.125454, acc 0.921875
2017-03-02T17:58:10.590847: step 19811, loss 0.15708, acc 0.953125
2017-03-02T17:58:10.660993: step 19812, loss 0.11807, acc 0.96875
2017-03-02T17:58:10.736652: step 19813, loss 0.093734, acc 0.953125
2017-03-02T17:58:10.804507: step 19814, loss 0.193463, acc 0.921875
2017-03-02T17:58:10.875914: step 19815, loss 0.140013, acc 0.953125
2017-03-02T17:58:10.951292: step 19816, loss 0.154803, acc 0.9375
2017-03-02T17:58:11.022703: step 19817, loss 0.156172, acc 0.9375
2017-03-02T17:58:11.105077: step 19818, loss 0.189866, acc 0.921875
2017-03-02T17:58:11.173657: step 19819, loss 0.0954457, acc 0.9375
2017-03-02T17:58:11.246973: step 19820, loss 0.149169, acc 0.9375
2017-03-02T17:58:11.319781: step 19821, loss 0.153623, acc 0.90625
2017-03-02T17:58:11.390889: step 19822, loss 0.217565, acc 0.90625
2017-03-02T17:58:11.469840: step 19823, loss 0.127848, acc 0.953125
2017-03-02T17:58:11.536582: step 19824, loss 0.183398, acc 0.921875
2017-03-02T17:58:11.601087: step 19825, loss 0.191029, acc 0.921875
2017-03-02T17:58:11.681778: step 19826, loss 0.0829319, acc 0.96875
2017-03-02T17:58:11.753602: step 19827, loss 0.0979832, acc 0.96875
2017-03-02T17:58:11.827388: step 19828, loss 0.153336, acc 0.953125
2017-03-02T17:58:11.893539: step 19829, loss 0.220969, acc 0.890625
2017-03-02T17:58:11.967477: step 19830, loss 0.279706, acc 0.875
2017-03-02T17:58:12.043117: step 19831, loss 0.0684999, acc 0.953125
2017-03-02T17:58:12.107931: step 19832, loss 0.104622, acc 0.953125
2017-03-02T17:58:12.179041: step 19833, loss 0.0910101, acc 0.96875
2017-03-02T17:58:12.259083: step 19834, loss 0.100983, acc 0.953125
2017-03-02T17:58:12.331533: step 19835, loss 0.194397, acc 0.875
2017-03-02T17:58:12.407058: step 19836, loss 0.13474, acc 0.921875
2017-03-02T17:58:12.489823: step 19837, loss 0.130684, acc 0.921875
2017-03-02T17:58:12.564707: step 19838, loss 0.243385, acc 0.875
2017-03-02T17:58:12.638204: step 19839, loss 0.10713, acc 0.9375
2017-03-02T17:58:12.719366: step 19840, loss 0.0978419, acc 0.96875
2017-03-02T17:58:12.789434: step 19841, loss 0.128078, acc 0.953125
2017-03-02T17:58:12.863055: step 19842, loss 0.163738, acc 0.921875
2017-03-02T17:58:12.928602: step 19843, loss 0.127846, acc 0.921875
2017-03-02T17:58:13.001306: step 19844, loss 0.197004, acc 0.90625
2017-03-02T17:58:13.073812: step 19845, loss 0.0983873, acc 0.953125
2017-03-02T17:58:13.145650: step 19846, loss 0.0809924, acc 0.953125
2017-03-02T17:58:13.210138: step 19847, loss 0.204066, acc 0.90625
2017-03-02T17:58:13.296632: step 19848, loss 0.131936, acc 0.9375
2017-03-02T17:58:13.368871: step 19849, loss 0.241347, acc 0.921875
2017-03-02T17:58:13.442872: step 19850, loss 0.218453, acc 0.921875
2017-03-02T17:58:13.517694: step 19851, loss 0.102699, acc 0.984375
2017-03-02T17:58:13.585513: step 19852, loss 0.246762, acc 0.875
2017-03-02T17:58:13.666881: step 19853, loss 0.235655, acc 0.90625
2017-03-02T17:58:13.743142: step 19854, loss 0.157031, acc 0.953125
2017-03-02T17:58:13.819945: step 19855, loss 0.121257, acc 0.953125
2017-03-02T17:58:13.919262: step 19856, loss 0.203189, acc 0.890625
2017-03-02T17:58:14.016098: step 19857, loss 0.109164, acc 0.953125
2017-03-02T17:58:14.089752: step 19858, loss 0.0978069, acc 0.953125
2017-03-02T17:58:14.173466: step 19859, loss 0.208841, acc 0.90625
2017-03-02T17:58:14.243471: step 19860, loss 0.110895, acc 0.953125
2017-03-02T17:58:14.303680: step 19861, loss 0.0837315, acc 0.96875
2017-03-02T17:58:14.372166: step 19862, loss 0.206232, acc 0.921875
2017-03-02T17:58:14.443754: step 19863, loss 0.242691, acc 0.859375
2017-03-02T17:58:14.528871: step 19864, loss 0.237582, acc 0.90625
2017-03-02T17:58:14.600080: step 19865, loss 0.23532, acc 0.9375
2017-03-02T17:58:14.673279: step 19866, loss 0.110617, acc 0.9375
2017-03-02T17:58:14.750601: step 19867, loss 0.155153, acc 0.921875
2017-03-02T17:58:14.829069: step 19868, loss 0.128934, acc 0.9375
2017-03-02T17:58:14.900137: step 19869, loss 0.156656, acc 0.9375
2017-03-02T17:58:14.968520: step 19870, loss 0.12553, acc 0.9375
2017-03-02T17:58:15.041725: step 19871, loss 0.185766, acc 0.921875
2017-03-02T17:58:15.114512: step 19872, loss 0.147876, acc 0.921875
2017-03-02T17:58:15.189524: step 19873, loss 0.234214, acc 0.890625
2017-03-02T17:58:15.258363: step 19874, loss 0.152805, acc 0.921875
2017-03-02T17:58:15.330369: step 19875, loss 0.0985409, acc 0.984375
2017-03-02T17:58:15.399964: step 19876, loss 0.128584, acc 0.953125
2017-03-02T17:58:15.473309: step 19877, loss 0.131768, acc 0.921875
2017-03-02T17:58:15.550204: step 19878, loss 0.142701, acc 0.921875
2017-03-02T17:58:15.622426: step 19879, loss 0.140538, acc 0.921875
2017-03-02T17:58:15.696202: step 19880, loss 0.192864, acc 0.875
2017-03-02T17:58:15.772627: step 19881, loss 0.131128, acc 0.953125
2017-03-02T17:58:15.844090: step 19882, loss 0.175577, acc 0.9375
2017-03-02T17:58:15.925867: step 19883, loss 0.160794, acc 0.890625
2017-03-02T17:58:15.998781: step 19884, loss 0.224067, acc 0.90625
2017-03-02T17:58:16.069750: step 19885, loss 0.151693, acc 0.96875
2017-03-02T17:58:16.140699: step 19886, loss 0.169558, acc 0.90625
2017-03-02T17:58:16.215536: step 19887, loss 0.151975, acc 0.921875
2017-03-02T17:58:16.291479: step 19888, loss 0.162164, acc 0.921875
2017-03-02T17:58:16.367174: step 19889, loss 0.106507, acc 0.96875
2017-03-02T17:58:16.443254: step 19890, loss 0.0988303, acc 0.984375
2017-03-02T17:58:16.525187: step 19891, loss 0.185986, acc 0.9375
2017-03-02T17:58:16.600974: step 19892, loss 0.123841, acc 0.921875
2017-03-02T17:58:16.674119: step 19893, loss 0.180804, acc 0.90625
2017-03-02T17:58:16.747116: step 19894, loss 0.107987, acc 0.9375
2017-03-02T17:58:16.817913: step 19895, loss 0.120442, acc 0.96875
2017-03-02T17:58:16.895613: step 19896, loss 0.258499, acc 0.890625
2017-03-02T17:58:16.972670: step 19897, loss 0.159672, acc 0.953125
2017-03-02T17:58:17.042673: step 19898, loss 0.061421, acc 0.96875
2017-03-02T17:58:17.116041: step 19899, loss 0.167654, acc 0.90625
2017-03-02T17:58:17.188928: step 19900, loss 0.202139, acc 0.90625

Evaluation:
2017-03-02T17:58:17.226179: step 19900, loss 2.44085, acc 0.660418

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-19900

2017-03-02T17:58:17.672033: step 19901, loss 0.133536, acc 0.9375
2017-03-02T17:58:17.739150: step 19902, loss 0.0789792, acc 0.953125
2017-03-02T17:58:17.812705: step 19903, loss 0.186272, acc 0.921875
2017-03-02T17:58:17.884726: step 19904, loss 0.151946, acc 0.921875
2017-03-02T17:58:17.955942: step 19905, loss 0.207166, acc 0.921875
2017-03-02T17:58:18.030924: step 19906, loss 0.218703, acc 0.875
2017-03-02T17:58:18.101610: step 19907, loss 0.145989, acc 0.90625
2017-03-02T17:58:18.172471: step 19908, loss 0.175183, acc 0.921875
2017-03-02T17:58:18.246982: step 19909, loss 0.12391, acc 0.953125
2017-03-02T17:58:18.321300: step 19910, loss 0.0751481, acc 0.96875
2017-03-02T17:58:18.389765: step 19911, loss 0.208235, acc 0.875
2017-03-02T17:58:18.462725: step 19912, loss 0.188007, acc 0.90625
2017-03-02T17:58:18.529203: step 19913, loss 0.105241, acc 0.9375
2017-03-02T17:58:18.604876: step 19914, loss 0.178526, acc 0.921875
2017-03-02T17:58:18.677016: step 19915, loss 0.297569, acc 0.859375
2017-03-02T17:58:18.749386: step 19916, loss 0.274456, acc 0.890625
2017-03-02T17:58:18.826648: step 19917, loss 0.0502474, acc 0.96875
2017-03-02T17:58:18.908543: step 19918, loss 0.0401153, acc 0.984375
2017-03-02T17:58:18.990464: step 19919, loss 0.175798, acc 0.921875
2017-03-02T17:58:19.059615: step 19920, loss 0.0859329, acc 0.9375
2017-03-02T17:58:19.130084: step 19921, loss 0.072508, acc 0.953125
2017-03-02T17:58:19.203002: step 19922, loss 0.111049, acc 0.9375
2017-03-02T17:58:19.278756: step 19923, loss 0.173083, acc 0.953125
2017-03-02T17:58:19.371501: step 19924, loss 0.0909209, acc 0.96875
2017-03-02T17:58:19.446153: step 19925, loss 0.171032, acc 0.9375
2017-03-02T17:58:19.529268: step 19926, loss 0.240857, acc 0.890625
2017-03-02T17:58:19.602310: step 19927, loss 0.0924213, acc 0.953125
2017-03-02T17:58:19.679923: step 19928, loss 0.118511, acc 0.921875
2017-03-02T17:58:19.749006: step 19929, loss 0.161221, acc 0.921875
2017-03-02T17:58:19.818685: step 19930, loss 0.129864, acc 0.921875
2017-03-02T17:58:19.888633: step 19931, loss 0.208008, acc 0.96875
2017-03-02T17:58:19.961092: step 19932, loss 0.181319, acc 0.90625
2017-03-02T17:58:20.034324: step 19933, loss 0.213901, acc 0.90625
2017-03-02T17:58:20.121622: step 19934, loss 0.198123, acc 0.9375
2017-03-02T17:58:20.199190: step 19935, loss 0.0430021, acc 1
2017-03-02T17:58:20.277726: step 19936, loss 0.22787, acc 0.890625
2017-03-02T17:58:20.352120: step 19937, loss 0.263321, acc 0.890625
2017-03-02T17:58:20.418791: step 19938, loss 0.228517, acc 0.890625
2017-03-02T17:58:20.489907: step 19939, loss 0.111902, acc 0.9375
2017-03-02T17:58:20.576372: step 19940, loss 0.183969, acc 0.9375
2017-03-02T17:58:20.651497: step 19941, loss 0.0784633, acc 0.953125
2017-03-02T17:58:20.726385: step 19942, loss 0.127494, acc 0.9375
2017-03-02T17:58:20.801228: step 19943, loss 0.0906157, acc 0.9375
2017-03-02T17:58:20.875853: step 19944, loss 0.313022, acc 0.90625
2017-03-02T17:58:20.941021: step 19945, loss 0.119162, acc 0.953125
2017-03-02T17:58:21.011371: step 19946, loss 0.122317, acc 0.9375
2017-03-02T17:58:21.083390: step 19947, loss 0.119751, acc 0.953125
2017-03-02T17:58:21.153335: step 19948, loss 0.256935, acc 0.890625
2017-03-02T17:58:21.223596: step 19949, loss 0.171608, acc 0.921875
2017-03-02T17:58:21.307079: step 19950, loss 0.192569, acc 0.9375
2017-03-02T17:58:21.382736: step 19951, loss 0.129022, acc 0.953125
2017-03-02T17:58:21.464726: step 19952, loss 0.188796, acc 0.90625
2017-03-02T17:58:21.541672: step 19953, loss 0.264131, acc 0.859375
2017-03-02T17:58:21.623853: step 19954, loss 0.183914, acc 0.90625
2017-03-02T17:58:21.699073: step 19955, loss 0.0683006, acc 0.96875
2017-03-02T17:58:21.771632: step 19956, loss 0.0443121, acc 1
2017-03-02T17:58:21.841745: step 19957, loss 0.31692, acc 0.875
2017-03-02T17:58:21.922523: step 19958, loss 0.0790899, acc 0.96875
2017-03-02T17:58:21.994561: step 19959, loss 0.204772, acc 0.90625
2017-03-02T17:58:22.068022: step 19960, loss 0.10943, acc 0.9375
2017-03-02T17:58:22.147645: step 19961, loss 0.18087, acc 0.875
2017-03-02T17:58:22.222913: step 19962, loss 0.21935, acc 0.9375
2017-03-02T17:58:22.304206: step 19963, loss 0.166706, acc 0.9375
2017-03-02T17:58:22.383465: step 19964, loss 0.0450603, acc 1
2017-03-02T17:58:22.457210: step 19965, loss 0.170274, acc 0.9375
2017-03-02T17:58:22.528958: step 19966, loss 0.161075, acc 0.90625
2017-03-02T17:58:22.609305: step 19967, loss 0.0905193, acc 0.984375
2017-03-02T17:58:22.685263: step 19968, loss 0.127033, acc 0.921875
2017-03-02T17:58:22.757651: step 19969, loss 0.145705, acc 0.921875
2017-03-02T17:58:22.829843: step 19970, loss 0.153292, acc 0.9375
2017-03-02T17:58:22.899081: step 19971, loss 0.229863, acc 0.890625
2017-03-02T17:58:22.972841: step 19972, loss 0.0999152, acc 0.96875
2017-03-02T17:58:23.051606: step 19973, loss 0.189592, acc 0.9375
2017-03-02T17:58:23.129295: step 19974, loss 0.0882, acc 0.984375
2017-03-02T17:58:23.201158: step 19975, loss 0.0790692, acc 0.984375
2017-03-02T17:58:23.266982: step 19976, loss 0.18281, acc 0.921875
2017-03-02T17:58:23.369829: step 19977, loss 0.175319, acc 0.890625
2017-03-02T17:58:23.444066: step 19978, loss 0.133407, acc 0.953125
2017-03-02T17:58:23.514278: step 19979, loss 0.275225, acc 0.890625
2017-03-02T17:58:23.589185: step 19980, loss 0.123812, acc 0.96875
2017-03-02T17:58:23.659722: step 19981, loss 0.154205, acc 0.9375
2017-03-02T17:58:23.729989: step 19982, loss 0.154585, acc 0.9375
2017-03-02T17:58:23.806852: step 19983, loss 0.166602, acc 0.953125
2017-03-02T17:58:23.887169: step 19984, loss 0.199533, acc 0.890625
2017-03-02T17:58:23.972818: step 19985, loss 0.149558, acc 0.9375
2017-03-02T17:58:24.046598: step 19986, loss 0.0810977, acc 0.96875
2017-03-02T17:58:24.129925: step 19987, loss 0.0513126, acc 1
2017-03-02T17:58:24.202950: step 19988, loss 0.16969, acc 0.9375
2017-03-02T17:58:24.277279: step 19989, loss 0.113791, acc 0.9375
2017-03-02T17:58:24.366133: step 19990, loss 0.199788, acc 0.90625
2017-03-02T17:58:24.438782: step 19991, loss 0.19984, acc 0.890625
2017-03-02T17:58:24.510067: step 19992, loss 0.190347, acc 1
2017-03-02T17:58:24.591774: step 19993, loss 0.122761, acc 0.953125
2017-03-02T17:58:24.666798: step 19994, loss 0.195901, acc 0.890625
2017-03-02T17:58:24.738343: step 19995, loss 0.189573, acc 0.90625
2017-03-02T17:58:24.821657: step 19996, loss 0.108735, acc 0.921875
2017-03-02T17:58:24.907848: step 19997, loss 0.126906, acc 0.9375
2017-03-02T17:58:24.984522: step 19998, loss 0.236258, acc 0.9375
2017-03-02T17:58:25.052730: step 19999, loss 0.0789715, acc 0.96875
2017-03-02T17:58:25.122116: step 20000, loss 0.180876, acc 0.9375

Evaluation:
2017-03-02T17:58:25.158252: step 20000, loss 2.41262, acc 0.64744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20000

2017-03-02T17:58:25.658111: step 20001, loss 0.0781091, acc 0.96875
2017-03-02T17:58:25.728781: step 20002, loss 0.0903758, acc 0.96875
2017-03-02T17:58:25.799977: step 20003, loss 0.28065, acc 0.921875
2017-03-02T17:58:25.881164: step 20004, loss 0.118519, acc 0.90625
2017-03-02T17:58:25.955614: step 20005, loss 0.085862, acc 0.96875
2017-03-02T17:58:26.026407: step 20006, loss 0.154446, acc 0.921875
2017-03-02T17:58:26.094822: step 20007, loss 0.0858883, acc 0.96875
2017-03-02T17:58:26.186019: step 20008, loss 0.222954, acc 0.859375
2017-03-02T17:58:26.264908: step 20009, loss 0.0976928, acc 0.9375
2017-03-02T17:58:26.341711: step 20010, loss 0.16448, acc 0.921875
2017-03-02T17:58:26.414320: step 20011, loss 0.151113, acc 0.921875
2017-03-02T17:58:26.491525: step 20012, loss 0.11656, acc 0.9375
2017-03-02T17:58:26.564095: step 20013, loss 0.095054, acc 0.953125
2017-03-02T17:58:26.636164: step 20014, loss 0.150094, acc 0.921875
2017-03-02T17:58:26.713185: step 20015, loss 0.0974395, acc 0.96875
2017-03-02T17:58:26.788580: step 20016, loss 0.118533, acc 0.953125
2017-03-02T17:58:26.863954: step 20017, loss 0.100792, acc 0.96875
2017-03-02T17:58:26.940663: step 20018, loss 0.135699, acc 0.953125
2017-03-02T17:58:27.011229: step 20019, loss 0.12955, acc 0.9375
2017-03-02T17:58:27.081661: step 20020, loss 0.120046, acc 0.9375
2017-03-02T17:58:27.154873: step 20021, loss 0.180773, acc 0.921875
2017-03-02T17:58:27.235213: step 20022, loss 0.165699, acc 0.90625
2017-03-02T17:58:27.313331: step 20023, loss 0.123397, acc 0.953125
2017-03-02T17:58:27.392057: step 20024, loss 0.115358, acc 0.953125
2017-03-02T17:58:27.457177: step 20025, loss 0.144991, acc 0.90625
2017-03-02T17:58:27.542000: step 20026, loss 0.141155, acc 0.921875
2017-03-02T17:58:27.618011: step 20027, loss 0.123719, acc 0.953125
2017-03-02T17:58:27.696540: step 20028, loss 0.146534, acc 0.921875
2017-03-02T17:58:27.771852: step 20029, loss 0.120233, acc 0.921875
2017-03-02T17:58:27.843665: step 20030, loss 0.155409, acc 0.921875
2017-03-02T17:58:27.921691: step 20031, loss 0.189919, acc 0.921875
2017-03-02T17:58:27.994448: step 20032, loss 0.157294, acc 0.96875
2017-03-02T17:58:28.064297: step 20033, loss 0.10406, acc 0.953125
2017-03-02T17:58:28.143416: step 20034, loss 0.0690865, acc 0.984375
2017-03-02T17:58:28.227664: step 20035, loss 0.17038, acc 0.921875
2017-03-02T17:58:28.326111: step 20036, loss 0.119406, acc 0.9375
2017-03-02T17:58:28.394886: step 20037, loss 0.112858, acc 0.9375
2017-03-02T17:58:28.469623: step 20038, loss 0.146053, acc 0.921875
2017-03-02T17:58:28.537342: step 20039, loss 0.108944, acc 0.953125
2017-03-02T17:58:28.611173: step 20040, loss 0.113468, acc 0.96875
2017-03-02T17:58:28.686722: step 20041, loss 0.099351, acc 0.96875
2017-03-02T17:58:28.756660: step 20042, loss 0.137069, acc 0.90625
2017-03-02T17:58:28.823105: step 20043, loss 0.0534505, acc 0.984375
2017-03-02T17:58:28.900723: step 20044, loss 0.163663, acc 0.921875
2017-03-02T17:58:28.977273: step 20045, loss 0.120654, acc 0.953125
2017-03-02T17:58:29.050762: step 20046, loss 0.159691, acc 0.90625
2017-03-02T17:58:29.126707: step 20047, loss 0.116899, acc 0.921875
2017-03-02T17:58:29.207473: step 20048, loss 0.0928602, acc 0.984375
2017-03-02T17:58:29.284357: step 20049, loss 0.194099, acc 0.953125
2017-03-02T17:58:29.357779: step 20050, loss 0.250724, acc 0.90625
2017-03-02T17:58:29.429918: step 20051, loss 0.127417, acc 0.9375
2017-03-02T17:58:29.497432: step 20052, loss 0.162106, acc 0.953125
2017-03-02T17:58:29.577523: step 20053, loss 0.185367, acc 0.890625
2017-03-02T17:58:29.650029: step 20054, loss 0.0396667, acc 1
2017-03-02T17:58:29.727295: step 20055, loss 0.0985526, acc 0.953125
2017-03-02T17:58:29.807951: step 20056, loss 0.127051, acc 0.921875
2017-03-02T17:58:29.878946: step 20057, loss 0.170235, acc 0.953125
2017-03-02T17:58:29.957856: step 20058, loss 0.20328, acc 0.921875
2017-03-02T17:58:30.033399: step 20059, loss 0.199302, acc 0.921875
2017-03-02T17:58:30.100316: step 20060, loss 0.0603473, acc 0.96875
2017-03-02T17:58:30.169055: step 20061, loss 0.157468, acc 0.9375
2017-03-02T17:58:30.241828: step 20062, loss 0.106354, acc 0.953125
2017-03-02T17:58:30.315727: step 20063, loss 0.170503, acc 0.890625
2017-03-02T17:58:30.392477: step 20064, loss 0.130126, acc 0.953125
2017-03-02T17:58:30.464394: step 20065, loss 0.0586799, acc 0.984375
2017-03-02T17:58:30.536486: step 20066, loss 0.237381, acc 0.921875
2017-03-02T17:58:30.612580: step 20067, loss 0.284571, acc 0.890625
2017-03-02T17:58:30.689310: step 20068, loss 0.0917071, acc 0.953125
2017-03-02T17:58:30.779962: step 20069, loss 0.147418, acc 0.9375
2017-03-02T17:58:30.849525: step 20070, loss 0.0273088, acc 1
2017-03-02T17:58:30.916847: step 20071, loss 0.185482, acc 0.90625
2017-03-02T17:58:30.992540: step 20072, loss 0.196629, acc 0.90625
2017-03-02T17:58:31.065010: step 20073, loss 0.0663607, acc 0.96875
2017-03-02T17:58:31.136582: step 20074, loss 0.049335, acc 0.96875
2017-03-02T17:58:31.200994: step 20075, loss 0.144911, acc 0.9375
2017-03-02T17:58:31.275092: step 20076, loss 0.0675668, acc 0.984375
2017-03-02T17:58:31.347225: step 20077, loss 0.203863, acc 0.890625
2017-03-02T17:58:31.434611: step 20078, loss 0.073849, acc 0.984375
2017-03-02T17:58:31.506086: step 20079, loss 0.142521, acc 0.96875
2017-03-02T17:58:31.565306: step 20080, loss 0.108819, acc 0.953125
2017-03-02T17:58:31.637498: step 20081, loss 0.263908, acc 0.890625
2017-03-02T17:58:31.711512: step 20082, loss 0.302354, acc 0.890625
2017-03-02T17:58:31.784253: step 20083, loss 0.131275, acc 0.953125
2017-03-02T17:58:31.856773: step 20084, loss 0.205559, acc 0.953125
2017-03-02T17:58:31.927949: step 20085, loss 0.178952, acc 0.96875
2017-03-02T17:58:32.001016: step 20086, loss 0.192378, acc 0.9375
2017-03-02T17:58:32.076527: step 20087, loss 0.227463, acc 0.90625
2017-03-02T17:58:32.147768: step 20088, loss 0.0897314, acc 0.953125
2017-03-02T17:58:32.245790: step 20089, loss 0.091675, acc 0.96875
2017-03-02T17:58:32.315837: step 20090, loss 0.15111, acc 0.9375
2017-03-02T17:58:32.388378: step 20091, loss 0.108611, acc 0.953125
2017-03-02T17:58:32.473755: step 20092, loss 0.216634, acc 0.890625
2017-03-02T17:58:32.557697: step 20093, loss 0.121036, acc 0.984375
2017-03-02T17:58:32.631884: step 20094, loss 0.259322, acc 0.875
2017-03-02T17:58:32.714553: step 20095, loss 0.116155, acc 0.921875
2017-03-02T17:58:32.793383: step 20096, loss 0.205831, acc 0.90625
2017-03-02T17:58:32.862376: step 20097, loss 0.183956, acc 0.90625
2017-03-02T17:58:32.932369: step 20098, loss 0.146241, acc 0.90625
2017-03-02T17:58:33.003338: step 20099, loss 0.145324, acc 0.9375
2017-03-02T17:58:33.074935: step 20100, loss 0.0535995, acc 0.984375

Evaluation:
2017-03-02T17:58:33.114278: step 20100, loss 2.47385, acc 0.643836

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20100

2017-03-02T17:58:33.559037: step 20101, loss 0.221534, acc 0.890625
2017-03-02T17:58:33.628169: step 20102, loss 0.118907, acc 0.953125
2017-03-02T17:58:33.703213: step 20103, loss 0.138027, acc 0.953125
2017-03-02T17:58:33.772670: step 20104, loss 0.147192, acc 0.9375
2017-03-02T17:58:33.860883: step 20105, loss 0.133691, acc 0.953125
2017-03-02T17:58:33.931497: step 20106, loss 0.243134, acc 0.875
2017-03-02T17:58:34.009431: step 20107, loss 0.060067, acc 0.984375
2017-03-02T17:58:34.088220: step 20108, loss 0.105036, acc 0.984375
2017-03-02T17:58:34.159244: step 20109, loss 0.124409, acc 0.9375
2017-03-02T17:58:34.231025: step 20110, loss 0.073041, acc 1
2017-03-02T17:58:34.303972: step 20111, loss 0.261496, acc 0.90625
2017-03-02T17:58:34.373366: step 20112, loss 0.307322, acc 0.84375
2017-03-02T17:58:34.446563: step 20113, loss 0.0901322, acc 0.984375
2017-03-02T17:58:34.527032: step 20114, loss 0.106974, acc 0.9375
2017-03-02T17:58:34.595129: step 20115, loss 0.250029, acc 0.875
2017-03-02T17:58:34.664595: step 20116, loss 0.108791, acc 0.9375
2017-03-02T17:58:34.741850: step 20117, loss 0.184848, acc 0.90625
2017-03-02T17:58:34.817396: step 20118, loss 0.123752, acc 0.9375
2017-03-02T17:58:34.899358: step 20119, loss 0.0583506, acc 1
2017-03-02T17:58:34.972757: step 20120, loss 0.0851269, acc 0.984375
2017-03-02T17:58:35.050438: step 20121, loss 0.185848, acc 0.90625
2017-03-02T17:58:35.119383: step 20122, loss 0.107023, acc 0.96875
2017-03-02T17:58:35.195354: step 20123, loss 0.282074, acc 0.859375
2017-03-02T17:58:35.268480: step 20124, loss 0.140097, acc 0.921875
2017-03-02T17:58:35.342325: step 20125, loss 0.150788, acc 0.90625
2017-03-02T17:58:35.414023: step 20126, loss 0.224962, acc 0.90625
2017-03-02T17:58:35.496616: step 20127, loss 0.173748, acc 0.9375
2017-03-02T17:58:35.574526: step 20128, loss 0.256236, acc 0.875
2017-03-02T17:58:35.653524: step 20129, loss 0.151946, acc 0.9375
2017-03-02T17:58:35.724481: step 20130, loss 0.177426, acc 0.953125
2017-03-02T17:58:35.797497: step 20131, loss 0.134407, acc 0.9375
2017-03-02T17:58:35.877156: step 20132, loss 0.190601, acc 0.921875
2017-03-02T17:58:35.948581: step 20133, loss 0.129118, acc 0.953125
2017-03-02T17:58:36.019155: step 20134, loss 0.0718463, acc 0.96875
2017-03-02T17:58:36.101127: step 20135, loss 0.24199, acc 0.921875
2017-03-02T17:58:36.164806: step 20136, loss 0.126745, acc 0.953125
2017-03-02T17:58:36.235609: step 20137, loss 0.182708, acc 0.921875
2017-03-02T17:58:36.310086: step 20138, loss 0.323067, acc 0.875
2017-03-02T17:58:36.380149: step 20139, loss 0.322099, acc 0.84375
2017-03-02T17:58:36.447951: step 20140, loss 0.13173, acc 0.984375
2017-03-02T17:58:36.523861: step 20141, loss 0.137935, acc 0.9375
2017-03-02T17:58:36.608253: step 20142, loss 0.156194, acc 0.953125
2017-03-02T17:58:36.687028: step 20143, loss 0.167689, acc 0.90625
2017-03-02T17:58:36.761647: step 20144, loss 0.105922, acc 0.96875
2017-03-02T17:58:36.832208: step 20145, loss 0.11093, acc 0.953125
2017-03-02T17:58:36.908127: step 20146, loss 0.0871001, acc 0.96875
2017-03-02T17:58:36.985046: step 20147, loss 0.202745, acc 0.890625
2017-03-02T17:58:37.052019: step 20148, loss 0.139106, acc 0.9375
2017-03-02T17:58:37.140097: step 20149, loss 0.20843, acc 0.90625
2017-03-02T17:58:37.223296: step 20150, loss 0.147533, acc 0.9375
2017-03-02T17:58:37.297994: step 20151, loss 0.0973744, acc 0.9375
2017-03-02T17:58:37.376698: step 20152, loss 0.25555, acc 0.921875
2017-03-02T17:58:37.448125: step 20153, loss 0.124894, acc 0.921875
2017-03-02T17:58:37.521958: step 20154, loss 0.11633, acc 0.96875
2017-03-02T17:58:37.597289: step 20155, loss 0.162171, acc 0.921875
2017-03-02T17:58:37.682479: step 20156, loss 0.119368, acc 0.9375
2017-03-02T17:58:37.753087: step 20157, loss 0.224985, acc 0.859375
2017-03-02T17:58:37.822656: step 20158, loss 0.183264, acc 0.921875
2017-03-02T17:58:37.914578: step 20159, loss 0.18255, acc 0.9375
2017-03-02T17:58:37.989951: step 20160, loss 0.201432, acc 0.921875
2017-03-02T17:58:38.059969: step 20161, loss 0.283246, acc 0.859375
2017-03-02T17:58:38.131607: step 20162, loss 0.134779, acc 0.9375
2017-03-02T17:58:38.202385: step 20163, loss 0.233095, acc 0.890625
2017-03-02T17:58:38.274016: step 20164, loss 0.0843951, acc 0.96875
2017-03-02T17:58:38.348544: step 20165, loss 0.210314, acc 0.921875
2017-03-02T17:58:38.418418: step 20166, loss 0.117944, acc 0.9375
2017-03-02T17:58:38.484894: step 20167, loss 0.173109, acc 0.90625
2017-03-02T17:58:38.558438: step 20168, loss 0.146011, acc 0.921875
2017-03-02T17:58:38.631715: step 20169, loss 0.221672, acc 0.90625
2017-03-02T17:58:38.702603: step 20170, loss 0.194206, acc 0.921875
2017-03-02T17:58:38.775615: step 20171, loss 0.104434, acc 0.953125
2017-03-02T17:58:38.856221: step 20172, loss 0.217107, acc 0.921875
2017-03-02T17:58:38.927659: step 20173, loss 0.268178, acc 0.84375
2017-03-02T17:58:39.003969: step 20174, loss 0.189803, acc 0.90625
2017-03-02T17:58:39.074533: step 20175, loss 0.206132, acc 0.875
2017-03-02T17:58:39.139653: step 20176, loss 0.140131, acc 0.921875
2017-03-02T17:58:39.232048: step 20177, loss 0.125851, acc 0.921875
2017-03-02T17:58:39.305839: step 20178, loss 0.115199, acc 0.9375
2017-03-02T17:58:39.390490: step 20179, loss 0.0946736, acc 0.953125
2017-03-02T17:58:39.461416: step 20180, loss 0.129901, acc 0.9375
2017-03-02T17:58:39.529619: step 20181, loss 0.153603, acc 0.9375
2017-03-02T17:58:39.599488: step 20182, loss 0.276364, acc 0.90625
2017-03-02T17:58:39.672992: step 20183, loss 0.121697, acc 0.9375
2017-03-02T17:58:39.747481: step 20184, loss 0.104174, acc 0.953125
2017-03-02T17:58:39.825669: step 20185, loss 0.121235, acc 0.921875
2017-03-02T17:58:39.895789: step 20186, loss 0.0848538, acc 0.953125
2017-03-02T17:58:39.968770: step 20187, loss 0.188604, acc 0.890625
2017-03-02T17:58:40.035835: step 20188, loss 0.0559419, acc 1
2017-03-02T17:58:40.110431: step 20189, loss 0.187287, acc 0.9375
2017-03-02T17:58:40.183604: step 20190, loss 0.24565, acc 0.921875
2017-03-02T17:58:40.262094: step 20191, loss 0.0630735, acc 0.96875
2017-03-02T17:58:40.336833: step 20192, loss 0.195486, acc 0.921875
2017-03-02T17:58:40.414175: step 20193, loss 0.185993, acc 0.921875
2017-03-02T17:58:40.481382: step 20194, loss 0.245635, acc 0.90625
2017-03-02T17:58:40.554498: step 20195, loss 0.115139, acc 0.96875
2017-03-02T17:58:40.629668: step 20196, loss 0.11296, acc 0.953125
2017-03-02T17:58:40.707708: step 20197, loss 0.175272, acc 0.953125
2017-03-02T17:58:40.783037: step 20198, loss 0.10525, acc 0.9375
2017-03-02T17:58:40.856110: step 20199, loss 0.123412, acc 0.90625
2017-03-02T17:58:40.934130: step 20200, loss 0.0764522, acc 0.96875

Evaluation:
2017-03-02T17:58:40.975288: step 20200, loss 2.42389, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20200

2017-03-02T17:58:41.435409: step 20201, loss 0.12249, acc 0.9375
2017-03-02T17:58:41.511458: step 20202, loss 0.210087, acc 0.890625
2017-03-02T17:58:41.584754: step 20203, loss 0.128482, acc 0.921875
2017-03-02T17:58:41.657464: step 20204, loss 0.240057, acc 0.90625
2017-03-02T17:58:41.730206: step 20205, loss 0.15041, acc 0.9375
2017-03-02T17:58:41.804857: step 20206, loss 0.161502, acc 0.953125
2017-03-02T17:58:41.870457: step 20207, loss 0.112704, acc 0.9375
2017-03-02T17:58:41.940681: step 20208, loss 0.0653, acc 0.96875
2017-03-02T17:58:42.009798: step 20209, loss 0.202906, acc 0.90625
2017-03-02T17:58:42.079606: step 20210, loss 0.208918, acc 0.890625
2017-03-02T17:58:42.148613: step 20211, loss 0.268418, acc 0.875
2017-03-02T17:58:42.221593: step 20212, loss 0.110504, acc 0.90625
2017-03-02T17:58:42.292015: step 20213, loss 0.113284, acc 0.953125
2017-03-02T17:58:42.360647: step 20214, loss 0.148071, acc 0.921875
2017-03-02T17:58:42.435438: step 20215, loss 0.338659, acc 0.875
2017-03-02T17:58:42.511257: step 20216, loss 0.0960666, acc 0.96875
2017-03-02T17:58:42.586941: step 20217, loss 0.19556, acc 0.90625
2017-03-02T17:58:42.656510: step 20218, loss 0.286876, acc 0.859375
2017-03-02T17:58:42.729014: step 20219, loss 0.138407, acc 0.9375
2017-03-02T17:58:42.806187: step 20220, loss 0.105063, acc 0.96875
2017-03-02T17:58:42.883843: step 20221, loss 0.17281, acc 0.90625
2017-03-02T17:58:42.955329: step 20222, loss 0.219446, acc 0.890625
2017-03-02T17:58:43.019549: step 20223, loss 0.119268, acc 0.9375
2017-03-02T17:58:43.099785: step 20224, loss 0.104103, acc 0.96875
2017-03-02T17:58:43.175686: step 20225, loss 0.127475, acc 0.953125
2017-03-02T17:58:43.243295: step 20226, loss 0.209612, acc 0.921875
2017-03-02T17:58:43.310717: step 20227, loss 0.166091, acc 0.921875
2017-03-02T17:58:43.382160: step 20228, loss 0.165878, acc 0.9375
2017-03-02T17:58:43.457663: step 20229, loss 0.108669, acc 0.953125
2017-03-02T17:58:43.535394: step 20230, loss 0.278238, acc 0.890625
2017-03-02T17:58:43.609448: step 20231, loss 0.133894, acc 0.953125
2017-03-02T17:58:43.681609: step 20232, loss 0.222549, acc 0.90625
2017-03-02T17:58:43.761531: step 20233, loss 0.201695, acc 0.921875
2017-03-02T17:58:43.835187: step 20234, loss 0.116915, acc 0.9375
2017-03-02T17:58:43.906051: step 20235, loss 0.0813593, acc 0.96875
2017-03-02T17:58:43.969168: step 20236, loss 0.108648, acc 0.953125
2017-03-02T17:58:44.041472: step 20237, loss 0.182508, acc 0.921875
2017-03-02T17:58:44.123904: step 20238, loss 0.209337, acc 0.90625
2017-03-02T17:58:44.192426: step 20239, loss 0.0887145, acc 0.96875
2017-03-02T17:58:44.263889: step 20240, loss 0.0962729, acc 0.953125
2017-03-02T17:58:44.335697: step 20241, loss 0.148091, acc 0.90625
2017-03-02T17:58:44.408438: step 20242, loss 0.177782, acc 0.921875
2017-03-02T17:58:44.477338: step 20243, loss 0.270592, acc 0.875
2017-03-02T17:58:44.549811: step 20244, loss 0.137408, acc 0.921875
2017-03-02T17:58:44.619731: step 20245, loss 0.117993, acc 0.96875
2017-03-02T17:58:44.680890: step 20246, loss 0.150508, acc 0.90625
2017-03-02T17:58:44.756120: step 20247, loss 0.184122, acc 0.921875
2017-03-02T17:58:44.827685: step 20248, loss 0.15138, acc 0.921875
2017-03-02T17:58:44.899168: step 20249, loss 0.269021, acc 0.890625
2017-03-02T17:58:44.970057: step 20250, loss 0.0979644, acc 0.953125
2017-03-02T17:58:45.043438: step 20251, loss 0.18209, acc 0.9375
2017-03-02T17:58:45.115914: step 20252, loss 0.246021, acc 0.890625
2017-03-02T17:58:45.186314: step 20253, loss 0.162615, acc 0.953125
2017-03-02T17:58:45.256575: step 20254, loss 0.151778, acc 0.921875
2017-03-02T17:58:45.326565: step 20255, loss 0.104278, acc 0.953125
2017-03-02T17:58:45.396549: step 20256, loss 0.150598, acc 0.921875
2017-03-02T17:58:45.470952: step 20257, loss 0.170707, acc 0.921875
2017-03-02T17:58:45.546704: step 20258, loss 0.188294, acc 0.90625
2017-03-02T17:58:45.628709: step 20259, loss 0.153246, acc 0.921875
2017-03-02T17:58:45.701344: step 20260, loss 0.190383, acc 0.9375
2017-03-02T17:58:45.774278: step 20261, loss 0.163005, acc 0.90625
2017-03-02T17:58:45.854058: step 20262, loss 0.126853, acc 0.9375
2017-03-02T17:58:45.942948: step 20263, loss 0.139271, acc 0.9375
2017-03-02T17:58:46.016825: step 20264, loss 0.135705, acc 0.90625
2017-03-02T17:58:46.084560: step 20265, loss 0.162903, acc 0.9375
2017-03-02T17:58:46.159220: step 20266, loss 0.132913, acc 0.9375
2017-03-02T17:58:46.232168: step 20267, loss 0.111848, acc 0.9375
2017-03-02T17:58:46.302824: step 20268, loss 0.107684, acc 0.953125
2017-03-02T17:58:46.374445: step 20269, loss 0.183775, acc 0.921875
2017-03-02T17:58:46.463734: step 20270, loss 0.161983, acc 0.921875
2017-03-02T17:58:46.543231: step 20271, loss 0.107048, acc 0.9375
2017-03-02T17:58:46.615661: step 20272, loss 0.190203, acc 0.921875
2017-03-02T17:58:46.685373: step 20273, loss 0.128187, acc 0.9375
2017-03-02T17:58:46.757329: step 20274, loss 0.0864702, acc 0.96875
2017-03-02T17:58:46.828815: step 20275, loss 0.171571, acc 0.953125
2017-03-02T17:58:46.905355: step 20276, loss 0.0620066, acc 0.984375
2017-03-02T17:58:46.982923: step 20277, loss 0.0863815, acc 0.96875
2017-03-02T17:58:47.061164: step 20278, loss 0.189319, acc 0.9375
2017-03-02T17:58:47.145925: step 20279, loss 0.159195, acc 0.921875
2017-03-02T17:58:47.218115: step 20280, loss 0.173852, acc 0.921875
2017-03-02T17:58:47.288232: step 20281, loss 0.0987504, acc 0.96875
2017-03-02T17:58:47.355479: step 20282, loss 0.223608, acc 0.859375
2017-03-02T17:58:47.425103: step 20283, loss 0.136062, acc 0.953125
2017-03-02T17:58:47.495352: step 20284, loss 0.220708, acc 0.921875
2017-03-02T17:58:47.565616: step 20285, loss 0.082328, acc 0.984375
2017-03-02T17:58:47.643057: step 20286, loss 0.105685, acc 0.96875
2017-03-02T17:58:47.718991: step 20287, loss 0.166572, acc 0.90625
2017-03-02T17:58:47.791298: step 20288, loss 0.125277, acc 0.9375
2017-03-02T17:58:47.867764: step 20289, loss 0.0815876, acc 0.984375
2017-03-02T17:58:47.945102: step 20290, loss 0.164453, acc 0.9375
2017-03-02T17:58:48.021656: step 20291, loss 0.194656, acc 0.890625
2017-03-02T17:58:48.087863: step 20292, loss 0.120034, acc 0.984375
2017-03-02T17:58:48.165423: step 20293, loss 0.0373889, acc 0.984375
2017-03-02T17:58:48.242440: step 20294, loss 0.134955, acc 0.96875
2017-03-02T17:58:48.317813: step 20295, loss 0.208148, acc 0.890625
2017-03-02T17:58:48.388558: step 20296, loss 0.106057, acc 0.953125
2017-03-02T17:58:48.470527: step 20297, loss 0.204454, acc 0.9375
2017-03-02T17:58:48.533904: step 20298, loss 0.104613, acc 0.953125
2017-03-02T17:58:48.604346: step 20299, loss 0.180841, acc 0.90625
2017-03-02T17:58:48.668050: step 20300, loss 0.107852, acc 0.96875

Evaluation:
2017-03-02T17:58:48.696465: step 20300, loss 2.43211, acc 0.648162

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20300

2017-03-02T17:58:49.203579: step 20301, loss 0.134755, acc 0.96875
2017-03-02T17:58:49.276848: step 20302, loss 0.137162, acc 0.90625
2017-03-02T17:58:49.360586: step 20303, loss 0.236696, acc 0.890625
2017-03-02T17:58:49.437996: step 20304, loss 0.228309, acc 0.90625
2017-03-02T17:58:49.515293: step 20305, loss 0.0827846, acc 0.953125
2017-03-02T17:58:49.588866: step 20306, loss 0.117879, acc 0.921875
2017-03-02T17:58:49.668289: step 20307, loss 0.0989312, acc 0.953125
2017-03-02T17:58:49.738601: step 20308, loss 0.178737, acc 0.90625
2017-03-02T17:58:49.818337: step 20309, loss 0.181027, acc 0.953125
2017-03-02T17:58:49.900850: step 20310, loss 0.170347, acc 0.9375
2017-03-02T17:58:49.982707: step 20311, loss 0.105554, acc 0.953125
2017-03-02T17:58:50.061168: step 20312, loss 0.207035, acc 0.921875
2017-03-02T17:58:50.130977: step 20313, loss 0.240428, acc 0.90625
2017-03-02T17:58:50.198744: step 20314, loss 0.14113, acc 0.96875
2017-03-02T17:58:50.292518: step 20315, loss 0.131259, acc 0.90625
2017-03-02T17:58:50.365455: step 20316, loss 0.225434, acc 0.921875
2017-03-02T17:58:50.443260: step 20317, loss 0.136529, acc 0.9375
2017-03-02T17:58:50.519619: step 20318, loss 0.176876, acc 0.921875
2017-03-02T17:58:50.600651: step 20319, loss 0.109278, acc 0.9375
2017-03-02T17:58:50.673872: step 20320, loss 0.324569, acc 0.890625
2017-03-02T17:58:50.758417: step 20321, loss 0.232848, acc 0.84375
2017-03-02T17:58:50.831310: step 20322, loss 0.101905, acc 0.96875
2017-03-02T17:58:50.907930: step 20323, loss 0.0863595, acc 0.984375
2017-03-02T17:58:50.985183: step 20324, loss 0.123039, acc 0.921875
2017-03-02T17:58:51.055253: step 20325, loss 0.19064, acc 0.875
2017-03-02T17:58:51.132616: step 20326, loss 0.232042, acc 0.875
2017-03-02T17:58:51.199295: step 20327, loss 0.192258, acc 0.9375
2017-03-02T17:58:51.270663: step 20328, loss 0.0647537, acc 0.953125
2017-03-02T17:58:51.342210: step 20329, loss 0.0874359, acc 0.953125
2017-03-02T17:58:51.420347: step 20330, loss 0.0610022, acc 0.984375
2017-03-02T17:58:51.488809: step 20331, loss 0.100267, acc 0.9375
2017-03-02T17:58:51.555666: step 20332, loss 0.0944813, acc 0.953125
2017-03-02T17:58:51.628487: step 20333, loss 0.102894, acc 0.9375
2017-03-02T17:58:51.701217: step 20334, loss 0.273944, acc 0.890625
2017-03-02T17:58:51.771589: step 20335, loss 0.259134, acc 0.890625
2017-03-02T17:58:51.839702: step 20336, loss 0.139879, acc 0.921875
2017-03-02T17:58:51.914434: step 20337, loss 0.252597, acc 0.90625
2017-03-02T17:58:51.988978: step 20338, loss 0.177551, acc 0.90625
2017-03-02T17:58:52.064805: step 20339, loss 0.19068, acc 0.90625
2017-03-02T17:58:52.136492: step 20340, loss 0.262199, acc 0.859375
2017-03-02T17:58:52.205626: step 20341, loss 0.161739, acc 0.953125
2017-03-02T17:58:52.295887: step 20342, loss 0.108973, acc 0.953125
2017-03-02T17:58:52.374611: step 20343, loss 0.0625796, acc 0.96875
2017-03-02T17:58:52.449343: step 20344, loss 0.105381, acc 0.96875
2017-03-02T17:58:52.521039: step 20345, loss 0.192291, acc 0.9375
2017-03-02T17:58:52.598456: step 20346, loss 0.209477, acc 0.921875
2017-03-02T17:58:52.674940: step 20347, loss 0.0902139, acc 0.953125
2017-03-02T17:58:52.754706: step 20348, loss 0.227101, acc 0.953125
2017-03-02T17:58:52.825368: step 20349, loss 0.0739639, acc 0.96875
2017-03-02T17:58:52.893683: step 20350, loss 0.0549228, acc 0.96875
2017-03-02T17:58:52.966978: step 20351, loss 0.206971, acc 0.921875
2017-03-02T17:58:53.037453: step 20352, loss 0.231696, acc 0.875
2017-03-02T17:58:53.110292: step 20353, loss 0.248031, acc 0.90625
2017-03-02T17:58:53.188054: step 20354, loss 0.226165, acc 0.90625
2017-03-02T17:58:53.261214: step 20355, loss 0.107217, acc 0.96875
2017-03-02T17:58:53.334620: step 20356, loss 0.17216, acc 0.953125
2017-03-02T17:58:53.405237: step 20357, loss 0.210844, acc 0.890625
2017-03-02T17:58:53.471958: step 20358, loss 0.232092, acc 0.9375
2017-03-02T17:58:53.538503: step 20359, loss 0.248031, acc 0.921875
2017-03-02T17:58:53.607010: step 20360, loss 0.145632, acc 0.90625
2017-03-02T17:58:53.679276: step 20361, loss 0.0823143, acc 0.953125
2017-03-02T17:58:53.750984: step 20362, loss 0.196632, acc 0.890625
2017-03-02T17:58:53.825533: step 20363, loss 0.122, acc 0.96875
2017-03-02T17:58:53.913207: step 20364, loss 0.121902, acc 0.96875
2017-03-02T17:58:53.989107: step 20365, loss 0.0625248, acc 0.984375
2017-03-02T17:58:54.066911: step 20366, loss 0.18508, acc 0.953125
2017-03-02T17:58:54.137549: step 20367, loss 0.100887, acc 0.9375
2017-03-02T17:58:54.204513: step 20368, loss 0.174111, acc 0.90625
2017-03-02T17:58:54.281879: step 20369, loss 0.144545, acc 0.90625
2017-03-02T17:58:54.349873: step 20370, loss 0.0767906, acc 0.96875
2017-03-02T17:58:54.422699: step 20371, loss 0.139624, acc 0.953125
2017-03-02T17:58:54.499470: step 20372, loss 0.170568, acc 0.96875
2017-03-02T17:58:54.581972: step 20373, loss 0.185382, acc 0.890625
2017-03-02T17:58:54.658220: step 20374, loss 0.11493, acc 0.9375
2017-03-02T17:58:54.726152: step 20375, loss 0.179957, acc 0.921875
2017-03-02T17:58:54.806533: step 20376, loss 0.0744633, acc 0.9375
2017-03-02T17:58:54.886842: step 20377, loss 0.215744, acc 0.90625
2017-03-02T17:58:54.956778: step 20378, loss 0.256004, acc 0.90625
2017-03-02T17:58:55.026756: step 20379, loss 0.112533, acc 0.9375
2017-03-02T17:58:55.106050: step 20380, loss 0.102008, acc 0.96875
2017-03-02T17:58:55.178643: step 20381, loss 0.13684, acc 0.9375
2017-03-02T17:58:55.249770: step 20382, loss 0.112475, acc 0.953125
2017-03-02T17:58:55.324347: step 20383, loss 0.0790065, acc 0.984375
2017-03-02T17:58:55.397455: step 20384, loss 0.506608, acc 0.75
2017-03-02T17:58:55.471377: step 20385, loss 0.0862692, acc 0.953125
2017-03-02T17:58:55.550054: step 20386, loss 0.179709, acc 0.921875
2017-03-02T17:58:55.623959: step 20387, loss 0.0978546, acc 0.953125
2017-03-02T17:58:55.694890: step 20388, loss 0.0924764, acc 0.96875
2017-03-02T17:58:55.771142: step 20389, loss 0.206252, acc 0.9375
2017-03-02T17:58:55.844401: step 20390, loss 0.0707225, acc 0.96875
2017-03-02T17:58:55.939495: step 20391, loss 0.149164, acc 0.921875
2017-03-02T17:58:56.011045: step 20392, loss 0.184005, acc 0.90625
2017-03-02T17:58:56.083393: step 20393, loss 0.124379, acc 0.9375
2017-03-02T17:58:56.152857: step 20394, loss 0.14347, acc 0.953125
2017-03-02T17:58:56.227849: step 20395, loss 0.23909, acc 0.890625
2017-03-02T17:58:56.298795: step 20396, loss 0.246936, acc 0.90625
2017-03-02T17:58:56.367935: step 20397, loss 0.181372, acc 0.9375
2017-03-02T17:58:56.438552: step 20398, loss 0.202945, acc 0.921875
2017-03-02T17:58:56.521175: step 20399, loss 0.196456, acc 0.921875
2017-03-02T17:58:56.598647: step 20400, loss 0.196354, acc 0.921875

Evaluation:
2017-03-02T17:58:56.634504: step 20400, loss 2.467, acc 0.65465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20400

2017-03-02T17:58:57.078634: step 20401, loss 0.161739, acc 0.9375
2017-03-02T17:58:57.150305: step 20402, loss 0.11165, acc 0.96875
2017-03-02T17:58:57.234327: step 20403, loss 0.0639726, acc 1
2017-03-02T17:58:57.308395: step 20404, loss 0.264554, acc 0.890625
2017-03-02T17:58:57.392686: step 20405, loss 0.168966, acc 0.9375
2017-03-02T17:58:57.474714: step 20406, loss 0.108129, acc 0.953125
2017-03-02T17:58:57.545155: step 20407, loss 0.105792, acc 0.953125
2017-03-02T17:58:57.620196: step 20408, loss 0.111751, acc 0.9375
2017-03-02T17:58:57.686943: step 20409, loss 0.0950159, acc 0.953125
2017-03-02T17:58:57.751696: step 20410, loss 0.126286, acc 0.9375
2017-03-02T17:58:57.827138: step 20411, loss 0.0607297, acc 0.984375
2017-03-02T17:58:57.898195: step 20412, loss 0.108036, acc 0.96875
2017-03-02T17:58:57.968208: step 20413, loss 0.111651, acc 0.96875
2017-03-02T17:58:58.046227: step 20414, loss 0.128363, acc 0.921875
2017-03-02T17:58:58.121147: step 20415, loss 0.0889715, acc 0.953125
2017-03-02T17:58:58.193572: step 20416, loss 0.154548, acc 0.90625
2017-03-02T17:58:58.267894: step 20417, loss 0.117489, acc 0.953125
2017-03-02T17:58:58.341471: step 20418, loss 0.143836, acc 0.9375
2017-03-02T17:58:58.433485: step 20419, loss 0.187588, acc 0.921875
2017-03-02T17:58:58.503598: step 20420, loss 0.0807201, acc 0.953125
2017-03-02T17:58:58.580341: step 20421, loss 0.0567873, acc 1
2017-03-02T17:58:58.651388: step 20422, loss 0.151705, acc 0.921875
2017-03-02T17:58:58.722749: step 20423, loss 0.0190842, acc 1
2017-03-02T17:58:58.801712: step 20424, loss 0.185995, acc 0.9375
2017-03-02T17:58:58.884629: step 20425, loss 0.104154, acc 0.9375
2017-03-02T17:58:58.960286: step 20426, loss 0.134882, acc 0.953125
2017-03-02T17:58:59.032039: step 20427, loss 0.0981079, acc 0.9375
2017-03-02T17:58:59.103668: step 20428, loss 0.117561, acc 0.9375
2017-03-02T17:58:59.171690: step 20429, loss 0.175041, acc 0.921875
2017-03-02T17:58:59.248023: step 20430, loss 0.208463, acc 0.890625
2017-03-02T17:58:59.324440: step 20431, loss 0.140226, acc 0.953125
2017-03-02T17:58:59.396594: step 20432, loss 0.203167, acc 0.890625
2017-03-02T17:58:59.467137: step 20433, loss 0.142787, acc 0.9375
2017-03-02T17:58:59.533700: step 20434, loss 0.154548, acc 0.921875
2017-03-02T17:58:59.611855: step 20435, loss 0.121233, acc 0.9375
2017-03-02T17:58:59.690729: step 20436, loss 0.223146, acc 0.859375
2017-03-02T17:58:59.765223: step 20437, loss 0.158858, acc 0.890625
2017-03-02T17:58:59.830418: step 20438, loss 0.072964, acc 0.96875
2017-03-02T17:58:59.942109: step 20439, loss 0.119322, acc 0.9375
2017-03-02T17:59:00.008968: step 20440, loss 0.122588, acc 0.9375
2017-03-02T17:59:00.091439: step 20441, loss 0.0780708, acc 0.96875
2017-03-02T17:59:00.163560: step 20442, loss 0.122736, acc 0.921875
2017-03-02T17:59:00.270604: step 20443, loss 0.220163, acc 0.859375
2017-03-02T17:59:00.340416: step 20444, loss 0.0813349, acc 0.96875
2017-03-02T17:59:00.419429: step 20445, loss 0.218138, acc 0.90625
2017-03-02T17:59:00.490388: step 20446, loss 0.137289, acc 0.96875
2017-03-02T17:59:00.556775: step 20447, loss 0.0939177, acc 0.953125
2017-03-02T17:59:00.629703: step 20448, loss 0.120565, acc 0.9375
2017-03-02T17:59:00.704527: step 20449, loss 0.201761, acc 0.890625
2017-03-02T17:59:00.776409: step 20450, loss 0.216593, acc 0.90625
2017-03-02T17:59:00.852648: step 20451, loss 0.15443, acc 0.921875
2017-03-02T17:59:00.932817: step 20452, loss 0.0825065, acc 0.953125
2017-03-02T17:59:01.004018: step 20453, loss 0.14129, acc 0.9375
2017-03-02T17:59:01.063815: step 20454, loss 0.134605, acc 0.953125
2017-03-02T17:59:01.134369: step 20455, loss 0.0767237, acc 0.96875
2017-03-02T17:59:01.206829: step 20456, loss 0.028255, acc 0.984375
2017-03-02T17:59:01.278168: step 20457, loss 0.119923, acc 0.9375
2017-03-02T17:59:01.357547: step 20458, loss 0.329129, acc 0.875
2017-03-02T17:59:01.431728: step 20459, loss 0.160076, acc 0.921875
2017-03-02T17:59:01.508010: step 20460, loss 0.0949958, acc 0.953125
2017-03-02T17:59:01.580684: step 20461, loss 0.105123, acc 0.953125
2017-03-02T17:59:01.646712: step 20462, loss 0.133734, acc 0.953125
2017-03-02T17:59:01.720863: step 20463, loss 0.13093, acc 0.953125
2017-03-02T17:59:01.801175: step 20464, loss 0.139394, acc 0.9375
2017-03-02T17:59:01.866802: step 20465, loss 0.157627, acc 0.921875
2017-03-02T17:59:01.930999: step 20466, loss 0.101876, acc 0.96875
2017-03-02T17:59:01.993784: step 20467, loss 0.271674, acc 0.859375
2017-03-02T17:59:02.076158: step 20468, loss 0.214668, acc 0.890625
2017-03-02T17:59:02.153939: step 20469, loss 0.0413455, acc 0.96875
2017-03-02T17:59:02.234366: step 20470, loss 0.0545084, acc 0.984375
2017-03-02T17:59:02.307889: step 20471, loss 0.163761, acc 0.953125
2017-03-02T17:59:02.379897: step 20472, loss 0.210418, acc 0.875
2017-03-02T17:59:02.452543: step 20473, loss 0.0518933, acc 1
2017-03-02T17:59:02.528534: step 20474, loss 0.159453, acc 0.921875
2017-03-02T17:59:02.602873: step 20475, loss 0.182371, acc 0.90625
2017-03-02T17:59:02.675061: step 20476, loss 0.120009, acc 0.9375
2017-03-02T17:59:02.768977: step 20477, loss 0.210493, acc 0.921875
2017-03-02T17:59:02.851497: step 20478, loss 0.104541, acc 0.9375
2017-03-02T17:59:02.929282: step 20479, loss 0.127935, acc 0.96875
2017-03-02T17:59:03.006351: step 20480, loss 0.144606, acc 0.953125
2017-03-02T17:59:03.083091: step 20481, loss 0.186989, acc 0.921875
2017-03-02T17:59:03.157054: step 20482, loss 0.223715, acc 0.921875
2017-03-02T17:59:03.226235: step 20483, loss 0.263453, acc 0.90625
2017-03-02T17:59:03.302281: step 20484, loss 0.118133, acc 0.953125
2017-03-02T17:59:03.392182: step 20485, loss 0.252119, acc 0.890625
2017-03-02T17:59:03.470372: step 20486, loss 0.132067, acc 0.921875
2017-03-02T17:59:03.545063: step 20487, loss 0.100026, acc 0.953125
2017-03-02T17:59:03.618554: step 20488, loss 0.0313012, acc 0.984375
2017-03-02T17:59:03.699574: step 20489, loss 0.184301, acc 0.90625
2017-03-02T17:59:03.773831: step 20490, loss 0.145821, acc 0.953125
2017-03-02T17:59:03.852170: step 20491, loss 0.259507, acc 0.875
2017-03-02T17:59:03.930510: step 20492, loss 0.053622, acc 1
2017-03-02T17:59:04.007228: step 20493, loss 0.155436, acc 0.921875
2017-03-02T17:59:04.102935: step 20494, loss 0.162322, acc 0.9375
2017-03-02T17:59:04.186558: step 20495, loss 0.237024, acc 0.90625
2017-03-02T17:59:04.265168: step 20496, loss 0.26223, acc 0.90625
2017-03-02T17:59:04.331921: step 20497, loss 0.142804, acc 0.9375
2017-03-02T17:59:04.411119: step 20498, loss 0.199001, acc 0.875
2017-03-02T17:59:04.482979: step 20499, loss 0.0980479, acc 0.953125
2017-03-02T17:59:04.563278: step 20500, loss 0.212667, acc 0.953125

Evaluation:
2017-03-02T17:59:04.594095: step 20500, loss 2.48433, acc 0.619322

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20500

2017-03-02T17:59:05.067612: step 20501, loss 0.313988, acc 0.859375
2017-03-02T17:59:05.140471: step 20502, loss 0.172562, acc 0.953125
2017-03-02T17:59:05.212424: step 20503, loss 0.24651, acc 0.90625
2017-03-02T17:59:05.286561: step 20504, loss 0.223553, acc 0.90625
2017-03-02T17:59:05.355469: step 20505, loss 0.209328, acc 0.90625
2017-03-02T17:59:05.430220: step 20506, loss 0.194759, acc 0.90625
2017-03-02T17:59:05.506896: step 20507, loss 0.2339, acc 0.90625
2017-03-02T17:59:05.582610: step 20508, loss 0.047967, acc 0.984375
2017-03-02T17:59:05.656635: step 20509, loss 0.133474, acc 0.9375
2017-03-02T17:59:05.731549: step 20510, loss 0.185738, acc 0.9375
2017-03-02T17:59:05.805947: step 20511, loss 0.242959, acc 0.890625
2017-03-02T17:59:05.883630: step 20512, loss 0.104388, acc 0.953125
2017-03-02T17:59:05.967338: step 20513, loss 0.186911, acc 0.921875
2017-03-02T17:59:06.034245: step 20514, loss 0.181749, acc 0.921875
2017-03-02T17:59:06.099216: step 20515, loss 0.154915, acc 0.921875
2017-03-02T17:59:06.172666: step 20516, loss 0.0837687, acc 0.953125
2017-03-02T17:59:06.246278: step 20517, loss 0.129198, acc 0.9375
2017-03-02T17:59:06.317154: step 20518, loss 0.179084, acc 0.90625
2017-03-02T17:59:06.390479: step 20519, loss 0.137667, acc 0.90625
2017-03-02T17:59:06.458773: step 20520, loss 0.169417, acc 0.90625
2017-03-02T17:59:06.528957: step 20521, loss 0.26427, acc 0.890625
2017-03-02T17:59:06.603738: step 20522, loss 0.0798363, acc 1
2017-03-02T17:59:06.673465: step 20523, loss 0.221571, acc 0.90625
2017-03-02T17:59:06.746452: step 20524, loss 0.101037, acc 0.953125
2017-03-02T17:59:06.812380: step 20525, loss 0.152959, acc 0.90625
2017-03-02T17:59:06.920334: step 20526, loss 0.118497, acc 0.9375
2017-03-02T17:59:06.995614: step 20527, loss 0.132846, acc 0.9375
2017-03-02T17:59:07.071537: step 20528, loss 0.249785, acc 0.9375
2017-03-02T17:59:07.145236: step 20529, loss 0.121187, acc 0.953125
2017-03-02T17:59:07.219276: step 20530, loss 0.30835, acc 0.90625
2017-03-02T17:59:07.292367: step 20531, loss 0.0693536, acc 0.96875
2017-03-02T17:59:07.365498: step 20532, loss 0.200616, acc 0.921875
2017-03-02T17:59:07.436202: step 20533, loss 0.0897522, acc 0.9375
2017-03-02T17:59:07.507500: step 20534, loss 0.294071, acc 0.8125
2017-03-02T17:59:07.579208: step 20535, loss 0.262231, acc 0.890625
2017-03-02T17:59:07.642354: step 20536, loss 0.100926, acc 0.984375
2017-03-02T17:59:07.711968: step 20537, loss 0.1183, acc 0.921875
2017-03-02T17:59:07.791195: step 20538, loss 0.360616, acc 0.859375
2017-03-02T17:59:07.863170: step 20539, loss 0.147156, acc 0.953125
2017-03-02T17:59:07.936483: step 20540, loss 0.134005, acc 0.9375
2017-03-02T17:59:08.013319: step 20541, loss 0.211432, acc 0.875
2017-03-02T17:59:08.079065: step 20542, loss 0.131785, acc 0.9375
2017-03-02T17:59:08.150985: step 20543, loss 0.0750757, acc 0.96875
2017-03-02T17:59:08.226616: step 20544, loss 0.210509, acc 0.875
2017-03-02T17:59:08.297392: step 20545, loss 0.0591402, acc 0.984375
2017-03-02T17:59:08.371380: step 20546, loss 0.0554378, acc 0.96875
2017-03-02T17:59:08.442615: step 20547, loss 0.0935917, acc 0.9375
2017-03-02T17:59:08.524586: step 20548, loss 0.252871, acc 0.890625
2017-03-02T17:59:08.598202: step 20549, loss 0.164893, acc 0.921875
2017-03-02T17:59:08.672702: step 20550, loss 0.062732, acc 0.96875
2017-03-02T17:59:08.739572: step 20551, loss 0.163857, acc 0.9375
2017-03-02T17:59:08.815225: step 20552, loss 0.198899, acc 0.890625
2017-03-02T17:59:08.883292: step 20553, loss 0.0675644, acc 0.984375
2017-03-02T17:59:08.958766: step 20554, loss 0.137409, acc 0.96875
2017-03-02T17:59:09.030622: step 20555, loss 0.223418, acc 0.890625
2017-03-02T17:59:09.101953: step 20556, loss 0.208098, acc 0.890625
2017-03-02T17:59:09.174996: step 20557, loss 0.136229, acc 0.9375
2017-03-02T17:59:09.247773: step 20558, loss 0.105001, acc 0.953125
2017-03-02T17:59:09.320153: step 20559, loss 0.181959, acc 0.90625
2017-03-02T17:59:09.400518: step 20560, loss 0.165213, acc 0.953125
2017-03-02T17:59:09.478748: step 20561, loss 0.213031, acc 0.9375
2017-03-02T17:59:09.553883: step 20562, loss 0.235733, acc 0.90625
2017-03-02T17:59:09.631474: step 20563, loss 0.180323, acc 0.921875
2017-03-02T17:59:09.701560: step 20564, loss 0.130364, acc 0.9375
2017-03-02T17:59:09.778631: step 20565, loss 0.13081, acc 0.921875
2017-03-02T17:59:09.849724: step 20566, loss 0.113438, acc 0.921875
2017-03-02T17:59:09.930808: step 20567, loss 0.201746, acc 0.90625
2017-03-02T17:59:10.004455: step 20568, loss 0.090954, acc 0.96875
2017-03-02T17:59:10.084817: step 20569, loss 0.160885, acc 0.9375
2017-03-02T17:59:10.162526: step 20570, loss 0.237949, acc 0.84375
2017-03-02T17:59:10.227291: step 20571, loss 0.252222, acc 0.875
2017-03-02T17:59:10.302092: step 20572, loss 0.164275, acc 0.953125
2017-03-02T17:59:10.376577: step 20573, loss 0.0861455, acc 0.96875
2017-03-02T17:59:10.454033: step 20574, loss 0.27341, acc 0.859375
2017-03-02T17:59:10.529427: step 20575, loss 0.332071, acc 0.84375
2017-03-02T17:59:10.609053: step 20576, loss 0.0838405, acc 0.953125
2017-03-02T17:59:10.680653: step 20577, loss 0.13021, acc 0.953125
2017-03-02T17:59:10.758223: step 20578, loss 0.230882, acc 0.921875
2017-03-02T17:59:10.821891: step 20579, loss 0.180713, acc 0.953125
2017-03-02T17:59:10.885278: step 20580, loss 0.507705, acc 0.5
2017-03-02T17:59:10.975924: step 20581, loss 0.151376, acc 0.921875
2017-03-02T17:59:11.065020: step 20582, loss 0.114846, acc 0.984375
2017-03-02T17:59:11.133459: step 20583, loss 0.0748859, acc 0.96875
2017-03-02T17:59:11.210791: step 20584, loss 0.056507, acc 0.984375
2017-03-02T17:59:11.283605: step 20585, loss 0.158839, acc 0.921875
2017-03-02T17:59:11.357467: step 20586, loss 0.135836, acc 0.921875
2017-03-02T17:59:11.431558: step 20587, loss 0.133209, acc 0.9375
2017-03-02T17:59:11.508112: step 20588, loss 0.0400335, acc 1
2017-03-02T17:59:11.584415: step 20589, loss 0.0764553, acc 1
2017-03-02T17:59:11.658855: step 20590, loss 0.154603, acc 0.921875
2017-03-02T17:59:11.733149: step 20591, loss 0.0889486, acc 0.953125
2017-03-02T17:59:11.812501: step 20592, loss 0.143804, acc 0.9375
2017-03-02T17:59:11.894387: step 20593, loss 0.272534, acc 0.859375
2017-03-02T17:59:11.968529: step 20594, loss 0.179994, acc 0.921875
2017-03-02T17:59:12.046158: step 20595, loss 0.169536, acc 0.9375
2017-03-02T17:59:12.116714: step 20596, loss 0.274876, acc 0.875
2017-03-02T17:59:12.201097: step 20597, loss 0.174984, acc 0.921875
2017-03-02T17:59:12.276296: step 20598, loss 0.177427, acc 0.90625
2017-03-02T17:59:12.348182: step 20599, loss 0.139936, acc 0.90625
2017-03-02T17:59:12.422742: step 20600, loss 0.197571, acc 0.875

Evaluation:
2017-03-02T17:59:12.461136: step 20600, loss 2.38743, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20600

2017-03-02T17:59:12.925209: step 20601, loss 0.113785, acc 0.96875
2017-03-02T17:59:13.000715: step 20602, loss 0.154168, acc 0.921875
2017-03-02T17:59:13.079282: step 20603, loss 0.198379, acc 0.90625
2017-03-02T17:59:13.153803: step 20604, loss 0.156343, acc 0.9375
2017-03-02T17:59:13.221584: step 20605, loss 0.173137, acc 0.921875
2017-03-02T17:59:13.296752: step 20606, loss 0.232041, acc 0.921875
2017-03-02T17:59:13.369141: step 20607, loss 0.0292881, acc 1
2017-03-02T17:59:13.441893: step 20608, loss 0.120423, acc 0.96875
2017-03-02T17:59:13.504888: step 20609, loss 0.225009, acc 0.890625
2017-03-02T17:59:13.608677: step 20610, loss 0.195183, acc 0.875
2017-03-02T17:59:13.671669: step 20611, loss 0.142175, acc 0.9375
2017-03-02T17:59:13.733132: step 20612, loss 0.110142, acc 0.9375
2017-03-02T17:59:13.843535: step 20613, loss 0.163189, acc 0.953125
2017-03-02T17:59:13.915487: step 20614, loss 0.0930204, acc 0.984375
2017-03-02T17:59:13.988861: step 20615, loss 0.11903, acc 0.9375
2017-03-02T17:59:14.061752: step 20616, loss 0.177725, acc 0.9375
2017-03-02T17:59:14.134218: step 20617, loss 0.111049, acc 0.953125
2017-03-02T17:59:14.210680: step 20618, loss 0.0465104, acc 0.984375
2017-03-02T17:59:14.285718: step 20619, loss 0.117688, acc 0.9375
2017-03-02T17:59:14.363348: step 20620, loss 0.123365, acc 0.9375
2017-03-02T17:59:14.425350: step 20621, loss 0.124474, acc 0.953125
2017-03-02T17:59:14.500207: step 20622, loss 0.0905018, acc 0.984375
2017-03-02T17:59:14.572946: step 20623, loss 0.084245, acc 0.96875
2017-03-02T17:59:14.643743: step 20624, loss 0.106762, acc 0.953125
2017-03-02T17:59:14.719247: step 20625, loss 0.138396, acc 0.96875
2017-03-02T17:59:14.790073: step 20626, loss 0.190317, acc 0.90625
2017-03-02T17:59:14.865152: step 20627, loss 0.24031, acc 0.875
2017-03-02T17:59:14.946960: step 20628, loss 0.189369, acc 0.9375
2017-03-02T17:59:15.018210: step 20629, loss 0.254863, acc 0.890625
2017-03-02T17:59:15.089501: step 20630, loss 0.0694586, acc 0.96875
2017-03-02T17:59:15.162279: step 20631, loss 0.129957, acc 0.96875
2017-03-02T17:59:15.240922: step 20632, loss 0.2314, acc 0.90625
2017-03-02T17:59:15.314129: step 20633, loss 0.248202, acc 0.90625
2017-03-02T17:59:15.389655: step 20634, loss 0.19937, acc 0.9375
2017-03-02T17:59:15.471496: step 20635, loss 0.138689, acc 0.9375
2017-03-02T17:59:15.563864: step 20636, loss 0.151041, acc 0.9375
2017-03-02T17:59:15.659028: step 20637, loss 0.1809, acc 0.921875
2017-03-02T17:59:15.726303: step 20638, loss 0.192237, acc 0.859375
2017-03-02T17:59:15.792618: step 20639, loss 0.0627214, acc 0.96875
2017-03-02T17:59:15.885577: step 20640, loss 0.130576, acc 0.9375
2017-03-02T17:59:15.965553: step 20641, loss 0.202721, acc 0.890625
2017-03-02T17:59:16.041792: step 20642, loss 0.0922318, acc 0.953125
2017-03-02T17:59:16.117568: step 20643, loss 0.14415, acc 0.9375
2017-03-02T17:59:16.190599: step 20644, loss 0.119735, acc 0.953125
2017-03-02T17:59:16.263218: step 20645, loss 0.255876, acc 0.875
2017-03-02T17:59:16.348286: step 20646, loss 0.114204, acc 0.96875
2017-03-02T17:59:16.424939: step 20647, loss 0.139506, acc 0.953125
2017-03-02T17:59:16.494607: step 20648, loss 0.206561, acc 0.875
2017-03-02T17:59:16.567301: step 20649, loss 0.201957, acc 0.90625
2017-03-02T17:59:16.641173: step 20650, loss 0.111962, acc 0.96875
2017-03-02T17:59:16.716923: step 20651, loss 0.16028, acc 0.90625
2017-03-02T17:59:16.789907: step 20652, loss 0.113173, acc 0.984375
2017-03-02T17:59:16.862663: step 20653, loss 0.111514, acc 0.96875
2017-03-02T17:59:16.937238: step 20654, loss 0.162767, acc 0.921875
2017-03-02T17:59:17.007083: step 20655, loss 0.12053, acc 0.921875
2017-03-02T17:59:17.083653: step 20656, loss 0.16755, acc 0.921875
2017-03-02T17:59:17.151838: step 20657, loss 0.303611, acc 0.875
2017-03-02T17:59:17.225693: step 20658, loss 0.135887, acc 0.921875
2017-03-02T17:59:17.299319: step 20659, loss 0.0772101, acc 0.96875
2017-03-02T17:59:17.386163: step 20660, loss 0.149708, acc 0.9375
2017-03-02T17:59:17.454583: step 20661, loss 0.090698, acc 0.953125
2017-03-02T17:59:17.525558: step 20662, loss 0.177499, acc 0.953125
2017-03-02T17:59:17.632491: step 20663, loss 0.0249977, acc 1
2017-03-02T17:59:17.708054: step 20664, loss 0.116403, acc 0.953125
2017-03-02T17:59:17.777717: step 20665, loss 0.135925, acc 0.9375
2017-03-02T17:59:17.846250: step 20666, loss 0.291827, acc 0.859375
2017-03-02T17:59:17.921806: step 20667, loss 0.233785, acc 0.90625
2017-03-02T17:59:17.992030: step 20668, loss 0.145538, acc 0.953125
2017-03-02T17:59:18.074588: step 20669, loss 0.120105, acc 0.921875
2017-03-02T17:59:18.149895: step 20670, loss 0.151317, acc 0.9375
2017-03-02T17:59:18.222641: step 20671, loss 0.0884206, acc 0.9375
2017-03-02T17:59:18.297290: step 20672, loss 0.146139, acc 0.9375
2017-03-02T17:59:18.383406: step 20673, loss 0.106213, acc 0.953125
2017-03-02T17:59:18.456998: step 20674, loss 0.119486, acc 0.953125
2017-03-02T17:59:18.523537: step 20675, loss 0.0923008, acc 0.96875
2017-03-02T17:59:18.601887: step 20676, loss 0.0636558, acc 0.96875
2017-03-02T17:59:18.675801: step 20677, loss 0.127669, acc 0.921875
2017-03-02T17:59:18.754222: step 20678, loss 0.0867674, acc 0.9375
2017-03-02T17:59:18.827204: step 20679, loss 0.0969924, acc 0.96875
2017-03-02T17:59:18.903204: step 20680, loss 0.123175, acc 0.9375
2017-03-02T17:59:18.978157: step 20681, loss 0.184085, acc 0.921875
2017-03-02T17:59:19.053509: step 20682, loss 0.169077, acc 0.953125
2017-03-02T17:59:19.123294: step 20683, loss 0.105838, acc 0.953125
2017-03-02T17:59:19.197704: step 20684, loss 0.168027, acc 0.921875
2017-03-02T17:59:19.268596: step 20685, loss 0.0868988, acc 0.96875
2017-03-02T17:59:19.341045: step 20686, loss 0.104302, acc 0.953125
2017-03-02T17:59:19.409326: step 20687, loss 0.105343, acc 0.953125
2017-03-02T17:59:19.482325: step 20688, loss 0.158536, acc 0.9375
2017-03-02T17:59:19.568962: step 20689, loss 0.117346, acc 0.953125
2017-03-02T17:59:19.651428: step 20690, loss 0.0800816, acc 0.953125
2017-03-02T17:59:19.726929: step 20691, loss 0.212037, acc 0.875
2017-03-02T17:59:19.806303: step 20692, loss 0.168639, acc 0.9375
2017-03-02T17:59:19.871795: step 20693, loss 0.182754, acc 0.9375
2017-03-02T17:59:19.939576: step 20694, loss 0.128998, acc 0.96875
2017-03-02T17:59:20.010141: step 20695, loss 0.244365, acc 0.90625
2017-03-02T17:59:20.091070: step 20696, loss 0.13245, acc 0.953125
2017-03-02T17:59:20.166971: step 20697, loss 0.0519799, acc 0.96875
2017-03-02T17:59:20.238527: step 20698, loss 0.176402, acc 0.921875
2017-03-02T17:59:20.316559: step 20699, loss 0.223971, acc 0.90625
2017-03-02T17:59:20.392893: step 20700, loss 0.165753, acc 0.9375

Evaluation:
2017-03-02T17:59:20.423005: step 20700, loss 2.46197, acc 0.652487

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20700

2017-03-02T17:59:20.857414: step 20701, loss 0.200468, acc 0.9375
2017-03-02T17:59:20.931087: step 20702, loss 0.173735, acc 0.921875
2017-03-02T17:59:21.006498: step 20703, loss 0.251851, acc 0.921875
2017-03-02T17:59:21.080537: step 20704, loss 0.127274, acc 0.9375
2017-03-02T17:59:21.162262: step 20705, loss 0.125508, acc 0.953125
2017-03-02T17:59:21.231812: step 20706, loss 0.161888, acc 0.9375
2017-03-02T17:59:21.299726: step 20707, loss 0.174451, acc 0.90625
2017-03-02T17:59:21.376921: step 20708, loss 0.163553, acc 0.90625
2017-03-02T17:59:21.451959: step 20709, loss 0.0934668, acc 0.96875
2017-03-02T17:59:21.523971: step 20710, loss 0.211625, acc 0.9375
2017-03-02T17:59:21.595088: step 20711, loss 0.130315, acc 0.9375
2017-03-02T17:59:21.665638: step 20712, loss 0.103766, acc 0.953125
2017-03-02T17:59:21.738556: step 20713, loss 0.191596, acc 0.921875
2017-03-02T17:59:21.802177: step 20714, loss 0.183268, acc 0.953125
2017-03-02T17:59:21.871707: step 20715, loss 0.105505, acc 0.96875
2017-03-02T17:59:21.938624: step 20716, loss 0.2539, acc 0.875
2017-03-02T17:59:22.010898: step 20717, loss 0.0974614, acc 0.953125
2017-03-02T17:59:22.091158: step 20718, loss 0.188803, acc 0.875
2017-03-02T17:59:22.163349: step 20719, loss 0.156571, acc 0.90625
2017-03-02T17:59:22.239767: step 20720, loss 0.23815, acc 0.890625
2017-03-02T17:59:22.312764: step 20721, loss 0.266493, acc 0.859375
2017-03-02T17:59:22.385436: step 20722, loss 0.166804, acc 0.921875
2017-03-02T17:59:22.462252: step 20723, loss 0.242123, acc 0.890625
2017-03-02T17:59:22.531716: step 20724, loss 0.23663, acc 0.90625
2017-03-02T17:59:22.600572: step 20725, loss 0.066551, acc 0.96875
2017-03-02T17:59:22.672639: step 20726, loss 0.120727, acc 0.96875
2017-03-02T17:59:22.744337: step 20727, loss 0.0467864, acc 1
2017-03-02T17:59:22.813373: step 20728, loss 0.192232, acc 0.90625
2017-03-02T17:59:22.890869: step 20729, loss 0.234705, acc 0.890625
2017-03-02T17:59:22.963244: step 20730, loss 0.161191, acc 0.921875
2017-03-02T17:59:23.034902: step 20731, loss 0.144553, acc 0.9375
2017-03-02T17:59:23.104304: step 20732, loss 0.233751, acc 0.921875
2017-03-02T17:59:23.172815: step 20733, loss 0.179209, acc 0.9375
2017-03-02T17:59:23.237928: step 20734, loss 0.144207, acc 0.90625
2017-03-02T17:59:23.310205: step 20735, loss 0.354042, acc 0.828125
2017-03-02T17:59:23.384915: step 20736, loss 0.227032, acc 0.9375
2017-03-02T17:59:23.457403: step 20737, loss 0.159614, acc 0.90625
2017-03-02T17:59:23.526673: step 20738, loss 0.112472, acc 0.953125
2017-03-02T17:59:23.596510: step 20739, loss 0.13431, acc 0.953125
2017-03-02T17:59:23.668067: step 20740, loss 0.163817, acc 0.890625
2017-03-02T17:59:23.735884: step 20741, loss 0.127416, acc 0.96875
2017-03-02T17:59:23.805612: step 20742, loss 0.110856, acc 0.953125
2017-03-02T17:59:23.872913: step 20743, loss 0.130284, acc 0.9375
2017-03-02T17:59:23.942007: step 20744, loss 0.187533, acc 0.90625
2017-03-02T17:59:24.009184: step 20745, loss 0.118282, acc 0.9375
2017-03-02T17:59:24.086014: step 20746, loss 0.162068, acc 0.9375
2017-03-02T17:59:24.164175: step 20747, loss 0.143574, acc 0.9375
2017-03-02T17:59:24.240885: step 20748, loss 0.0895408, acc 0.96875
2017-03-02T17:59:24.314941: step 20749, loss 0.202108, acc 0.90625
2017-03-02T17:59:24.388661: step 20750, loss 0.165253, acc 0.9375
2017-03-02T17:59:24.462525: step 20751, loss 0.175749, acc 0.9375
2017-03-02T17:59:24.531752: step 20752, loss 0.0709447, acc 0.96875
2017-03-02T17:59:24.602499: step 20753, loss 0.1658, acc 0.921875
2017-03-02T17:59:24.671138: step 20754, loss 0.185088, acc 0.921875
2017-03-02T17:59:24.759081: step 20755, loss 0.124021, acc 0.9375
2017-03-02T17:59:24.837420: step 20756, loss 0.200199, acc 0.90625
2017-03-02T17:59:24.905557: step 20757, loss 0.194618, acc 0.90625
2017-03-02T17:59:24.986925: step 20758, loss 0.160066, acc 0.890625
2017-03-02T17:59:25.056620: step 20759, loss 0.157798, acc 0.96875
2017-03-02T17:59:25.134905: step 20760, loss 0.121718, acc 0.96875
2017-03-02T17:59:25.210090: step 20761, loss 0.154582, acc 0.921875
2017-03-02T17:59:25.279837: step 20762, loss 0.0923495, acc 0.953125
2017-03-02T17:59:25.347233: step 20763, loss 0.0741256, acc 0.984375
2017-03-02T17:59:25.426467: step 20764, loss 0.154113, acc 0.9375
2017-03-02T17:59:25.510950: step 20765, loss 0.182352, acc 0.921875
2017-03-02T17:59:25.586332: step 20766, loss 0.174647, acc 0.90625
2017-03-02T17:59:25.660591: step 20767, loss 0.238889, acc 0.890625
2017-03-02T17:59:25.734373: step 20768, loss 0.2004, acc 0.9375
2017-03-02T17:59:25.802089: step 20769, loss 0.141842, acc 0.953125
2017-03-02T17:59:25.874955: step 20770, loss 0.171144, acc 0.9375
2017-03-02T17:59:25.940670: step 20771, loss 0.0967199, acc 0.96875
2017-03-02T17:59:26.016853: step 20772, loss 0.173854, acc 0.921875
2017-03-02T17:59:26.089897: step 20773, loss 0.22623, acc 0.921875
2017-03-02T17:59:26.166053: step 20774, loss 0.172313, acc 0.890625
2017-03-02T17:59:26.239849: step 20775, loss 0.0610728, acc 0.984375
2017-03-02T17:59:26.311302: step 20776, loss 0.000226132, acc 1
2017-03-02T17:59:26.385624: step 20777, loss 0.0646531, acc 0.984375
2017-03-02T17:59:26.462923: step 20778, loss 0.0966206, acc 0.953125
2017-03-02T17:59:26.540208: step 20779, loss 0.0959278, acc 0.96875
2017-03-02T17:59:26.618293: step 20780, loss 0.0655097, acc 0.96875
2017-03-02T17:59:26.684246: step 20781, loss 0.160931, acc 0.875
2017-03-02T17:59:26.756945: step 20782, loss 0.047353, acc 0.984375
2017-03-02T17:59:26.826709: step 20783, loss 0.226893, acc 0.921875
2017-03-02T17:59:26.898569: step 20784, loss 0.122567, acc 0.953125
2017-03-02T17:59:26.970363: step 20785, loss 0.196985, acc 0.90625
2017-03-02T17:59:27.041792: step 20786, loss 0.139675, acc 0.9375
2017-03-02T17:59:27.112866: step 20787, loss 0.202967, acc 0.90625
2017-03-02T17:59:27.181214: step 20788, loss 0.110573, acc 0.9375
2017-03-02T17:59:27.253763: step 20789, loss 0.182614, acc 0.9375
2017-03-02T17:59:27.327598: step 20790, loss 0.0746301, acc 0.96875
2017-03-02T17:59:27.406851: step 20791, loss 0.115257, acc 0.9375
2017-03-02T17:59:27.481113: step 20792, loss 0.186253, acc 0.921875
2017-03-02T17:59:27.554831: step 20793, loss 0.139114, acc 0.9375
2017-03-02T17:59:27.628794: step 20794, loss 0.126058, acc 0.921875
2017-03-02T17:59:27.701122: step 20795, loss 0.120949, acc 0.9375
2017-03-02T17:59:27.776145: step 20796, loss 0.148879, acc 0.921875
2017-03-02T17:59:27.845946: step 20797, loss 0.0866097, acc 0.96875
2017-03-02T17:59:27.925731: step 20798, loss 0.191418, acc 0.921875
2017-03-02T17:59:28.007766: step 20799, loss 0.184578, acc 0.875
2017-03-02T17:59:28.082279: step 20800, loss 0.145251, acc 0.921875

Evaluation:
2017-03-02T17:59:28.118798: step 20800, loss 2.45779, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20800

2017-03-02T17:59:28.586807: step 20801, loss 0.114235, acc 0.9375
2017-03-02T17:59:28.661801: step 20802, loss 0.133979, acc 0.9375
2017-03-02T17:59:28.733001: step 20803, loss 0.119871, acc 0.953125
2017-03-02T17:59:28.804723: step 20804, loss 0.132008, acc 0.921875
2017-03-02T17:59:28.880024: step 20805, loss 0.328336, acc 0.875
2017-03-02T17:59:28.956410: step 20806, loss 0.135333, acc 0.9375
2017-03-02T17:59:29.030834: step 20807, loss 0.189488, acc 0.921875
2017-03-02T17:59:29.110579: step 20808, loss 0.270369, acc 0.90625
2017-03-02T17:59:29.200531: step 20809, loss 0.188887, acc 0.90625
2017-03-02T17:59:29.280422: step 20810, loss 0.263363, acc 0.875
2017-03-02T17:59:29.359149: step 20811, loss 0.0732652, acc 0.96875
2017-03-02T17:59:29.437744: step 20812, loss 0.0396529, acc 0.984375
2017-03-02T17:59:29.505385: step 20813, loss 0.27672, acc 0.90625
2017-03-02T17:59:29.575890: step 20814, loss 0.0573803, acc 1
2017-03-02T17:59:29.658483: step 20815, loss 0.15538, acc 0.9375
2017-03-02T17:59:29.746281: step 20816, loss 0.307607, acc 0.875
2017-03-02T17:59:29.823675: step 20817, loss 0.165773, acc 0.953125
2017-03-02T17:59:29.897980: step 20818, loss 0.247918, acc 0.890625
2017-03-02T17:59:29.963292: step 20819, loss 0.180487, acc 0.890625
2017-03-02T17:59:30.033991: step 20820, loss 0.160716, acc 0.9375
2017-03-02T17:59:30.098411: step 20821, loss 0.202878, acc 0.90625
2017-03-02T17:59:30.164043: step 20822, loss 0.149447, acc 0.953125
2017-03-02T17:59:30.236946: step 20823, loss 0.145427, acc 0.9375
2017-03-02T17:59:30.316561: step 20824, loss 0.12807, acc 0.9375
2017-03-02T17:59:30.399230: step 20825, loss 0.145148, acc 0.9375
2017-03-02T17:59:30.477892: step 20826, loss 0.0521896, acc 0.96875
2017-03-02T17:59:30.551220: step 20827, loss 0.15593, acc 0.90625
2017-03-02T17:59:30.621259: step 20828, loss 0.0693851, acc 0.96875
2017-03-02T17:59:30.693760: step 20829, loss 0.16137, acc 0.921875
2017-03-02T17:59:30.765935: step 20830, loss 0.118275, acc 0.9375
2017-03-02T17:59:30.841061: step 20831, loss 0.080479, acc 0.96875
2017-03-02T17:59:30.908687: step 20832, loss 0.213533, acc 0.90625
2017-03-02T17:59:30.992113: step 20833, loss 0.114897, acc 0.953125
2017-03-02T17:59:31.069430: step 20834, loss 0.13269, acc 0.921875
2017-03-02T17:59:31.147600: step 20835, loss 0.111161, acc 0.96875
2017-03-02T17:59:31.221915: step 20836, loss 0.0941436, acc 0.984375
2017-03-02T17:59:31.300242: step 20837, loss 0.167021, acc 0.90625
2017-03-02T17:59:31.377206: step 20838, loss 0.122094, acc 0.921875
2017-03-02T17:59:31.458625: step 20839, loss 0.167964, acc 0.921875
2017-03-02T17:59:31.530039: step 20840, loss 0.136306, acc 0.921875
2017-03-02T17:59:31.600435: step 20841, loss 0.117538, acc 0.9375
2017-03-02T17:59:31.683289: step 20842, loss 0.193562, acc 0.90625
2017-03-02T17:59:31.765958: step 20843, loss 0.186682, acc 0.921875
2017-03-02T17:59:31.843737: step 20844, loss 0.157741, acc 0.9375
2017-03-02T17:59:31.928162: step 20845, loss 0.205335, acc 0.90625
2017-03-02T17:59:32.008049: step 20846, loss 0.280542, acc 0.875
2017-03-02T17:59:32.088998: step 20847, loss 0.0669941, acc 0.96875
2017-03-02T17:59:32.162466: step 20848, loss 0.165251, acc 0.921875
2017-03-02T17:59:32.231353: step 20849, loss 0.0986698, acc 0.953125
2017-03-02T17:59:32.302131: step 20850, loss 0.229026, acc 0.90625
2017-03-02T17:59:32.371729: step 20851, loss 0.18432, acc 0.921875
2017-03-02T17:59:32.443776: step 20852, loss 0.209559, acc 0.875
2017-03-02T17:59:32.519738: step 20853, loss 0.14493, acc 0.9375
2017-03-02T17:59:32.595014: step 20854, loss 0.11532, acc 0.96875
2017-03-02T17:59:32.673127: step 20855, loss 0.101574, acc 0.96875
2017-03-02T17:59:32.743984: step 20856, loss 0.077318, acc 0.984375
2017-03-02T17:59:32.820355: step 20857, loss 0.200292, acc 0.90625
2017-03-02T17:59:32.888683: step 20858, loss 0.277948, acc 0.859375
2017-03-02T17:59:32.958094: step 20859, loss 0.0942709, acc 0.96875
2017-03-02T17:59:33.031037: step 20860, loss 0.229718, acc 0.90625
2017-03-02T17:59:33.102676: step 20861, loss 0.183841, acc 0.890625
2017-03-02T17:59:33.174278: step 20862, loss 0.156101, acc 0.9375
2017-03-02T17:59:33.265138: step 20863, loss 0.213156, acc 0.953125
2017-03-02T17:59:33.346683: step 20864, loss 0.0853753, acc 0.96875
2017-03-02T17:59:33.426563: step 20865, loss 0.100274, acc 0.9375
2017-03-02T17:59:33.517813: step 20866, loss 0.146084, acc 0.96875
2017-03-02T17:59:33.590789: step 20867, loss 0.198031, acc 0.921875
2017-03-02T17:59:33.663194: step 20868, loss 0.229418, acc 0.90625
2017-03-02T17:59:33.736246: step 20869, loss 0.0741772, acc 0.96875
2017-03-02T17:59:33.816691: step 20870, loss 0.13761, acc 0.96875
2017-03-02T17:59:33.888435: step 20871, loss 0.100908, acc 0.984375
2017-03-02T17:59:33.961866: step 20872, loss 0.191256, acc 0.90625
2017-03-02T17:59:34.036950: step 20873, loss 0.150676, acc 0.921875
2017-03-02T17:59:34.113032: step 20874, loss 0.194948, acc 0.890625
2017-03-02T17:59:34.192753: step 20875, loss 0.127235, acc 0.96875
2017-03-02T17:59:34.263533: step 20876, loss 0.134089, acc 0.921875
2017-03-02T17:59:34.349313: step 20877, loss 0.167933, acc 0.921875
2017-03-02T17:59:34.421800: step 20878, loss 0.154764, acc 0.9375
2017-03-02T17:59:34.496918: step 20879, loss 0.16802, acc 0.890625
2017-03-02T17:59:34.560102: step 20880, loss 0.192352, acc 0.90625
2017-03-02T17:59:34.636631: step 20881, loss 0.0789279, acc 0.9375
2017-03-02T17:59:34.711497: step 20882, loss 0.12264, acc 0.9375
2017-03-02T17:59:34.783325: step 20883, loss 0.12617, acc 0.921875
2017-03-02T17:59:34.858403: step 20884, loss 0.107306, acc 0.921875
2017-03-02T17:59:34.916962: step 20885, loss 0.232639, acc 0.875
2017-03-02T17:59:34.990308: step 20886, loss 0.140247, acc 0.90625
2017-03-02T17:59:35.057955: step 20887, loss 0.156444, acc 0.921875
2017-03-02T17:59:35.129436: step 20888, loss 0.149701, acc 0.9375
2017-03-02T17:59:35.211467: step 20889, loss 0.0852896, acc 0.96875
2017-03-02T17:59:35.285449: step 20890, loss 0.193641, acc 0.921875
2017-03-02T17:59:35.358905: step 20891, loss 0.136476, acc 0.9375
2017-03-02T17:59:35.451938: step 20892, loss 0.165917, acc 0.890625
2017-03-02T17:59:35.525740: step 20893, loss 0.142497, acc 0.9375
2017-03-02T17:59:35.598922: step 20894, loss 0.19235, acc 0.9375
2017-03-02T17:59:35.670013: step 20895, loss 0.162179, acc 0.921875
2017-03-02T17:59:35.737046: step 20896, loss 0.12856, acc 0.921875
2017-03-02T17:59:35.810807: step 20897, loss 0.20813, acc 0.890625
2017-03-02T17:59:35.887067: step 20898, loss 0.15387, acc 0.953125
2017-03-02T17:59:35.963034: step 20899, loss 0.15144, acc 0.90625
2017-03-02T17:59:36.033026: step 20900, loss 0.14267, acc 0.90625

Evaluation:
2017-03-02T17:59:36.068994: step 20900, loss 2.45087, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-20900

2017-03-02T17:59:36.507086: step 20901, loss 0.0862144, acc 0.96875
2017-03-02T17:59:36.579688: step 20902, loss 0.159741, acc 0.921875
2017-03-02T17:59:36.653169: step 20903, loss 0.115068, acc 0.921875
2017-03-02T17:59:36.727043: step 20904, loss 0.244718, acc 0.890625
2017-03-02T17:59:36.799689: step 20905, loss 0.174448, acc 0.90625
2017-03-02T17:59:36.868683: step 20906, loss 0.133607, acc 0.96875
2017-03-02T17:59:36.940839: step 20907, loss 0.082989, acc 0.96875
2017-03-02T17:59:37.010526: step 20908, loss 0.0745372, acc 0.953125
2017-03-02T17:59:37.079782: step 20909, loss 0.0994453, acc 0.984375
2017-03-02T17:59:37.145157: step 20910, loss 0.0999453, acc 0.953125
2017-03-02T17:59:37.216648: step 20911, loss 0.205865, acc 0.921875
2017-03-02T17:59:37.293956: step 20912, loss 0.0747188, acc 0.96875
2017-03-02T17:59:37.367792: step 20913, loss 0.293508, acc 0.859375
2017-03-02T17:59:37.447776: step 20914, loss 0.154892, acc 0.9375
2017-03-02T17:59:37.521577: step 20915, loss 0.0976696, acc 0.96875
2017-03-02T17:59:37.593950: step 20916, loss 0.120851, acc 0.9375
2017-03-02T17:59:37.674679: step 20917, loss 0.202857, acc 0.921875
2017-03-02T17:59:37.744014: step 20918, loss 0.109202, acc 0.96875
2017-03-02T17:59:37.814259: step 20919, loss 0.177861, acc 0.921875
2017-03-02T17:59:37.888470: step 20920, loss 0.140431, acc 0.9375
2017-03-02T17:59:37.963445: step 20921, loss 0.221417, acc 0.890625
2017-03-02T17:59:38.051242: step 20922, loss 0.151442, acc 0.9375
2017-03-02T17:59:38.119119: step 20923, loss 0.220875, acc 0.921875
2017-03-02T17:59:38.191175: step 20924, loss 0.0475316, acc 0.96875
2017-03-02T17:59:38.262740: step 20925, loss 0.122376, acc 0.953125
2017-03-02T17:59:38.342934: step 20926, loss 0.230976, acc 0.890625
2017-03-02T17:59:38.414563: step 20927, loss 0.129754, acc 0.9375
2017-03-02T17:59:38.494263: step 20928, loss 0.0989619, acc 0.96875
2017-03-02T17:59:38.563283: step 20929, loss 0.198336, acc 0.90625
2017-03-02T17:59:38.635567: step 20930, loss 0.156702, acc 0.921875
2017-03-02T17:59:38.708997: step 20931, loss 0.349663, acc 0.875
2017-03-02T17:59:38.798622: step 20932, loss 0.138016, acc 0.9375
2017-03-02T17:59:38.881503: step 20933, loss 0.176824, acc 0.921875
2017-03-02T17:59:38.956128: step 20934, loss 0.0762564, acc 0.96875
2017-03-02T17:59:39.021879: step 20935, loss 0.116893, acc 0.953125
2017-03-02T17:59:39.094804: step 20936, loss 0.124944, acc 0.9375
2017-03-02T17:59:39.161522: step 20937, loss 0.163231, acc 0.9375
2017-03-02T17:59:39.231572: step 20938, loss 0.222361, acc 0.90625
2017-03-02T17:59:39.299229: step 20939, loss 0.121195, acc 0.96875
2017-03-02T17:59:39.376126: step 20940, loss 0.238548, acc 0.921875
2017-03-02T17:59:39.441397: step 20941, loss 0.234028, acc 0.921875
2017-03-02T17:59:39.515702: step 20942, loss 0.0930697, acc 0.96875
2017-03-02T17:59:39.590501: step 20943, loss 0.0875201, acc 0.984375
2017-03-02T17:59:39.664173: step 20944, loss 0.138456, acc 0.9375
2017-03-02T17:59:39.739280: step 20945, loss 0.12927, acc 0.953125
2017-03-02T17:59:39.809064: step 20946, loss 0.122073, acc 0.9375
2017-03-02T17:59:39.877871: step 20947, loss 0.122042, acc 0.96875
2017-03-02T17:59:39.951830: step 20948, loss 0.114837, acc 0.9375
2017-03-02T17:59:40.025015: step 20949, loss 0.26732, acc 0.875
2017-03-02T17:59:40.099057: step 20950, loss 0.146917, acc 0.9375
2017-03-02T17:59:40.168375: step 20951, loss 0.138118, acc 0.953125
2017-03-02T17:59:40.253153: step 20952, loss 0.229795, acc 0.9375
2017-03-02T17:59:40.324734: step 20953, loss 0.21695, acc 0.921875
2017-03-02T17:59:40.400241: step 20954, loss 0.105801, acc 0.96875
2017-03-02T17:59:40.467599: step 20955, loss 0.122806, acc 0.953125
2017-03-02T17:59:40.529830: step 20956, loss 0.297768, acc 0.890625
2017-03-02T17:59:40.594199: step 20957, loss 0.105449, acc 0.96875
2017-03-02T17:59:40.672753: step 20958, loss 0.223434, acc 0.921875
2017-03-02T17:59:40.747876: step 20959, loss 0.216413, acc 0.890625
2017-03-02T17:59:40.816621: step 20960, loss 0.116738, acc 0.9375
2017-03-02T17:59:40.901342: step 20961, loss 0.0936215, acc 0.984375
2017-03-02T17:59:40.972782: step 20962, loss 0.127618, acc 0.96875
2017-03-02T17:59:41.044554: step 20963, loss 0.169828, acc 0.9375
2017-03-02T17:59:41.124920: step 20964, loss 0.0533027, acc 0.984375
2017-03-02T17:59:41.192410: step 20965, loss 0.11796, acc 0.96875
2017-03-02T17:59:41.261646: step 20966, loss 0.140254, acc 0.953125
2017-03-02T17:59:41.334603: step 20967, loss 0.110858, acc 0.96875
2017-03-02T17:59:41.410684: step 20968, loss 0.146634, acc 0.9375
2017-03-02T17:59:41.483005: step 20969, loss 0.187, acc 0.9375
2017-03-02T17:59:41.561341: step 20970, loss 0.136262, acc 0.9375
2017-03-02T17:59:41.636158: step 20971, loss 0.182898, acc 0.921875
2017-03-02T17:59:41.706057: step 20972, loss 0.00592286, acc 1
2017-03-02T17:59:41.780440: step 20973, loss 0.0483881, acc 0.984375
2017-03-02T17:59:41.847439: step 20974, loss 0.236306, acc 0.875
2017-03-02T17:59:41.918935: step 20975, loss 0.0901344, acc 0.9375
2017-03-02T17:59:41.991774: step 20976, loss 0.0651997, acc 0.96875
2017-03-02T17:59:42.068361: step 20977, loss 0.158968, acc 0.921875
2017-03-02T17:59:42.158262: step 20978, loss 0.121015, acc 0.953125
2017-03-02T17:59:42.231913: step 20979, loss 0.0494016, acc 0.984375
2017-03-02T17:59:42.310688: step 20980, loss 0.0882397, acc 0.9375
2017-03-02T17:59:42.386154: step 20981, loss 0.0838047, acc 0.953125
2017-03-02T17:59:42.464510: step 20982, loss 0.127765, acc 0.96875
2017-03-02T17:59:42.532692: step 20983, loss 0.246152, acc 0.90625
2017-03-02T17:59:42.610266: step 20984, loss 0.0754667, acc 0.96875
2017-03-02T17:59:42.679599: step 20985, loss 0.0735884, acc 0.96875
2017-03-02T17:59:42.750694: step 20986, loss 0.0935885, acc 0.96875
2017-03-02T17:59:42.822261: step 20987, loss 0.237544, acc 0.921875
2017-03-02T17:59:42.895774: step 20988, loss 0.0941661, acc 0.9375
2017-03-02T17:59:42.965766: step 20989, loss 0.0955081, acc 0.953125
2017-03-02T17:59:43.036765: step 20990, loss 0.0911452, acc 0.96875
2017-03-02T17:59:43.114735: step 20991, loss 0.106734, acc 0.953125
2017-03-02T17:59:43.185768: step 20992, loss 0.136148, acc 0.953125
2017-03-02T17:59:43.255245: step 20993, loss 0.104845, acc 0.921875
2017-03-02T17:59:43.319432: step 20994, loss 0.189879, acc 0.96875
2017-03-02T17:59:43.392725: step 20995, loss 0.315587, acc 0.859375
2017-03-02T17:59:43.471268: step 20996, loss 0.206507, acc 0.9375
2017-03-02T17:59:43.542274: step 20997, loss 0.163703, acc 0.921875
2017-03-02T17:59:43.615318: step 20998, loss 0.110753, acc 0.984375
2017-03-02T17:59:43.687040: step 20999, loss 0.260158, acc 0.875
2017-03-02T17:59:43.761612: step 21000, loss 0.0984208, acc 0.953125

Evaluation:
2017-03-02T17:59:43.793190: step 21000, loss 2.51751, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21000

2017-03-02T17:59:44.245496: step 21001, loss 0.0863618, acc 0.984375
2017-03-02T17:59:44.317968: step 21002, loss 0.201517, acc 0.90625
2017-03-02T17:59:44.396062: step 21003, loss 0.188621, acc 0.890625
2017-03-02T17:59:44.469045: step 21004, loss 0.177474, acc 0.9375
2017-03-02T17:59:44.549204: step 21005, loss 0.141416, acc 0.921875
2017-03-02T17:59:44.624969: step 21006, loss 0.0451614, acc 0.984375
2017-03-02T17:59:44.694274: step 21007, loss 0.0841612, acc 0.953125
2017-03-02T17:59:44.762196: step 21008, loss 0.187462, acc 0.921875
2017-03-02T17:59:44.834689: step 21009, loss 0.224982, acc 0.890625
2017-03-02T17:59:44.908007: step 21010, loss 0.140105, acc 0.9375
2017-03-02T17:59:44.985003: step 21011, loss 0.273002, acc 0.890625
2017-03-02T17:59:45.065060: step 21012, loss 0.0766153, acc 0.953125
2017-03-02T17:59:45.142127: step 21013, loss 0.181071, acc 0.921875
2017-03-02T17:59:45.211397: step 21014, loss 0.129012, acc 0.9375
2017-03-02T17:59:45.286008: step 21015, loss 0.268963, acc 0.875
2017-03-02T17:59:45.364719: step 21016, loss 0.128554, acc 0.9375
2017-03-02T17:59:45.433534: step 21017, loss 0.0565515, acc 0.984375
2017-03-02T17:59:45.507111: step 21018, loss 0.128023, acc 0.9375
2017-03-02T17:59:45.570592: step 21019, loss 0.300105, acc 0.84375
2017-03-02T17:59:45.648179: step 21020, loss 0.130558, acc 0.921875
2017-03-02T17:59:45.723315: step 21021, loss 0.0705815, acc 0.96875
2017-03-02T17:59:45.798209: step 21022, loss 0.161419, acc 0.90625
2017-03-02T17:59:45.883458: step 21023, loss 0.180253, acc 0.953125
2017-03-02T17:59:45.962592: step 21024, loss 0.120132, acc 0.96875
2017-03-02T17:59:46.036242: step 21025, loss 0.0972683, acc 0.96875
2017-03-02T17:59:46.109463: step 21026, loss 0.127196, acc 0.9375
2017-03-02T17:59:46.181867: step 21027, loss 0.204443, acc 0.90625
2017-03-02T17:59:46.280333: step 21028, loss 0.0875838, acc 0.96875
2017-03-02T17:59:46.357091: step 21029, loss 0.112102, acc 0.9375
2017-03-02T17:59:46.431470: step 21030, loss 0.14896, acc 0.921875
2017-03-02T17:59:46.504666: step 21031, loss 0.162228, acc 0.90625
2017-03-02T17:59:46.575004: step 21032, loss 0.108264, acc 0.96875
2017-03-02T17:59:46.650224: step 21033, loss 0.154257, acc 0.953125
2017-03-02T17:59:46.720986: step 21034, loss 0.184271, acc 0.9375
2017-03-02T17:59:46.791111: step 21035, loss 0.0985955, acc 0.953125
2017-03-02T17:59:46.863107: step 21036, loss 0.240904, acc 0.875
2017-03-02T17:59:46.937082: step 21037, loss 0.164679, acc 0.921875
2017-03-02T17:59:47.017030: step 21038, loss 0.100371, acc 0.953125
2017-03-02T17:59:47.090398: step 21039, loss 0.281003, acc 0.859375
2017-03-02T17:59:47.167009: step 21040, loss 0.1791, acc 0.90625
2017-03-02T17:59:47.238639: step 21041, loss 0.250089, acc 0.90625
2017-03-02T17:59:47.308567: step 21042, loss 0.127202, acc 0.921875
2017-03-02T17:59:47.390785: step 21043, loss 0.243889, acc 0.890625
2017-03-02T17:59:47.461806: step 21044, loss 0.275879, acc 0.875
2017-03-02T17:59:47.534753: step 21045, loss 0.130812, acc 0.9375
2017-03-02T17:59:47.609920: step 21046, loss 0.13999, acc 0.9375
2017-03-02T17:59:47.682820: step 21047, loss 0.0620575, acc 0.984375
2017-03-02T17:59:47.757423: step 21048, loss 0.175689, acc 0.9375
2017-03-02T17:59:47.833258: step 21049, loss 0.086177, acc 0.984375
2017-03-02T17:59:47.919611: step 21050, loss 0.264051, acc 0.875
2017-03-02T17:59:47.991345: step 21051, loss 0.175818, acc 0.9375
2017-03-02T17:59:48.066661: step 21052, loss 0.144422, acc 0.921875
2017-03-02T17:59:48.135864: step 21053, loss 0.189711, acc 0.921875
2017-03-02T17:59:48.207780: step 21054, loss 0.0663253, acc 0.96875
2017-03-02T17:59:48.288897: step 21055, loss 0.102939, acc 0.953125
2017-03-02T17:59:48.365868: step 21056, loss 0.306733, acc 0.859375
2017-03-02T17:59:48.442831: step 21057, loss 0.142013, acc 0.921875
2017-03-02T17:59:48.516796: step 21058, loss 0.0649554, acc 0.984375
2017-03-02T17:59:48.590831: step 21059, loss 0.221761, acc 0.90625
2017-03-02T17:59:48.662548: step 21060, loss 0.0560592, acc 1
2017-03-02T17:59:48.736081: step 21061, loss 0.175289, acc 0.921875
2017-03-02T17:59:48.810599: step 21062, loss 0.212938, acc 0.890625
2017-03-02T17:59:48.886833: step 21063, loss 0.223656, acc 0.890625
2017-03-02T17:59:48.963699: step 21064, loss 0.150719, acc 0.921875
2017-03-02T17:59:49.031375: step 21065, loss 0.109871, acc 0.953125
2017-03-02T17:59:49.116734: step 21066, loss 0.191589, acc 0.921875
2017-03-02T17:59:49.199797: step 21067, loss 0.2689, acc 0.890625
2017-03-02T17:59:49.275275: step 21068, loss 0.13449, acc 0.953125
2017-03-02T17:59:49.363973: step 21069, loss 0.136362, acc 0.9375
2017-03-02T17:59:49.433668: step 21070, loss 0.0877831, acc 0.953125
2017-03-02T17:59:49.505640: step 21071, loss 0.137058, acc 0.921875
2017-03-02T17:59:49.575039: step 21072, loss 0.194308, acc 0.921875
2017-03-02T17:59:49.655919: step 21073, loss 0.0964294, acc 0.96875
2017-03-02T17:59:49.724916: step 21074, loss 0.124038, acc 0.9375
2017-03-02T17:59:49.808193: step 21075, loss 0.147224, acc 0.921875
2017-03-02T17:59:49.873693: step 21076, loss 0.153562, acc 0.9375
2017-03-02T17:59:49.954392: step 21077, loss 0.234629, acc 0.921875
2017-03-02T17:59:50.027459: step 21078, loss 0.15671, acc 0.90625
2017-03-02T17:59:50.100060: step 21079, loss 0.254849, acc 0.90625
2017-03-02T17:59:50.169610: step 21080, loss 0.214776, acc 0.921875
2017-03-02T17:59:50.240543: step 21081, loss 0.159843, acc 0.90625
2017-03-02T17:59:50.311612: step 21082, loss 0.162484, acc 0.921875
2017-03-02T17:59:50.381915: step 21083, loss 0.146977, acc 0.9375
2017-03-02T17:59:50.465530: step 21084, loss 0.215453, acc 0.890625
2017-03-02T17:59:50.544952: step 21085, loss 0.167559, acc 0.90625
2017-03-02T17:59:50.616336: step 21086, loss 0.0594161, acc 0.96875
2017-03-02T17:59:50.683567: step 21087, loss 0.135377, acc 0.921875
2017-03-02T17:59:50.758445: step 21088, loss 0.130431, acc 0.9375
2017-03-02T17:59:50.837706: step 21089, loss 0.305767, acc 0.875
2017-03-02T17:59:50.901662: step 21090, loss 0.178858, acc 0.90625
2017-03-02T17:59:50.975881: step 21091, loss 0.148259, acc 0.953125
2017-03-02T17:59:51.045705: step 21092, loss 0.165144, acc 0.9375
2017-03-02T17:59:51.116569: step 21093, loss 0.146902, acc 0.9375
2017-03-02T17:59:51.192326: step 21094, loss 0.147692, acc 0.921875
2017-03-02T17:59:51.271760: step 21095, loss 0.114082, acc 0.96875
2017-03-02T17:59:51.341448: step 21096, loss 0.142008, acc 0.9375
2017-03-02T17:59:51.418052: step 21097, loss 0.114672, acc 0.96875
2017-03-02T17:59:51.489644: step 21098, loss 0.0639967, acc 0.984375
2017-03-02T17:59:51.556284: step 21099, loss 0.157491, acc 0.90625
2017-03-02T17:59:51.628776: step 21100, loss 0.141432, acc 0.9375

Evaluation:
2017-03-02T17:59:51.670316: step 21100, loss 2.5021, acc 0.65465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21100

2017-03-02T17:59:52.137556: step 21101, loss 0.253204, acc 0.890625
2017-03-02T17:59:52.207078: step 21102, loss 0.0713744, acc 0.96875
2017-03-02T17:59:52.276018: step 21103, loss 0.122004, acc 0.921875
2017-03-02T17:59:52.353187: step 21104, loss 0.0766472, acc 0.96875
2017-03-02T17:59:52.427262: step 21105, loss 0.101455, acc 0.9375
2017-03-02T17:59:52.503335: step 21106, loss 0.214305, acc 0.921875
2017-03-02T17:59:52.577857: step 21107, loss 0.220181, acc 0.890625
2017-03-02T17:59:52.650229: step 21108, loss 0.22305, acc 0.890625
2017-03-02T17:59:52.721739: step 21109, loss 0.151347, acc 0.9375
2017-03-02T17:59:52.796514: step 21110, loss 0.171839, acc 0.9375
2017-03-02T17:59:52.868865: step 21111, loss 0.172134, acc 0.96875
2017-03-02T17:59:52.937019: step 21112, loss 0.0422476, acc 0.984375
2017-03-02T17:59:53.011475: step 21113, loss 0.139647, acc 0.921875
2017-03-02T17:59:53.075960: step 21114, loss 0.122732, acc 0.921875
2017-03-02T17:59:53.150268: step 21115, loss 0.0716073, acc 0.96875
2017-03-02T17:59:53.219491: step 21116, loss 0.214073, acc 0.953125
2017-03-02T17:59:53.287286: step 21117, loss 0.211856, acc 0.890625
2017-03-02T17:59:53.363725: step 21118, loss 0.126207, acc 0.9375
2017-03-02T17:59:53.447066: step 21119, loss 0.212897, acc 0.9375
2017-03-02T17:59:53.520059: step 21120, loss 0.0777659, acc 0.96875
2017-03-02T17:59:53.590790: step 21121, loss 0.161433, acc 0.90625
2017-03-02T17:59:53.665896: step 21122, loss 0.0869115, acc 0.9375
2017-03-02T17:59:53.738456: step 21123, loss 0.130491, acc 0.90625
2017-03-02T17:59:53.812826: step 21124, loss 0.235681, acc 0.921875
2017-03-02T17:59:53.902826: step 21125, loss 0.126814, acc 0.953125
2017-03-02T17:59:53.974281: step 21126, loss 0.122082, acc 0.96875
2017-03-02T17:59:54.053055: step 21127, loss 0.156514, acc 0.9375
2017-03-02T17:59:54.118296: step 21128, loss 0.205961, acc 0.90625
2017-03-02T17:59:54.188174: step 21129, loss 0.158508, acc 0.9375
2017-03-02T17:59:54.257004: step 21130, loss 0.146855, acc 0.9375
2017-03-02T17:59:54.323775: step 21131, loss 0.0850614, acc 0.953125
2017-03-02T17:59:54.399170: step 21132, loss 0.065141, acc 0.96875
2017-03-02T17:59:54.473820: step 21133, loss 0.215337, acc 0.890625
2017-03-02T17:59:54.546601: step 21134, loss 0.0595974, acc 0.96875
2017-03-02T17:59:54.617929: step 21135, loss 0.113484, acc 0.9375
2017-03-02T17:59:54.680588: step 21136, loss 0.0571135, acc 0.984375
2017-03-02T17:59:54.753412: step 21137, loss 0.210922, acc 0.890625
2017-03-02T17:59:54.830132: step 21138, loss 0.20422, acc 0.90625
2017-03-02T17:59:54.904308: step 21139, loss 0.206402, acc 0.890625
2017-03-02T17:59:54.976769: step 21140, loss 0.269529, acc 0.90625
2017-03-02T17:59:55.054018: step 21141, loss 0.0505646, acc 0.984375
2017-03-02T17:59:55.126056: step 21142, loss 0.310503, acc 0.828125
2017-03-02T17:59:55.201829: step 21143, loss 0.0413096, acc 0.96875
2017-03-02T17:59:55.278170: step 21144, loss 0.0897548, acc 0.953125
2017-03-02T17:59:55.360049: step 21145, loss 0.112929, acc 0.96875
2017-03-02T17:59:55.432111: step 21146, loss 0.208008, acc 0.90625
2017-03-02T17:59:55.503904: step 21147, loss 0.198584, acc 0.90625
2017-03-02T17:59:55.576997: step 21148, loss 0.142136, acc 0.953125
2017-03-02T17:59:55.640411: step 21149, loss 0.181527, acc 0.921875
2017-03-02T17:59:55.712886: step 21150, loss 0.231682, acc 0.921875
2017-03-02T17:59:55.784339: step 21151, loss 0.233891, acc 0.90625
2017-03-02T17:59:55.863291: step 21152, loss 0.160261, acc 0.9375
2017-03-02T17:59:55.937058: step 21153, loss 0.134424, acc 0.9375
2017-03-02T17:59:56.013419: step 21154, loss 0.052697, acc 0.984375
2017-03-02T17:59:56.099802: step 21155, loss 0.0853465, acc 0.953125
2017-03-02T17:59:56.167139: step 21156, loss 0.169972, acc 0.9375
2017-03-02T17:59:56.239872: step 21157, loss 0.195774, acc 0.890625
2017-03-02T17:59:56.303924: step 21158, loss 0.200228, acc 0.9375
2017-03-02T17:59:56.371176: step 21159, loss 0.130596, acc 0.9375
2017-03-02T17:59:56.440517: step 21160, loss 0.236915, acc 0.875
2017-03-02T17:59:56.513720: step 21161, loss 0.110831, acc 0.953125
2017-03-02T17:59:56.584155: step 21162, loss 0.188926, acc 0.90625
2017-03-02T17:59:56.655697: step 21163, loss 0.186936, acc 0.90625
2017-03-02T17:59:56.727772: step 21164, loss 0.335967, acc 0.84375
2017-03-02T17:59:56.795398: step 21165, loss 0.128381, acc 0.953125
2017-03-02T17:59:56.864173: step 21166, loss 0.152668, acc 0.921875
2017-03-02T17:59:56.936182: step 21167, loss 0.183205, acc 0.890625
2017-03-02T17:59:57.009983: step 21168, loss 0.105131, acc 1
2017-03-02T17:59:57.081788: step 21169, loss 0.26753, acc 0.890625
2017-03-02T17:59:57.154249: step 21170, loss 0.139665, acc 0.90625
2017-03-02T17:59:57.223074: step 21171, loss 0.0897693, acc 0.96875
2017-03-02T17:59:57.293462: step 21172, loss 0.0952682, acc 0.953125
2017-03-02T17:59:57.367004: step 21173, loss 0.114943, acc 0.953125
2017-03-02T17:59:57.445670: step 21174, loss 0.183547, acc 0.875
2017-03-02T17:59:57.520967: step 21175, loss 0.165921, acc 0.890625
2017-03-02T17:59:57.596153: step 21176, loss 0.14019, acc 0.96875
2017-03-02T17:59:57.667994: step 21177, loss 0.143989, acc 0.953125
2017-03-02T17:59:57.735760: step 21178, loss 0.133939, acc 0.96875
2017-03-02T17:59:57.808979: step 21179, loss 0.084165, acc 0.953125
2017-03-02T17:59:57.883190: step 21180, loss 0.149952, acc 0.921875
2017-03-02T17:59:57.953882: step 21181, loss 0.125783, acc 0.90625
2017-03-02T17:59:58.028373: step 21182, loss 0.0516927, acc 0.984375
2017-03-02T17:59:58.106671: step 21183, loss 0.220478, acc 0.890625
2017-03-02T17:59:58.180538: step 21184, loss 0.292528, acc 0.875
2017-03-02T17:59:58.253780: step 21185, loss 0.146326, acc 0.96875
2017-03-02T17:59:58.333925: step 21186, loss 0.148655, acc 0.921875
2017-03-02T17:59:58.406647: step 21187, loss 0.141455, acc 0.9375
2017-03-02T17:59:58.478728: step 21188, loss 0.142143, acc 0.9375
2017-03-02T17:59:58.556526: step 21189, loss 0.156887, acc 0.9375
2017-03-02T17:59:58.627658: step 21190, loss 0.218208, acc 0.890625
2017-03-02T17:59:58.695512: step 21191, loss 0.119155, acc 0.9375
2017-03-02T17:59:58.773191: step 21192, loss 0.144351, acc 0.9375
2017-03-02T17:59:58.857008: step 21193, loss 0.0768576, acc 1
2017-03-02T17:59:58.932597: step 21194, loss 0.100815, acc 0.953125
2017-03-02T17:59:59.007193: step 21195, loss 0.0194438, acc 1
2017-03-02T17:59:59.085110: step 21196, loss 0.157923, acc 0.921875
2017-03-02T17:59:59.153112: step 21197, loss 0.230125, acc 0.90625
2017-03-02T17:59:59.221956: step 21198, loss 0.107953, acc 0.953125
2017-03-02T17:59:59.294189: step 21199, loss 0.158224, acc 0.921875
2017-03-02T17:59:59.367377: step 21200, loss 0.157815, acc 0.921875

Evaluation:
2017-03-02T17:59:59.406214: step 21200, loss 2.52721, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21200

2017-03-02T17:59:59.851554: step 21201, loss 0.160871, acc 0.9375
2017-03-02T17:59:59.930458: step 21202, loss 0.189536, acc 0.953125
2017-03-02T18:00:00.002499: step 21203, loss 0.0676028, acc 0.984375
2017-03-02T18:00:00.084510: step 21204, loss 0.120599, acc 0.953125
2017-03-02T18:00:00.158780: step 21205, loss 0.224248, acc 0.875
2017-03-02T18:00:00.227620: step 21206, loss 0.158265, acc 0.953125
2017-03-02T18:00:00.306459: step 21207, loss 0.0798639, acc 0.9375
2017-03-02T18:00:00.379372: step 21208, loss 0.170494, acc 0.9375
2017-03-02T18:00:00.449769: step 21209, loss 0.139091, acc 0.9375
2017-03-02T18:00:00.526425: step 21210, loss 0.11639, acc 0.953125
2017-03-02T18:00:00.712636: step 21211, loss 0.160844, acc 0.9375
2017-03-02T18:00:00.788987: step 21212, loss 0.107107, acc 0.9375
2017-03-02T18:00:00.851613: step 21213, loss 0.128652, acc 0.96875
2017-03-02T18:00:00.924604: step 21214, loss 0.203125, acc 0.890625
2017-03-02T18:00:00.991281: step 21215, loss 0.0889385, acc 0.953125
2017-03-02T18:00:01.062652: step 21216, loss 0.136243, acc 0.9375
2017-03-02T18:00:01.138003: step 21217, loss 0.130217, acc 0.953125
2017-03-02T18:00:01.206008: step 21218, loss 0.213997, acc 0.90625
2017-03-02T18:00:01.281084: step 21219, loss 0.0767361, acc 0.984375
2017-03-02T18:00:01.348194: step 21220, loss 0.193286, acc 0.890625
2017-03-02T18:00:01.415406: step 21221, loss 0.142892, acc 0.90625
2017-03-02T18:00:01.492668: step 21222, loss 0.115721, acc 0.96875
2017-03-02T18:00:01.572007: step 21223, loss 0.143752, acc 0.9375
2017-03-02T18:00:01.646762: step 21224, loss 0.266892, acc 0.90625
2017-03-02T18:00:01.717833: step 21225, loss 0.117973, acc 0.9375
2017-03-02T18:00:01.790420: step 21226, loss 0.136588, acc 0.9375
2017-03-02T18:00:01.867059: step 21227, loss 0.0576863, acc 0.953125
2017-03-02T18:00:01.941148: step 21228, loss 0.141469, acc 0.953125
2017-03-02T18:00:02.011837: step 21229, loss 0.223231, acc 0.953125
2017-03-02T18:00:02.081583: step 21230, loss 0.13237, acc 0.953125
2017-03-02T18:00:02.161981: step 21231, loss 0.133903, acc 0.953125
2017-03-02T18:00:02.237557: step 21232, loss 0.104871, acc 0.953125
2017-03-02T18:00:02.310043: step 21233, loss 0.0980293, acc 0.96875
2017-03-02T18:00:02.382824: step 21234, loss 0.136047, acc 0.9375
2017-03-02T18:00:02.452798: step 21235, loss 0.0990241, acc 0.96875
2017-03-02T18:00:02.529363: step 21236, loss 0.231367, acc 0.890625
2017-03-02T18:00:02.600414: step 21237, loss 0.0647874, acc 0.984375
2017-03-02T18:00:02.669227: step 21238, loss 0.15325, acc 0.921875
2017-03-02T18:00:02.746154: step 21239, loss 0.259334, acc 0.875
2017-03-02T18:00:02.821490: step 21240, loss 0.225799, acc 0.859375
2017-03-02T18:00:02.899123: step 21241, loss 0.101671, acc 0.953125
2017-03-02T18:00:02.972574: step 21242, loss 0.235078, acc 0.890625
2017-03-02T18:00:03.052018: step 21243, loss 0.271632, acc 0.921875
2017-03-02T18:00:03.122953: step 21244, loss 0.125906, acc 0.9375
2017-03-02T18:00:03.198817: step 21245, loss 0.121582, acc 0.953125
2017-03-02T18:00:03.273651: step 21246, loss 0.126452, acc 0.96875
2017-03-02T18:00:03.350422: step 21247, loss 0.088579, acc 0.9375
2017-03-02T18:00:03.440384: step 21248, loss 0.14413, acc 0.96875
2017-03-02T18:00:03.516529: step 21249, loss 0.144805, acc 0.953125
2017-03-02T18:00:03.591491: step 21250, loss 0.142248, acc 0.921875
2017-03-02T18:00:03.659073: step 21251, loss 0.121791, acc 0.9375
2017-03-02T18:00:03.726877: step 21252, loss 0.122553, acc 0.953125
2017-03-02T18:00:03.799525: step 21253, loss 0.0759164, acc 0.984375
2017-03-02T18:00:03.871526: step 21254, loss 0.10538, acc 0.9375
2017-03-02T18:00:03.942083: step 21255, loss 0.097941, acc 0.953125
2017-03-02T18:00:04.011496: step 21256, loss 0.167792, acc 0.90625
2017-03-02T18:00:04.079420: step 21257, loss 0.20131, acc 0.96875
2017-03-02T18:00:04.153346: step 21258, loss 0.0803867, acc 0.96875
2017-03-02T18:00:04.229805: step 21259, loss 0.0428043, acc 0.984375
2017-03-02T18:00:04.306462: step 21260, loss 0.158955, acc 0.921875
2017-03-02T18:00:04.376784: step 21261, loss 0.142257, acc 0.9375
2017-03-02T18:00:04.447628: step 21262, loss 0.0769225, acc 0.984375
2017-03-02T18:00:04.514844: step 21263, loss 0.167711, acc 0.9375
2017-03-02T18:00:04.590510: step 21264, loss 0.100667, acc 0.9375
2017-03-02T18:00:04.668466: step 21265, loss 0.225018, acc 0.890625
2017-03-02T18:00:04.735744: step 21266, loss 0.204089, acc 0.90625
2017-03-02T18:00:04.798418: step 21267, loss 0.264559, acc 0.859375
2017-03-02T18:00:04.875656: step 21268, loss 0.177462, acc 0.90625
2017-03-02T18:00:04.950439: step 21269, loss 0.195082, acc 0.921875
2017-03-02T18:00:05.026380: step 21270, loss 0.155734, acc 0.890625
2017-03-02T18:00:05.101283: step 21271, loss 0.197675, acc 0.875
2017-03-02T18:00:05.174670: step 21272, loss 0.129834, acc 0.953125
2017-03-02T18:00:05.249526: step 21273, loss 0.138894, acc 0.96875
2017-03-02T18:00:05.322823: step 21274, loss 0.11224, acc 0.953125
2017-03-02T18:00:05.394768: step 21275, loss 0.190159, acc 0.921875
2017-03-02T18:00:05.464858: step 21276, loss 0.131919, acc 0.921875
2017-03-02T18:00:05.541787: step 21277, loss 0.188759, acc 0.90625
2017-03-02T18:00:05.611551: step 21278, loss 0.0747073, acc 0.96875
2017-03-02T18:00:05.683687: step 21279, loss 0.110841, acc 0.953125
2017-03-02T18:00:05.759182: step 21280, loss 0.146477, acc 0.953125
2017-03-02T18:00:05.849027: step 21281, loss 0.243264, acc 0.875
2017-03-02T18:00:05.922521: step 21282, loss 0.120883, acc 0.9375
2017-03-02T18:00:06.001331: step 21283, loss 0.195564, acc 0.90625
2017-03-02T18:00:06.068798: step 21284, loss 0.196492, acc 0.9375
2017-03-02T18:00:06.139054: step 21285, loss 0.0643107, acc 0.984375
2017-03-02T18:00:06.210949: step 21286, loss 0.0887494, acc 0.9375
2017-03-02T18:00:06.281041: step 21287, loss 0.125602, acc 0.921875
2017-03-02T18:00:06.352518: step 21288, loss 0.132815, acc 0.9375
2017-03-02T18:00:06.432065: step 21289, loss 0.215017, acc 0.90625
2017-03-02T18:00:06.506123: step 21290, loss 0.160167, acc 0.9375
2017-03-02T18:00:06.580500: step 21291, loss 0.242276, acc 0.9375
2017-03-02T18:00:06.654758: step 21292, loss 0.110368, acc 0.96875
2017-03-02T18:00:06.727920: step 21293, loss 0.164131, acc 0.90625
2017-03-02T18:00:06.808661: step 21294, loss 0.128206, acc 0.9375
2017-03-02T18:00:06.880748: step 21295, loss 0.105764, acc 0.953125
2017-03-02T18:00:06.960418: step 21296, loss 0.14654, acc 0.96875
2017-03-02T18:00:07.035949: step 21297, loss 0.237501, acc 0.875
2017-03-02T18:00:07.105022: step 21298, loss 0.0718817, acc 0.984375
2017-03-02T18:00:07.172544: step 21299, loss 0.202038, acc 0.90625
2017-03-02T18:00:07.253407: step 21300, loss 0.157559, acc 0.953125

Evaluation:
2017-03-02T18:00:07.291839: step 21300, loss 2.55058, acc 0.653929

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21300

2017-03-02T18:00:07.759641: step 21301, loss 0.165694, acc 0.9375
2017-03-02T18:00:07.838079: step 21302, loss 0.152924, acc 0.921875
2017-03-02T18:00:07.922653: step 21303, loss 0.126184, acc 0.921875
2017-03-02T18:00:07.991343: step 21304, loss 0.127526, acc 0.9375
2017-03-02T18:00:08.075596: step 21305, loss 0.0883413, acc 0.953125
2017-03-02T18:00:08.152449: step 21306, loss 0.103762, acc 0.9375
2017-03-02T18:00:08.231660: step 21307, loss 0.0885162, acc 0.9375
2017-03-02T18:00:08.304708: step 21308, loss 0.129377, acc 0.953125
2017-03-02T18:00:08.385114: step 21309, loss 0.143435, acc 0.953125
2017-03-02T18:00:08.463264: step 21310, loss 0.183054, acc 0.90625
2017-03-02T18:00:08.537139: step 21311, loss 0.147871, acc 0.953125
2017-03-02T18:00:08.600843: step 21312, loss 0.14136, acc 0.96875
2017-03-02T18:00:08.677696: step 21313, loss 0.235144, acc 0.890625
2017-03-02T18:00:08.760495: step 21314, loss 0.140327, acc 0.9375
2017-03-02T18:00:08.836398: step 21315, loss 0.174627, acc 0.90625
2017-03-02T18:00:08.909727: step 21316, loss 0.0517603, acc 0.984375
2017-03-02T18:00:08.985685: step 21317, loss 0.126934, acc 0.921875
2017-03-02T18:00:09.059070: step 21318, loss 0.167736, acc 0.921875
2017-03-02T18:00:09.142724: step 21319, loss 0.237298, acc 0.921875
2017-03-02T18:00:09.221556: step 21320, loss 0.217004, acc 0.90625
2017-03-02T18:00:09.292190: step 21321, loss 0.226322, acc 0.875
2017-03-02T18:00:09.364570: step 21322, loss 0.224358, acc 0.890625
2017-03-02T18:00:09.440443: step 21323, loss 0.127034, acc 0.9375
2017-03-02T18:00:09.509502: step 21324, loss 0.113529, acc 0.984375
2017-03-02T18:00:09.578194: step 21325, loss 0.167485, acc 0.90625
2017-03-02T18:00:09.651148: step 21326, loss 0.166046, acc 0.953125
2017-03-02T18:00:09.718770: step 21327, loss 0.0793476, acc 0.96875
2017-03-02T18:00:09.828377: step 21328, loss 0.146573, acc 0.90625
2017-03-02T18:00:09.903342: step 21329, loss 0.0537832, acc 0.96875
2017-03-02T18:00:09.974430: step 21330, loss 0.198821, acc 0.9375
2017-03-02T18:00:10.046713: step 21331, loss 0.257613, acc 0.90625
2017-03-02T18:00:10.125325: step 21332, loss 0.148213, acc 0.953125
2017-03-02T18:00:10.194858: step 21333, loss 0.212399, acc 0.9375
2017-03-02T18:00:10.271107: step 21334, loss 0.0657068, acc 0.984375
2017-03-02T18:00:10.341046: step 21335, loss 0.23756, acc 0.921875
2017-03-02T18:00:10.420699: step 21336, loss 0.232813, acc 0.890625
2017-03-02T18:00:10.495968: step 21337, loss 0.0621329, acc 0.96875
2017-03-02T18:00:10.567888: step 21338, loss 0.192625, acc 0.90625
2017-03-02T18:00:10.639250: step 21339, loss 0.149587, acc 0.921875
2017-03-02T18:00:10.712779: step 21340, loss 0.186728, acc 0.921875
2017-03-02T18:00:10.781371: step 21341, loss 0.160233, acc 0.90625
2017-03-02T18:00:10.863752: step 21342, loss 0.2299, acc 0.90625
2017-03-02T18:00:10.933392: step 21343, loss 0.24805, acc 0.9375
2017-03-02T18:00:11.007241: step 21344, loss 0.13893, acc 0.953125
2017-03-02T18:00:11.079003: step 21345, loss 0.324909, acc 0.859375
2017-03-02T18:00:11.154434: step 21346, loss 0.151141, acc 0.90625
2017-03-02T18:00:11.228981: step 21347, loss 0.0922132, acc 0.96875
2017-03-02T18:00:11.321344: step 21348, loss 0.0561418, acc 0.96875
2017-03-02T18:00:11.392387: step 21349, loss 0.169832, acc 0.953125
2017-03-02T18:00:11.465358: step 21350, loss 0.220231, acc 0.9375
2017-03-02T18:00:11.537634: step 21351, loss 0.0471941, acc 0.96875
2017-03-02T18:00:11.614320: step 21352, loss 0.0858644, acc 0.96875
2017-03-02T18:00:11.682222: step 21353, loss 0.201681, acc 0.921875
2017-03-02T18:00:11.747460: step 21354, loss 0.25868, acc 0.890625
2017-03-02T18:00:11.830415: step 21355, loss 0.240582, acc 0.90625
2017-03-02T18:00:11.914794: step 21356, loss 0.219114, acc 0.875
2017-03-02T18:00:11.992367: step 21357, loss 0.190075, acc 0.90625
2017-03-02T18:00:12.076770: step 21358, loss 0.110358, acc 0.9375
2017-03-02T18:00:12.153975: step 21359, loss 0.241218, acc 0.90625
2017-03-02T18:00:12.224364: step 21360, loss 0.152154, acc 0.9375
2017-03-02T18:00:12.301561: step 21361, loss 0.0541156, acc 0.984375
2017-03-02T18:00:12.367613: step 21362, loss 0.189305, acc 0.90625
2017-03-02T18:00:12.439804: step 21363, loss 0.0922502, acc 0.96875
2017-03-02T18:00:12.516150: step 21364, loss 0.176919, acc 1
2017-03-02T18:00:12.593358: step 21365, loss 0.0907885, acc 0.9375
2017-03-02T18:00:12.681082: step 21366, loss 0.204223, acc 0.921875
2017-03-02T18:00:12.753969: step 21367, loss 0.155881, acc 0.9375
2017-03-02T18:00:12.822623: step 21368, loss 0.0876779, acc 0.96875
2017-03-02T18:00:12.891216: step 21369, loss 0.176297, acc 0.953125
2017-03-02T18:00:12.968390: step 21370, loss 0.201358, acc 0.890625
2017-03-02T18:00:13.042595: step 21371, loss 0.160743, acc 0.921875
2017-03-02T18:00:13.111537: step 21372, loss 0.140143, acc 0.921875
2017-03-02T18:00:13.184208: step 21373, loss 0.0980588, acc 0.984375
2017-03-02T18:00:13.252343: step 21374, loss 0.12473, acc 0.9375
2017-03-02T18:00:13.325330: step 21375, loss 0.129558, acc 0.921875
2017-03-02T18:00:13.407166: step 21376, loss 0.183607, acc 0.90625
2017-03-02T18:00:13.478867: step 21377, loss 0.159507, acc 0.890625
2017-03-02T18:00:13.554461: step 21378, loss 0.198958, acc 0.890625
2017-03-02T18:00:13.634732: step 21379, loss 0.122861, acc 0.921875
2017-03-02T18:00:13.708386: step 21380, loss 0.115621, acc 0.984375
2017-03-02T18:00:13.776645: step 21381, loss 0.182877, acc 0.9375
2017-03-02T18:00:13.857869: step 21382, loss 0.195762, acc 0.9375
2017-03-02T18:00:13.932224: step 21383, loss 0.0868835, acc 0.96875
2017-03-02T18:00:14.006722: step 21384, loss 0.118582, acc 0.90625
2017-03-02T18:00:14.084196: step 21385, loss 0.129114, acc 0.921875
2017-03-02T18:00:14.153996: step 21386, loss 0.142476, acc 0.96875
2017-03-02T18:00:14.222904: step 21387, loss 0.191553, acc 0.890625
2017-03-02T18:00:14.296960: step 21388, loss 0.119327, acc 0.953125
2017-03-02T18:00:14.376636: step 21389, loss 0.179958, acc 0.890625
2017-03-02T18:00:14.449959: step 21390, loss 0.0898702, acc 0.96875
2017-03-02T18:00:14.524694: step 21391, loss 0.1345, acc 0.96875
2017-03-02T18:00:14.607499: step 21392, loss 0.103513, acc 0.96875
2017-03-02T18:00:14.681792: step 21393, loss 0.175471, acc 0.9375
2017-03-02T18:00:14.754656: step 21394, loss 0.129836, acc 0.9375
2017-03-02T18:00:14.828385: step 21395, loss 0.123814, acc 0.9375
2017-03-02T18:00:14.901567: step 21396, loss 0.166433, acc 0.921875
2017-03-02T18:00:14.982723: step 21397, loss 0.122959, acc 0.921875
2017-03-02T18:00:15.070182: step 21398, loss 0.181551, acc 0.921875
2017-03-02T18:00:15.143021: step 21399, loss 0.092429, acc 0.921875
2017-03-02T18:00:15.211255: step 21400, loss 0.171469, acc 0.9375

Evaluation:
2017-03-02T18:00:15.247426: step 21400, loss 2.52877, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21400

2017-03-02T18:00:15.703378: step 21401, loss 0.0663981, acc 0.984375
2017-03-02T18:00:15.771767: step 21402, loss 0.181279, acc 0.875
2017-03-02T18:00:15.842228: step 21403, loss 0.27662, acc 0.875
2017-03-02T18:00:15.906447: step 21404, loss 0.120299, acc 0.953125
2017-03-02T18:00:15.973000: step 21405, loss 0.0782917, acc 0.984375
2017-03-02T18:00:16.061747: step 21406, loss 0.198171, acc 0.921875
2017-03-02T18:00:16.135327: step 21407, loss 0.192763, acc 0.90625
2017-03-02T18:00:16.208358: step 21408, loss 0.109717, acc 0.953125
2017-03-02T18:00:16.280423: step 21409, loss 0.104012, acc 0.9375
2017-03-02T18:00:16.350955: step 21410, loss 0.181408, acc 0.9375
2017-03-02T18:00:16.423254: step 21411, loss 0.142327, acc 0.9375
2017-03-02T18:00:16.497105: step 21412, loss 0.075156, acc 0.96875
2017-03-02T18:00:16.565025: step 21413, loss 0.133007, acc 0.953125
2017-03-02T18:00:16.644557: step 21414, loss 0.100685, acc 0.953125
2017-03-02T18:00:16.715258: step 21415, loss 0.139601, acc 0.921875
2017-03-02T18:00:16.793851: step 21416, loss 0.171712, acc 0.90625
2017-03-02T18:00:16.870936: step 21417, loss 0.123241, acc 0.9375
2017-03-02T18:00:16.947466: step 21418, loss 0.157515, acc 0.890625
2017-03-02T18:00:17.021797: step 21419, loss 0.137731, acc 0.953125
2017-03-02T18:00:17.095398: step 21420, loss 0.0957902, acc 0.96875
2017-03-02T18:00:17.167123: step 21421, loss 0.225654, acc 0.921875
2017-03-02T18:00:17.236718: step 21422, loss 0.144315, acc 0.921875
2017-03-02T18:00:17.301528: step 21423, loss 0.283003, acc 0.875
2017-03-02T18:00:17.372670: step 21424, loss 0.164114, acc 0.90625
2017-03-02T18:00:17.451122: step 21425, loss 0.181186, acc 0.921875
2017-03-02T18:00:17.535956: step 21426, loss 0.335968, acc 0.828125
2017-03-02T18:00:17.617293: step 21427, loss 0.125677, acc 0.921875
2017-03-02T18:00:17.685160: step 21428, loss 0.050089, acc 0.96875
2017-03-02T18:00:17.762321: step 21429, loss 0.119399, acc 0.9375
2017-03-02T18:00:17.849062: step 21430, loss 0.169014, acc 0.953125
2017-03-02T18:00:17.919864: step 21431, loss 0.177937, acc 0.953125
2017-03-02T18:00:17.998686: step 21432, loss 0.150291, acc 0.9375
2017-03-02T18:00:18.073406: step 21433, loss 0.140054, acc 0.9375
2017-03-02T18:00:18.156391: step 21434, loss 0.160256, acc 0.90625
2017-03-02T18:00:18.234862: step 21435, loss 0.163429, acc 0.9375
2017-03-02T18:00:18.309784: step 21436, loss 0.106259, acc 0.953125
2017-03-02T18:00:18.381406: step 21437, loss 0.226615, acc 0.859375
2017-03-02T18:00:18.452239: step 21438, loss 0.21376, acc 0.90625
2017-03-02T18:00:18.525686: step 21439, loss 0.163383, acc 0.90625
2017-03-02T18:00:18.592864: step 21440, loss 0.116129, acc 0.921875
2017-03-02T18:00:18.665140: step 21441, loss 0.131569, acc 0.96875
2017-03-02T18:00:18.758308: step 21442, loss 0.0444988, acc 1
2017-03-02T18:00:18.833790: step 21443, loss 0.0295454, acc 1
2017-03-02T18:00:18.904638: step 21444, loss 0.197905, acc 0.9375
2017-03-02T18:00:18.979945: step 21445, loss 0.195943, acc 0.921875
2017-03-02T18:00:19.050375: step 21446, loss 0.162853, acc 0.921875
2017-03-02T18:00:19.125088: step 21447, loss 0.18749, acc 0.90625
2017-03-02T18:00:19.215700: step 21448, loss 0.0467519, acc 1
2017-03-02T18:00:19.301809: step 21449, loss 0.0935059, acc 0.953125
2017-03-02T18:00:19.376731: step 21450, loss 0.185327, acc 0.890625
2017-03-02T18:00:19.457688: step 21451, loss 0.130766, acc 0.9375
2017-03-02T18:00:19.533376: step 21452, loss 0.111938, acc 0.953125
2017-03-02T18:00:19.610287: step 21453, loss 0.0716756, acc 0.96875
2017-03-02T18:00:19.686382: step 21454, loss 0.119434, acc 0.921875
2017-03-02T18:00:19.769821: step 21455, loss 0.267345, acc 0.921875
2017-03-02T18:00:19.834959: step 21456, loss 0.0990242, acc 0.96875
2017-03-02T18:00:19.913408: step 21457, loss 0.196453, acc 0.921875
2017-03-02T18:00:19.982303: step 21458, loss 0.16011, acc 0.9375
2017-03-02T18:00:20.042224: step 21459, loss 0.126602, acc 0.953125
2017-03-02T18:00:20.120904: step 21460, loss 0.156658, acc 0.9375
2017-03-02T18:00:20.192072: step 21461, loss 0.0708662, acc 0.96875
2017-03-02T18:00:20.257141: step 21462, loss 0.194116, acc 0.90625
2017-03-02T18:00:20.334065: step 21463, loss 0.247756, acc 0.890625
2017-03-02T18:00:20.415107: step 21464, loss 0.0660141, acc 1
2017-03-02T18:00:20.480241: step 21465, loss 0.112998, acc 0.921875
2017-03-02T18:00:20.550103: step 21466, loss 0.150229, acc 0.921875
2017-03-02T18:00:20.626031: step 21467, loss 0.145911, acc 0.921875
2017-03-02T18:00:20.687652: step 21468, loss 0.0645614, acc 0.953125
2017-03-02T18:00:20.771402: step 21469, loss 0.0874352, acc 0.96875
2017-03-02T18:00:20.846880: step 21470, loss 0.284862, acc 0.890625
2017-03-02T18:00:20.929841: step 21471, loss 0.202235, acc 0.859375
2017-03-02T18:00:21.001859: step 21472, loss 0.125575, acc 0.96875
2017-03-02T18:00:21.074887: step 21473, loss 0.0678075, acc 1
2017-03-02T18:00:21.147334: step 21474, loss 0.107302, acc 0.953125
2017-03-02T18:00:21.223409: step 21475, loss 0.157428, acc 0.921875
2017-03-02T18:00:21.296384: step 21476, loss 0.104644, acc 0.96875
2017-03-02T18:00:21.366064: step 21477, loss 0.193378, acc 0.953125
2017-03-02T18:00:21.436360: step 21478, loss 0.110204, acc 0.96875
2017-03-02T18:00:21.509608: step 21479, loss 0.132981, acc 0.96875
2017-03-02T18:00:21.583949: step 21480, loss 0.133953, acc 0.90625
2017-03-02T18:00:21.656238: step 21481, loss 0.187787, acc 0.921875
2017-03-02T18:00:21.729926: step 21482, loss 0.181072, acc 0.9375
2017-03-02T18:00:21.810341: step 21483, loss 0.137122, acc 0.921875
2017-03-02T18:00:21.894491: step 21484, loss 0.0964336, acc 0.9375
2017-03-02T18:00:21.971473: step 21485, loss 0.187166, acc 0.9375
2017-03-02T18:00:22.038407: step 21486, loss 0.178294, acc 0.90625
2017-03-02T18:00:22.108079: step 21487, loss 0.204381, acc 0.890625
2017-03-02T18:00:22.185380: step 21488, loss 0.119023, acc 0.96875
2017-03-02T18:00:22.273445: step 21489, loss 0.146105, acc 0.921875
2017-03-02T18:00:22.351368: step 21490, loss 0.133708, acc 0.9375
2017-03-02T18:00:22.416537: step 21491, loss 0.142549, acc 0.9375
2017-03-02T18:00:22.494019: step 21492, loss 0.0855161, acc 0.96875
2017-03-02T18:00:22.566311: step 21493, loss 0.0933425, acc 0.9375
2017-03-02T18:00:22.668482: step 21494, loss 0.0848487, acc 0.9375
2017-03-02T18:00:22.743181: step 21495, loss 0.131389, acc 0.9375
2017-03-02T18:00:22.815923: step 21496, loss 0.212522, acc 0.875
2017-03-02T18:00:22.886920: step 21497, loss 0.0939473, acc 0.96875
2017-03-02T18:00:22.962531: step 21498, loss 0.177937, acc 0.90625
2017-03-02T18:00:23.045121: step 21499, loss 0.238487, acc 0.875
2017-03-02T18:00:23.117471: step 21500, loss 0.0878703, acc 0.96875

Evaluation:
2017-03-02T18:00:23.149878: step 21500, loss 2.56261, acc 0.638789

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21500

2017-03-02T18:00:23.617101: step 21501, loss 0.237174, acc 0.84375
2017-03-02T18:00:23.691163: step 21502, loss 0.172416, acc 0.9375
2017-03-02T18:00:23.762359: step 21503, loss 0.25537, acc 0.9375
2017-03-02T18:00:23.840635: step 21504, loss 0.174009, acc 0.9375
2017-03-02T18:00:23.919933: step 21505, loss 0.145074, acc 0.90625
2017-03-02T18:00:23.994392: step 21506, loss 0.0667257, acc 1
2017-03-02T18:00:24.063563: step 21507, loss 0.0840875, acc 0.953125
2017-03-02T18:00:24.132044: step 21508, loss 0.0884006, acc 0.96875
2017-03-02T18:00:24.210436: step 21509, loss 0.136089, acc 0.953125
2017-03-02T18:00:24.283138: step 21510, loss 0.202341, acc 0.921875
2017-03-02T18:00:24.360402: step 21511, loss 0.151799, acc 0.921875
2017-03-02T18:00:24.442464: step 21512, loss 0.108266, acc 0.953125
2017-03-02T18:00:24.514567: step 21513, loss 0.172839, acc 0.921875
2017-03-02T18:00:24.572219: step 21514, loss 0.186177, acc 0.921875
2017-03-02T18:00:24.649851: step 21515, loss 0.0704178, acc 0.953125
2017-03-02T18:00:24.723938: step 21516, loss 0.214868, acc 0.90625
2017-03-02T18:00:24.802725: step 21517, loss 0.20036, acc 0.90625
2017-03-02T18:00:24.872771: step 21518, loss 0.104732, acc 0.953125
2017-03-02T18:00:24.937263: step 21519, loss 0.381427, acc 0.828125
2017-03-02T18:00:25.011834: step 21520, loss 0.169538, acc 0.90625
2017-03-02T18:00:25.086417: step 21521, loss 0.181638, acc 0.921875
2017-03-02T18:00:25.157800: step 21522, loss 0.237295, acc 0.90625
2017-03-02T18:00:25.234012: step 21523, loss 0.152766, acc 0.953125
2017-03-02T18:00:25.307774: step 21524, loss 0.137755, acc 0.9375
2017-03-02T18:00:25.375999: step 21525, loss 0.23851, acc 0.875
2017-03-02T18:00:25.447289: step 21526, loss 0.126467, acc 0.9375
2017-03-02T18:00:25.514247: step 21527, loss 0.0617595, acc 0.96875
2017-03-02T18:00:25.586149: step 21528, loss 0.0466162, acc 1
2017-03-02T18:00:25.665946: step 21529, loss 0.245921, acc 0.890625
2017-03-02T18:00:25.737135: step 21530, loss 0.178045, acc 0.9375
2017-03-02T18:00:25.806419: step 21531, loss 0.170388, acc 0.921875
2017-03-02T18:00:25.878242: step 21532, loss 0.097725, acc 0.953125
2017-03-02T18:00:25.954145: step 21533, loss 0.0895045, acc 0.953125
2017-03-02T18:00:26.023369: step 21534, loss 0.0574197, acc 0.96875
2017-03-02T18:00:26.092056: step 21535, loss 0.162074, acc 0.9375
2017-03-02T18:00:26.163344: step 21536, loss 0.172207, acc 0.90625
2017-03-02T18:00:26.230733: step 21537, loss 0.167051, acc 0.921875
2017-03-02T18:00:26.306473: step 21538, loss 0.222006, acc 0.90625
2017-03-02T18:00:26.386068: step 21539, loss 0.0834803, acc 0.953125
2017-03-02T18:00:26.457930: step 21540, loss 0.082579, acc 0.984375
2017-03-02T18:00:26.526273: step 21541, loss 0.251891, acc 0.90625
2017-03-02T18:00:26.598026: step 21542, loss 0.101321, acc 0.96875
2017-03-02T18:00:26.682430: step 21543, loss 0.0971142, acc 0.953125
2017-03-02T18:00:26.755215: step 21544, loss 0.206873, acc 0.9375
2017-03-02T18:00:26.822332: step 21545, loss 0.258214, acc 0.875
2017-03-02T18:00:26.899229: step 21546, loss 0.192944, acc 0.890625
2017-03-02T18:00:26.969002: step 21547, loss 0.230834, acc 0.921875
2017-03-02T18:00:27.044597: step 21548, loss 0.237619, acc 0.890625
2017-03-02T18:00:27.118764: step 21549, loss 0.107405, acc 0.9375
2017-03-02T18:00:27.199026: step 21550, loss 0.122332, acc 0.9375
2017-03-02T18:00:27.268523: step 21551, loss 0.0559805, acc 0.96875
2017-03-02T18:00:27.342831: step 21552, loss 0.154394, acc 0.921875
2017-03-02T18:00:27.413307: step 21553, loss 0.252349, acc 0.890625
2017-03-02T18:00:27.487186: step 21554, loss 0.135518, acc 0.96875
2017-03-02T18:00:27.555766: step 21555, loss 0.22648, acc 0.921875
2017-03-02T18:00:27.628195: step 21556, loss 0.344794, acc 0.875
2017-03-02T18:00:27.718940: step 21557, loss 0.129265, acc 0.953125
2017-03-02T18:00:27.787797: step 21558, loss 0.168987, acc 0.921875
2017-03-02T18:00:27.859495: step 21559, loss 0.278366, acc 0.875
2017-03-02T18:00:27.935197: step 21560, loss 0.771277, acc 0.75
2017-03-02T18:00:28.015671: step 21561, loss 0.171636, acc 0.921875
2017-03-02T18:00:28.085881: step 21562, loss 0.232329, acc 0.890625
2017-03-02T18:00:28.162340: step 21563, loss 0.121336, acc 0.96875
2017-03-02T18:00:28.232816: step 21564, loss 0.130494, acc 0.921875
2017-03-02T18:00:28.298074: step 21565, loss 0.187608, acc 0.921875
2017-03-02T18:00:28.384410: step 21566, loss 0.112, acc 0.953125
2017-03-02T18:00:28.459780: step 21567, loss 0.164153, acc 0.890625
2017-03-02T18:00:28.534484: step 21568, loss 0.107763, acc 0.953125
2017-03-02T18:00:28.602158: step 21569, loss 0.145563, acc 0.9375
2017-03-02T18:00:28.680557: step 21570, loss 0.224132, acc 0.890625
2017-03-02T18:00:28.758880: step 21571, loss 0.0798733, acc 0.984375
2017-03-02T18:00:28.830873: step 21572, loss 0.142258, acc 0.953125
2017-03-02T18:00:28.913891: step 21573, loss 0.0788476, acc 0.984375
2017-03-02T18:00:28.984774: step 21574, loss 0.128714, acc 0.953125
2017-03-02T18:00:29.059008: step 21575, loss 0.166474, acc 0.9375
2017-03-02T18:00:29.137455: step 21576, loss 0.176584, acc 0.953125
2017-03-02T18:00:29.216877: step 21577, loss 0.160034, acc 0.9375
2017-03-02T18:00:29.287402: step 21578, loss 0.201471, acc 0.9375
2017-03-02T18:00:29.363057: step 21579, loss 0.0952988, acc 0.953125
2017-03-02T18:00:29.433411: step 21580, loss 0.215734, acc 0.90625
2017-03-02T18:00:29.502046: step 21581, loss 0.116547, acc 0.921875
2017-03-02T18:00:29.580319: step 21582, loss 0.121147, acc 0.953125
2017-03-02T18:00:29.642623: step 21583, loss 0.145016, acc 0.921875
2017-03-02T18:00:29.710249: step 21584, loss 0.108203, acc 0.9375
2017-03-02T18:00:29.795442: step 21585, loss 0.0586154, acc 0.96875
2017-03-02T18:00:29.884624: step 21586, loss 0.133284, acc 0.9375
2017-03-02T18:00:29.960151: step 21587, loss 0.0435151, acc 0.984375
2017-03-02T18:00:30.054094: step 21588, loss 0.191988, acc 0.90625
2017-03-02T18:00:30.122855: step 21589, loss 0.129223, acc 0.96875
2017-03-02T18:00:30.197327: step 21590, loss 0.0919467, acc 0.96875
2017-03-02T18:00:30.275872: step 21591, loss 0.106278, acc 0.9375
2017-03-02T18:00:30.346037: step 21592, loss 0.135722, acc 0.90625
2017-03-02T18:00:30.410497: step 21593, loss 0.123918, acc 0.953125
2017-03-02T18:00:30.482282: step 21594, loss 0.0685696, acc 0.96875
2017-03-02T18:00:30.556392: step 21595, loss 0.131317, acc 0.9375
2017-03-02T18:00:30.629496: step 21596, loss 0.0607981, acc 0.984375
2017-03-02T18:00:30.703402: step 21597, loss 0.108344, acc 0.9375
2017-03-02T18:00:30.776612: step 21598, loss 0.22161, acc 0.890625
2017-03-02T18:00:30.846174: step 21599, loss 0.129847, acc 0.953125
2017-03-02T18:00:30.922407: step 21600, loss 0.162332, acc 0.9375

Evaluation:
2017-03-02T18:00:30.950195: step 21600, loss 2.59969, acc 0.658255

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21600

2017-03-02T18:00:31.434648: step 21601, loss 0.105832, acc 0.9375
2017-03-02T18:00:31.514088: step 21602, loss 0.135068, acc 0.921875
2017-03-02T18:00:31.589244: step 21603, loss 0.160602, acc 0.890625
2017-03-02T18:00:31.672723: step 21604, loss 0.137016, acc 0.96875
2017-03-02T18:00:31.755650: step 21605, loss 0.153638, acc 0.921875
2017-03-02T18:00:31.848315: step 21606, loss 0.0553711, acc 0.96875
2017-03-02T18:00:31.924378: step 21607, loss 0.212159, acc 0.875
2017-03-02T18:00:31.998128: step 21608, loss 0.222805, acc 0.890625
2017-03-02T18:00:32.071454: step 21609, loss 0.157048, acc 0.921875
2017-03-02T18:00:32.142088: step 21610, loss 0.282058, acc 0.921875
2017-03-02T18:00:32.215583: step 21611, loss 0.155989, acc 0.953125
2017-03-02T18:00:32.292197: step 21612, loss 0.0769032, acc 0.96875
2017-03-02T18:00:32.358948: step 21613, loss 0.20103, acc 0.90625
2017-03-02T18:00:32.432693: step 21614, loss 0.108204, acc 0.953125
2017-03-02T18:00:32.506844: step 21615, loss 0.178124, acc 0.953125
2017-03-02T18:00:32.582396: step 21616, loss 0.167655, acc 0.921875
2017-03-02T18:00:32.654565: step 21617, loss 0.112975, acc 0.96875
2017-03-02T18:00:32.746062: step 21618, loss 0.112815, acc 0.984375
2017-03-02T18:00:32.831014: step 21619, loss 0.116469, acc 0.953125
2017-03-02T18:00:32.908753: step 21620, loss 0.0743657, acc 0.96875
2017-03-02T18:00:32.981413: step 21621, loss 0.36597, acc 0.84375
2017-03-02T18:00:33.052451: step 21622, loss 0.168687, acc 0.96875
2017-03-02T18:00:33.126156: step 21623, loss 0.121249, acc 0.953125
2017-03-02T18:00:33.195319: step 21624, loss 0.218706, acc 0.890625
2017-03-02T18:00:33.266958: step 21625, loss 0.264625, acc 0.875
2017-03-02T18:00:33.352619: step 21626, loss 0.194766, acc 0.921875
2017-03-02T18:00:33.430926: step 21627, loss 0.136378, acc 0.953125
2017-03-02T18:00:33.504740: step 21628, loss 0.12836, acc 0.953125
2017-03-02T18:00:33.579585: step 21629, loss 0.0727876, acc 0.953125
2017-03-02T18:00:33.653713: step 21630, loss 0.180299, acc 0.921875
2017-03-02T18:00:33.722256: step 21631, loss 0.176754, acc 0.9375
2017-03-02T18:00:33.792243: step 21632, loss 0.126378, acc 0.96875
2017-03-02T18:00:33.868488: step 21633, loss 0.158907, acc 0.9375
2017-03-02T18:00:33.961186: step 21634, loss 0.0573554, acc 0.984375
2017-03-02T18:00:34.033943: step 21635, loss 0.199478, acc 0.890625
2017-03-02T18:00:34.107509: step 21636, loss 0.140412, acc 0.921875
2017-03-02T18:00:34.179723: step 21637, loss 0.167335, acc 0.90625
2017-03-02T18:00:34.250640: step 21638, loss 0.0855734, acc 0.9375
2017-03-02T18:00:34.325682: step 21639, loss 0.0878067, acc 0.96875
2017-03-02T18:00:34.394625: step 21640, loss 0.206838, acc 0.921875
2017-03-02T18:00:34.468290: step 21641, loss 0.210111, acc 0.890625
2017-03-02T18:00:34.535698: step 21642, loss 0.123421, acc 0.921875
2017-03-02T18:00:34.610878: step 21643, loss 0.199805, acc 0.96875
2017-03-02T18:00:34.698231: step 21644, loss 0.116127, acc 0.953125
2017-03-02T18:00:34.772347: step 21645, loss 0.209963, acc 0.921875
2017-03-02T18:00:34.850159: step 21646, loss 0.104668, acc 0.96875
2017-03-02T18:00:34.924299: step 21647, loss 0.229816, acc 0.875
2017-03-02T18:00:34.999243: step 21648, loss 0.131614, acc 0.921875
2017-03-02T18:00:35.086647: step 21649, loss 0.134843, acc 0.9375
2017-03-02T18:00:35.155857: step 21650, loss 0.103792, acc 0.96875
2017-03-02T18:00:35.221491: step 21651, loss 0.133951, acc 0.9375
2017-03-02T18:00:35.298792: step 21652, loss 0.164346, acc 0.953125
2017-03-02T18:00:35.373760: step 21653, loss 0.112906, acc 0.953125
2017-03-02T18:00:35.453742: step 21654, loss 0.177882, acc 0.90625
2017-03-02T18:00:35.530528: step 21655, loss 0.170307, acc 0.9375
2017-03-02T18:00:35.608229: step 21656, loss 0.171919, acc 0.90625
2017-03-02T18:00:35.679889: step 21657, loss 0.18559, acc 0.953125
2017-03-02T18:00:35.755558: step 21658, loss 0.153969, acc 0.90625
2017-03-02T18:00:35.828768: step 21659, loss 0.147137, acc 0.953125
2017-03-02T18:00:35.899924: step 21660, loss 0.121216, acc 0.953125
2017-03-02T18:00:35.970482: step 21661, loss 0.16355, acc 0.953125
2017-03-02T18:00:36.045436: step 21662, loss 0.158414, acc 0.921875
2017-03-02T18:00:36.115801: step 21663, loss 0.166494, acc 0.9375
2017-03-02T18:00:36.190149: step 21664, loss 0.175587, acc 0.90625
2017-03-02T18:00:36.266087: step 21665, loss 0.124889, acc 0.9375
2017-03-02T18:00:36.340225: step 21666, loss 0.109037, acc 0.96875
2017-03-02T18:00:36.413160: step 21667, loss 0.22516, acc 0.890625
2017-03-02T18:00:36.480323: step 21668, loss 0.0852093, acc 0.953125
2017-03-02T18:00:36.549012: step 21669, loss 0.062276, acc 0.96875
2017-03-02T18:00:36.620665: step 21670, loss 0.160794, acc 0.90625
2017-03-02T18:00:36.707359: step 21671, loss 0.134796, acc 0.921875
2017-03-02T18:00:36.783206: step 21672, loss 0.147297, acc 0.9375
2017-03-02T18:00:36.871876: step 21673, loss 0.0704294, acc 0.96875
2017-03-02T18:00:36.942843: step 21674, loss 0.19909, acc 0.90625
2017-03-02T18:00:37.014159: step 21675, loss 0.139918, acc 0.953125
2017-03-02T18:00:37.092959: step 21676, loss 0.211926, acc 0.921875
2017-03-02T18:00:37.160689: step 21677, loss 0.150612, acc 0.90625
2017-03-02T18:00:37.231015: step 21678, loss 0.214734, acc 0.90625
2017-03-02T18:00:37.310396: step 21679, loss 0.194985, acc 0.921875
2017-03-02T18:00:37.391751: step 21680, loss 0.124708, acc 0.9375
2017-03-02T18:00:37.470216: step 21681, loss 0.174475, acc 0.921875
2017-03-02T18:00:37.544297: step 21682, loss 0.148902, acc 0.921875
2017-03-02T18:00:37.619724: step 21683, loss 0.165877, acc 0.90625
2017-03-02T18:00:37.694071: step 21684, loss 0.114271, acc 0.953125
2017-03-02T18:00:37.777418: step 21685, loss 0.266254, acc 0.890625
2017-03-02T18:00:37.846012: step 21686, loss 0.146816, acc 0.921875
2017-03-02T18:00:37.918805: step 21687, loss 0.083139, acc 0.984375
2017-03-02T18:00:37.986790: step 21688, loss 0.165244, acc 0.9375
2017-03-02T18:00:38.069255: step 21689, loss 0.107124, acc 0.9375
2017-03-02T18:00:38.150365: step 21690, loss 0.22551, acc 0.890625
2017-03-02T18:00:38.230833: step 21691, loss 0.143405, acc 0.96875
2017-03-02T18:00:38.315416: step 21692, loss 0.146569, acc 0.96875
2017-03-02T18:00:38.395446: step 21693, loss 0.186884, acc 0.921875
2017-03-02T18:00:38.474179: step 21694, loss 0.199857, acc 0.890625
2017-03-02T18:00:38.555520: step 21695, loss 0.182028, acc 0.953125
2017-03-02T18:00:38.630802: step 21696, loss 0.0950692, acc 0.96875
2017-03-02T18:00:38.701088: step 21697, loss 0.0937225, acc 0.953125
2017-03-02T18:00:38.791978: step 21698, loss 0.0737515, acc 0.953125
2017-03-02T18:00:38.863977: step 21699, loss 0.145982, acc 0.921875
2017-03-02T18:00:38.942926: step 21700, loss 0.0838555, acc 0.953125

Evaluation:
2017-03-02T18:00:38.979960: step 21700, loss 2.61048, acc 0.650324

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21700

2017-03-02T18:00:39.427664: step 21701, loss 0.10976, acc 0.9375
2017-03-02T18:00:39.499833: step 21702, loss 0.222698, acc 0.9375
2017-03-02T18:00:39.570338: step 21703, loss 0.223633, acc 0.890625
2017-03-02T18:00:39.649306: step 21704, loss 0.196568, acc 0.90625
2017-03-02T18:00:39.725173: step 21705, loss 0.112839, acc 0.953125
2017-03-02T18:00:39.797703: step 21706, loss 0.0446838, acc 0.984375
2017-03-02T18:00:39.871017: step 21707, loss 0.25768, acc 0.875
2017-03-02T18:00:39.938253: step 21708, loss 0.0947146, acc 0.96875
2017-03-02T18:00:40.004610: step 21709, loss 0.308649, acc 0.875
2017-03-02T18:00:40.073577: step 21710, loss 0.235033, acc 0.84375
2017-03-02T18:00:40.143612: step 21711, loss 0.187071, acc 0.90625
2017-03-02T18:00:40.216612: step 21712, loss 0.0941919, acc 0.96875
2017-03-02T18:00:40.292704: step 21713, loss 0.199197, acc 0.921875
2017-03-02T18:00:40.372100: step 21714, loss 0.107794, acc 0.96875
2017-03-02T18:00:40.445478: step 21715, loss 0.160585, acc 0.90625
2017-03-02T18:00:40.517169: step 21716, loss 0.279573, acc 0.890625
2017-03-02T18:00:40.594015: step 21717, loss 0.170306, acc 0.9375
2017-03-02T18:00:40.665429: step 21718, loss 0.114171, acc 0.953125
2017-03-02T18:00:40.734997: step 21719, loss 0.214889, acc 0.890625
2017-03-02T18:00:40.805671: step 21720, loss 0.212915, acc 0.890625
2017-03-02T18:00:40.876952: step 21721, loss 0.213085, acc 0.875
2017-03-02T18:00:40.950092: step 21722, loss 0.070042, acc 0.984375
2017-03-02T18:00:41.019800: step 21723, loss 0.179947, acc 0.953125
2017-03-02T18:00:41.093332: step 21724, loss 0.117979, acc 0.953125
2017-03-02T18:00:41.182660: step 21725, loss 0.128859, acc 0.921875
2017-03-02T18:00:41.259813: step 21726, loss 0.222223, acc 0.84375
2017-03-02T18:00:41.331290: step 21727, loss 0.253102, acc 0.875
2017-03-02T18:00:41.402105: step 21728, loss 0.15564, acc 0.90625
2017-03-02T18:00:41.473629: step 21729, loss 0.132347, acc 0.921875
2017-03-02T18:00:41.549012: step 21730, loss 0.156738, acc 0.9375
2017-03-02T18:00:41.628144: step 21731, loss 0.116846, acc 0.9375
2017-03-02T18:00:41.702622: step 21732, loss 0.15611, acc 0.96875
2017-03-02T18:00:41.778307: step 21733, loss 0.120079, acc 0.96875
2017-03-02T18:00:41.851450: step 21734, loss 0.238623, acc 0.953125
2017-03-02T18:00:41.919026: step 21735, loss 0.124128, acc 0.9375
2017-03-02T18:00:41.994899: step 21736, loss 0.0879857, acc 0.984375
2017-03-02T18:00:42.068962: step 21737, loss 0.155356, acc 0.921875
2017-03-02T18:00:42.135475: step 21738, loss 0.20933, acc 0.890625
2017-03-02T18:00:42.213393: step 21739, loss 0.109775, acc 0.96875
2017-03-02T18:00:42.275991: step 21740, loss 0.283944, acc 0.890625
2017-03-02T18:00:42.349668: step 21741, loss 0.118796, acc 0.9375
2017-03-02T18:00:42.423360: step 21742, loss 0.223776, acc 0.921875
2017-03-02T18:00:42.498106: step 21743, loss 0.185781, acc 0.953125
2017-03-02T18:00:42.569926: step 21744, loss 0.24392, acc 0.90625
2017-03-02T18:00:42.646065: step 21745, loss 0.16569, acc 0.90625
2017-03-02T18:00:42.717031: step 21746, loss 0.130795, acc 0.9375
2017-03-02T18:00:42.787926: step 21747, loss 0.147246, acc 0.90625
2017-03-02T18:00:42.860589: step 21748, loss 0.103316, acc 0.953125
2017-03-02T18:00:42.937476: step 21749, loss 0.197344, acc 0.953125
2017-03-02T18:00:43.018308: step 21750, loss 0.192399, acc 0.921875
2017-03-02T18:00:43.091814: step 21751, loss 0.0920124, acc 0.9375
2017-03-02T18:00:43.167192: step 21752, loss 0.221749, acc 0.90625
2017-03-02T18:00:43.240460: step 21753, loss 0.228273, acc 0.90625
2017-03-02T18:00:43.314906: step 21754, loss 0.113526, acc 0.953125
2017-03-02T18:00:43.375698: step 21755, loss 0.12282, acc 0.953125
2017-03-02T18:00:43.460140: step 21756, loss 0.0200871, acc 1
2017-03-02T18:00:43.530559: step 21757, loss 0.168207, acc 0.921875
2017-03-02T18:00:43.600614: step 21758, loss 0.202325, acc 0.90625
2017-03-02T18:00:43.681787: step 21759, loss 0.116054, acc 0.921875
2017-03-02T18:00:43.757866: step 21760, loss 0.0978161, acc 0.9375
2017-03-02T18:00:43.840923: step 21761, loss 0.142345, acc 0.921875
2017-03-02T18:00:43.917431: step 21762, loss 0.148417, acc 0.953125
2017-03-02T18:00:43.987348: step 21763, loss 0.107899, acc 0.9375
2017-03-02T18:00:44.052768: step 21764, loss 0.181571, acc 0.875
2017-03-02T18:00:44.118223: step 21765, loss 0.114769, acc 0.9375
2017-03-02T18:00:44.181979: step 21766, loss 0.225672, acc 0.921875
2017-03-02T18:00:44.255959: step 21767, loss 0.118428, acc 0.9375
2017-03-02T18:00:44.330034: step 21768, loss 0.112008, acc 0.984375
2017-03-02T18:00:44.404168: step 21769, loss 0.168691, acc 0.9375
2017-03-02T18:00:44.481514: step 21770, loss 0.15362, acc 0.90625
2017-03-02T18:00:44.556524: step 21771, loss 0.0976585, acc 0.96875
2017-03-02T18:00:44.621838: step 21772, loss 0.06826, acc 0.984375
2017-03-02T18:00:44.696403: step 21773, loss 0.163712, acc 0.90625
2017-03-02T18:00:44.767921: step 21774, loss 0.156744, acc 0.9375
2017-03-02T18:00:44.836427: step 21775, loss 0.147648, acc 0.953125
2017-03-02T18:00:44.908468: step 21776, loss 0.167305, acc 0.9375
2017-03-02T18:00:44.995980: step 21777, loss 0.136304, acc 0.9375
2017-03-02T18:00:45.068197: step 21778, loss 0.191855, acc 0.90625
2017-03-02T18:00:45.140002: step 21779, loss 0.135615, acc 0.921875
2017-03-02T18:00:45.212039: step 21780, loss 0.134335, acc 0.9375
2017-03-02T18:00:45.281748: step 21781, loss 0.0472781, acc 1
2017-03-02T18:00:45.352111: step 21782, loss 0.123247, acc 0.9375
2017-03-02T18:00:45.422928: step 21783, loss 0.167681, acc 0.9375
2017-03-02T18:00:45.499146: step 21784, loss 0.105383, acc 0.9375
2017-03-02T18:00:45.566807: step 21785, loss 0.111998, acc 0.953125
2017-03-02T18:00:45.652387: step 21786, loss 0.125665, acc 0.953125
2017-03-02T18:00:45.725792: step 21787, loss 0.135308, acc 0.953125
2017-03-02T18:00:45.800344: step 21788, loss 0.0965729, acc 0.953125
2017-03-02T18:00:45.872664: step 21789, loss 0.0854976, acc 1
2017-03-02T18:00:45.946631: step 21790, loss 0.0558295, acc 0.984375
2017-03-02T18:00:46.036964: step 21791, loss 0.141257, acc 0.90625
2017-03-02T18:00:46.109681: step 21792, loss 0.172109, acc 0.921875
2017-03-02T18:00:46.178190: step 21793, loss 0.127983, acc 0.9375
2017-03-02T18:00:46.245053: step 21794, loss 0.129009, acc 0.9375
2017-03-02T18:00:46.315111: step 21795, loss 0.0894424, acc 0.953125
2017-03-02T18:00:46.386583: step 21796, loss 0.186321, acc 0.90625
2017-03-02T18:00:46.465734: step 21797, loss 0.116829, acc 0.9375
2017-03-02T18:00:46.538538: step 21798, loss 0.285216, acc 0.890625
2017-03-02T18:00:46.614667: step 21799, loss 0.187555, acc 0.921875
2017-03-02T18:00:46.691246: step 21800, loss 0.251701, acc 0.90625

Evaluation:
2017-03-02T18:00:46.727251: step 21800, loss 2.57083, acc 0.635184

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21800

2017-03-02T18:00:47.175650: step 21801, loss 0.129043, acc 0.9375
2017-03-02T18:00:47.248133: step 21802, loss 0.155183, acc 0.921875
2017-03-02T18:00:47.321459: step 21803, loss 0.100708, acc 0.9375
2017-03-02T18:00:47.393506: step 21804, loss 0.125268, acc 0.953125
2017-03-02T18:00:47.486210: step 21805, loss 0.120428, acc 0.921875
2017-03-02T18:00:47.559442: step 21806, loss 0.216001, acc 0.90625
2017-03-02T18:00:47.629854: step 21807, loss 0.211891, acc 0.890625
2017-03-02T18:00:47.708369: step 21808, loss 0.0986696, acc 0.984375
2017-03-02T18:00:47.784472: step 21809, loss 0.171515, acc 0.9375
2017-03-02T18:00:47.852724: step 21810, loss 0.158925, acc 0.9375
2017-03-02T18:00:47.922434: step 21811, loss 0.111913, acc 0.953125
2017-03-02T18:00:47.993239: step 21812, loss 0.220146, acc 0.90625
2017-03-02T18:00:48.066369: step 21813, loss 0.0830371, acc 0.96875
2017-03-02T18:00:48.144160: step 21814, loss 0.166999, acc 0.921875
2017-03-02T18:00:48.208431: step 21815, loss 0.203563, acc 0.875
2017-03-02T18:00:48.277863: step 21816, loss 0.195329, acc 0.90625
2017-03-02T18:00:48.348585: step 21817, loss 0.0971599, acc 0.96875
2017-03-02T18:00:48.427445: step 21818, loss 0.146961, acc 0.953125
2017-03-02T18:00:48.502766: step 21819, loss 0.246135, acc 0.875
2017-03-02T18:00:48.576407: step 21820, loss 0.114147, acc 0.953125
2017-03-02T18:00:48.655542: step 21821, loss 0.138077, acc 0.9375
2017-03-02T18:00:48.727156: step 21822, loss 0.195311, acc 0.921875
2017-03-02T18:00:48.800136: step 21823, loss 0.16407, acc 0.953125
2017-03-02T18:00:48.877169: step 21824, loss 0.164983, acc 0.90625
2017-03-02T18:00:48.950893: step 21825, loss 0.0587998, acc 1
2017-03-02T18:00:49.023027: step 21826, loss 0.213049, acc 0.90625
2017-03-02T18:00:49.097119: step 21827, loss 0.123407, acc 0.9375
2017-03-02T18:00:49.171989: step 21828, loss 0.258222, acc 0.875
2017-03-02T18:00:49.258775: step 21829, loss 0.193067, acc 0.90625
2017-03-02T18:00:49.335768: step 21830, loss 0.134829, acc 0.9375
2017-03-02T18:00:49.415789: step 21831, loss 0.092152, acc 0.953125
2017-03-02T18:00:49.498737: step 21832, loss 0.145645, acc 0.921875
2017-03-02T18:00:49.566446: step 21833, loss 0.195304, acc 0.90625
2017-03-02T18:00:49.635877: step 21834, loss 0.23646, acc 0.921875
2017-03-02T18:00:49.711175: step 21835, loss 0.255431, acc 0.921875
2017-03-02T18:00:49.785295: step 21836, loss 0.0857555, acc 0.984375
2017-03-02T18:00:49.856301: step 21837, loss 0.0229481, acc 1
2017-03-02T18:00:49.933524: step 21838, loss 0.171101, acc 0.890625
2017-03-02T18:00:50.000571: step 21839, loss 0.188174, acc 0.921875
2017-03-02T18:00:50.075325: step 21840, loss 0.102996, acc 0.9375
2017-03-02T18:00:50.149943: step 21841, loss 0.230592, acc 0.90625
2017-03-02T18:00:50.228273: step 21842, loss 0.138464, acc 0.921875
2017-03-02T18:00:50.302070: step 21843, loss 0.128275, acc 0.9375
2017-03-02T18:00:50.369610: step 21844, loss 0.151107, acc 0.890625
2017-03-02T18:00:50.443436: step 21845, loss 0.14276, acc 0.9375
2017-03-02T18:00:50.515543: step 21846, loss 0.115409, acc 0.953125
2017-03-02T18:00:50.592182: step 21847, loss 0.119568, acc 0.9375
2017-03-02T18:00:50.677828: step 21848, loss 0.208769, acc 0.890625
2017-03-02T18:00:50.750859: step 21849, loss 0.170291, acc 0.90625
2017-03-02T18:00:50.825123: step 21850, loss 0.121322, acc 0.953125
2017-03-02T18:00:50.895527: step 21851, loss 0.13757, acc 0.921875
2017-03-02T18:00:50.968633: step 21852, loss 0.117835, acc 0.953125
2017-03-02T18:00:51.032166: step 21853, loss 0.21414, acc 0.90625
2017-03-02T18:00:51.100871: step 21854, loss 0.153002, acc 0.921875
2017-03-02T18:00:51.175143: step 21855, loss 0.0462063, acc 1
2017-03-02T18:00:51.240877: step 21856, loss 0.11211, acc 0.9375
2017-03-02T18:00:51.314781: step 21857, loss 0.148213, acc 0.9375
2017-03-02T18:00:51.379060: step 21858, loss 0.147228, acc 0.9375
2017-03-02T18:00:51.451636: step 21859, loss 0.156896, acc 0.890625
2017-03-02T18:00:51.533239: step 21860, loss 0.248111, acc 0.921875
2017-03-02T18:00:51.608396: step 21861, loss 0.0973552, acc 0.96875
2017-03-02T18:00:51.678389: step 21862, loss 0.167394, acc 0.953125
2017-03-02T18:00:51.746587: step 21863, loss 0.0944227, acc 0.953125
2017-03-02T18:00:51.818065: step 21864, loss 0.158271, acc 0.921875
2017-03-02T18:00:51.892488: step 21865, loss 0.124222, acc 0.953125
2017-03-02T18:00:51.968850: step 21866, loss 0.124705, acc 0.9375
2017-03-02T18:00:52.054161: step 21867, loss 0.0513339, acc 0.984375
2017-03-02T18:00:52.130703: step 21868, loss 0.216613, acc 0.90625
2017-03-02T18:00:52.216148: step 21869, loss 0.210198, acc 0.921875
2017-03-02T18:00:52.298841: step 21870, loss 0.300536, acc 0.859375
2017-03-02T18:00:52.367167: step 21871, loss 0.15252, acc 0.9375
2017-03-02T18:00:52.448338: step 21872, loss 0.0619292, acc 0.984375
2017-03-02T18:00:52.524198: step 21873, loss 0.195967, acc 0.921875
2017-03-02T18:00:52.600249: step 21874, loss 0.105718, acc 0.921875
2017-03-02T18:00:52.677126: step 21875, loss 0.0501519, acc 0.984375
2017-03-02T18:00:52.756867: step 21876, loss 0.124123, acc 0.921875
2017-03-02T18:00:52.829191: step 21877, loss 0.232394, acc 0.90625
2017-03-02T18:00:52.912044: step 21878, loss 0.154152, acc 0.921875
2017-03-02T18:00:52.989284: step 21879, loss 0.277821, acc 0.859375
2017-03-02T18:00:53.058911: step 21880, loss 0.25268, acc 0.9375
2017-03-02T18:00:53.132295: step 21881, loss 0.0481365, acc 0.984375
2017-03-02T18:00:53.220557: step 21882, loss 0.0982031, acc 0.953125
2017-03-02T18:00:53.296001: step 21883, loss 0.20721, acc 0.875
2017-03-02T18:00:53.368913: step 21884, loss 0.0455179, acc 0.984375
2017-03-02T18:00:53.444247: step 21885, loss 0.0982687, acc 0.984375
2017-03-02T18:00:53.513922: step 21886, loss 0.215953, acc 0.890625
2017-03-02T18:00:53.588814: step 21887, loss 0.079071, acc 0.984375
2017-03-02T18:00:53.662843: step 21888, loss 0.0858681, acc 0.953125
2017-03-02T18:00:53.732615: step 21889, loss 0.174172, acc 0.9375
2017-03-02T18:00:53.801512: step 21890, loss 0.215829, acc 0.890625
2017-03-02T18:00:53.869984: step 21891, loss 0.218135, acc 0.890625
2017-03-02T18:00:53.943737: step 21892, loss 0.103838, acc 0.9375
2017-03-02T18:00:54.015020: step 21893, loss 0.238352, acc 0.90625
2017-03-02T18:00:54.086193: step 21894, loss 0.218883, acc 0.9375
2017-03-02T18:00:54.155616: step 21895, loss 0.128547, acc 0.921875
2017-03-02T18:00:54.231235: step 21896, loss 0.314806, acc 0.84375
2017-03-02T18:00:54.300543: step 21897, loss 0.114819, acc 0.953125
2017-03-02T18:00:54.374623: step 21898, loss 0.0607582, acc 0.96875
2017-03-02T18:00:54.445288: step 21899, loss 0.266628, acc 0.890625
2017-03-02T18:00:54.510660: step 21900, loss 0.105167, acc 0.953125

Evaluation:
2017-03-02T18:00:54.550308: step 21900, loss 2.59428, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-21900

2017-03-02T18:00:54.995885: step 21901, loss 0.0936515, acc 0.96875
2017-03-02T18:00:55.064790: step 21902, loss 0.180903, acc 0.921875
2017-03-02T18:00:55.136602: step 21903, loss 0.164695, acc 0.9375
2017-03-02T18:00:55.207373: step 21904, loss 0.228631, acc 0.890625
2017-03-02T18:00:55.289535: step 21905, loss 0.18354, acc 0.890625
2017-03-02T18:00:55.365812: step 21906, loss 0.19956, acc 0.9375
2017-03-02T18:00:55.439477: step 21907, loss 0.183172, acc 0.9375
2017-03-02T18:00:55.517028: step 21908, loss 0.113772, acc 0.953125
2017-03-02T18:00:55.596431: step 21909, loss 0.109319, acc 0.96875
2017-03-02T18:00:55.673297: step 21910, loss 0.131813, acc 0.9375
2017-03-02T18:00:55.743229: step 21911, loss 0.184318, acc 0.921875
2017-03-02T18:00:55.810936: step 21912, loss 0.0694362, acc 0.984375
2017-03-02T18:00:55.882206: step 21913, loss 0.102745, acc 0.9375
2017-03-02T18:00:55.955025: step 21914, loss 0.143786, acc 0.921875
2017-03-02T18:00:56.030125: step 21915, loss 0.243492, acc 0.890625
2017-03-02T18:00:56.103575: step 21916, loss 0.216422, acc 0.875
2017-03-02T18:00:56.185094: step 21917, loss 0.203588, acc 0.90625
2017-03-02T18:00:56.258951: step 21918, loss 0.13536, acc 0.953125
2017-03-02T18:00:56.327739: step 21919, loss 0.130333, acc 0.96875
2017-03-02T18:00:56.390976: step 21920, loss 0.0514624, acc 0.984375
2017-03-02T18:00:56.456037: step 21921, loss 0.105326, acc 0.953125
2017-03-02T18:00:56.526753: step 21922, loss 0.102814, acc 0.921875
2017-03-02T18:00:56.608473: step 21923, loss 0.147234, acc 0.921875
2017-03-02T18:00:56.679760: step 21924, loss 0.117439, acc 0.9375
2017-03-02T18:00:56.753941: step 21925, loss 0.192244, acc 0.9375
2017-03-02T18:00:56.825938: step 21926, loss 0.237846, acc 0.875
2017-03-02T18:00:56.892086: step 21927, loss 0.164095, acc 0.921875
2017-03-02T18:00:56.959151: step 21928, loss 0.120351, acc 0.953125
2017-03-02T18:00:57.034500: step 21929, loss 0.17363, acc 0.9375
2017-03-02T18:00:57.108205: step 21930, loss 0.213194, acc 0.921875
2017-03-02T18:00:57.171919: step 21931, loss 0.134091, acc 0.9375
2017-03-02T18:00:57.237601: step 21932, loss 0.0632166, acc 0.984375
2017-03-02T18:00:57.305195: step 21933, loss 0.140145, acc 0.890625
2017-03-02T18:00:57.379219: step 21934, loss 0.0903587, acc 0.984375
2017-03-02T18:00:57.453985: step 21935, loss 0.121271, acc 0.96875
2017-03-02T18:00:57.523447: step 21936, loss 0.104754, acc 0.96875
2017-03-02T18:00:57.594402: step 21937, loss 0.116416, acc 0.96875
2017-03-02T18:00:57.666597: step 21938, loss 0.174791, acc 0.921875
2017-03-02T18:00:57.737769: step 21939, loss 0.246766, acc 0.9375
2017-03-02T18:00:57.808622: step 21940, loss 0.123726, acc 0.9375
2017-03-02T18:00:57.875486: step 21941, loss 0.378348, acc 0.859375
2017-03-02T18:00:57.942742: step 21942, loss 0.0969767, acc 0.96875
2017-03-02T18:00:58.017235: step 21943, loss 0.0819671, acc 0.96875
2017-03-02T18:00:58.093676: step 21944, loss 0.134964, acc 0.953125
2017-03-02T18:00:58.183986: step 21945, loss 0.160906, acc 0.921875
2017-03-02T18:00:58.257229: step 21946, loss 0.109557, acc 0.9375
2017-03-02T18:00:58.341683: step 21947, loss 0.243673, acc 0.875
2017-03-02T18:00:58.415619: step 21948, loss 0.106153, acc 0.921875
2017-03-02T18:00:58.473643: step 21949, loss 0.138128, acc 0.921875
2017-03-02T18:00:58.543106: step 21950, loss 0.135064, acc 0.953125
2017-03-02T18:00:58.613232: step 21951, loss 0.198554, acc 0.890625
2017-03-02T18:00:58.681833: step 21952, loss 0.175126, acc 1
2017-03-02T18:00:58.762829: step 21953, loss 0.34351, acc 0.859375
2017-03-02T18:00:58.851893: step 21954, loss 0.179129, acc 0.921875
2017-03-02T18:00:58.926004: step 21955, loss 0.227375, acc 0.875
2017-03-02T18:00:58.994009: step 21956, loss 0.0842689, acc 0.953125
2017-03-02T18:00:59.070693: step 21957, loss 0.232556, acc 0.875
2017-03-02T18:00:59.141992: step 21958, loss 0.0565146, acc 0.984375
2017-03-02T18:00:59.216330: step 21959, loss 0.108426, acc 0.96875
2017-03-02T18:00:59.283945: step 21960, loss 0.177821, acc 0.9375
2017-03-02T18:00:59.357940: step 21961, loss 0.185121, acc 0.9375
2017-03-02T18:00:59.428909: step 21962, loss 0.122179, acc 0.953125
2017-03-02T18:00:59.503284: step 21963, loss 0.139192, acc 0.9375
2017-03-02T18:00:59.585182: step 21964, loss 0.170743, acc 0.921875
2017-03-02T18:00:59.657605: step 21965, loss 0.163341, acc 0.890625
2017-03-02T18:00:59.737893: step 21966, loss 0.126789, acc 0.953125
2017-03-02T18:00:59.811915: step 21967, loss 0.0758393, acc 0.984375
2017-03-02T18:00:59.885481: step 21968, loss 0.0426793, acc 1
2017-03-02T18:00:59.955383: step 21969, loss 0.103067, acc 0.921875
2017-03-02T18:01:00.028614: step 21970, loss 0.10359, acc 0.921875
2017-03-02T18:01:00.090573: step 21971, loss 0.161822, acc 0.953125
2017-03-02T18:01:00.166687: step 21972, loss 0.140184, acc 0.9375
2017-03-02T18:01:00.241933: step 21973, loss 0.102941, acc 0.953125
2017-03-02T18:01:00.310956: step 21974, loss 0.0658154, acc 0.96875
2017-03-02T18:01:00.372380: step 21975, loss 0.125756, acc 0.90625
2017-03-02T18:01:00.445821: step 21976, loss 0.0656883, acc 0.984375
2017-03-02T18:01:00.519739: step 21977, loss 0.126642, acc 0.9375
2017-03-02T18:01:00.587274: step 21978, loss 0.0983064, acc 0.953125
2017-03-02T18:01:00.654143: step 21979, loss 0.0881748, acc 0.984375
2017-03-02T18:01:00.725488: step 21980, loss 0.227145, acc 0.890625
2017-03-02T18:01:00.797866: step 21981, loss 0.0852013, acc 0.953125
2017-03-02T18:01:00.881848: step 21982, loss 0.0892921, acc 0.96875
2017-03-02T18:01:00.959195: step 21983, loss 0.14768, acc 0.90625
2017-03-02T18:01:01.036240: step 21984, loss 0.133798, acc 0.953125
2017-03-02T18:01:01.112133: step 21985, loss 0.15249, acc 0.953125
2017-03-02T18:01:01.189798: step 21986, loss 0.114921, acc 0.9375
2017-03-02T18:01:01.258851: step 21987, loss 0.114912, acc 0.96875
2017-03-02T18:01:01.336935: step 21988, loss 0.130362, acc 0.9375
2017-03-02T18:01:01.404229: step 21989, loss 0.163169, acc 0.921875
2017-03-02T18:01:01.485574: step 21990, loss 0.159542, acc 0.90625
2017-03-02T18:01:01.568390: step 21991, loss 0.114862, acc 0.96875
2017-03-02T18:01:01.637799: step 21992, loss 0.101665, acc 0.984375
2017-03-02T18:01:01.709171: step 21993, loss 0.0556902, acc 0.96875
2017-03-02T18:01:01.781205: step 21994, loss 0.1143, acc 0.953125
2017-03-02T18:01:01.853241: step 21995, loss 0.171627, acc 0.921875
2017-03-02T18:01:01.925382: step 21996, loss 0.0745428, acc 0.953125
2017-03-02T18:01:01.997780: step 21997, loss 0.187638, acc 0.890625
2017-03-02T18:01:02.064870: step 21998, loss 0.237738, acc 0.921875
2017-03-02T18:01:02.140377: step 21999, loss 0.135885, acc 0.953125
2017-03-02T18:01:02.214729: step 22000, loss 0.202193, acc 0.84375

Evaluation:
2017-03-02T18:01:02.248178: step 22000, loss 2.64613, acc 0.64744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22000

2017-03-02T18:01:02.690860: step 22001, loss 0.106443, acc 0.9375
2017-03-02T18:01:02.771901: step 22002, loss 0.195196, acc 0.9375
2017-03-02T18:01:02.835826: step 22003, loss 0.0957675, acc 0.96875
2017-03-02T18:01:02.899971: step 22004, loss 0.145012, acc 0.921875
2017-03-02T18:01:02.970313: step 22005, loss 0.138609, acc 0.890625
2017-03-02T18:01:03.044895: step 22006, loss 0.130913, acc 0.953125
2017-03-02T18:01:03.115035: step 22007, loss 0.277898, acc 0.890625
2017-03-02T18:01:03.192222: step 22008, loss 0.0761507, acc 0.984375
2017-03-02T18:01:03.256731: step 22009, loss 0.148632, acc 0.9375
2017-03-02T18:01:03.315943: step 22010, loss 0.163645, acc 0.953125
2017-03-02T18:01:03.381261: step 22011, loss 0.148555, acc 0.953125
2017-03-02T18:01:03.452039: step 22012, loss 0.0456218, acc 0.984375
2017-03-02T18:01:03.519775: step 22013, loss 0.134954, acc 0.953125
2017-03-02T18:01:03.595916: step 22014, loss 0.104941, acc 0.96875
2017-03-02T18:01:03.666631: step 22015, loss 0.235163, acc 0.921875
2017-03-02T18:01:03.738861: step 22016, loss 0.143002, acc 0.921875
2017-03-02T18:01:03.810520: step 22017, loss 0.0745774, acc 0.96875
2017-03-02T18:01:03.882300: step 22018, loss 0.135739, acc 0.9375
2017-03-02T18:01:03.952433: step 22019, loss 0.211061, acc 0.90625
2017-03-02T18:01:04.018698: step 22020, loss 0.223008, acc 0.90625
2017-03-02T18:01:04.086110: step 22021, loss 0.145131, acc 0.9375
2017-03-02T18:01:04.164829: step 22022, loss 0.136771, acc 0.921875
2017-03-02T18:01:04.243298: step 22023, loss 0.0522391, acc 0.984375
2017-03-02T18:01:04.313647: step 22024, loss 0.11898, acc 0.953125
2017-03-02T18:01:04.386365: step 22025, loss 0.123947, acc 0.921875
2017-03-02T18:01:04.455614: step 22026, loss 0.167834, acc 0.9375
2017-03-02T18:01:04.526534: step 22027, loss 0.131316, acc 0.9375
2017-03-02T18:01:04.601279: step 22028, loss 0.157708, acc 0.9375
2017-03-02T18:01:04.670522: step 22029, loss 0.176704, acc 0.9375
2017-03-02T18:01:04.736316: step 22030, loss 0.130673, acc 0.96875
2017-03-02T18:01:04.805534: step 22031, loss 0.168398, acc 0.921875
2017-03-02T18:01:04.888104: step 22032, loss 0.197796, acc 0.859375
2017-03-02T18:01:04.953256: step 22033, loss 0.13927, acc 0.921875
2017-03-02T18:01:05.026659: step 22034, loss 0.100947, acc 0.953125
2017-03-02T18:01:05.104226: step 22035, loss 0.106375, acc 0.9375
2017-03-02T18:01:05.177342: step 22036, loss 0.383499, acc 0.84375
2017-03-02T18:01:05.250715: step 22037, loss 0.0763948, acc 0.984375
2017-03-02T18:01:05.336871: step 22038, loss 0.0702071, acc 0.953125
2017-03-02T18:01:05.412615: step 22039, loss 0.132753, acc 0.953125
2017-03-02T18:01:05.495583: step 22040, loss 0.189582, acc 0.90625
2017-03-02T18:01:05.576870: step 22041, loss 0.114898, acc 0.96875
2017-03-02T18:01:05.650411: step 22042, loss 0.0968384, acc 0.984375
2017-03-02T18:01:05.720554: step 22043, loss 0.127509, acc 0.953125
2017-03-02T18:01:05.799887: step 22044, loss 0.265449, acc 0.828125
2017-03-02T18:01:05.866990: step 22045, loss 0.136374, acc 0.9375
2017-03-02T18:01:05.942197: step 22046, loss 0.169349, acc 0.921875
2017-03-02T18:01:06.014934: step 22047, loss 0.130854, acc 0.9375
2017-03-02T18:01:06.080829: step 22048, loss 0.136655, acc 0.953125
2017-03-02T18:01:06.149511: step 22049, loss 0.214912, acc 0.890625
2017-03-02T18:01:06.225727: step 22050, loss 0.112556, acc 0.96875
2017-03-02T18:01:06.295725: step 22051, loss 0.269385, acc 0.90625
2017-03-02T18:01:06.374672: step 22052, loss 0.229922, acc 0.90625
2017-03-02T18:01:06.463180: step 22053, loss 0.173901, acc 0.921875
2017-03-02T18:01:06.535008: step 22054, loss 0.116051, acc 0.96875
2017-03-02T18:01:06.608372: step 22055, loss 0.14103, acc 0.953125
2017-03-02T18:01:06.677520: step 22056, loss 0.238095, acc 0.90625
2017-03-02T18:01:06.749913: step 22057, loss 0.183111, acc 0.921875
2017-03-02T18:01:06.818505: step 22058, loss 0.236248, acc 0.890625
2017-03-02T18:01:06.884214: step 22059, loss 0.153014, acc 0.953125
2017-03-02T18:01:06.964576: step 22060, loss 0.181943, acc 0.90625
2017-03-02T18:01:07.033856: step 22061, loss 0.143996, acc 0.953125
2017-03-02T18:01:07.107286: step 22062, loss 0.0879366, acc 0.953125
2017-03-02T18:01:07.173594: step 22063, loss 0.149619, acc 0.921875
2017-03-02T18:01:07.242849: step 22064, loss 0.19701, acc 0.921875
2017-03-02T18:01:07.315124: step 22065, loss 0.265049, acc 0.90625
2017-03-02T18:01:07.385881: step 22066, loss 0.10471, acc 0.9375
2017-03-02T18:01:07.450814: step 22067, loss 0.155396, acc 0.890625
2017-03-02T18:01:07.518866: step 22068, loss 0.123435, acc 0.921875
2017-03-02T18:01:07.592448: step 22069, loss 0.125494, acc 0.9375
2017-03-02T18:01:07.663168: step 22070, loss 0.0830933, acc 0.96875
2017-03-02T18:01:07.749078: step 22071, loss 0.112534, acc 0.953125
2017-03-02T18:01:07.822884: step 22072, loss 0.160718, acc 0.90625
2017-03-02T18:01:07.902970: step 22073, loss 0.194202, acc 0.90625
2017-03-02T18:01:07.974323: step 22074, loss 0.0935337, acc 0.953125
2017-03-02T18:01:08.037046: step 22075, loss 0.242842, acc 0.90625
2017-03-02T18:01:08.109063: step 22076, loss 0.153005, acc 0.9375
2017-03-02T18:01:08.177963: step 22077, loss 0.121026, acc 0.921875
2017-03-02T18:01:08.248220: step 22078, loss 0.106536, acc 0.96875
2017-03-02T18:01:08.335350: step 22079, loss 0.105308, acc 0.9375
2017-03-02T18:01:08.408427: step 22080, loss 0.0935993, acc 0.96875
2017-03-02T18:01:08.484525: step 22081, loss 0.228923, acc 0.921875
2017-03-02T18:01:08.563544: step 22082, loss 0.142871, acc 0.9375
2017-03-02T18:01:08.635880: step 22083, loss 0.142647, acc 0.9375
2017-03-02T18:01:08.722892: step 22084, loss 0.159082, acc 0.90625
2017-03-02T18:01:08.797465: step 22085, loss 0.162742, acc 0.921875
2017-03-02T18:01:08.878896: step 22086, loss 0.0695517, acc 0.96875
2017-03-02T18:01:08.948344: step 22087, loss 0.269653, acc 0.921875
2017-03-02T18:01:09.020226: step 22088, loss 0.10797, acc 0.9375
2017-03-02T18:01:09.097255: step 22089, loss 0.218035, acc 0.921875
2017-03-02T18:01:09.172972: step 22090, loss 0.125925, acc 0.9375
2017-03-02T18:01:09.248132: step 22091, loss 0.111446, acc 0.96875
2017-03-02T18:01:09.323812: step 22092, loss 0.125767, acc 0.921875
2017-03-02T18:01:09.407688: step 22093, loss 0.116267, acc 0.9375
2017-03-02T18:01:09.487988: step 22094, loss 0.0839985, acc 0.96875
2017-03-02T18:01:09.554080: step 22095, loss 0.165014, acc 0.953125
2017-03-02T18:01:09.625898: step 22096, loss 0.131811, acc 0.953125
2017-03-02T18:01:09.691593: step 22097, loss 0.199989, acc 0.90625
2017-03-02T18:01:09.763714: step 22098, loss 0.212176, acc 0.90625
2017-03-02T18:01:09.833078: step 22099, loss 0.165967, acc 0.9375
2017-03-02T18:01:09.905791: step 22100, loss 0.174969, acc 0.953125

Evaluation:
2017-03-02T18:01:09.941953: step 22100, loss 2.6856, acc 0.638068

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22100

2017-03-02T18:01:10.390725: step 22101, loss 0.180414, acc 0.90625
2017-03-02T18:01:10.466320: step 22102, loss 0.181898, acc 0.9375
2017-03-02T18:01:10.544624: step 22103, loss 0.143159, acc 0.9375
2017-03-02T18:01:10.615684: step 22104, loss 0.135691, acc 0.953125
2017-03-02T18:01:10.692628: step 22105, loss 0.123174, acc 0.953125
2017-03-02T18:01:10.761424: step 22106, loss 0.198345, acc 0.90625
2017-03-02T18:01:10.830262: step 22107, loss 0.120907, acc 0.9375
2017-03-02T18:01:10.907471: step 22108, loss 0.166554, acc 0.921875
2017-03-02T18:01:10.975813: step 22109, loss 0.170689, acc 0.875
2017-03-02T18:01:11.045006: step 22110, loss 0.177147, acc 0.921875
2017-03-02T18:01:11.116837: step 22111, loss 0.0997585, acc 0.953125
2017-03-02T18:01:11.194812: step 22112, loss 0.166449, acc 0.953125
2017-03-02T18:01:11.270250: step 22113, loss 0.194405, acc 0.9375
2017-03-02T18:01:11.350647: step 22114, loss 0.10032, acc 0.9375
2017-03-02T18:01:11.416181: step 22115, loss 0.199404, acc 0.9375
2017-03-02T18:01:11.482788: step 22116, loss 0.187208, acc 0.875
2017-03-02T18:01:11.561346: step 22117, loss 0.122406, acc 0.9375
2017-03-02T18:01:11.632442: step 22118, loss 0.164917, acc 0.9375
2017-03-02T18:01:11.713285: step 22119, loss 0.155456, acc 0.921875
2017-03-02T18:01:11.788615: step 22120, loss 0.254804, acc 0.890625
2017-03-02T18:01:11.861321: step 22121, loss 0.101042, acc 0.96875
2017-03-02T18:01:11.944069: step 22122, loss 0.208711, acc 0.9375
2017-03-02T18:01:12.019311: step 22123, loss 0.11894, acc 0.9375
2017-03-02T18:01:12.097248: step 22124, loss 0.119962, acc 0.9375
2017-03-02T18:01:12.178012: step 22125, loss 0.0945741, acc 0.9375
2017-03-02T18:01:12.257712: step 22126, loss 0.15055, acc 0.90625
2017-03-02T18:01:12.326247: step 22127, loss 0.0914946, acc 0.984375
2017-03-02T18:01:12.392430: step 22128, loss 0.118242, acc 0.96875
2017-03-02T18:01:12.462785: step 22129, loss 0.22956, acc 0.859375
2017-03-02T18:01:12.549657: step 22130, loss 0.106798, acc 0.953125
2017-03-02T18:01:12.628766: step 22131, loss 0.185346, acc 0.90625
2017-03-02T18:01:12.716739: step 22132, loss 0.124902, acc 0.953125
2017-03-02T18:01:12.788854: step 22133, loss 0.0933405, acc 0.96875
2017-03-02T18:01:12.860564: step 22134, loss 0.200318, acc 0.859375
2017-03-02T18:01:12.946941: step 22135, loss 0.0759408, acc 0.96875
2017-03-02T18:01:13.014678: step 22136, loss 0.229525, acc 0.890625
2017-03-02T18:01:13.089598: step 22137, loss 0.15015, acc 0.921875
2017-03-02T18:01:13.158560: step 22138, loss 0.17178, acc 0.90625
2017-03-02T18:01:13.229072: step 22139, loss 0.274633, acc 0.875
2017-03-02T18:01:13.304558: step 22140, loss 0.0572126, acc 0.96875
2017-03-02T18:01:13.370977: step 22141, loss 0.107049, acc 0.96875
2017-03-02T18:01:13.443692: step 22142, loss 0.231656, acc 0.90625
2017-03-02T18:01:13.514501: step 22143, loss 0.23847, acc 0.90625
2017-03-02T18:01:13.586379: step 22144, loss 0.19678, acc 0.921875
2017-03-02T18:01:13.672294: step 22145, loss 0.211379, acc 0.875
2017-03-02T18:01:13.737282: step 22146, loss 0.225317, acc 0.875
2017-03-02T18:01:13.805982: step 22147, loss 0.125941, acc 0.9375
2017-03-02T18:01:13.865132: step 22148, loss 0.000811967, acc 1
2017-03-02T18:01:13.959122: step 22149, loss 0.182247, acc 0.90625
2017-03-02T18:01:14.031592: step 22150, loss 0.159527, acc 0.90625
2017-03-02T18:01:14.103628: step 22151, loss 0.194485, acc 0.921875
2017-03-02T18:01:14.174400: step 22152, loss 0.150199, acc 0.921875
2017-03-02T18:01:14.247957: step 22153, loss 0.203801, acc 0.921875
2017-03-02T18:01:14.324463: step 22154, loss 0.179567, acc 0.921875
2017-03-02T18:01:14.394819: step 22155, loss 0.0909212, acc 0.984375
2017-03-02T18:01:14.463691: step 22156, loss 0.069379, acc 0.96875
2017-03-02T18:01:14.544399: step 22157, loss 0.100614, acc 0.984375
2017-03-02T18:01:14.617206: step 22158, loss 0.239066, acc 0.90625
2017-03-02T18:01:14.690184: step 22159, loss 0.151094, acc 0.953125
2017-03-02T18:01:14.762604: step 22160, loss 0.146716, acc 0.9375
2017-03-02T18:01:14.834908: step 22161, loss 0.157679, acc 0.9375
2017-03-02T18:01:14.902506: step 22162, loss 0.105429, acc 0.96875
2017-03-02T18:01:14.992460: step 22163, loss 0.174707, acc 0.921875
2017-03-02T18:01:15.062943: step 22164, loss 0.199886, acc 0.9375
2017-03-02T18:01:15.157197: step 22165, loss 0.174526, acc 0.9375
2017-03-02T18:01:15.229374: step 22166, loss 0.195766, acc 0.921875
2017-03-02T18:01:15.305434: step 22167, loss 0.264645, acc 0.90625
2017-03-02T18:01:15.394851: step 22168, loss 0.11261, acc 0.9375
2017-03-02T18:01:15.461566: step 22169, loss 0.154057, acc 0.90625
2017-03-02T18:01:15.531644: step 22170, loss 0.166897, acc 0.9375
2017-03-02T18:01:15.610693: step 22171, loss 0.143291, acc 0.890625
2017-03-02T18:01:15.685125: step 22172, loss 0.0743009, acc 0.96875
2017-03-02T18:01:15.758183: step 22173, loss 0.130485, acc 0.953125
2017-03-02T18:01:15.829556: step 22174, loss 0.30001, acc 0.875
2017-03-02T18:01:15.902128: step 22175, loss 0.0395905, acc 1
2017-03-02T18:01:15.976983: step 22176, loss 0.186422, acc 0.921875
2017-03-02T18:01:16.048933: step 22177, loss 0.164619, acc 0.9375
2017-03-02T18:01:16.120402: step 22178, loss 0.158212, acc 0.9375
2017-03-02T18:01:16.202865: step 22179, loss 0.140657, acc 0.921875
2017-03-02T18:01:16.277504: step 22180, loss 0.0733442, acc 0.96875
2017-03-02T18:01:16.349358: step 22181, loss 0.167847, acc 0.953125
2017-03-02T18:01:16.424875: step 22182, loss 0.0771895, acc 0.96875
2017-03-02T18:01:16.490476: step 22183, loss 0.151829, acc 0.921875
2017-03-02T18:01:16.559312: step 22184, loss 0.114925, acc 0.96875
2017-03-02T18:01:16.633042: step 22185, loss 0.183869, acc 0.90625
2017-03-02T18:01:16.705114: step 22186, loss 0.0905174, acc 0.9375
2017-03-02T18:01:16.776672: step 22187, loss 0.0990877, acc 0.9375
2017-03-02T18:01:16.849691: step 22188, loss 0.297357, acc 0.84375
2017-03-02T18:01:16.922285: step 22189, loss 0.208214, acc 0.90625
2017-03-02T18:01:16.996970: step 22190, loss 0.0882472, acc 0.953125
2017-03-02T18:01:17.066152: step 22191, loss 0.236814, acc 0.890625
2017-03-02T18:01:17.162697: step 22192, loss 0.13465, acc 0.9375
2017-03-02T18:01:17.231134: step 22193, loss 0.224854, acc 0.890625
2017-03-02T18:01:17.302373: step 22194, loss 0.13501, acc 0.9375
2017-03-02T18:01:17.379630: step 22195, loss 0.151908, acc 0.921875
2017-03-02T18:01:17.460854: step 22196, loss 0.144063, acc 0.953125
2017-03-02T18:01:17.535946: step 22197, loss 0.24222, acc 0.90625
2017-03-02T18:01:17.613757: step 22198, loss 0.125025, acc 0.96875
2017-03-02T18:01:17.682062: step 22199, loss 0.263474, acc 0.890625
2017-03-02T18:01:17.752883: step 22200, loss 0.0627527, acc 0.953125

Evaluation:
2017-03-02T18:01:17.788349: step 22200, loss 2.63433, acc 0.651045

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22200

2017-03-02T18:01:18.251755: step 22201, loss 0.134153, acc 0.96875
2017-03-02T18:01:18.325007: step 22202, loss 0.223861, acc 0.859375
2017-03-02T18:01:18.395004: step 22203, loss 0.0868778, acc 0.953125
2017-03-02T18:01:18.466569: step 22204, loss 0.170596, acc 0.921875
2017-03-02T18:01:18.537173: step 22205, loss 0.138185, acc 0.953125
2017-03-02T18:01:18.606628: step 22206, loss 0.197727, acc 0.90625
2017-03-02T18:01:18.672040: step 22207, loss 0.102039, acc 0.953125
2017-03-02T18:01:18.748251: step 22208, loss 0.228691, acc 0.90625
2017-03-02T18:01:18.818849: step 22209, loss 0.347516, acc 0.875
2017-03-02T18:01:18.895326: step 22210, loss 0.16685, acc 0.953125
2017-03-02T18:01:18.956732: step 22211, loss 0.215007, acc 0.90625
2017-03-02T18:01:19.032743: step 22212, loss 0.102877, acc 0.953125
2017-03-02T18:01:19.105568: step 22213, loss 0.21618, acc 0.875
2017-03-02T18:01:19.183942: step 22214, loss 0.203022, acc 0.859375
2017-03-02T18:01:19.252027: step 22215, loss 0.166459, acc 0.921875
2017-03-02T18:01:19.322272: step 22216, loss 0.127007, acc 0.953125
2017-03-02T18:01:19.393405: step 22217, loss 0.157777, acc 0.90625
2017-03-02T18:01:19.461606: step 22218, loss 0.0978631, acc 0.96875
2017-03-02T18:01:19.540594: step 22219, loss 0.223755, acc 0.90625
2017-03-02T18:01:19.613378: step 22220, loss 0.146482, acc 0.953125
2017-03-02T18:01:19.684082: step 22221, loss 0.129735, acc 0.9375
2017-03-02T18:01:19.756401: step 22222, loss 0.10633, acc 0.96875
2017-03-02T18:01:19.828784: step 22223, loss 0.192826, acc 0.890625
2017-03-02T18:01:19.902153: step 22224, loss 0.153503, acc 0.953125
2017-03-02T18:01:19.973494: step 22225, loss 0.0941421, acc 0.96875
2017-03-02T18:01:20.043258: step 22226, loss 0.163647, acc 0.9375
2017-03-02T18:01:20.112049: step 22227, loss 0.0673146, acc 0.984375
2017-03-02T18:01:20.197140: step 22228, loss 0.114246, acc 0.953125
2017-03-02T18:01:20.289408: step 22229, loss 0.0980364, acc 0.96875
2017-03-02T18:01:20.359855: step 22230, loss 0.164643, acc 0.90625
2017-03-02T18:01:20.432917: step 22231, loss 0.230619, acc 0.90625
2017-03-02T18:01:20.510282: step 22232, loss 0.145182, acc 0.953125
2017-03-02T18:01:20.585739: step 22233, loss 0.14294, acc 0.921875
2017-03-02T18:01:20.654634: step 22234, loss 0.101101, acc 0.984375
2017-03-02T18:01:20.722559: step 22235, loss 0.0319264, acc 1
2017-03-02T18:01:20.792612: step 22236, loss 0.13149, acc 0.96875
2017-03-02T18:01:20.878250: step 22237, loss 0.199462, acc 0.90625
2017-03-02T18:01:20.955849: step 22238, loss 0.22256, acc 0.890625
2017-03-02T18:01:21.034776: step 22239, loss 0.174214, acc 0.921875
2017-03-02T18:01:21.111322: step 22240, loss 0.115841, acc 0.921875
2017-03-02T18:01:21.178466: step 22241, loss 0.151969, acc 0.9375
2017-03-02T18:01:21.257479: step 22242, loss 0.191889, acc 0.90625
2017-03-02T18:01:21.330213: step 22243, loss 0.151764, acc 0.9375
2017-03-02T18:01:21.398752: step 22244, loss 0.230116, acc 0.90625
2017-03-02T18:01:21.469960: step 22245, loss 0.180795, acc 0.90625
2017-03-02T18:01:21.533780: step 22246, loss 0.122021, acc 0.984375
2017-03-02T18:01:21.605874: step 22247, loss 0.0857073, acc 0.984375
2017-03-02T18:01:21.677765: step 22248, loss 0.180075, acc 0.9375
2017-03-02T18:01:21.754431: step 22249, loss 0.0553252, acc 0.984375
2017-03-02T18:01:21.830213: step 22250, loss 0.16046, acc 0.953125
2017-03-02T18:01:21.897185: step 22251, loss 0.102334, acc 0.96875
2017-03-02T18:01:21.980194: step 22252, loss 0.210971, acc 0.890625
2017-03-02T18:01:22.043534: step 22253, loss 0.0920055, acc 0.984375
2017-03-02T18:01:22.118478: step 22254, loss 0.0300887, acc 1
2017-03-02T18:01:22.194471: step 22255, loss 0.132053, acc 0.90625
2017-03-02T18:01:22.274226: step 22256, loss 0.141705, acc 0.921875
2017-03-02T18:01:22.353330: step 22257, loss 0.156033, acc 0.9375
2017-03-02T18:01:22.424804: step 22258, loss 0.204186, acc 0.9375
2017-03-02T18:01:22.503151: step 22259, loss 0.208349, acc 0.9375
2017-03-02T18:01:22.575301: step 22260, loss 0.189925, acc 0.90625
2017-03-02T18:01:22.651616: step 22261, loss 0.144996, acc 0.9375
2017-03-02T18:01:22.725337: step 22262, loss 0.138415, acc 0.921875
2017-03-02T18:01:22.793511: step 22263, loss 0.214873, acc 0.875
2017-03-02T18:01:22.856386: step 22264, loss 0.0532966, acc 0.96875
2017-03-02T18:01:22.928158: step 22265, loss 0.0903863, acc 0.953125
2017-03-02T18:01:23.009007: step 22266, loss 0.236529, acc 0.921875
2017-03-02T18:01:23.082608: step 22267, loss 0.0992239, acc 0.9375
2017-03-02T18:01:23.157066: step 22268, loss 0.183831, acc 0.90625
2017-03-02T18:01:23.228614: step 22269, loss 0.135778, acc 0.953125
2017-03-02T18:01:23.297818: step 22270, loss 0.270614, acc 0.921875
2017-03-02T18:01:23.384408: step 22271, loss 0.0697855, acc 0.96875
2017-03-02T18:01:23.447940: step 22272, loss 0.124312, acc 0.953125
2017-03-02T18:01:23.523975: step 22273, loss 0.151411, acc 0.921875
2017-03-02T18:01:23.597049: step 22274, loss 0.0654422, acc 0.953125
2017-03-02T18:01:23.668700: step 22275, loss 0.0914101, acc 0.96875
2017-03-02T18:01:23.743255: step 22276, loss 0.117474, acc 0.953125
2017-03-02T18:01:23.816493: step 22277, loss 0.281776, acc 0.890625
2017-03-02T18:01:23.896408: step 22278, loss 0.0812577, acc 0.984375
2017-03-02T18:01:23.968528: step 22279, loss 0.158463, acc 0.921875
2017-03-02T18:01:24.039872: step 22280, loss 0.150149, acc 0.9375
2017-03-02T18:01:24.115901: step 22281, loss 0.171178, acc 0.90625
2017-03-02T18:01:24.186981: step 22282, loss 0.0924281, acc 0.96875
2017-03-02T18:01:24.260775: step 22283, loss 0.103932, acc 0.953125
2017-03-02T18:01:24.337121: step 22284, loss 0.156078, acc 0.9375
2017-03-02T18:01:24.435381: step 22285, loss 0.182432, acc 0.921875
2017-03-02T18:01:24.504645: step 22286, loss 0.13251, acc 0.953125
2017-03-02T18:01:24.594165: step 22287, loss 0.127477, acc 0.9375
2017-03-02T18:01:24.661856: step 22288, loss 0.148316, acc 0.921875
2017-03-02T18:01:24.734130: step 22289, loss 0.219019, acc 0.890625
2017-03-02T18:01:24.810715: step 22290, loss 0.0833592, acc 0.953125
2017-03-02T18:01:24.889179: step 22291, loss 0.0707419, acc 0.984375
2017-03-02T18:01:24.962338: step 22292, loss 0.171455, acc 0.9375
2017-03-02T18:01:25.037725: step 22293, loss 0.153956, acc 0.921875
2017-03-02T18:01:25.109543: step 22294, loss 0.1219, acc 0.953125
2017-03-02T18:01:25.173138: step 22295, loss 0.145134, acc 0.90625
2017-03-02T18:01:25.244451: step 22296, loss 0.0640203, acc 0.96875
2017-03-02T18:01:25.317048: step 22297, loss 0.243122, acc 0.890625
2017-03-02T18:01:25.391530: step 22298, loss 0.121706, acc 0.953125
2017-03-02T18:01:25.463939: step 22299, loss 0.207158, acc 0.890625
2017-03-02T18:01:25.538672: step 22300, loss 0.220221, acc 0.9375

Evaluation:
2017-03-02T18:01:25.579519: step 22300, loss 2.70288, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22300

2017-03-02T18:01:26.029789: step 22301, loss 0.142471, acc 0.921875
2017-03-02T18:01:26.109488: step 22302, loss 0.19171, acc 0.890625
2017-03-02T18:01:26.175639: step 22303, loss 0.104721, acc 0.953125
2017-03-02T18:01:26.245371: step 22304, loss 0.160825, acc 0.90625
2017-03-02T18:01:26.315723: step 22305, loss 0.096504, acc 0.96875
2017-03-02T18:01:26.384495: step 22306, loss 0.0794267, acc 0.984375
2017-03-02T18:01:26.452783: step 22307, loss 0.233541, acc 0.890625
2017-03-02T18:01:26.526881: step 22308, loss 0.179215, acc 0.921875
2017-03-02T18:01:26.597729: step 22309, loss 0.202293, acc 0.921875
2017-03-02T18:01:26.668059: step 22310, loss 0.106892, acc 0.953125
2017-03-02T18:01:26.742162: step 22311, loss 0.130357, acc 0.921875
2017-03-02T18:01:26.808748: step 22312, loss 0.16841, acc 0.9375
2017-03-02T18:01:26.878259: step 22313, loss 0.167444, acc 0.90625
2017-03-02T18:01:26.944416: step 22314, loss 0.154609, acc 0.921875
2017-03-02T18:01:27.010611: step 22315, loss 0.194223, acc 0.90625
2017-03-02T18:01:27.078188: step 22316, loss 0.129078, acc 0.953125
2017-03-02T18:01:27.148465: step 22317, loss 0.111497, acc 0.953125
2017-03-02T18:01:27.230140: step 22318, loss 0.14905, acc 0.921875
2017-03-02T18:01:27.303507: step 22319, loss 0.131418, acc 0.953125
2017-03-02T18:01:27.379317: step 22320, loss 0.141601, acc 0.9375
2017-03-02T18:01:27.455025: step 22321, loss 0.127445, acc 0.9375
2017-03-02T18:01:27.528368: step 22322, loss 0.21529, acc 0.859375
2017-03-02T18:01:27.610643: step 22323, loss 0.175548, acc 0.9375
2017-03-02T18:01:27.677698: step 22324, loss 0.278267, acc 0.84375
2017-03-02T18:01:27.760223: step 22325, loss 0.155355, acc 0.9375
2017-03-02T18:01:27.832609: step 22326, loss 0.096703, acc 0.953125
2017-03-02T18:01:27.905998: step 22327, loss 0.0865735, acc 0.9375
2017-03-02T18:01:27.976484: step 22328, loss 0.107561, acc 0.953125
2017-03-02T18:01:28.047940: step 22329, loss 0.174571, acc 0.953125
2017-03-02T18:01:28.119469: step 22330, loss 0.163282, acc 0.90625
2017-03-02T18:01:28.191530: step 22331, loss 0.0777634, acc 0.96875
2017-03-02T18:01:28.268687: step 22332, loss 0.089792, acc 0.953125
2017-03-02T18:01:28.344453: step 22333, loss 0.102435, acc 0.96875
2017-03-02T18:01:28.427524: step 22334, loss 0.152369, acc 0.953125
2017-03-02T18:01:28.497127: step 22335, loss 0.118399, acc 0.921875
2017-03-02T18:01:28.568737: step 22336, loss 0.104456, acc 0.9375
2017-03-02T18:01:28.640232: step 22337, loss 0.14771, acc 0.90625
2017-03-02T18:01:28.713157: step 22338, loss 0.160786, acc 0.953125
2017-03-02T18:01:28.789841: step 22339, loss 0.0941435, acc 0.9375
2017-03-02T18:01:28.865936: step 22340, loss 0.201191, acc 0.859375
2017-03-02T18:01:28.932298: step 22341, loss 0.197887, acc 0.890625
2017-03-02T18:01:28.997994: step 22342, loss 0.202807, acc 0.90625
2017-03-02T18:01:29.070712: step 22343, loss 0.122771, acc 0.9375
2017-03-02T18:01:29.138611: step 22344, loss 2.38418e-07, acc 1
2017-03-02T18:01:29.213986: step 22345, loss 0.204718, acc 0.890625
2017-03-02T18:01:29.302826: step 22346, loss 0.146998, acc 0.9375
2017-03-02T18:01:29.376635: step 22347, loss 0.136232, acc 0.953125
2017-03-02T18:01:29.450565: step 22348, loss 0.121195, acc 0.953125
2017-03-02T18:01:29.523549: step 22349, loss 0.114778, acc 0.9375
2017-03-02T18:01:29.591799: step 22350, loss 0.230342, acc 0.90625
2017-03-02T18:01:29.656411: step 22351, loss 0.176201, acc 0.9375
2017-03-02T18:01:29.725618: step 22352, loss 0.148665, acc 0.90625
2017-03-02T18:01:29.803697: step 22353, loss 0.102481, acc 0.953125
2017-03-02T18:01:29.878776: step 22354, loss 0.147217, acc 0.9375
2017-03-02T18:01:29.949768: step 22355, loss 0.128871, acc 0.984375
2017-03-02T18:01:30.025979: step 22356, loss 0.193481, acc 0.921875
2017-03-02T18:01:30.098565: step 22357, loss 0.107285, acc 0.953125
2017-03-02T18:01:30.179367: step 22358, loss 0.141974, acc 0.921875
2017-03-02T18:01:30.254540: step 22359, loss 0.235161, acc 0.90625
2017-03-02T18:01:30.314843: step 22360, loss 0.085241, acc 0.96875
2017-03-02T18:01:30.377453: step 22361, loss 0.159847, acc 0.921875
2017-03-02T18:01:30.457212: step 22362, loss 0.216307, acc 0.9375
2017-03-02T18:01:30.533424: step 22363, loss 0.119909, acc 0.9375
2017-03-02T18:01:30.601431: step 22364, loss 0.141162, acc 0.921875
2017-03-02T18:01:30.671835: step 22365, loss 0.313795, acc 0.875
2017-03-02T18:01:30.756173: step 22366, loss 0.162809, acc 0.890625
2017-03-02T18:01:30.842416: step 22367, loss 0.117347, acc 0.953125
2017-03-02T18:01:30.918669: step 22368, loss 0.0918349, acc 0.9375
2017-03-02T18:01:30.986329: step 22369, loss 0.196249, acc 0.921875
2017-03-02T18:01:31.058227: step 22370, loss 0.136683, acc 0.953125
2017-03-02T18:01:31.134458: step 22371, loss 0.155124, acc 0.921875
2017-03-02T18:01:31.204418: step 22372, loss 0.149736, acc 0.9375
2017-03-02T18:01:31.277596: step 22373, loss 0.122784, acc 0.953125
2017-03-02T18:01:31.350067: step 22374, loss 0.142303, acc 0.9375
2017-03-02T18:01:31.438372: step 22375, loss 0.0498295, acc 0.96875
2017-03-02T18:01:31.507312: step 22376, loss 0.0774525, acc 0.96875
2017-03-02T18:01:31.583330: step 22377, loss 0.0741246, acc 0.96875
2017-03-02T18:01:31.659998: step 22378, loss 0.0657113, acc 0.96875
2017-03-02T18:01:31.734063: step 22379, loss 0.170822, acc 0.9375
2017-03-02T18:01:31.804680: step 22380, loss 0.0954379, acc 0.96875
2017-03-02T18:01:31.886055: step 22381, loss 0.110259, acc 0.9375
2017-03-02T18:01:31.974565: step 22382, loss 0.165446, acc 0.90625
2017-03-02T18:01:32.048640: step 22383, loss 0.0873496, acc 0.984375
2017-03-02T18:01:32.117856: step 22384, loss 0.0727029, acc 0.96875
2017-03-02T18:01:32.195432: step 22385, loss 0.0883753, acc 0.953125
2017-03-02T18:01:32.268123: step 22386, loss 0.0867915, acc 0.9375
2017-03-02T18:01:32.350050: step 22387, loss 0.319982, acc 0.84375
2017-03-02T18:01:32.419418: step 22388, loss 0.390689, acc 0.84375
2017-03-02T18:01:32.492126: step 22389, loss 0.128338, acc 0.90625
2017-03-02T18:01:32.562868: step 22390, loss 0.209023, acc 0.875
2017-03-02T18:01:32.635633: step 22391, loss 0.0894594, acc 0.96875
2017-03-02T18:01:32.708023: step 22392, loss 0.149818, acc 0.953125
2017-03-02T18:01:32.789755: step 22393, loss 0.19756, acc 0.9375
2017-03-02T18:01:32.863916: step 22394, loss 0.140584, acc 0.953125
2017-03-02T18:01:32.944506: step 22395, loss 0.167697, acc 0.9375
2017-03-02T18:01:33.022229: step 22396, loss 0.11871, acc 0.953125
2017-03-02T18:01:33.089169: step 22397, loss 0.0996395, acc 0.9375
2017-03-02T18:01:33.153858: step 22398, loss 0.216806, acc 0.890625
2017-03-02T18:01:33.223064: step 22399, loss 0.252713, acc 0.890625
2017-03-02T18:01:33.308088: step 22400, loss 0.148521, acc 0.921875

Evaluation:
2017-03-02T18:01:33.343884: step 22400, loss 2.69706, acc 0.625811

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22400

2017-03-02T18:01:33.781174: step 22401, loss 0.120097, acc 0.953125
2017-03-02T18:01:33.859562: step 22402, loss 0.135361, acc 0.953125
2017-03-02T18:01:33.934610: step 22403, loss 0.0668507, acc 0.984375
2017-03-02T18:01:34.010468: step 22404, loss 0.135208, acc 0.953125
2017-03-02T18:01:34.094328: step 22405, loss 0.128646, acc 0.9375
2017-03-02T18:01:34.167358: step 22406, loss 0.225666, acc 0.859375
2017-03-02T18:01:34.241131: step 22407, loss 0.0944625, acc 0.96875
2017-03-02T18:01:34.316384: step 22408, loss 0.100988, acc 0.953125
2017-03-02T18:01:34.396535: step 22409, loss 0.150784, acc 0.9375
2017-03-02T18:01:34.465252: step 22410, loss 0.0894501, acc 0.984375
2017-03-02T18:01:34.532333: step 22411, loss 0.0438509, acc 0.984375
2017-03-02T18:01:34.605116: step 22412, loss 0.166156, acc 0.9375
2017-03-02T18:01:34.685731: step 22413, loss 0.109935, acc 0.953125
2017-03-02T18:01:34.758783: step 22414, loss 0.156873, acc 0.921875
2017-03-02T18:01:34.831847: step 22415, loss 0.107886, acc 0.96875
2017-03-02T18:01:34.905610: step 22416, loss 0.165867, acc 0.9375
2017-03-02T18:01:34.982583: step 22417, loss 0.172374, acc 0.921875
2017-03-02T18:01:35.061313: step 22418, loss 0.0843483, acc 0.953125
2017-03-02T18:01:35.130037: step 22419, loss 0.166255, acc 0.921875
2017-03-02T18:01:35.200784: step 22420, loss 0.162239, acc 0.9375
2017-03-02T18:01:35.270484: step 22421, loss 0.178412, acc 0.9375
2017-03-02T18:01:35.347855: step 22422, loss 0.155029, acc 0.9375
2017-03-02T18:01:35.434027: step 22423, loss 0.203863, acc 0.953125
2017-03-02T18:01:35.504128: step 22424, loss 0.181774, acc 0.921875
2017-03-02T18:01:35.586293: step 22425, loss 0.229415, acc 0.890625
2017-03-02T18:01:35.673701: step 22426, loss 0.162861, acc 0.921875
2017-03-02T18:01:35.750201: step 22427, loss 0.201559, acc 0.953125
2017-03-02T18:01:35.818162: step 22428, loss 0.153671, acc 0.9375
2017-03-02T18:01:35.886610: step 22429, loss 0.164242, acc 0.921875
2017-03-02T18:01:35.954999: step 22430, loss 0.133661, acc 0.921875
2017-03-02T18:01:36.026348: step 22431, loss 0.165071, acc 0.953125
2017-03-02T18:01:36.099234: step 22432, loss 0.151513, acc 0.9375
2017-03-02T18:01:36.170771: step 22433, loss 0.103369, acc 0.984375
2017-03-02T18:01:36.243025: step 22434, loss 0.070778, acc 0.96875
2017-03-02T18:01:36.316864: step 22435, loss 0.218088, acc 0.875
2017-03-02T18:01:36.390367: step 22436, loss 0.117165, acc 0.9375
2017-03-02T18:01:36.464105: step 22437, loss 0.105685, acc 0.96875
2017-03-02T18:01:36.536070: step 22438, loss 0.167249, acc 0.921875
2017-03-02T18:01:36.604403: step 22439, loss 0.21355, acc 0.921875
2017-03-02T18:01:36.676210: step 22440, loss 0.106537, acc 0.96875
2017-03-02T18:01:36.755648: step 22441, loss 0.100777, acc 0.953125
2017-03-02T18:01:36.828155: step 22442, loss 0.0966594, acc 0.953125
2017-03-02T18:01:36.903102: step 22443, loss 0.228408, acc 0.890625
2017-03-02T18:01:36.979210: step 22444, loss 0.15647, acc 0.9375
2017-03-02T18:01:37.050515: step 22445, loss 0.281286, acc 0.875
2017-03-02T18:01:37.120405: step 22446, loss 0.140557, acc 0.953125
2017-03-02T18:01:37.193340: step 22447, loss 0.17603, acc 0.921875
2017-03-02T18:01:37.269724: step 22448, loss 0.0666096, acc 0.984375
2017-03-02T18:01:37.336208: step 22449, loss 0.099625, acc 0.96875
2017-03-02T18:01:37.427136: step 22450, loss 0.136331, acc 0.921875
2017-03-02T18:01:37.501070: step 22451, loss 0.199972, acc 0.890625
2017-03-02T18:01:37.574542: step 22452, loss 0.155987, acc 0.90625
2017-03-02T18:01:37.657748: step 22453, loss 0.117796, acc 0.9375
2017-03-02T18:01:37.740998: step 22454, loss 0.104276, acc 0.9375
2017-03-02T18:01:37.813800: step 22455, loss 0.107041, acc 0.96875
2017-03-02T18:01:37.901543: step 22456, loss 0.0796859, acc 0.96875
2017-03-02T18:01:37.990782: step 22457, loss 0.149534, acc 0.9375
2017-03-02T18:01:38.077240: step 22458, loss 0.128898, acc 0.953125
2017-03-02T18:01:38.167525: step 22459, loss 0.224833, acc 0.921875
2017-03-02T18:01:38.242266: step 22460, loss 0.131946, acc 0.9375
2017-03-02T18:01:38.318984: step 22461, loss 0.0844052, acc 0.953125
2017-03-02T18:01:38.393036: step 22462, loss 0.222867, acc 0.90625
2017-03-02T18:01:38.470987: step 22463, loss 0.101698, acc 0.9375
2017-03-02T18:01:38.547452: step 22464, loss 0.220925, acc 0.859375
2017-03-02T18:01:38.620812: step 22465, loss 0.155655, acc 0.90625
2017-03-02T18:01:38.694481: step 22466, loss 0.17935, acc 0.9375
2017-03-02T18:01:38.766525: step 22467, loss 0.303406, acc 0.875
2017-03-02T18:01:38.840120: step 22468, loss 0.121242, acc 0.9375
2017-03-02T18:01:38.912978: step 22469, loss 0.105138, acc 0.953125
2017-03-02T18:01:38.981468: step 22470, loss 0.118278, acc 0.953125
2017-03-02T18:01:39.059539: step 22471, loss 0.170275, acc 0.9375
2017-03-02T18:01:39.127772: step 22472, loss 0.171245, acc 0.90625
2017-03-02T18:01:39.204744: step 22473, loss 0.0653068, acc 0.96875
2017-03-02T18:01:39.279663: step 22474, loss 0.253513, acc 0.90625
2017-03-02T18:01:39.349479: step 22475, loss 0.189005, acc 0.90625
2017-03-02T18:01:39.420049: step 22476, loss 0.251225, acc 0.875
2017-03-02T18:01:39.495683: step 22477, loss 0.14721, acc 0.9375
2017-03-02T18:01:39.572436: step 22478, loss 0.0654917, acc 0.96875
2017-03-02T18:01:39.645411: step 22479, loss 0.130531, acc 0.9375
2017-03-02T18:01:39.716932: step 22480, loss 0.0916668, acc 0.96875
2017-03-02T18:01:39.808832: step 22481, loss 0.234345, acc 0.90625
2017-03-02T18:01:39.879589: step 22482, loss 0.197151, acc 0.921875
2017-03-02T18:01:39.949699: step 22483, loss 0.0987874, acc 0.96875
2017-03-02T18:01:40.018084: step 22484, loss 0.157781, acc 0.921875
2017-03-02T18:01:40.086912: step 22485, loss 0.142819, acc 0.9375
2017-03-02T18:01:40.157524: step 22486, loss 0.127113, acc 0.9375
2017-03-02T18:01:40.229596: step 22487, loss 0.13961, acc 0.9375
2017-03-02T18:01:40.298496: step 22488, loss 0.196828, acc 0.921875
2017-03-02T18:01:40.364296: step 22489, loss 0.177542, acc 0.90625
2017-03-02T18:01:40.435673: step 22490, loss 0.228128, acc 0.9375
2017-03-02T18:01:40.520190: step 22491, loss 0.105721, acc 0.9375
2017-03-02T18:01:40.596234: step 22492, loss 0.100024, acc 0.9375
2017-03-02T18:01:40.660801: step 22493, loss 0.194597, acc 0.90625
2017-03-02T18:01:40.720431: step 22494, loss 0.162823, acc 0.921875
2017-03-02T18:01:40.784604: step 22495, loss 0.12525, acc 0.9375
2017-03-02T18:01:40.857177: step 22496, loss 0.2607, acc 0.90625
2017-03-02T18:01:40.925713: step 22497, loss 0.088955, acc 0.96875
2017-03-02T18:01:40.995205: step 22498, loss 0.146245, acc 0.953125
2017-03-02T18:01:41.077943: step 22499, loss 0.183345, acc 0.953125
2017-03-02T18:01:41.153118: step 22500, loss 0.160903, acc 0.875

Evaluation:
2017-03-02T18:01:41.187415: step 22500, loss 2.70515, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22500

2017-03-02T18:01:41.634213: step 22501, loss 0.218336, acc 0.828125
2017-03-02T18:01:41.704551: step 22502, loss 0.143529, acc 0.921875
2017-03-02T18:01:41.780320: step 22503, loss 0.0668008, acc 0.96875
2017-03-02T18:01:41.852897: step 22504, loss 0.144393, acc 0.96875
2017-03-02T18:01:41.923222: step 22505, loss 0.106446, acc 0.953125
2017-03-02T18:01:41.994736: step 22506, loss 0.115906, acc 0.953125
2017-03-02T18:01:42.064214: step 22507, loss 0.0739961, acc 0.96875
2017-03-02T18:01:42.129845: step 22508, loss 0.129561, acc 0.9375
2017-03-02T18:01:42.205322: step 22509, loss 0.0597246, acc 0.984375
2017-03-02T18:01:42.282685: step 22510, loss 0.146373, acc 0.921875
2017-03-02T18:01:42.371030: step 22511, loss 0.129094, acc 0.921875
2017-03-02T18:01:42.443544: step 22512, loss 0.0452418, acc 0.984375
2017-03-02T18:01:42.519487: step 22513, loss 0.187266, acc 0.921875
2017-03-02T18:01:42.592389: step 22514, loss 0.250733, acc 0.90625
2017-03-02T18:01:42.664880: step 22515, loss 0.147301, acc 0.9375
2017-03-02T18:01:42.734492: step 22516, loss 0.196092, acc 0.875
2017-03-02T18:01:42.800633: step 22517, loss 0.161521, acc 0.9375
2017-03-02T18:01:42.876220: step 22518, loss 0.170201, acc 0.921875
2017-03-02T18:01:42.951343: step 22519, loss 0.224363, acc 0.9375
2017-03-02T18:01:43.024802: step 22520, loss 0.12459, acc 0.9375
2017-03-02T18:01:43.096843: step 22521, loss 0.135365, acc 0.921875
2017-03-02T18:01:43.169436: step 22522, loss 0.138915, acc 0.890625
2017-03-02T18:01:43.242008: step 22523, loss 0.182652, acc 0.890625
2017-03-02T18:01:43.315848: step 22524, loss 0.158096, acc 0.921875
2017-03-02T18:01:43.388894: step 22525, loss 0.145095, acc 0.9375
2017-03-02T18:01:43.454279: step 22526, loss 0.174357, acc 0.90625
2017-03-02T18:01:43.535755: step 22527, loss 0.225927, acc 0.859375
2017-03-02T18:01:43.609417: step 22528, loss 0.150405, acc 0.90625
2017-03-02T18:01:43.683364: step 22529, loss 0.286856, acc 0.890625
2017-03-02T18:01:43.757011: step 22530, loss 0.229065, acc 0.921875
2017-03-02T18:01:43.831022: step 22531, loss 0.159194, acc 0.90625
2017-03-02T18:01:43.899954: step 22532, loss 0.119518, acc 0.953125
2017-03-02T18:01:43.973227: step 22533, loss 0.131852, acc 0.921875
2017-03-02T18:01:44.048007: step 22534, loss 0.137637, acc 0.953125
2017-03-02T18:01:44.116203: step 22535, loss 0.284037, acc 0.890625
2017-03-02T18:01:44.183631: step 22536, loss 0.105492, acc 0.96875
2017-03-02T18:01:44.261571: step 22537, loss 0.134205, acc 0.890625
2017-03-02T18:01:44.335540: step 22538, loss 0.099938, acc 0.984375
2017-03-02T18:01:44.406520: step 22539, loss 0.109229, acc 1
2017-03-02T18:01:44.473160: step 22540, loss 0.158503, acc 1
2017-03-02T18:01:44.553050: step 22541, loss 0.13199, acc 0.953125
2017-03-02T18:01:44.622783: step 22542, loss 0.185339, acc 0.90625
2017-03-02T18:01:44.684084: step 22543, loss 0.120866, acc 0.921875
2017-03-02T18:01:44.761722: step 22544, loss 0.0270059, acc 0.984375
2017-03-02T18:01:44.834696: step 22545, loss 0.163692, acc 0.9375
2017-03-02T18:01:44.911612: step 22546, loss 0.11696, acc 0.96875
2017-03-02T18:01:44.987664: step 22547, loss 0.187474, acc 0.921875
2017-03-02T18:01:45.062675: step 22548, loss 0.183054, acc 0.890625
2017-03-02T18:01:45.139190: step 22549, loss 0.11326, acc 0.953125
2017-03-02T18:01:45.211887: step 22550, loss 0.134099, acc 0.953125
2017-03-02T18:01:45.286426: step 22551, loss 0.171835, acc 0.9375
2017-03-02T18:01:45.355955: step 22552, loss 0.138712, acc 0.921875
2017-03-02T18:01:45.435924: step 22553, loss 0.109576, acc 0.953125
2017-03-02T18:01:45.510214: step 22554, loss 0.246623, acc 0.90625
2017-03-02T18:01:45.580835: step 22555, loss 0.105003, acc 0.96875
2017-03-02T18:01:45.654053: step 22556, loss 0.183615, acc 0.890625
2017-03-02T18:01:45.725888: step 22557, loss 0.207782, acc 0.90625
2017-03-02T18:01:45.798471: step 22558, loss 0.0876587, acc 0.96875
2017-03-02T18:01:45.874338: step 22559, loss 0.175505, acc 0.890625
2017-03-02T18:01:45.946312: step 22560, loss 0.177865, acc 0.9375
2017-03-02T18:01:46.014181: step 22561, loss 0.176734, acc 0.953125
2017-03-02T18:01:46.087039: step 22562, loss 0.112693, acc 0.9375
2017-03-02T18:01:46.156849: step 22563, loss 0.166751, acc 0.921875
2017-03-02T18:01:46.227243: step 22564, loss 0.123755, acc 0.9375
2017-03-02T18:01:46.293614: step 22565, loss 0.220607, acc 0.9375
2017-03-02T18:01:46.362261: step 22566, loss 0.088668, acc 0.96875
2017-03-02T18:01:46.436203: step 22567, loss 0.111065, acc 0.953125
2017-03-02T18:01:46.508033: step 22568, loss 0.183682, acc 0.890625
2017-03-02T18:01:46.589998: step 22569, loss 0.168064, acc 0.921875
2017-03-02T18:01:46.663208: step 22570, loss 0.130342, acc 0.921875
2017-03-02T18:01:46.735978: step 22571, loss 0.16054, acc 0.9375
2017-03-02T18:01:46.805175: step 22572, loss 0.187792, acc 0.921875
2017-03-02T18:01:46.870001: step 22573, loss 0.117903, acc 0.953125
2017-03-02T18:01:46.930084: step 22574, loss 0.104306, acc 0.921875
2017-03-02T18:01:46.997323: step 22575, loss 0.149278, acc 0.90625
2017-03-02T18:01:47.078467: step 22576, loss 0.22921, acc 0.90625
2017-03-02T18:01:47.148349: step 22577, loss 0.0958227, acc 0.9375
2017-03-02T18:01:47.222265: step 22578, loss 0.166768, acc 0.9375
2017-03-02T18:01:47.293756: step 22579, loss 0.196725, acc 0.859375
2017-03-02T18:01:47.367313: step 22580, loss 0.0909542, acc 0.96875
2017-03-02T18:01:47.440222: step 22581, loss 0.160895, acc 0.90625
2017-03-02T18:01:47.513584: step 22582, loss 0.22491, acc 0.875
2017-03-02T18:01:47.588726: step 22583, loss 0.177624, acc 0.90625
2017-03-02T18:01:47.661828: step 22584, loss 0.219386, acc 0.953125
2017-03-02T18:01:47.729540: step 22585, loss 0.203266, acc 0.890625
2017-03-02T18:01:47.800898: step 22586, loss 0.0948112, acc 0.96875
2017-03-02T18:01:47.873038: step 22587, loss 0.107919, acc 0.9375
2017-03-02T18:01:47.951241: step 22588, loss 0.18733, acc 0.921875
2017-03-02T18:01:48.053880: step 22589, loss 0.118828, acc 0.9375
2017-03-02T18:01:48.121588: step 22590, loss 0.181238, acc 0.9375
2017-03-02T18:01:48.195153: step 22591, loss 0.131703, acc 0.9375
2017-03-02T18:01:48.277376: step 22592, loss 0.183163, acc 0.9375
2017-03-02T18:01:48.353196: step 22593, loss 0.0797889, acc 0.96875
2017-03-02T18:01:48.426284: step 22594, loss 0.289275, acc 0.875
2017-03-02T18:01:48.503191: step 22595, loss 0.133199, acc 0.921875
2017-03-02T18:01:48.581442: step 22596, loss 0.144974, acc 0.953125
2017-03-02T18:01:48.654689: step 22597, loss 0.215075, acc 0.859375
2017-03-02T18:01:48.728574: step 22598, loss 0.0866494, acc 0.96875
2017-03-02T18:01:48.801448: step 22599, loss 0.160669, acc 0.9375
2017-03-02T18:01:48.873269: step 22600, loss 0.135931, acc 0.9375

Evaluation:
2017-03-02T18:01:48.908634: step 22600, loss 2.70338, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22600

2017-03-02T18:01:49.355705: step 22601, loss 0.0932852, acc 0.953125
2017-03-02T18:01:49.425132: step 22602, loss 0.190545, acc 0.9375
2017-03-02T18:01:49.491848: step 22603, loss 0.0854607, acc 0.953125
2017-03-02T18:01:49.562620: step 22604, loss 0.156058, acc 0.921875
2017-03-02T18:01:49.636143: step 22605, loss 0.181147, acc 0.96875
2017-03-02T18:01:49.708871: step 22606, loss 0.169455, acc 0.875
2017-03-02T18:01:49.778554: step 22607, loss 0.122899, acc 0.90625
2017-03-02T18:01:49.846578: step 22608, loss 0.167874, acc 0.90625
2017-03-02T18:01:49.915454: step 22609, loss 0.138255, acc 0.9375
2017-03-02T18:01:49.987931: step 22610, loss 0.18409, acc 0.875
2017-03-02T18:01:50.055197: step 22611, loss 0.0979965, acc 0.96875
2017-03-02T18:01:50.134078: step 22612, loss 0.141436, acc 0.921875
2017-03-02T18:01:50.209554: step 22613, loss 0.149339, acc 0.90625
2017-03-02T18:01:50.283240: step 22614, loss 0.119975, acc 0.96875
2017-03-02T18:01:50.361016: step 22615, loss 0.202426, acc 0.875
2017-03-02T18:01:50.430114: step 22616, loss 0.0583641, acc 0.96875
2017-03-02T18:01:50.511196: step 22617, loss 0.0680647, acc 0.96875
2017-03-02T18:01:50.584249: step 22618, loss 0.11128, acc 0.921875
2017-03-02T18:01:50.658559: step 22619, loss 0.132478, acc 0.96875
2017-03-02T18:01:50.730078: step 22620, loss 0.156572, acc 0.9375
2017-03-02T18:01:50.804732: step 22621, loss 0.116227, acc 0.921875
2017-03-02T18:01:50.876350: step 22622, loss 0.196758, acc 0.90625
2017-03-02T18:01:50.947331: step 22623, loss 0.117864, acc 0.921875
2017-03-02T18:01:51.018336: step 22624, loss 0.19713, acc 0.859375
2017-03-02T18:01:51.091714: step 22625, loss 0.161585, acc 0.921875
2017-03-02T18:01:51.158127: step 22626, loss 0.0739156, acc 0.953125
2017-03-02T18:01:51.231029: step 22627, loss 0.16094, acc 0.9375
2017-03-02T18:01:51.302711: step 22628, loss 0.107367, acc 0.9375
2017-03-02T18:01:51.384135: step 22629, loss 0.308719, acc 0.890625
2017-03-02T18:01:51.460427: step 22630, loss 0.216905, acc 0.875
2017-03-02T18:01:51.544581: step 22631, loss 0.154017, acc 0.9375
2017-03-02T18:01:51.618829: step 22632, loss 0.0932661, acc 0.9375
2017-03-02T18:01:51.688847: step 22633, loss 0.254189, acc 0.90625
2017-03-02T18:01:51.770801: step 22634, loss 0.136513, acc 0.953125
2017-03-02T18:01:51.840502: step 22635, loss 0.11889, acc 0.953125
2017-03-02T18:01:51.907726: step 22636, loss 0.109068, acc 0.953125
2017-03-02T18:01:51.993215: step 22637, loss 0.217876, acc 0.953125
2017-03-02T18:01:52.091331: step 22638, loss 0.311435, acc 0.921875
2017-03-02T18:01:52.164143: step 22639, loss 0.15247, acc 0.890625
2017-03-02T18:01:52.237280: step 22640, loss 0.0758203, acc 0.953125
2017-03-02T18:01:52.325467: step 22641, loss 0.159103, acc 0.9375
2017-03-02T18:01:52.399908: step 22642, loss 0.0662669, acc 0.96875
2017-03-02T18:01:52.469330: step 22643, loss 0.261974, acc 0.90625
2017-03-02T18:01:52.541682: step 22644, loss 0.105771, acc 0.953125
2017-03-02T18:01:52.616587: step 22645, loss 0.220655, acc 0.890625
2017-03-02T18:01:52.688197: step 22646, loss 0.0980633, acc 0.953125
2017-03-02T18:01:52.759174: step 22647, loss 0.133002, acc 0.953125
2017-03-02T18:01:52.832824: step 22648, loss 0.165634, acc 0.921875
2017-03-02T18:01:52.906047: step 22649, loss 0.119484, acc 0.921875
2017-03-02T18:01:52.975400: step 22650, loss 0.197836, acc 0.921875
2017-03-02T18:01:53.053087: step 22651, loss 0.0853704, acc 0.96875
2017-03-02T18:01:53.123077: step 22652, loss 0.226388, acc 0.9375
2017-03-02T18:01:53.190296: step 22653, loss 0.135738, acc 0.921875
2017-03-02T18:01:53.258586: step 22654, loss 0.196357, acc 0.921875
2017-03-02T18:01:53.330501: step 22655, loss 0.137631, acc 0.875
2017-03-02T18:01:53.410792: step 22656, loss 0.101803, acc 0.9375
2017-03-02T18:01:53.480153: step 22657, loss 0.0706126, acc 0.96875
2017-03-02T18:01:53.551716: step 22658, loss 0.121733, acc 0.953125
2017-03-02T18:01:53.616856: step 22659, loss 0.162978, acc 0.921875
2017-03-02T18:01:53.693110: step 22660, loss 0.195731, acc 0.9375
2017-03-02T18:01:53.769888: step 22661, loss 0.0967534, acc 0.953125
2017-03-02T18:01:53.840135: step 22662, loss 0.180834, acc 0.921875
2017-03-02T18:01:53.904678: step 22663, loss 0.148139, acc 0.953125
2017-03-02T18:01:53.976678: step 22664, loss 0.205207, acc 0.921875
2017-03-02T18:01:54.050539: step 22665, loss 0.0798875, acc 0.96875
2017-03-02T18:01:54.129091: step 22666, loss 0.101543, acc 0.953125
2017-03-02T18:01:54.205565: step 22667, loss 0.159407, acc 0.9375
2017-03-02T18:01:54.280143: step 22668, loss 0.206359, acc 0.921875
2017-03-02T18:01:54.354843: step 22669, loss 0.1324, acc 0.921875
2017-03-02T18:01:54.431857: step 22670, loss 0.181518, acc 0.921875
2017-03-02T18:01:54.501013: step 22671, loss 0.146268, acc 0.9375
2017-03-02T18:01:54.572633: step 22672, loss 0.230701, acc 0.875
2017-03-02T18:01:54.642726: step 22673, loss 0.174076, acc 0.921875
2017-03-02T18:01:54.714663: step 22674, loss 0.270874, acc 0.890625
2017-03-02T18:01:54.787363: step 22675, loss 0.0923773, acc 0.953125
2017-03-02T18:01:54.863507: step 22676, loss 0.189791, acc 0.890625
2017-03-02T18:01:54.938792: step 22677, loss 0.120345, acc 0.953125
2017-03-02T18:01:55.026118: step 22678, loss 0.127058, acc 0.921875
2017-03-02T18:01:55.099450: step 22679, loss 0.124302, acc 0.9375
2017-03-02T18:01:55.177704: step 22680, loss 0.112217, acc 0.953125
2017-03-02T18:01:55.246867: step 22681, loss 0.128565, acc 0.921875
2017-03-02T18:01:55.314575: step 22682, loss 0.129669, acc 0.953125
2017-03-02T18:01:55.383545: step 22683, loss 0.173734, acc 0.9375
2017-03-02T18:01:55.460329: step 22684, loss 0.148668, acc 0.953125
2017-03-02T18:01:55.532609: step 22685, loss 0.15985, acc 0.90625
2017-03-02T18:01:55.609968: step 22686, loss 0.16401, acc 0.96875
2017-03-02T18:01:55.685587: step 22687, loss 0.17124, acc 0.9375
2017-03-02T18:01:55.756401: step 22688, loss 0.184153, acc 0.921875
2017-03-02T18:01:55.828208: step 22689, loss 0.12354, acc 0.953125
2017-03-02T18:01:55.901009: step 22690, loss 0.242613, acc 0.921875
2017-03-02T18:01:55.969680: step 22691, loss 0.0963443, acc 0.984375
2017-03-02T18:01:56.043203: step 22692, loss 0.17604, acc 0.9375
2017-03-02T18:01:56.118547: step 22693, loss 0.16866, acc 0.953125
2017-03-02T18:01:56.197585: step 22694, loss 0.114446, acc 0.953125
2017-03-02T18:01:56.275181: step 22695, loss 0.122226, acc 0.9375
2017-03-02T18:01:56.349656: step 22696, loss 0.146725, acc 0.921875
2017-03-02T18:01:56.417265: step 22697, loss 0.128131, acc 0.953125
2017-03-02T18:01:56.490798: step 22698, loss 0.203693, acc 0.921875
2017-03-02T18:01:56.578903: step 22699, loss 0.170193, acc 0.90625
2017-03-02T18:01:56.645897: step 22700, loss 0.139316, acc 0.921875

Evaluation:
2017-03-02T18:01:56.677936: step 22700, loss 2.70422, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22700

2017-03-02T18:01:57.158601: step 22701, loss 0.0768435, acc 0.953125
2017-03-02T18:01:57.232963: step 22702, loss 0.189272, acc 0.9375
2017-03-02T18:01:57.301486: step 22703, loss 0.120624, acc 0.921875
2017-03-02T18:01:57.376371: step 22704, loss 0.0913354, acc 0.921875
2017-03-02T18:01:57.450505: step 22705, loss 0.203795, acc 0.90625
2017-03-02T18:01:57.523750: step 22706, loss 0.205572, acc 0.921875
2017-03-02T18:01:57.597861: step 22707, loss 0.0684338, acc 0.984375
2017-03-02T18:01:57.671013: step 22708, loss 0.0710274, acc 0.953125
2017-03-02T18:01:57.747581: step 22709, loss 0.144438, acc 0.921875
2017-03-02T18:01:57.820671: step 22710, loss 0.239109, acc 0.890625
2017-03-02T18:01:57.908107: step 22711, loss 0.237041, acc 0.859375
2017-03-02T18:01:57.980627: step 22712, loss 0.14554, acc 0.921875
2017-03-02T18:01:58.045115: step 22713, loss 0.148749, acc 0.9375
2017-03-02T18:01:58.114723: step 22714, loss 0.17928, acc 0.9375
2017-03-02T18:01:58.184115: step 22715, loss 0.160998, acc 0.9375
2017-03-02T18:01:58.261829: step 22716, loss 0.187873, acc 0.921875
2017-03-02T18:01:58.336965: step 22717, loss 0.179195, acc 0.921875
2017-03-02T18:01:58.400796: step 22718, loss 0.102075, acc 0.953125
2017-03-02T18:01:58.473961: step 22719, loss 0.016088, acc 1
2017-03-02T18:01:58.551192: step 22720, loss 0.113795, acc 0.9375
2017-03-02T18:01:58.625221: step 22721, loss 0.225406, acc 0.90625
2017-03-02T18:01:58.701873: step 22722, loss 0.188981, acc 0.890625
2017-03-02T18:01:58.762516: step 22723, loss 0.153364, acc 0.9375
2017-03-02T18:01:58.846259: step 22724, loss 0.16515, acc 0.9375
2017-03-02T18:01:58.917328: step 22725, loss 0.119269, acc 0.921875
2017-03-02T18:01:58.993873: step 22726, loss 0.264162, acc 0.890625
2017-03-02T18:01:59.080685: step 22727, loss 0.104787, acc 0.9375
2017-03-02T18:01:59.152893: step 22728, loss 0.120163, acc 0.953125
2017-03-02T18:01:59.226304: step 22729, loss 0.304353, acc 0.875
2017-03-02T18:01:59.297657: step 22730, loss 0.0682817, acc 0.96875
2017-03-02T18:01:59.365477: step 22731, loss 0.0592909, acc 0.953125
2017-03-02T18:01:59.430654: step 22732, loss 0.219302, acc 0.890625
2017-03-02T18:01:59.501198: step 22733, loss 0.1101, acc 0.96875
2017-03-02T18:01:59.567954: step 22734, loss 0.127256, acc 0.9375
2017-03-02T18:01:59.636661: step 22735, loss 0.12071, acc 0.9375
2017-03-02T18:01:59.701891: step 22736, loss 3.12922e-06, acc 1
2017-03-02T18:01:59.772591: step 22737, loss 0.107839, acc 0.96875
2017-03-02T18:01:59.842641: step 22738, loss 0.145875, acc 0.921875
2017-03-02T18:01:59.912334: step 22739, loss 0.284287, acc 0.890625
2017-03-02T18:01:59.983909: step 22740, loss 0.169166, acc 0.90625
2017-03-02T18:02:00.052027: step 22741, loss 0.055148, acc 0.984375
2017-03-02T18:02:00.126452: step 22742, loss 0.139585, acc 0.953125
2017-03-02T18:02:00.196619: step 22743, loss 0.206893, acc 0.921875
2017-03-02T18:02:00.270106: step 22744, loss 0.0657684, acc 0.96875
2017-03-02T18:02:00.342714: step 22745, loss 0.0551917, acc 0.984375
2017-03-02T18:02:00.414778: step 22746, loss 0.0983155, acc 0.953125
2017-03-02T18:02:00.486126: step 22747, loss 0.119969, acc 0.921875
2017-03-02T18:02:00.559737: step 22748, loss 0.0543095, acc 0.984375
2017-03-02T18:02:00.631277: step 22749, loss 0.175593, acc 0.90625
2017-03-02T18:02:00.703741: step 22750, loss 0.154785, acc 0.921875
2017-03-02T18:02:00.769702: step 22751, loss 0.172413, acc 0.890625
2017-03-02T18:02:00.853168: step 22752, loss 0.0476451, acc 0.984375
2017-03-02T18:02:00.930793: step 22753, loss 0.171731, acc 0.921875
2017-03-02T18:02:01.014880: step 22754, loss 0.0699563, acc 0.9375
2017-03-02T18:02:01.087591: step 22755, loss 0.182093, acc 0.9375
2017-03-02T18:02:01.158376: step 22756, loss 0.14174, acc 0.921875
2017-03-02T18:02:01.239444: step 22757, loss 0.117135, acc 0.953125
2017-03-02T18:02:01.313889: step 22758, loss 0.0744274, acc 0.96875
2017-03-02T18:02:01.393666: step 22759, loss 0.092969, acc 0.96875
2017-03-02T18:02:01.469489: step 22760, loss 0.195468, acc 0.921875
2017-03-02T18:02:01.547667: step 22761, loss 0.0984009, acc 0.953125
2017-03-02T18:02:01.624649: step 22762, loss 0.141002, acc 0.9375
2017-03-02T18:02:01.709780: step 22763, loss 0.136632, acc 0.9375
2017-03-02T18:02:01.781507: step 22764, loss 0.114953, acc 0.953125
2017-03-02T18:02:01.853857: step 22765, loss 0.167155, acc 0.921875
2017-03-02T18:02:01.930172: step 22766, loss 0.127803, acc 0.96875
2017-03-02T18:02:02.000202: step 22767, loss 0.0953355, acc 0.984375
2017-03-02T18:02:02.071541: step 22768, loss 0.177801, acc 0.890625
2017-03-02T18:02:02.140766: step 22769, loss 0.19129, acc 0.921875
2017-03-02T18:02:02.211729: step 22770, loss 0.191033, acc 0.890625
2017-03-02T18:02:02.284420: step 22771, loss 0.0457771, acc 0.984375
2017-03-02T18:02:02.362387: step 22772, loss 0.260467, acc 0.921875
2017-03-02T18:02:02.445704: step 22773, loss 0.126782, acc 0.90625
2017-03-02T18:02:02.535771: step 22774, loss 0.0763994, acc 0.984375
2017-03-02T18:02:02.609189: step 22775, loss 0.134513, acc 0.9375
2017-03-02T18:02:02.680816: step 22776, loss 0.170281, acc 0.921875
2017-03-02T18:02:02.755887: step 22777, loss 0.202369, acc 0.890625
2017-03-02T18:02:02.838303: step 22778, loss 0.05666, acc 0.984375
2017-03-02T18:02:02.908376: step 22779, loss 0.189882, acc 0.9375
2017-03-02T18:02:02.980126: step 22780, loss 0.100977, acc 0.984375
2017-03-02T18:02:03.053745: step 22781, loss 0.194966, acc 0.875
2017-03-02T18:02:03.127609: step 22782, loss 0.195944, acc 0.90625
2017-03-02T18:02:03.200629: step 22783, loss 0.126456, acc 0.9375
2017-03-02T18:02:03.308599: step 22784, loss 0.252783, acc 0.890625
2017-03-02T18:02:03.381383: step 22785, loss 0.109391, acc 0.953125
2017-03-02T18:02:03.458947: step 22786, loss 0.0973322, acc 0.96875
2017-03-02T18:02:03.527455: step 22787, loss 0.134828, acc 0.90625
2017-03-02T18:02:03.596569: step 22788, loss 0.10182, acc 0.953125
2017-03-02T18:02:03.671520: step 22789, loss 0.146941, acc 0.90625
2017-03-02T18:02:03.742802: step 22790, loss 0.0865115, acc 0.984375
2017-03-02T18:02:03.815764: step 22791, loss 0.256708, acc 0.875
2017-03-02T18:02:03.890048: step 22792, loss 0.0955892, acc 0.9375
2017-03-02T18:02:03.963466: step 22793, loss 0.0951232, acc 0.953125
2017-03-02T18:02:04.040534: step 22794, loss 0.228786, acc 0.921875
2017-03-02T18:02:04.113149: step 22795, loss 0.186077, acc 0.9375
2017-03-02T18:02:04.187736: step 22796, loss 0.135422, acc 0.921875
2017-03-02T18:02:04.267261: step 22797, loss 0.106922, acc 0.953125
2017-03-02T18:02:04.345067: step 22798, loss 0.170411, acc 0.9375
2017-03-02T18:02:04.420673: step 22799, loss 0.436078, acc 0.84375
2017-03-02T18:02:04.492692: step 22800, loss 0.131967, acc 0.953125

Evaluation:
2017-03-02T18:02:04.528762: step 22800, loss 2.81539, acc 0.664744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22800

2017-03-02T18:02:05.044785: step 22801, loss 0.111239, acc 0.9375
2017-03-02T18:02:05.118588: step 22802, loss 0.0961666, acc 0.96875
2017-03-02T18:02:05.200546: step 22803, loss 0.237804, acc 0.890625
2017-03-02T18:02:05.278593: step 22804, loss 0.186067, acc 0.90625
2017-03-02T18:02:05.357177: step 22805, loss 0.0605391, acc 0.96875
2017-03-02T18:02:05.430561: step 22806, loss 0.179495, acc 0.9375
2017-03-02T18:02:05.510820: step 22807, loss 0.100486, acc 0.953125
2017-03-02T18:02:05.589018: step 22808, loss 0.126595, acc 0.9375
2017-03-02T18:02:05.657168: step 22809, loss 0.0670814, acc 0.953125
2017-03-02T18:02:05.718065: step 22810, loss 0.345606, acc 0.859375
2017-03-02T18:02:05.785205: step 22811, loss 0.181284, acc 0.953125
2017-03-02T18:02:05.854946: step 22812, loss 0.161727, acc 0.921875
2017-03-02T18:02:05.930947: step 22813, loss 0.106539, acc 0.9375
2017-03-02T18:02:06.000233: step 22814, loss 0.13767, acc 0.953125
2017-03-02T18:02:06.078576: step 22815, loss 0.0807698, acc 0.953125
2017-03-02T18:02:06.149890: step 22816, loss 0.260608, acc 0.890625
2017-03-02T18:02:06.221297: step 22817, loss 0.164456, acc 0.921875
2017-03-02T18:02:06.286922: step 22818, loss 0.263038, acc 0.875
2017-03-02T18:02:06.341674: step 22819, loss 0.167482, acc 0.921875
2017-03-02T18:02:06.413426: step 22820, loss 0.100687, acc 0.96875
2017-03-02T18:02:06.484849: step 22821, loss 0.128508, acc 0.9375
2017-03-02T18:02:06.568457: step 22822, loss 0.0737304, acc 0.96875
2017-03-02T18:02:06.641287: step 22823, loss 0.0835034, acc 0.953125
2017-03-02T18:02:06.721681: step 22824, loss 0.20814, acc 0.921875
2017-03-02T18:02:06.791828: step 22825, loss 0.271096, acc 0.875
2017-03-02T18:02:06.865664: step 22826, loss 0.205601, acc 0.921875
2017-03-02T18:02:06.932095: step 22827, loss 0.209757, acc 0.90625
2017-03-02T18:02:07.004438: step 22828, loss 0.109268, acc 0.921875
2017-03-02T18:02:07.075559: step 22829, loss 0.222669, acc 0.890625
2017-03-02T18:02:07.144524: step 22830, loss 0.132716, acc 0.9375
2017-03-02T18:02:07.215891: step 22831, loss 0.244653, acc 0.90625
2017-03-02T18:02:07.288131: step 22832, loss 0.0717832, acc 0.984375
2017-03-02T18:02:07.364491: step 22833, loss 0.17402, acc 0.890625
2017-03-02T18:02:07.439528: step 22834, loss 0.212513, acc 0.921875
2017-03-02T18:02:07.517693: step 22835, loss 0.21102, acc 0.875
2017-03-02T18:02:07.590411: step 22836, loss 0.138372, acc 0.9375
2017-03-02T18:02:07.664566: step 22837, loss 0.28729, acc 0.90625
2017-03-02T18:02:07.732887: step 22838, loss 0.272116, acc 0.90625
2017-03-02T18:02:07.811391: step 22839, loss 0.184594, acc 0.921875
2017-03-02T18:02:07.889368: step 22840, loss 0.146291, acc 0.953125
2017-03-02T18:02:07.962142: step 22841, loss 0.192848, acc 0.875
2017-03-02T18:02:08.035799: step 22842, loss 0.257633, acc 0.890625
2017-03-02T18:02:08.108189: step 22843, loss 0.125739, acc 0.921875
2017-03-02T18:02:08.178243: step 22844, loss 0.171344, acc 0.890625
2017-03-02T18:02:08.249113: step 22845, loss 0.17347, acc 0.921875
2017-03-02T18:02:08.324686: step 22846, loss 0.141535, acc 0.96875
2017-03-02T18:02:08.395112: step 22847, loss 0.157665, acc 0.921875
2017-03-02T18:02:08.467443: step 22848, loss 0.136112, acc 0.9375
2017-03-02T18:02:08.538974: step 22849, loss 0.162413, acc 0.9375
2017-03-02T18:02:08.618279: step 22850, loss 0.188351, acc 0.9375
2017-03-02T18:02:08.692340: step 22851, loss 0.0877563, acc 0.96875
2017-03-02T18:02:08.772328: step 22852, loss 0.219337, acc 0.90625
2017-03-02T18:02:08.839559: step 22853, loss 0.13252, acc 0.9375
2017-03-02T18:02:08.914466: step 22854, loss 0.119365, acc 0.953125
2017-03-02T18:02:08.985830: step 22855, loss 0.123779, acc 0.921875
2017-03-02T18:02:09.059413: step 22856, loss 0.05489, acc 0.984375
2017-03-02T18:02:09.133059: step 22857, loss 0.205719, acc 0.953125
2017-03-02T18:02:09.213265: step 22858, loss 0.101387, acc 0.96875
2017-03-02T18:02:09.280912: step 22859, loss 0.0755534, acc 0.96875
2017-03-02T18:02:09.353659: step 22860, loss 0.191666, acc 0.90625
2017-03-02T18:02:09.419788: step 22861, loss 0.0693843, acc 1
2017-03-02T18:02:09.497689: step 22862, loss 0.186728, acc 0.9375
2017-03-02T18:02:09.559592: step 22863, loss 0.125847, acc 0.953125
2017-03-02T18:02:09.629550: step 22864, loss 0.218502, acc 0.890625
2017-03-02T18:02:09.703613: step 22865, loss 0.116167, acc 0.9375
2017-03-02T18:02:09.768217: step 22866, loss 0.115955, acc 0.953125
2017-03-02T18:02:09.831583: step 22867, loss 0.291762, acc 0.859375
2017-03-02T18:02:09.903095: step 22868, loss 0.177534, acc 0.890625
2017-03-02T18:02:09.970801: step 22869, loss 0.154998, acc 0.9375
2017-03-02T18:02:10.040419: step 22870, loss 0.0929078, acc 0.984375
2017-03-02T18:02:10.108096: step 22871, loss 0.186834, acc 0.921875
2017-03-02T18:02:10.183285: step 22872, loss 0.0986715, acc 0.96875
2017-03-02T18:02:10.252485: step 22873, loss 0.21608, acc 0.875
2017-03-02T18:02:10.320596: step 22874, loss 0.212915, acc 0.921875
2017-03-02T18:02:10.392656: step 22875, loss 0.200999, acc 0.921875
2017-03-02T18:02:10.463212: step 22876, loss 0.152399, acc 0.953125
2017-03-02T18:02:10.548096: step 22877, loss 0.138963, acc 0.921875
2017-03-02T18:02:10.629973: step 22878, loss 0.159253, acc 0.90625
2017-03-02T18:02:10.721524: step 22879, loss 0.188743, acc 0.9375
2017-03-02T18:02:10.792607: step 22880, loss 0.125454, acc 0.9375
2017-03-02T18:02:10.872247: step 22881, loss 0.180982, acc 0.90625
2017-03-02T18:02:10.950794: step 22882, loss 0.0786842, acc 0.96875
2017-03-02T18:02:11.022376: step 22883, loss 0.341808, acc 0.859375
2017-03-02T18:02:11.103357: step 22884, loss 0.151759, acc 0.96875
2017-03-02T18:02:11.171298: step 22885, loss 0.0913704, acc 0.96875
2017-03-02T18:02:11.240737: step 22886, loss 0.125842, acc 0.96875
2017-03-02T18:02:11.317825: step 22887, loss 0.194433, acc 0.9375
2017-03-02T18:02:11.391980: step 22888, loss 0.30625, acc 0.875
2017-03-02T18:02:11.466325: step 22889, loss 0.281227, acc 0.875
2017-03-02T18:02:11.542226: step 22890, loss 0.130717, acc 0.953125
2017-03-02T18:02:11.616037: step 22891, loss 0.156352, acc 0.921875
2017-03-02T18:02:11.689829: step 22892, loss 0.161361, acc 0.921875
2017-03-02T18:02:11.764419: step 22893, loss 0.182217, acc 0.9375
2017-03-02T18:02:11.829910: step 22894, loss 0.120404, acc 0.96875
2017-03-02T18:02:11.903437: step 22895, loss 0.167408, acc 0.921875
2017-03-02T18:02:11.979276: step 22896, loss 0.187679, acc 0.90625
2017-03-02T18:02:12.052182: step 22897, loss 0.0371367, acc 0.984375
2017-03-02T18:02:12.124681: step 22898, loss 0.125529, acc 0.9375
2017-03-02T18:02:12.207859: step 22899, loss 0.07467, acc 0.96875
2017-03-02T18:02:12.280701: step 22900, loss 0.128848, acc 0.9375

Evaluation:
2017-03-02T18:02:12.315159: step 22900, loss 2.65551, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-22900

2017-03-02T18:02:12.786674: step 22901, loss 0.126711, acc 0.921875
2017-03-02T18:02:12.857175: step 22902, loss 0.136009, acc 0.9375
2017-03-02T18:02:12.930809: step 22903, loss 0.187282, acc 0.90625
2017-03-02T18:02:13.003197: step 22904, loss 0.104248, acc 0.96875
2017-03-02T18:02:13.074653: step 22905, loss 0.389484, acc 0.859375
2017-03-02T18:02:13.142742: step 22906, loss 0.0994984, acc 0.96875
2017-03-02T18:02:13.205608: step 22907, loss 0.319029, acc 0.875
2017-03-02T18:02:13.275807: step 22908, loss 0.20035, acc 0.890625
2017-03-02T18:02:13.345893: step 22909, loss 0.207683, acc 0.859375
2017-03-02T18:02:13.416745: step 22910, loss 0.186664, acc 0.953125
2017-03-02T18:02:13.496561: step 22911, loss 0.162553, acc 0.90625
2017-03-02T18:02:13.566404: step 22912, loss 0.221233, acc 0.90625
2017-03-02T18:02:13.664382: step 22913, loss 0.15085, acc 0.921875
2017-03-02T18:02:13.746046: step 22914, loss 0.196648, acc 0.90625
2017-03-02T18:02:13.821987: step 22915, loss 0.100217, acc 0.96875
2017-03-02T18:02:13.892962: step 22916, loss 0.0823542, acc 0.96875
2017-03-02T18:02:13.957533: step 22917, loss 0.0495483, acc 0.984375
2017-03-02T18:02:14.031352: step 22918, loss 0.147975, acc 0.9375
2017-03-02T18:02:14.117396: step 22919, loss 0.131993, acc 0.9375
2017-03-02T18:02:14.193200: step 22920, loss 0.141037, acc 0.921875
2017-03-02T18:02:14.266342: step 22921, loss 0.0797675, acc 0.96875
2017-03-02T18:02:14.335282: step 22922, loss 0.186766, acc 0.953125
2017-03-02T18:02:14.405736: step 22923, loss 0.108604, acc 0.96875
2017-03-02T18:02:14.488576: step 22924, loss 0.044568, acc 0.984375
2017-03-02T18:02:14.567383: step 22925, loss 0.254456, acc 0.875
2017-03-02T18:02:14.640656: step 22926, loss 0.0949259, acc 0.953125
2017-03-02T18:02:14.719504: step 22927, loss 0.160192, acc 0.921875
2017-03-02T18:02:14.801708: step 22928, loss 0.151988, acc 0.921875
2017-03-02T18:02:14.877893: step 22929, loss 0.114545, acc 0.96875
2017-03-02T18:02:14.948850: step 22930, loss 0.101335, acc 0.9375
2017-03-02T18:02:15.013483: step 22931, loss 0.214854, acc 0.90625
2017-03-02T18:02:15.077008: step 22932, loss 1.81492e-05, acc 1
2017-03-02T18:02:15.159712: step 22933, loss 0.138558, acc 0.96875
2017-03-02T18:02:15.238098: step 22934, loss 0.117063, acc 0.9375
2017-03-02T18:02:15.305003: step 22935, loss 0.076868, acc 0.96875
2017-03-02T18:02:15.383271: step 22936, loss 0.106016, acc 0.953125
2017-03-02T18:02:15.451074: step 22937, loss 0.126219, acc 0.921875
2017-03-02T18:02:15.521916: step 22938, loss 0.109752, acc 0.953125
2017-03-02T18:02:15.594406: step 22939, loss 0.244047, acc 0.890625
2017-03-02T18:02:15.666622: step 22940, loss 0.126177, acc 0.953125
2017-03-02T18:02:15.737637: step 22941, loss 0.37029, acc 0.828125
2017-03-02T18:02:15.810460: step 22942, loss 0.216817, acc 0.90625
2017-03-02T18:02:15.882097: step 22943, loss 0.131882, acc 0.90625
2017-03-02T18:02:15.948752: step 22944, loss 0.0598985, acc 0.984375
2017-03-02T18:02:16.014493: step 22945, loss 0.11926, acc 0.921875
2017-03-02T18:02:16.083647: step 22946, loss 0.137674, acc 0.9375
2017-03-02T18:02:16.168569: step 22947, loss 0.115748, acc 0.96875
2017-03-02T18:02:16.243816: step 22948, loss 0.119065, acc 0.921875
2017-03-02T18:02:16.324667: step 22949, loss 0.12066, acc 0.921875
2017-03-02T18:02:16.400168: step 22950, loss 0.125542, acc 0.953125
2017-03-02T18:02:16.484495: step 22951, loss 0.125957, acc 0.9375
2017-03-02T18:02:16.561289: step 22952, loss 0.198534, acc 0.921875
2017-03-02T18:02:16.630838: step 22953, loss 0.140948, acc 0.90625
2017-03-02T18:02:16.703844: step 22954, loss 0.182437, acc 0.953125
2017-03-02T18:02:16.773090: step 22955, loss 0.136179, acc 0.921875
2017-03-02T18:02:16.841330: step 22956, loss 0.138034, acc 0.9375
2017-03-02T18:02:16.912685: step 22957, loss 0.262489, acc 0.859375
2017-03-02T18:02:16.983118: step 22958, loss 0.0368606, acc 0.984375
2017-03-02T18:02:17.060885: step 22959, loss 0.102834, acc 0.984375
2017-03-02T18:02:17.134171: step 22960, loss 0.0619003, acc 0.984375
2017-03-02T18:02:17.206635: step 22961, loss 0.0197729, acc 1
2017-03-02T18:02:17.285497: step 22962, loss 0.175054, acc 0.90625
2017-03-02T18:02:17.349476: step 22963, loss 0.0592953, acc 0.953125
2017-03-02T18:02:17.418717: step 22964, loss 0.107639, acc 0.953125
2017-03-02T18:02:17.492427: step 22965, loss 0.152001, acc 0.9375
2017-03-02T18:02:17.566367: step 22966, loss 0.140369, acc 0.921875
2017-03-02T18:02:17.644796: step 22967, loss 0.225713, acc 0.890625
2017-03-02T18:02:17.721620: step 22968, loss 0.133795, acc 0.96875
2017-03-02T18:02:17.795513: step 22969, loss 0.120895, acc 0.921875
2017-03-02T18:02:17.867611: step 22970, loss 0.111547, acc 0.9375
2017-03-02T18:02:17.950075: step 22971, loss 0.0697166, acc 0.96875
2017-03-02T18:02:18.019352: step 22972, loss 0.142673, acc 0.921875
2017-03-02T18:02:18.088324: step 22973, loss 0.0879163, acc 0.984375
2017-03-02T18:02:18.163064: step 22974, loss 0.125662, acc 0.953125
2017-03-02T18:02:18.237601: step 22975, loss 0.161493, acc 0.953125
2017-03-02T18:02:18.310881: step 22976, loss 0.141755, acc 0.921875
2017-03-02T18:02:18.384955: step 22977, loss 0.126816, acc 0.953125
2017-03-02T18:02:18.458239: step 22978, loss 0.0850377, acc 1
2017-03-02T18:02:18.523964: step 22979, loss 0.142262, acc 0.9375
2017-03-02T18:02:18.595376: step 22980, loss 0.153099, acc 0.9375
2017-03-02T18:02:18.671166: step 22981, loss 0.196901, acc 0.921875
2017-03-02T18:02:18.738402: step 22982, loss 0.182757, acc 0.90625
2017-03-02T18:02:18.806476: step 22983, loss 0.148007, acc 0.9375
2017-03-02T18:02:18.878720: step 22984, loss 0.112503, acc 0.921875
2017-03-02T18:02:18.952105: step 22985, loss 0.186559, acc 0.9375
2017-03-02T18:02:19.029246: step 22986, loss 0.268676, acc 0.921875
2017-03-02T18:02:19.105372: step 22987, loss 0.16599, acc 0.96875
2017-03-02T18:02:19.187936: step 22988, loss 0.194675, acc 0.90625
2017-03-02T18:02:19.262701: step 22989, loss 0.13122, acc 0.96875
2017-03-02T18:02:19.346127: step 22990, loss 0.181982, acc 0.90625
2017-03-02T18:02:19.416391: step 22991, loss 0.220639, acc 0.90625
2017-03-02T18:02:19.509323: step 22992, loss 0.148146, acc 0.921875
2017-03-02T18:02:19.581203: step 22993, loss 0.148768, acc 0.9375
2017-03-02T18:02:19.651638: step 22994, loss 0.127512, acc 0.921875
2017-03-02T18:02:19.728706: step 22995, loss 0.163603, acc 0.90625
2017-03-02T18:02:19.804674: step 22996, loss 0.106626, acc 0.953125
2017-03-02T18:02:19.877010: step 22997, loss 0.21835, acc 0.9375
2017-03-02T18:02:19.954153: step 22998, loss 0.216974, acc 0.921875
2017-03-02T18:02:20.031824: step 22999, loss 0.196011, acc 0.921875
2017-03-02T18:02:20.103394: step 23000, loss 0.0990122, acc 0.953125

Evaluation:
2017-03-02T18:02:20.141152: step 23000, loss 2.67793, acc 0.636626

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23000

2017-03-02T18:02:20.593654: step 23001, loss 0.0770135, acc 0.953125
2017-03-02T18:02:20.671635: step 23002, loss 0.200846, acc 0.90625
2017-03-02T18:02:20.743152: step 23003, loss 0.0718825, acc 0.984375
2017-03-02T18:02:20.813185: step 23004, loss 0.12699, acc 0.9375
2017-03-02T18:02:20.881636: step 23005, loss 0.109842, acc 0.953125
2017-03-02T18:02:20.956439: step 23006, loss 0.185824, acc 0.90625
2017-03-02T18:02:21.044825: step 23007, loss 0.133929, acc 0.90625
2017-03-02T18:02:21.117976: step 23008, loss 0.165629, acc 0.9375
2017-03-02T18:02:21.192509: step 23009, loss 0.101655, acc 0.953125
2017-03-02T18:02:21.274158: step 23010, loss 0.132534, acc 0.96875
2017-03-02T18:02:21.345086: step 23011, loss 0.0787073, acc 0.96875
2017-03-02T18:02:21.416061: step 23012, loss 0.280649, acc 0.890625
2017-03-02T18:02:21.497182: step 23013, loss 0.0872201, acc 0.984375
2017-03-02T18:02:21.560386: step 23014, loss 0.14006, acc 0.9375
2017-03-02T18:02:21.633377: step 23015, loss 0.112475, acc 0.953125
2017-03-02T18:02:21.708227: step 23016, loss 0.168232, acc 0.9375
2017-03-02T18:02:21.789195: step 23017, loss 0.18751, acc 0.921875
2017-03-02T18:02:21.862813: step 23018, loss 0.14004, acc 0.953125
2017-03-02T18:02:21.934445: step 23019, loss 0.0545025, acc 0.984375
2017-03-02T18:02:22.015333: step 23020, loss 0.0385452, acc 0.984375
2017-03-02T18:02:22.093584: step 23021, loss 0.146632, acc 0.953125
2017-03-02T18:02:22.161595: step 23022, loss 0.208992, acc 0.890625
2017-03-02T18:02:22.229097: step 23023, loss 0.157347, acc 0.921875
2017-03-02T18:02:22.299695: step 23024, loss 0.215722, acc 0.875
2017-03-02T18:02:22.374237: step 23025, loss 0.200339, acc 0.90625
2017-03-02T18:02:22.458134: step 23026, loss 0.248307, acc 0.921875
2017-03-02T18:02:22.533040: step 23027, loss 0.150668, acc 0.9375
2017-03-02T18:02:22.602006: step 23028, loss 0.330803, acc 0.890625
2017-03-02T18:02:22.674359: step 23029, loss 0.162151, acc 0.921875
2017-03-02T18:02:22.749012: step 23030, loss 0.132062, acc 0.9375
2017-03-02T18:02:22.825485: step 23031, loss 0.0708099, acc 0.984375
2017-03-02T18:02:22.888271: step 23032, loss 0.10915, acc 0.9375
2017-03-02T18:02:22.958632: step 23033, loss 0.235152, acc 0.921875
2017-03-02T18:02:23.033244: step 23034, loss 0.132629, acc 0.953125
2017-03-02T18:02:23.105505: step 23035, loss 0.113648, acc 0.953125
2017-03-02T18:02:23.177150: step 23036, loss 0.12094, acc 0.9375
2017-03-02T18:02:23.248946: step 23037, loss 0.173207, acc 0.96875
2017-03-02T18:02:23.321440: step 23038, loss 0.176569, acc 0.921875
2017-03-02T18:02:23.395215: step 23039, loss 0.116474, acc 0.9375
2017-03-02T18:02:23.467774: step 23040, loss 0.146107, acc 0.9375
2017-03-02T18:02:23.536642: step 23041, loss 0.160653, acc 0.90625
2017-03-02T18:02:23.606451: step 23042, loss 0.196744, acc 0.9375
2017-03-02T18:02:23.678238: step 23043, loss 0.237499, acc 0.890625
2017-03-02T18:02:23.752881: step 23044, loss 0.192634, acc 0.90625
2017-03-02T18:02:23.834390: step 23045, loss 0.186244, acc 0.921875
2017-03-02T18:02:23.905129: step 23046, loss 0.172053, acc 0.921875
2017-03-02T18:02:23.976325: step 23047, loss 0.121932, acc 0.9375
2017-03-02T18:02:24.046086: step 23048, loss 0.188349, acc 0.890625
2017-03-02T18:02:24.115493: step 23049, loss 0.142591, acc 0.9375
2017-03-02T18:02:24.189183: step 23050, loss 0.0750936, acc 0.953125
2017-03-02T18:02:24.258891: step 23051, loss 0.0998046, acc 0.921875
2017-03-02T18:02:24.327690: step 23052, loss 0.168451, acc 0.890625
2017-03-02T18:02:24.398749: step 23053, loss 0.25286, acc 0.859375
2017-03-02T18:02:24.478939: step 23054, loss 0.0889556, acc 0.96875
2017-03-02T18:02:24.548552: step 23055, loss 0.196084, acc 0.953125
2017-03-02T18:02:24.614699: step 23056, loss 0.129314, acc 0.921875
2017-03-02T18:02:24.703492: step 23057, loss 0.191547, acc 0.890625
2017-03-02T18:02:24.774899: step 23058, loss 0.274522, acc 0.90625
2017-03-02T18:02:24.854134: step 23059, loss 0.202332, acc 0.890625
2017-03-02T18:02:24.928436: step 23060, loss 0.126462, acc 0.9375
2017-03-02T18:02:25.010456: step 23061, loss 0.146152, acc 0.921875
2017-03-02T18:02:25.095133: step 23062, loss 0.14314, acc 0.921875
2017-03-02T18:02:25.171351: step 23063, loss 0.356182, acc 0.875
2017-03-02T18:02:25.243952: step 23064, loss 0.083548, acc 0.96875
2017-03-02T18:02:25.316721: step 23065, loss 0.179442, acc 0.921875
2017-03-02T18:02:25.403129: step 23066, loss 0.180056, acc 0.90625
2017-03-02T18:02:25.477537: step 23067, loss 0.14078, acc 0.921875
2017-03-02T18:02:25.553906: step 23068, loss 0.296393, acc 0.90625
2017-03-02T18:02:25.631983: step 23069, loss 0.101548, acc 0.953125
2017-03-02T18:02:25.700781: step 23070, loss 0.0765352, acc 0.953125
2017-03-02T18:02:25.767648: step 23071, loss 0.19994, acc 0.875
2017-03-02T18:02:25.841377: step 23072, loss 0.127216, acc 0.953125
2017-03-02T18:02:25.915877: step 23073, loss 0.100029, acc 0.9375
2017-03-02T18:02:25.990582: step 23074, loss 0.103743, acc 0.953125
2017-03-02T18:02:26.060855: step 23075, loss 0.181729, acc 0.90625
2017-03-02T18:02:26.132566: step 23076, loss 0.130018, acc 0.921875
2017-03-02T18:02:26.202194: step 23077, loss 0.216334, acc 0.90625
2017-03-02T18:02:26.278962: step 23078, loss 0.132048, acc 0.921875
2017-03-02T18:02:26.357619: step 23079, loss 0.150885, acc 0.921875
2017-03-02T18:02:26.429110: step 23080, loss 0.14737, acc 0.921875
2017-03-02T18:02:26.503912: step 23081, loss 0.166198, acc 0.9375
2017-03-02T18:02:26.567121: step 23082, loss 0.0819898, acc 0.96875
2017-03-02T18:02:26.642725: step 23083, loss 0.17625, acc 0.90625
2017-03-02T18:02:26.716368: step 23084, loss 0.145398, acc 0.96875
2017-03-02T18:02:26.783938: step 23085, loss 0.103782, acc 0.96875
2017-03-02T18:02:26.859632: step 23086, loss 0.116094, acc 0.9375
2017-03-02T18:02:26.935094: step 23087, loss 0.0693022, acc 1
2017-03-02T18:02:27.006100: step 23088, loss 0.266656, acc 0.859375
2017-03-02T18:02:27.073005: step 23089, loss 0.182024, acc 0.90625
2017-03-02T18:02:27.141472: step 23090, loss 0.268496, acc 0.890625
2017-03-02T18:02:27.221698: step 23091, loss 0.23207, acc 0.921875
2017-03-02T18:02:27.299640: step 23092, loss 0.08339, acc 0.953125
2017-03-02T18:02:27.378790: step 23093, loss 0.092914, acc 0.953125
2017-03-02T18:02:27.456529: step 23094, loss 0.206931, acc 0.953125
2017-03-02T18:02:27.534592: step 23095, loss 0.222181, acc 0.890625
2017-03-02T18:02:27.606254: step 23096, loss 0.111192, acc 0.953125
2017-03-02T18:02:27.680740: step 23097, loss 0.126111, acc 0.953125
2017-03-02T18:02:27.745675: step 23098, loss 0.217679, acc 0.875
2017-03-02T18:02:27.807396: step 23099, loss 0.210199, acc 0.890625
2017-03-02T18:02:27.877955: step 23100, loss 0.104891, acc 0.96875

Evaluation:
2017-03-02T18:02:27.917923: step 23100, loss 2.75193, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23100

2017-03-02T18:02:28.388486: step 23101, loss 0.172132, acc 0.953125
2017-03-02T18:02:28.454556: step 23102, loss 0.0600325, acc 0.984375
2017-03-02T18:02:28.526636: step 23103, loss 0.256779, acc 0.875
2017-03-02T18:02:28.599316: step 23104, loss 0.120161, acc 0.953125
2017-03-02T18:02:28.671533: step 23105, loss 0.141621, acc 0.9375
2017-03-02T18:02:28.745386: step 23106, loss 0.200133, acc 0.90625
2017-03-02T18:02:28.818945: step 23107, loss 0.0868162, acc 0.96875
2017-03-02T18:02:28.905464: step 23108, loss 0.149183, acc 0.921875
2017-03-02T18:02:28.993957: step 23109, loss 0.122657, acc 0.953125
2017-03-02T18:02:29.063717: step 23110, loss 0.106516, acc 0.953125
2017-03-02T18:02:29.132998: step 23111, loss 0.112393, acc 0.9375
2017-03-02T18:02:29.206036: step 23112, loss 0.119964, acc 0.953125
2017-03-02T18:02:29.291808: step 23113, loss 0.134893, acc 0.921875
2017-03-02T18:02:29.362232: step 23114, loss 0.231112, acc 0.90625
2017-03-02T18:02:29.443250: step 23115, loss 0.173852, acc 0.890625
2017-03-02T18:02:29.515534: step 23116, loss 0.0932716, acc 0.96875
2017-03-02T18:02:29.587571: step 23117, loss 0.227292, acc 0.859375
2017-03-02T18:02:29.662047: step 23118, loss 0.0899457, acc 0.953125
2017-03-02T18:02:29.733268: step 23119, loss 0.0633375, acc 0.984375
2017-03-02T18:02:29.805888: step 23120, loss 0.183682, acc 0.921875
2017-03-02T18:02:29.872129: step 23121, loss 0.106817, acc 0.9375
2017-03-02T18:02:29.959300: step 23122, loss 0.178934, acc 0.9375
2017-03-02T18:02:30.030135: step 23123, loss 0.2399, acc 0.890625
2017-03-02T18:02:30.104843: step 23124, loss 0.167343, acc 0.921875
2017-03-02T18:02:30.177581: step 23125, loss 0.083148, acc 0.96875
2017-03-02T18:02:30.253469: step 23126, loss 0.166827, acc 0.921875
2017-03-02T18:02:30.325263: step 23127, loss 0.0442261, acc 0.984375
2017-03-02T18:02:30.400517: step 23128, loss 0.0166803, acc 1
2017-03-02T18:02:30.472486: step 23129, loss 0.170568, acc 0.9375
2017-03-02T18:02:30.544413: step 23130, loss 0.100205, acc 0.953125
2017-03-02T18:02:30.625548: step 23131, loss 0.207473, acc 0.9375
2017-03-02T18:02:30.709490: step 23132, loss 0.111, acc 0.921875
2017-03-02T18:02:30.788478: step 23133, loss 0.20872, acc 0.90625
2017-03-02T18:02:30.858480: step 23134, loss 0.0970454, acc 0.96875
2017-03-02T18:02:30.928868: step 23135, loss 0.122959, acc 0.96875
2017-03-02T18:02:30.998553: step 23136, loss 0.0548077, acc 1
2017-03-02T18:02:31.075355: step 23137, loss 0.134244, acc 0.9375
2017-03-02T18:02:31.145502: step 23138, loss 0.16251, acc 0.921875
2017-03-02T18:02:31.214130: step 23139, loss 0.239738, acc 0.875
2017-03-02T18:02:31.290706: step 23140, loss 0.096051, acc 0.96875
2017-03-02T18:02:31.365164: step 23141, loss 0.215382, acc 0.890625
2017-03-02T18:02:31.440956: step 23142, loss 0.169284, acc 0.921875
2017-03-02T18:02:31.516192: step 23143, loss 0.199573, acc 0.90625
2017-03-02T18:02:31.593664: step 23144, loss 0.182698, acc 0.921875
2017-03-02T18:02:31.675689: step 23145, loss 0.122016, acc 0.9375
2017-03-02T18:02:31.746546: step 23146, loss 0.160029, acc 0.90625
2017-03-02T18:02:31.819341: step 23147, loss 0.1822, acc 0.890625
2017-03-02T18:02:31.890620: step 23148, loss 0.0321167, acc 0.984375
2017-03-02T18:02:31.958868: step 23149, loss 0.10641, acc 0.953125
2017-03-02T18:02:32.032436: step 23150, loss 0.055159, acc 0.984375
2017-03-02T18:02:32.107910: step 23151, loss 0.178705, acc 0.9375
2017-03-02T18:02:32.178541: step 23152, loss 0.0863727, acc 0.984375
2017-03-02T18:02:32.249478: step 23153, loss 0.335568, acc 0.890625
2017-03-02T18:02:32.321102: step 23154, loss 0.11414, acc 0.9375
2017-03-02T18:02:32.387604: step 23155, loss 0.196401, acc 0.9375
2017-03-02T18:02:32.463399: step 23156, loss 0.126168, acc 0.984375
2017-03-02T18:02:32.533625: step 23157, loss 0.114418, acc 0.953125
2017-03-02T18:02:32.602313: step 23158, loss 0.113297, acc 0.96875
2017-03-02T18:02:32.669449: step 23159, loss 0.200686, acc 0.921875
2017-03-02T18:02:32.740366: step 23160, loss 0.23123, acc 0.859375
2017-03-02T18:02:32.817664: step 23161, loss 0.0989362, acc 0.9375
2017-03-02T18:02:32.893340: step 23162, loss 0.0761146, acc 0.953125
2017-03-02T18:02:32.963095: step 23163, loss 0.294879, acc 0.828125
2017-03-02T18:02:33.036334: step 23164, loss 0.142422, acc 0.9375
2017-03-02T18:02:33.112673: step 23165, loss 0.14141, acc 0.953125
2017-03-02T18:02:33.185199: step 23166, loss 0.0576276, acc 1
2017-03-02T18:02:33.253055: step 23167, loss 0.144786, acc 0.921875
2017-03-02T18:02:33.320144: step 23168, loss 0.079768, acc 0.984375
2017-03-02T18:02:33.392628: step 23169, loss 0.148388, acc 0.9375
2017-03-02T18:02:33.475657: step 23170, loss 0.0776583, acc 0.984375
2017-03-02T18:02:33.540746: step 23171, loss 0.262175, acc 0.875
2017-03-02T18:02:33.611238: step 23172, loss 0.175677, acc 0.890625
2017-03-02T18:02:33.679996: step 23173, loss 0.163427, acc 0.9375
2017-03-02T18:02:33.751530: step 23174, loss 0.0820721, acc 0.96875
2017-03-02T18:02:33.825359: step 23175, loss 0.124704, acc 0.921875
2017-03-02T18:02:33.895882: step 23176, loss 0.0561836, acc 0.96875
2017-03-02T18:02:33.967603: step 23177, loss 0.0958273, acc 0.9375
2017-03-02T18:02:34.041714: step 23178, loss 0.178004, acc 0.953125
2017-03-02T18:02:34.112636: step 23179, loss 0.163041, acc 0.921875
2017-03-02T18:02:34.192378: step 23180, loss 0.394187, acc 0.84375
2017-03-02T18:02:34.268295: step 23181, loss 0.15751, acc 0.953125
2017-03-02T18:02:34.344042: step 23182, loss 0.0930816, acc 0.96875
2017-03-02T18:02:34.425027: step 23183, loss 0.123995, acc 0.953125
2017-03-02T18:02:34.497365: step 23184, loss 0.292473, acc 0.859375
2017-03-02T18:02:34.564953: step 23185, loss 0.0932866, acc 0.953125
2017-03-02T18:02:34.632811: step 23186, loss 0.153191, acc 0.9375
2017-03-02T18:02:34.712862: step 23187, loss 0.163171, acc 0.9375
2017-03-02T18:02:34.783990: step 23188, loss 0.163113, acc 0.921875
2017-03-02T18:02:34.853492: step 23189, loss 0.250427, acc 0.859375
2017-03-02T18:02:34.929917: step 23190, loss 0.160307, acc 0.953125
2017-03-02T18:02:35.002359: step 23191, loss 0.05177, acc 0.96875
2017-03-02T18:02:35.069565: step 23192, loss 0.107761, acc 0.953125
2017-03-02T18:02:35.144427: step 23193, loss 0.192879, acc 0.921875
2017-03-02T18:02:35.220618: step 23194, loss 0.0950413, acc 0.953125
2017-03-02T18:02:35.289130: step 23195, loss 0.100936, acc 0.953125
2017-03-02T18:02:35.358453: step 23196, loss 0.169056, acc 0.953125
2017-03-02T18:02:35.438688: step 23197, loss 0.0945266, acc 0.96875
2017-03-02T18:02:35.520873: step 23198, loss 0.205805, acc 0.90625
2017-03-02T18:02:35.595050: step 23199, loss 0.215097, acc 0.890625
2017-03-02T18:02:35.676617: step 23200, loss 0.0959657, acc 0.984375

Evaluation:
2017-03-02T18:02:35.715704: step 23200, loss 2.75111, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23200

2017-03-02T18:02:36.178241: step 23201, loss 0.100118, acc 0.96875
2017-03-02T18:02:36.254282: step 23202, loss 0.149999, acc 0.9375
2017-03-02T18:02:36.334144: step 23203, loss 0.15465, acc 0.953125
2017-03-02T18:02:36.407117: step 23204, loss 0.109836, acc 0.96875
2017-03-02T18:02:36.476426: step 23205, loss 0.119813, acc 0.9375
2017-03-02T18:02:36.546984: step 23206, loss 0.249733, acc 0.875
2017-03-02T18:02:36.611875: step 23207, loss 0.17837, acc 0.921875
2017-03-02T18:02:36.679962: step 23208, loss 0.251012, acc 0.90625
2017-03-02T18:02:36.750618: step 23209, loss 0.0838006, acc 0.953125
2017-03-02T18:02:36.827309: step 23210, loss 0.197687, acc 0.90625
2017-03-02T18:02:36.902396: step 23211, loss 0.169253, acc 0.90625
2017-03-02T18:02:36.977796: step 23212, loss 0.103303, acc 0.96875
2017-03-02T18:02:37.042250: step 23213, loss 0.212925, acc 0.90625
2017-03-02T18:02:37.113365: step 23214, loss 0.199103, acc 0.90625
2017-03-02T18:02:37.188019: step 23215, loss 0.149617, acc 0.90625
2017-03-02T18:02:37.262957: step 23216, loss 0.10942, acc 0.9375
2017-03-02T18:02:37.359232: step 23217, loss 0.0605957, acc 0.96875
2017-03-02T18:02:37.429882: step 23218, loss 0.047986, acc 0.984375
2017-03-02T18:02:37.502689: step 23219, loss 0.166742, acc 0.90625
2017-03-02T18:02:37.582874: step 23220, loss 0.136861, acc 0.9375
2017-03-02T18:02:37.652804: step 23221, loss 0.197696, acc 0.890625
2017-03-02T18:02:37.722538: step 23222, loss 0.135688, acc 0.921875
2017-03-02T18:02:37.803343: step 23223, loss 0.0973889, acc 0.96875
2017-03-02T18:02:37.878905: step 23224, loss 0.173734, acc 0.953125
2017-03-02T18:02:37.950102: step 23225, loss 0.0627867, acc 0.96875
2017-03-02T18:02:38.016593: step 23226, loss 0.189238, acc 0.9375
2017-03-02T18:02:38.079971: step 23227, loss 0.126825, acc 0.9375
2017-03-02T18:02:38.149966: step 23228, loss 0.0787472, acc 0.984375
2017-03-02T18:02:38.221726: step 23229, loss 0.212572, acc 0.875
2017-03-02T18:02:38.299301: step 23230, loss 0.215603, acc 0.890625
2017-03-02T18:02:38.377651: step 23231, loss 0.0930957, acc 0.953125
2017-03-02T18:02:38.458885: step 23232, loss 0.0901438, acc 0.953125
2017-03-02T18:02:38.538299: step 23233, loss 0.121944, acc 0.953125
2017-03-02T18:02:38.614306: step 23234, loss 0.202631, acc 0.921875
2017-03-02T18:02:38.689127: step 23235, loss 0.139664, acc 0.90625
2017-03-02T18:02:38.767183: step 23236, loss 0.21862, acc 0.875
2017-03-02T18:02:38.849512: step 23237, loss 0.0999152, acc 0.953125
2017-03-02T18:02:38.922368: step 23238, loss 0.205388, acc 0.90625
2017-03-02T18:02:38.993277: step 23239, loss 0.186425, acc 0.921875
2017-03-02T18:02:39.067588: step 23240, loss 0.0743096, acc 0.984375
2017-03-02T18:02:39.138192: step 23241, loss 0.155916, acc 0.921875
2017-03-02T18:02:39.215085: step 23242, loss 0.225229, acc 0.859375
2017-03-02T18:02:39.299017: step 23243, loss 0.0998321, acc 0.953125
2017-03-02T18:02:39.367300: step 23244, loss 0.0727216, acc 0.96875
2017-03-02T18:02:39.455438: step 23245, loss 0.142564, acc 0.921875
2017-03-02T18:02:39.524993: step 23246, loss 0.155705, acc 0.921875
2017-03-02T18:02:39.600989: step 23247, loss 0.0650706, acc 0.984375
2017-03-02T18:02:39.682984: step 23248, loss 0.0501261, acc 0.984375
2017-03-02T18:02:39.751640: step 23249, loss 0.04157, acc 0.984375
2017-03-02T18:02:39.817990: step 23250, loss 0.175274, acc 0.953125
2017-03-02T18:02:39.890957: step 23251, loss 0.12334, acc 0.921875
2017-03-02T18:02:39.960617: step 23252, loss 0.211019, acc 0.890625
2017-03-02T18:02:40.037069: step 23253, loss 0.204002, acc 0.9375
2017-03-02T18:02:40.106789: step 23254, loss 0.104865, acc 0.9375
2017-03-02T18:02:40.180028: step 23255, loss 0.212485, acc 0.9375
2017-03-02T18:02:40.252235: step 23256, loss 0.263344, acc 0.90625
2017-03-02T18:02:40.328995: step 23257, loss 0.162617, acc 0.953125
2017-03-02T18:02:40.398871: step 23258, loss 0.15423, acc 0.921875
2017-03-02T18:02:40.472496: step 23259, loss 0.26661, acc 0.890625
2017-03-02T18:02:40.542357: step 23260, loss 0.28313, acc 0.859375
2017-03-02T18:02:40.614008: step 23261, loss 0.105803, acc 0.9375
2017-03-02T18:02:40.684032: step 23262, loss 0.143153, acc 0.953125
2017-03-02T18:02:40.754937: step 23263, loss 0.246553, acc 0.890625
2017-03-02T18:02:40.824649: step 23264, loss 0.183913, acc 0.90625
2017-03-02T18:02:40.897938: step 23265, loss 0.204308, acc 0.921875
2017-03-02T18:02:40.972347: step 23266, loss 0.109343, acc 0.921875
2017-03-02T18:02:41.044036: step 23267, loss 0.139791, acc 0.9375
2017-03-02T18:02:41.118642: step 23268, loss 0.134019, acc 0.9375
2017-03-02T18:02:41.188602: step 23269, loss 0.128524, acc 0.953125
2017-03-02T18:02:41.264314: step 23270, loss 0.121124, acc 0.953125
2017-03-02T18:02:41.334154: step 23271, loss 0.231798, acc 0.90625
2017-03-02T18:02:41.405724: step 23272, loss 0.10725, acc 0.953125
2017-03-02T18:02:41.478092: step 23273, loss 0.140377, acc 0.9375
2017-03-02T18:02:41.554522: step 23274, loss 0.113567, acc 0.953125
2017-03-02T18:02:41.624736: step 23275, loss 0.191408, acc 0.921875
2017-03-02T18:02:41.695445: step 23276, loss 0.269159, acc 0.890625
2017-03-02T18:02:41.766700: step 23277, loss 0.169336, acc 0.90625
2017-03-02T18:02:41.837409: step 23278, loss 0.128127, acc 0.921875
2017-03-02T18:02:41.907389: step 23279, loss 0.148225, acc 0.921875
2017-03-02T18:02:41.981864: step 23280, loss 0.176251, acc 0.953125
2017-03-02T18:02:42.054198: step 23281, loss 0.135234, acc 0.9375
2017-03-02T18:02:42.124363: step 23282, loss 0.108346, acc 0.9375
2017-03-02T18:02:42.190752: step 23283, loss 0.156484, acc 0.9375
2017-03-02T18:02:42.265671: step 23284, loss 0.183116, acc 0.9375
2017-03-02T18:02:42.339056: step 23285, loss 0.115286, acc 0.953125
2017-03-02T18:02:42.413009: step 23286, loss 0.0360882, acc 1
2017-03-02T18:02:42.499104: step 23287, loss 0.22877, acc 0.921875
2017-03-02T18:02:42.571179: step 23288, loss 0.158389, acc 0.921875
2017-03-02T18:02:42.641365: step 23289, loss 0.142525, acc 0.953125
2017-03-02T18:02:42.715959: step 23290, loss 0.216385, acc 0.90625
2017-03-02T18:02:42.788668: step 23291, loss 0.164963, acc 0.9375
2017-03-02T18:02:42.857628: step 23292, loss 0.117423, acc 0.953125
2017-03-02T18:02:42.928587: step 23293, loss 0.12022, acc 0.953125
2017-03-02T18:02:43.006202: step 23294, loss 0.1269, acc 0.953125
2017-03-02T18:02:43.067231: step 23295, loss 0.124825, acc 0.90625
2017-03-02T18:02:43.150341: step 23296, loss 0.246503, acc 0.921875
2017-03-02T18:02:43.221346: step 23297, loss 0.185083, acc 0.9375
2017-03-02T18:02:43.294892: step 23298, loss 0.0348205, acc 0.984375
2017-03-02T18:02:43.359241: step 23299, loss 0.109994, acc 0.96875
2017-03-02T18:02:43.431470: step 23300, loss 0.28468, acc 0.84375

Evaluation:
2017-03-02T18:02:43.479154: step 23300, loss 2.78817, acc 0.643836

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23300

2017-03-02T18:02:43.929604: step 23301, loss 0.0924455, acc 0.953125
2017-03-02T18:02:43.998898: step 23302, loss 0.0854746, acc 0.953125
2017-03-02T18:02:44.079955: step 23303, loss 0.101794, acc 0.953125
2017-03-02T18:02:44.151440: step 23304, loss 0.17901, acc 0.953125
2017-03-02T18:02:44.222041: step 23305, loss 0.156622, acc 0.9375
2017-03-02T18:02:44.290713: step 23306, loss 0.0851643, acc 0.953125
2017-03-02T18:02:44.380153: step 23307, loss 0.145764, acc 0.9375
2017-03-02T18:02:44.455052: step 23308, loss 0.112811, acc 0.9375
2017-03-02T18:02:44.533188: step 23309, loss 0.193577, acc 0.921875
2017-03-02T18:02:44.607170: step 23310, loss 0.197251, acc 0.9375
2017-03-02T18:02:44.682109: step 23311, loss 0.172027, acc 0.9375
2017-03-02T18:02:44.757165: step 23312, loss 0.0294061, acc 1
2017-03-02T18:02:44.826411: step 23313, loss 0.178378, acc 0.90625
2017-03-02T18:02:44.892242: step 23314, loss 0.27031, acc 0.859375
2017-03-02T18:02:44.965060: step 23315, loss 0.201681, acc 0.921875
2017-03-02T18:02:45.037977: step 23316, loss 0.197808, acc 0.921875
2017-03-02T18:02:45.111745: step 23317, loss 0.30961, acc 0.890625
2017-03-02T18:02:45.184000: step 23318, loss 0.1545, acc 0.9375
2017-03-02T18:02:45.256581: step 23319, loss 0.0777678, acc 0.96875
2017-03-02T18:02:45.322576: step 23320, loss 0.12936, acc 0.953125
2017-03-02T18:02:45.391207: step 23321, loss 0.0982244, acc 0.953125
2017-03-02T18:02:45.461194: step 23322, loss 0.132448, acc 0.921875
2017-03-02T18:02:45.532242: step 23323, loss 0.118844, acc 0.9375
2017-03-02T18:02:45.603433: step 23324, loss 0.312914, acc 0.75
2017-03-02T18:02:45.688058: step 23325, loss 0.121769, acc 0.9375
2017-03-02T18:02:45.756383: step 23326, loss 0.0560754, acc 0.984375
2017-03-02T18:02:45.830277: step 23327, loss 0.118063, acc 0.953125
2017-03-02T18:02:45.904163: step 23328, loss 0.119744, acc 0.921875
2017-03-02T18:02:45.979975: step 23329, loss 0.0682771, acc 0.953125
2017-03-02T18:02:46.061102: step 23330, loss 0.183008, acc 0.921875
2017-03-02T18:02:46.131345: step 23331, loss 0.12587, acc 0.921875
2017-03-02T18:02:46.209731: step 23332, loss 0.210044, acc 0.875
2017-03-02T18:02:46.278232: step 23333, loss 0.108332, acc 0.9375
2017-03-02T18:02:46.345850: step 23334, loss 0.149642, acc 0.90625
2017-03-02T18:02:46.419527: step 23335, loss 0.161265, acc 0.921875
2017-03-02T18:02:46.490395: step 23336, loss 0.148079, acc 0.90625
2017-03-02T18:02:46.563627: step 23337, loss 0.317598, acc 0.859375
2017-03-02T18:02:46.643979: step 23338, loss 0.200306, acc 0.90625
2017-03-02T18:02:46.722210: step 23339, loss 0.152689, acc 0.921875
2017-03-02T18:02:46.799907: step 23340, loss 0.149408, acc 0.953125
2017-03-02T18:02:46.875986: step 23341, loss 0.0876425, acc 0.9375
2017-03-02T18:02:46.942197: step 23342, loss 0.0941875, acc 0.984375
2017-03-02T18:02:47.015984: step 23343, loss 0.150817, acc 0.9375
2017-03-02T18:02:47.089445: step 23344, loss 0.0981537, acc 0.9375
2017-03-02T18:02:47.180113: step 23345, loss 0.185889, acc 0.953125
2017-03-02T18:02:47.261037: step 23346, loss 0.135947, acc 0.953125
2017-03-02T18:02:47.334341: step 23347, loss 0.15642, acc 0.953125
2017-03-02T18:02:47.406708: step 23348, loss 0.151481, acc 0.921875
2017-03-02T18:02:47.478729: step 23349, loss 0.144081, acc 0.90625
2017-03-02T18:02:47.557296: step 23350, loss 0.207504, acc 0.921875
2017-03-02T18:02:47.640218: step 23351, loss 0.0726739, acc 0.984375
2017-03-02T18:02:47.709660: step 23352, loss 0.0524961, acc 1
2017-03-02T18:02:47.784806: step 23353, loss 0.181568, acc 0.90625
2017-03-02T18:02:47.860487: step 23354, loss 0.0704542, acc 0.984375
2017-03-02T18:02:47.933984: step 23355, loss 0.0964768, acc 0.9375
2017-03-02T18:02:48.000017: step 23356, loss 0.212752, acc 0.90625
2017-03-02T18:02:48.076823: step 23357, loss 0.164999, acc 0.9375
2017-03-02T18:02:48.149569: step 23358, loss 0.0654429, acc 0.953125
2017-03-02T18:02:48.221927: step 23359, loss 0.150654, acc 0.921875
2017-03-02T18:02:48.293834: step 23360, loss 0.230832, acc 0.890625
2017-03-02T18:02:48.366530: step 23361, loss 0.184535, acc 0.90625
2017-03-02T18:02:48.435990: step 23362, loss 0.0749436, acc 0.96875
2017-03-02T18:02:48.504033: step 23363, loss 0.0920037, acc 0.96875
2017-03-02T18:02:48.581714: step 23364, loss 0.139351, acc 0.921875
2017-03-02T18:02:48.653391: step 23365, loss 0.194897, acc 0.921875
2017-03-02T18:02:48.727626: step 23366, loss 0.2657, acc 0.890625
2017-03-02T18:02:48.796105: step 23367, loss 0.128971, acc 0.953125
2017-03-02T18:02:48.875850: step 23368, loss 0.195236, acc 0.890625
2017-03-02T18:02:48.950970: step 23369, loss 0.170098, acc 0.9375
2017-03-02T18:02:49.020158: step 23370, loss 0.148919, acc 0.921875
2017-03-02T18:02:49.101058: step 23371, loss 0.133995, acc 0.921875
2017-03-02T18:02:49.171169: step 23372, loss 0.119114, acc 0.9375
2017-03-02T18:02:49.244665: step 23373, loss 0.182162, acc 0.953125
2017-03-02T18:02:49.322196: step 23374, loss 0.0960286, acc 0.953125
2017-03-02T18:02:49.396558: step 23375, loss 0.123114, acc 0.9375
2017-03-02T18:02:49.473107: step 23376, loss 0.0920245, acc 0.96875
2017-03-02T18:02:49.547277: step 23377, loss 0.138713, acc 0.921875
2017-03-02T18:02:49.618273: step 23378, loss 0.133347, acc 0.9375
2017-03-02T18:02:49.686294: step 23379, loss 0.0990958, acc 0.96875
2017-03-02T18:02:49.752739: step 23380, loss 0.298936, acc 0.890625
2017-03-02T18:02:49.817889: step 23381, loss 0.153319, acc 0.9375
2017-03-02T18:02:49.886335: step 23382, loss 0.177004, acc 0.875
2017-03-02T18:02:49.957476: step 23383, loss 0.12165, acc 0.953125
2017-03-02T18:02:50.068122: step 23384, loss 0.11057, acc 0.953125
2017-03-02T18:02:50.146648: step 23385, loss 0.203675, acc 0.953125
2017-03-02T18:02:50.224550: step 23386, loss 0.0740462, acc 0.96875
2017-03-02T18:02:50.293580: step 23387, loss 0.0736518, acc 0.984375
2017-03-02T18:02:50.369759: step 23388, loss 0.221626, acc 0.921875
2017-03-02T18:02:50.453434: step 23389, loss 0.15894, acc 0.96875
2017-03-02T18:02:50.522234: step 23390, loss 0.149921, acc 0.921875
2017-03-02T18:02:50.603895: step 23391, loss 0.0765749, acc 0.984375
2017-03-02T18:02:50.677910: step 23392, loss 0.159424, acc 0.921875
2017-03-02T18:02:50.765345: step 23393, loss 0.204174, acc 0.859375
2017-03-02T18:02:50.842128: step 23394, loss 0.137335, acc 0.9375
2017-03-02T18:02:50.915255: step 23395, loss 0.108526, acc 0.953125
2017-03-02T18:02:50.983785: step 23396, loss 0.0470684, acc 1
2017-03-02T18:02:51.063280: step 23397, loss 0.124956, acc 0.9375
2017-03-02T18:02:51.129558: step 23398, loss 0.267126, acc 0.875
2017-03-02T18:02:51.196032: step 23399, loss 0.218845, acc 0.875
2017-03-02T18:02:51.268006: step 23400, loss 0.12683, acc 0.9375

Evaluation:
2017-03-02T18:02:51.305619: step 23400, loss 2.80198, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23400

2017-03-02T18:02:51.748974: step 23401, loss 0.103839, acc 0.953125
2017-03-02T18:02:51.821807: step 23402, loss 0.0938943, acc 0.96875
2017-03-02T18:02:51.889014: step 23403, loss 0.215622, acc 0.921875
2017-03-02T18:02:51.965562: step 23404, loss 0.137793, acc 0.953125
2017-03-02T18:02:52.039632: step 23405, loss 0.155222, acc 0.921875
2017-03-02T18:02:52.119379: step 23406, loss 0.0586935, acc 0.96875
2017-03-02T18:02:52.192060: step 23407, loss 0.208863, acc 0.953125
2017-03-02T18:02:52.264781: step 23408, loss 0.10916, acc 0.953125
2017-03-02T18:02:52.335291: step 23409, loss 0.150737, acc 0.921875
2017-03-02T18:02:52.414614: step 23410, loss 0.198718, acc 0.890625
2017-03-02T18:02:52.481743: step 23411, loss 0.0584594, acc 0.96875
2017-03-02T18:02:52.550227: step 23412, loss 0.197676, acc 0.90625
2017-03-02T18:02:52.624230: step 23413, loss 0.11869, acc 0.96875
2017-03-02T18:02:52.692763: step 23414, loss 0.144199, acc 0.921875
2017-03-02T18:02:52.768065: step 23415, loss 0.0876356, acc 0.9375
2017-03-02T18:02:52.840625: step 23416, loss 0.0705003, acc 0.96875
2017-03-02T18:02:52.913065: step 23417, loss 0.224577, acc 0.890625
2017-03-02T18:02:52.984775: step 23418, loss 0.125417, acc 0.9375
2017-03-02T18:02:53.056635: step 23419, loss 0.231782, acc 0.890625
2017-03-02T18:02:53.129147: step 23420, loss 0.28023, acc 0.890625
2017-03-02T18:02:53.205897: step 23421, loss 0.109595, acc 0.953125
2017-03-02T18:02:53.273222: step 23422, loss 0.166797, acc 0.921875
2017-03-02T18:02:53.344855: step 23423, loss 0.10331, acc 0.953125
2017-03-02T18:02:53.427152: step 23424, loss 0.159421, acc 0.9375
2017-03-02T18:02:53.507936: step 23425, loss 0.0657118, acc 0.984375
2017-03-02T18:02:53.575020: step 23426, loss 0.184039, acc 0.9375
2017-03-02T18:02:53.647213: step 23427, loss 0.120772, acc 0.96875
2017-03-02T18:02:53.733816: step 23428, loss 0.0746507, acc 1
2017-03-02T18:02:53.817399: step 23429, loss 0.148818, acc 0.921875
2017-03-02T18:02:53.886909: step 23430, loss 0.0885088, acc 0.953125
2017-03-02T18:02:53.950900: step 23431, loss 0.145391, acc 0.921875
2017-03-02T18:02:54.025229: step 23432, loss 0.186016, acc 0.921875
2017-03-02T18:02:54.094634: step 23433, loss 0.149114, acc 0.921875
2017-03-02T18:02:54.165650: step 23434, loss 0.104842, acc 0.953125
2017-03-02T18:02:54.231161: step 23435, loss 0.233692, acc 0.9375
2017-03-02T18:02:54.307699: step 23436, loss 0.15931, acc 0.921875
2017-03-02T18:02:54.376030: step 23437, loss 0.176498, acc 0.90625
2017-03-02T18:02:54.444391: step 23438, loss 0.149237, acc 0.90625
2017-03-02T18:02:54.515532: step 23439, loss 0.26867, acc 0.890625
2017-03-02T18:02:54.585107: step 23440, loss 0.231754, acc 0.90625
2017-03-02T18:02:54.648916: step 23441, loss 0.183443, acc 0.890625
2017-03-02T18:02:54.723249: step 23442, loss 0.15993, acc 0.9375
2017-03-02T18:02:54.798584: step 23443, loss 0.217055, acc 0.9375
2017-03-02T18:02:54.879659: step 23444, loss 0.207625, acc 0.890625
2017-03-02T18:02:54.954225: step 23445, loss 0.187561, acc 0.90625
2017-03-02T18:02:55.030618: step 23446, loss 0.166734, acc 0.921875
2017-03-02T18:02:55.103402: step 23447, loss 0.165298, acc 0.953125
2017-03-02T18:02:55.170887: step 23448, loss 0.199692, acc 0.90625
2017-03-02T18:02:55.239548: step 23449, loss 0.133933, acc 0.953125
2017-03-02T18:02:55.337155: step 23450, loss 0.1178, acc 0.9375
2017-03-02T18:02:55.418283: step 23451, loss 0.180563, acc 0.921875
2017-03-02T18:02:55.492720: step 23452, loss 0.139939, acc 0.9375
2017-03-02T18:02:55.566108: step 23453, loss 0.217002, acc 0.9375
2017-03-02T18:02:55.638027: step 23454, loss 0.311198, acc 0.890625
2017-03-02T18:02:55.709397: step 23455, loss 0.0694224, acc 0.9375
2017-03-02T18:02:55.784148: step 23456, loss 0.0629532, acc 0.96875
2017-03-02T18:02:55.856415: step 23457, loss 0.0922858, acc 0.984375
2017-03-02T18:02:55.923446: step 23458, loss 0.163162, acc 0.9375
2017-03-02T18:02:55.995186: step 23459, loss 0.112818, acc 0.921875
2017-03-02T18:02:56.064104: step 23460, loss 0.260722, acc 0.890625
2017-03-02T18:02:56.139425: step 23461, loss 0.132736, acc 0.921875
2017-03-02T18:02:56.220199: step 23462, loss 0.242612, acc 0.890625
2017-03-02T18:02:56.289370: step 23463, loss 0.159824, acc 0.921875
2017-03-02T18:02:56.361367: step 23464, loss 0.182608, acc 0.9375
2017-03-02T18:02:56.428150: step 23465, loss 0.166702, acc 0.9375
2017-03-02T18:02:56.497325: step 23466, loss 0.223725, acc 0.921875
2017-03-02T18:02:56.570512: step 23467, loss 0.0967214, acc 0.9375
2017-03-02T18:02:56.641076: step 23468, loss 0.121773, acc 0.9375
2017-03-02T18:02:56.706995: step 23469, loss 0.125283, acc 0.953125
2017-03-02T18:02:56.778347: step 23470, loss 0.119795, acc 0.921875
2017-03-02T18:02:56.866732: step 23471, loss 0.120239, acc 0.953125
2017-03-02T18:02:56.936243: step 23472, loss 0.091121, acc 0.953125
2017-03-02T18:02:57.009537: step 23473, loss 0.0793605, acc 0.96875
2017-03-02T18:02:57.082935: step 23474, loss 0.0659139, acc 0.96875
2017-03-02T18:02:57.159162: step 23475, loss 0.109993, acc 0.96875
2017-03-02T18:02:57.231692: step 23476, loss 0.177607, acc 0.921875
2017-03-02T18:02:57.304121: step 23477, loss 0.173126, acc 0.9375
2017-03-02T18:02:57.371921: step 23478, loss 0.243963, acc 0.921875
2017-03-02T18:02:57.429174: step 23479, loss 0.165651, acc 0.921875
2017-03-02T18:02:57.503049: step 23480, loss 0.180545, acc 0.9375
2017-03-02T18:02:57.570920: step 23481, loss 0.228749, acc 0.9375
2017-03-02T18:02:57.638824: step 23482, loss 0.102846, acc 0.953125
2017-03-02T18:02:57.708115: step 23483, loss 0.183389, acc 0.890625
2017-03-02T18:02:57.778863: step 23484, loss 0.307442, acc 0.859375
2017-03-02T18:02:57.846822: step 23485, loss 0.17041, acc 0.953125
2017-03-02T18:02:57.916593: step 23486, loss 0.20825, acc 0.90625
2017-03-02T18:02:57.986176: step 23487, loss 0.0769422, acc 0.96875
2017-03-02T18:02:58.051407: step 23488, loss 0.250237, acc 0.90625
2017-03-02T18:02:58.117708: step 23489, loss 0.134587, acc 0.9375
2017-03-02T18:02:58.191143: step 23490, loss 0.0588253, acc 1
2017-03-02T18:02:58.266952: step 23491, loss 0.191302, acc 0.90625
2017-03-02T18:02:58.345398: step 23492, loss 0.0985572, acc 0.953125
2017-03-02T18:02:58.410629: step 23493, loss 0.2051, acc 0.921875
2017-03-02T18:02:58.487701: step 23494, loss 0.117489, acc 0.96875
2017-03-02T18:02:58.562360: step 23495, loss 0.240639, acc 0.90625
2017-03-02T18:02:58.634709: step 23496, loss 0.21139, acc 0.953125
2017-03-02T18:02:58.717404: step 23497, loss 0.145215, acc 0.953125
2017-03-02T18:02:58.784034: step 23498, loss 0.0977478, acc 0.953125
2017-03-02T18:02:58.855223: step 23499, loss 0.138986, acc 0.90625
2017-03-02T18:02:58.922764: step 23500, loss 0.187305, acc 0.9375

Evaluation:
2017-03-02T18:02:58.958052: step 23500, loss 2.85085, acc 0.65465

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23500

2017-03-02T18:02:59.392996: step 23501, loss 0.0978334, acc 0.9375
2017-03-02T18:02:59.464607: step 23502, loss 0.203118, acc 0.90625
2017-03-02T18:02:59.536206: step 23503, loss 0.122495, acc 0.953125
2017-03-02T18:02:59.608763: step 23504, loss 0.24476, acc 0.875
2017-03-02T18:02:59.674691: step 23505, loss 0.152667, acc 0.90625
2017-03-02T18:02:59.750256: step 23506, loss 0.145458, acc 0.9375
2017-03-02T18:02:59.822537: step 23507, loss 0.176056, acc 0.9375
2017-03-02T18:02:59.894396: step 23508, loss 0.108035, acc 0.953125
2017-03-02T18:02:59.968175: step 23509, loss 0.161585, acc 0.90625
2017-03-02T18:03:00.048195: step 23510, loss 0.123672, acc 0.921875
2017-03-02T18:03:00.117678: step 23511, loss 0.112559, acc 0.9375
2017-03-02T18:03:00.190679: step 23512, loss 0.128476, acc 0.953125
2017-03-02T18:03:00.259261: step 23513, loss 0.196127, acc 0.921875
2017-03-02T18:03:00.340708: step 23514, loss 0.0671853, acc 0.96875
2017-03-02T18:03:00.410385: step 23515, loss 0.0921176, acc 0.953125
2017-03-02T18:03:00.482703: step 23516, loss 0.130198, acc 0.9375
2017-03-02T18:03:00.551502: step 23517, loss 0.128648, acc 0.9375
2017-03-02T18:03:00.619826: step 23518, loss 0.109638, acc 0.953125
2017-03-02T18:03:00.691123: step 23519, loss 0.138862, acc 0.921875
2017-03-02T18:03:00.770157: step 23520, loss 0.00051877, acc 1
2017-03-02T18:03:00.844972: step 23521, loss 0.0708517, acc 0.953125
2017-03-02T18:03:00.915176: step 23522, loss 0.170367, acc 0.90625
2017-03-02T18:03:00.978771: step 23523, loss 0.0587837, acc 0.96875
2017-03-02T18:03:01.052051: step 23524, loss 0.0221426, acc 1
2017-03-02T18:03:01.132998: step 23525, loss 0.175768, acc 0.921875
2017-03-02T18:03:01.210505: step 23526, loss 0.064046, acc 0.984375
2017-03-02T18:03:01.286344: step 23527, loss 0.239794, acc 0.890625
2017-03-02T18:03:01.359885: step 23528, loss 0.0725733, acc 0.96875
2017-03-02T18:03:01.441042: step 23529, loss 0.181541, acc 0.9375
2017-03-02T18:03:01.505828: step 23530, loss 0.120006, acc 0.953125
2017-03-02T18:03:01.574610: step 23531, loss 0.0963427, acc 0.953125
2017-03-02T18:03:01.646713: step 23532, loss 0.131978, acc 0.953125
2017-03-02T18:03:01.722546: step 23533, loss 0.122273, acc 0.9375
2017-03-02T18:03:01.800718: step 23534, loss 0.130496, acc 0.9375
2017-03-02T18:03:01.875032: step 23535, loss 0.177143, acc 0.921875
2017-03-02T18:03:01.952243: step 23536, loss 0.217363, acc 0.9375
2017-03-02T18:03:02.021419: step 23537, loss 0.0875907, acc 0.953125
2017-03-02T18:03:02.092453: step 23538, loss 0.111649, acc 0.9375
2017-03-02T18:03:02.166877: step 23539, loss 0.164622, acc 0.9375
2017-03-02T18:03:02.243106: step 23540, loss 0.206201, acc 0.90625
2017-03-02T18:03:02.302829: step 23541, loss 0.193122, acc 0.890625
2017-03-02T18:03:02.380723: step 23542, loss 0.167331, acc 0.921875
2017-03-02T18:03:02.452988: step 23543, loss 0.125202, acc 0.953125
2017-03-02T18:03:02.523669: step 23544, loss 0.177339, acc 0.890625
2017-03-02T18:03:02.595203: step 23545, loss 0.0801473, acc 0.953125
2017-03-02T18:03:02.687922: step 23546, loss 0.114986, acc 0.9375
2017-03-02T18:03:02.765133: step 23547, loss 0.13595, acc 0.921875
2017-03-02T18:03:02.839232: step 23548, loss 0.353086, acc 0.84375
2017-03-02T18:03:02.906847: step 23549, loss 0.224796, acc 0.890625
2017-03-02T18:03:02.972822: step 23550, loss 0.189969, acc 0.90625
2017-03-02T18:03:03.033032: step 23551, loss 0.148622, acc 0.9375
2017-03-02T18:03:03.104888: step 23552, loss 0.0796181, acc 0.984375
2017-03-02T18:03:03.175278: step 23553, loss 0.0822613, acc 0.984375
2017-03-02T18:03:03.257957: step 23554, loss 0.171592, acc 0.90625
2017-03-02T18:03:03.330715: step 23555, loss 0.244462, acc 0.875
2017-03-02T18:03:03.403391: step 23556, loss 0.150375, acc 0.953125
2017-03-02T18:03:03.474208: step 23557, loss 0.174598, acc 0.921875
2017-03-02T18:03:03.546764: step 23558, loss 0.05927, acc 0.96875
2017-03-02T18:03:03.614074: step 23559, loss 0.0929175, acc 0.96875
2017-03-02T18:03:03.685243: step 23560, loss 0.199548, acc 0.890625
2017-03-02T18:03:03.761826: step 23561, loss 0.101809, acc 0.953125
2017-03-02T18:03:03.833271: step 23562, loss 0.0991876, acc 0.9375
2017-03-02T18:03:03.911109: step 23563, loss 0.124762, acc 0.9375
2017-03-02T18:03:03.987759: step 23564, loss 0.172163, acc 0.921875
2017-03-02T18:03:04.061199: step 23565, loss 0.103313, acc 0.953125
2017-03-02T18:03:04.133980: step 23566, loss 0.118403, acc 0.90625
2017-03-02T18:03:04.193975: step 23567, loss 0.102342, acc 0.953125
2017-03-02T18:03:04.264580: step 23568, loss 0.130154, acc 0.953125
2017-03-02T18:03:04.338860: step 23569, loss 0.199135, acc 0.890625
2017-03-02T18:03:04.404143: step 23570, loss 0.157765, acc 0.90625
2017-03-02T18:03:04.478682: step 23571, loss 0.137169, acc 0.921875
2017-03-02T18:03:04.550017: step 23572, loss 0.133085, acc 0.921875
2017-03-02T18:03:04.623442: step 23573, loss 0.152567, acc 0.9375
2017-03-02T18:03:04.724627: step 23574, loss 0.264216, acc 0.890625
2017-03-02T18:03:04.797007: step 23575, loss 0.094205, acc 0.96875
2017-03-02T18:03:04.867482: step 23576, loss 0.145819, acc 0.90625
2017-03-02T18:03:04.946873: step 23577, loss 0.0936277, acc 0.953125
2017-03-02T18:03:05.015260: step 23578, loss 0.132821, acc 0.921875
2017-03-02T18:03:05.090641: step 23579, loss 0.149616, acc 0.9375
2017-03-02T18:03:05.171008: step 23580, loss 0.0922704, acc 0.9375
2017-03-02T18:03:05.242454: step 23581, loss 0.328081, acc 0.859375
2017-03-02T18:03:05.321151: step 23582, loss 0.199922, acc 0.890625
2017-03-02T18:03:05.391926: step 23583, loss 0.158196, acc 0.921875
2017-03-02T18:03:05.471295: step 23584, loss 0.0738752, acc 0.96875
2017-03-02T18:03:05.539856: step 23585, loss 0.0969797, acc 0.953125
2017-03-02T18:03:05.609065: step 23586, loss 0.208963, acc 0.921875
2017-03-02T18:03:05.674647: step 23587, loss 0.118973, acc 0.921875
2017-03-02T18:03:05.742461: step 23588, loss 0.208453, acc 0.875
2017-03-02T18:03:05.814878: step 23589, loss 0.172529, acc 0.921875
2017-03-02T18:03:05.886562: step 23590, loss 0.156793, acc 0.90625
2017-03-02T18:03:05.967455: step 23591, loss 0.19905, acc 0.9375
2017-03-02T18:03:06.042676: step 23592, loss 0.166652, acc 0.921875
2017-03-02T18:03:06.115599: step 23593, loss 0.102817, acc 0.96875
2017-03-02T18:03:06.189721: step 23594, loss 0.0797421, acc 0.96875
2017-03-02T18:03:06.265667: step 23595, loss 0.17133, acc 0.9375
2017-03-02T18:03:06.340194: step 23596, loss 0.116869, acc 0.96875
2017-03-02T18:03:06.420383: step 23597, loss 0.0745849, acc 0.984375
2017-03-02T18:03:06.486025: step 23598, loss 0.137705, acc 0.9375
2017-03-02T18:03:06.556444: step 23599, loss 0.164345, acc 0.90625
2017-03-02T18:03:06.632016: step 23600, loss 0.196591, acc 0.90625

Evaluation:
2017-03-02T18:03:06.667390: step 23600, loss 2.80119, acc 0.64672

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23600

2017-03-02T18:03:07.129650: step 23601, loss 0.149283, acc 0.953125
2017-03-02T18:03:07.206821: step 23602, loss 0.162422, acc 0.9375
2017-03-02T18:03:07.280990: step 23603, loss 0.159734, acc 0.90625
2017-03-02T18:03:07.354518: step 23604, loss 0.259611, acc 0.859375
2017-03-02T18:03:07.438706: step 23605, loss 0.122762, acc 0.9375
2017-03-02T18:03:07.522800: step 23606, loss 0.247637, acc 0.875
2017-03-02T18:03:07.593321: step 23607, loss 0.136856, acc 0.9375
2017-03-02T18:03:07.671129: step 23608, loss 0.12689, acc 0.9375
2017-03-02T18:03:07.749258: step 23609, loss 0.087426, acc 0.9375
2017-03-02T18:03:07.822592: step 23610, loss 0.137103, acc 0.90625
2017-03-02T18:03:07.899320: step 23611, loss 0.231981, acc 0.921875
2017-03-02T18:03:07.972615: step 23612, loss 0.144993, acc 0.953125
2017-03-02T18:03:08.060805: step 23613, loss 0.121915, acc 0.9375
2017-03-02T18:03:08.129728: step 23614, loss 0.0627157, acc 0.96875
2017-03-02T18:03:08.205657: step 23615, loss 0.368731, acc 0.859375
2017-03-02T18:03:08.277657: step 23616, loss 0.122057, acc 0.96875
2017-03-02T18:03:08.359107: step 23617, loss 0.083219, acc 0.96875
2017-03-02T18:03:08.428576: step 23618, loss 0.198306, acc 0.921875
2017-03-02T18:03:08.497200: step 23619, loss 0.0920002, acc 0.9375
2017-03-02T18:03:08.571456: step 23620, loss 0.123707, acc 0.953125
2017-03-02T18:03:08.646197: step 23621, loss 0.142863, acc 0.953125
2017-03-02T18:03:08.718796: step 23622, loss 0.109968, acc 0.953125
2017-03-02T18:03:08.794055: step 23623, loss 0.16573, acc 0.953125
2017-03-02T18:03:08.870866: step 23624, loss 0.0626324, acc 0.984375
2017-03-02T18:03:08.945068: step 23625, loss 0.199367, acc 0.9375
2017-03-02T18:03:09.015672: step 23626, loss 0.198336, acc 0.890625
2017-03-02T18:03:09.087909: step 23627, loss 0.17598, acc 0.953125
2017-03-02T18:03:09.161905: step 23628, loss 0.104585, acc 0.953125
2017-03-02T18:03:09.226489: step 23629, loss 0.0893856, acc 0.9375
2017-03-02T18:03:09.304308: step 23630, loss 0.0944012, acc 0.96875
2017-03-02T18:03:09.378624: step 23631, loss 0.251416, acc 0.9375
2017-03-02T18:03:09.450882: step 23632, loss 0.188839, acc 0.90625
2017-03-02T18:03:09.519916: step 23633, loss 0.229573, acc 0.921875
2017-03-02T18:03:09.596850: step 23634, loss 0.170724, acc 0.90625
2017-03-02T18:03:09.666915: step 23635, loss 0.159029, acc 0.921875
2017-03-02T18:03:09.737641: step 23636, loss 0.1982, acc 0.890625
2017-03-02T18:03:09.805799: step 23637, loss 0.143487, acc 0.921875
2017-03-02T18:03:09.880446: step 23638, loss 0.17038, acc 0.9375
2017-03-02T18:03:09.949288: step 23639, loss 0.151988, acc 0.9375
2017-03-02T18:03:10.030845: step 23640, loss 0.217647, acc 0.875
2017-03-02T18:03:10.107475: step 23641, loss 0.191484, acc 0.90625
2017-03-02T18:03:10.189409: step 23642, loss 0.0953923, acc 0.953125
2017-03-02T18:03:10.266223: step 23643, loss 0.13253, acc 0.90625
2017-03-02T18:03:10.339987: step 23644, loss 0.0798416, acc 0.9375
2017-03-02T18:03:10.413836: step 23645, loss 0.158657, acc 0.921875
2017-03-02T18:03:10.488755: step 23646, loss 0.101598, acc 0.953125
2017-03-02T18:03:10.554704: step 23647, loss 0.259579, acc 0.890625
2017-03-02T18:03:10.625418: step 23648, loss 0.190795, acc 0.921875
2017-03-02T18:03:10.700139: step 23649, loss 0.15352, acc 0.953125
2017-03-02T18:03:10.785053: step 23650, loss 0.147972, acc 0.90625
2017-03-02T18:03:10.856771: step 23651, loss 0.133719, acc 0.953125
2017-03-02T18:03:10.948582: step 23652, loss 0.171861, acc 0.9375
2017-03-02T18:03:11.017710: step 23653, loss 0.129695, acc 0.96875
2017-03-02T18:03:11.091406: step 23654, loss 0.192897, acc 0.921875
2017-03-02T18:03:11.163864: step 23655, loss 0.201091, acc 0.9375
2017-03-02T18:03:11.241113: step 23656, loss 0.0979166, acc 0.9375
2017-03-02T18:03:11.308925: step 23657, loss 0.177013, acc 0.90625
2017-03-02T18:03:11.381076: step 23658, loss 0.165214, acc 0.9375
2017-03-02T18:03:11.455041: step 23659, loss 0.103295, acc 0.96875
2017-03-02T18:03:11.533028: step 23660, loss 0.0947678, acc 0.96875
2017-03-02T18:03:11.606411: step 23661, loss 0.114458, acc 0.96875
2017-03-02T18:03:11.679860: step 23662, loss 0.135653, acc 0.921875
2017-03-02T18:03:11.745923: step 23663, loss 0.125451, acc 0.953125
2017-03-02T18:03:11.822172: step 23664, loss 0.0912458, acc 0.953125
2017-03-02T18:03:11.887632: step 23665, loss 0.211965, acc 0.90625
2017-03-02T18:03:11.956538: step 23666, loss 0.147996, acc 0.90625
2017-03-02T18:03:12.031362: step 23667, loss 0.200998, acc 0.875
2017-03-02T18:03:12.102036: step 23668, loss 0.159414, acc 0.953125
2017-03-02T18:03:12.178471: step 23669, loss 0.116693, acc 0.9375
2017-03-02T18:03:12.258992: step 23670, loss 0.17515, acc 0.90625
2017-03-02T18:03:12.331759: step 23671, loss 0.186677, acc 0.90625
2017-03-02T18:03:12.402478: step 23672, loss 0.136049, acc 0.9375
2017-03-02T18:03:12.469205: step 23673, loss 0.11145, acc 0.9375
2017-03-02T18:03:12.545623: step 23674, loss 0.234788, acc 0.953125
2017-03-02T18:03:12.617881: step 23675, loss 0.223508, acc 0.9375
2017-03-02T18:03:12.688315: step 23676, loss 0.113353, acc 0.9375
2017-03-02T18:03:12.758161: step 23677, loss 0.129275, acc 0.9375
2017-03-02T18:03:12.833100: step 23678, loss 0.155016, acc 0.9375
2017-03-02T18:03:12.932224: step 23679, loss 0.235775, acc 0.9375
2017-03-02T18:03:13.012509: step 23680, loss 0.101201, acc 0.96875
2017-03-02T18:03:13.083190: step 23681, loss 0.161569, acc 0.90625
2017-03-02T18:03:13.157257: step 23682, loss 0.175797, acc 0.90625
2017-03-02T18:03:13.232716: step 23683, loss 0.198351, acc 0.921875
2017-03-02T18:03:13.297684: step 23684, loss 0.180167, acc 0.90625
2017-03-02T18:03:13.368983: step 23685, loss 0.161872, acc 0.9375
2017-03-02T18:03:13.460802: step 23686, loss 0.219268, acc 0.875
2017-03-02T18:03:13.543401: step 23687, loss 0.176806, acc 0.921875
2017-03-02T18:03:13.627224: step 23688, loss 0.130657, acc 0.9375
2017-03-02T18:03:13.690290: step 23689, loss 0.0659597, acc 0.984375
2017-03-02T18:03:13.753825: step 23690, loss 0.170931, acc 0.890625
2017-03-02T18:03:13.826973: step 23691, loss 0.0583335, acc 0.984375
2017-03-02T18:03:13.896993: step 23692, loss 0.108199, acc 0.953125
2017-03-02T18:03:13.970727: step 23693, loss 0.138105, acc 0.953125
2017-03-02T18:03:14.040191: step 23694, loss 0.130866, acc 0.9375
2017-03-02T18:03:14.108654: step 23695, loss 0.162261, acc 0.9375
2017-03-02T18:03:14.180501: step 23696, loss 0.211004, acc 0.9375
2017-03-02T18:03:14.255237: step 23697, loss 0.182359, acc 0.921875
2017-03-02T18:03:14.324508: step 23698, loss 0.259072, acc 0.859375
2017-03-02T18:03:14.398385: step 23699, loss 0.196719, acc 0.921875
2017-03-02T18:03:14.484105: step 23700, loss 0.0820249, acc 0.96875

Evaluation:
2017-03-02T18:03:14.517214: step 23700, loss 2.83746, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23700

2017-03-02T18:03:14.967183: step 23701, loss 0.130277, acc 0.953125
2017-03-02T18:03:15.039582: step 23702, loss 0.185339, acc 0.921875
2017-03-02T18:03:15.110590: step 23703, loss 0.108598, acc 0.953125
2017-03-02T18:03:15.179973: step 23704, loss 0.117468, acc 0.9375
2017-03-02T18:03:15.253473: step 23705, loss 0.200775, acc 0.890625
2017-03-02T18:03:15.323613: step 23706, loss 0.10418, acc 0.96875
2017-03-02T18:03:15.396341: step 23707, loss 0.113217, acc 0.921875
2017-03-02T18:03:15.465473: step 23708, loss 0.197501, acc 0.90625
2017-03-02T18:03:15.543033: step 23709, loss 0.155394, acc 0.90625
2017-03-02T18:03:15.636175: step 23710, loss 0.101779, acc 0.953125
2017-03-02T18:03:15.707167: step 23711, loss 0.152244, acc 0.953125
2017-03-02T18:03:15.776142: step 23712, loss 0.277294, acc 0.90625
2017-03-02T18:03:15.841021: step 23713, loss 0.0780596, acc 0.96875
2017-03-02T18:03:15.919663: step 23714, loss 0.0480466, acc 0.984375
2017-03-02T18:03:16.003407: step 23715, loss 0.184083, acc 0.9375
2017-03-02T18:03:16.072138: step 23716, loss 0.6255, acc 0.5
2017-03-02T18:03:16.141720: step 23717, loss 0.0862789, acc 0.953125
2017-03-02T18:03:16.217140: step 23718, loss 0.173323, acc 0.921875
2017-03-02T18:03:16.280878: step 23719, loss 0.128215, acc 0.9375
2017-03-02T18:03:16.343211: step 23720, loss 0.107811, acc 0.953125
2017-03-02T18:03:16.411284: step 23721, loss 0.133654, acc 0.96875
2017-03-02T18:03:16.490771: step 23722, loss 0.250483, acc 0.90625
2017-03-02T18:03:16.568385: step 23723, loss 0.193666, acc 0.90625
2017-03-02T18:03:16.638738: step 23724, loss 0.127001, acc 0.96875
2017-03-02T18:03:16.703126: step 23725, loss 0.110341, acc 0.9375
2017-03-02T18:03:16.774085: step 23726, loss 0.184674, acc 0.921875
2017-03-02T18:03:16.842686: step 23727, loss 0.227474, acc 0.875
2017-03-02T18:03:16.914248: step 23728, loss 0.102286, acc 1
2017-03-02T18:03:16.992582: step 23729, loss 0.270315, acc 0.890625
2017-03-02T18:03:17.058809: step 23730, loss 0.0805779, acc 0.96875
2017-03-02T18:03:17.123734: step 23731, loss 0.169808, acc 0.890625
2017-03-02T18:03:17.204645: step 23732, loss 0.142676, acc 0.9375
2017-03-02T18:03:17.282607: step 23733, loss 0.162418, acc 0.921875
2017-03-02T18:03:17.360526: step 23734, loss 0.109965, acc 0.921875
2017-03-02T18:03:17.423159: step 23735, loss 0.0499095, acc 0.984375
2017-03-02T18:03:17.490375: step 23736, loss 0.145441, acc 0.921875
2017-03-02T18:03:17.562754: step 23737, loss 0.140307, acc 0.9375
2017-03-02T18:03:17.642961: step 23738, loss 0.0826163, acc 0.96875
2017-03-02T18:03:17.722613: step 23739, loss 0.156118, acc 0.921875
2017-03-02T18:03:17.804085: step 23740, loss 0.276983, acc 0.875
2017-03-02T18:03:17.874779: step 23741, loss 0.230966, acc 0.875
2017-03-02T18:03:17.954790: step 23742, loss 0.144401, acc 0.953125
2017-03-02T18:03:18.029366: step 23743, loss 0.13079, acc 0.9375
2017-03-02T18:03:18.103841: step 23744, loss 0.0817207, acc 0.984375
2017-03-02T18:03:18.174810: step 23745, loss 0.180877, acc 0.921875
2017-03-02T18:03:18.246030: step 23746, loss 0.173032, acc 0.90625
2017-03-02T18:03:18.323839: step 23747, loss 0.125986, acc 0.953125
2017-03-02T18:03:18.406930: step 23748, loss 0.0887971, acc 0.953125
2017-03-02T18:03:18.489852: step 23749, loss 0.0993453, acc 0.953125
2017-03-02T18:03:18.564753: step 23750, loss 0.122924, acc 0.9375
2017-03-02T18:03:18.643056: step 23751, loss 0.11041, acc 0.9375
2017-03-02T18:03:18.714290: step 23752, loss 0.111187, acc 0.953125
2017-03-02T18:03:18.774709: step 23753, loss 0.188098, acc 0.875
2017-03-02T18:03:18.848631: step 23754, loss 0.182402, acc 0.921875
2017-03-02T18:03:18.928121: step 23755, loss 0.0785956, acc 0.96875
2017-03-02T18:03:19.003680: step 23756, loss 0.161357, acc 0.90625
2017-03-02T18:03:19.073737: step 23757, loss 0.143612, acc 0.90625
2017-03-02T18:03:19.150154: step 23758, loss 0.0999585, acc 0.96875
2017-03-02T18:03:19.221901: step 23759, loss 0.0763155, acc 0.96875
2017-03-02T18:03:19.290183: step 23760, loss 0.0836433, acc 0.96875
2017-03-02T18:03:19.360084: step 23761, loss 0.143965, acc 0.90625
2017-03-02T18:03:19.429660: step 23762, loss 0.0944267, acc 0.953125
2017-03-02T18:03:19.496418: step 23763, loss 0.188774, acc 0.890625
2017-03-02T18:03:19.558971: step 23764, loss 0.183969, acc 0.921875
2017-03-02T18:03:19.628676: step 23765, loss 0.172771, acc 0.9375
2017-03-02T18:03:19.704205: step 23766, loss 0.114817, acc 0.9375
2017-03-02T18:03:19.780650: step 23767, loss 0.126779, acc 0.9375
2017-03-02T18:03:19.862493: step 23768, loss 0.252705, acc 0.875
2017-03-02T18:03:19.938527: step 23769, loss 0.211252, acc 0.90625
2017-03-02T18:03:20.009915: step 23770, loss 0.198214, acc 0.890625
2017-03-02T18:03:20.088947: step 23771, loss 0.0936013, acc 0.96875
2017-03-02T18:03:20.156315: step 23772, loss 0.134708, acc 0.96875
2017-03-02T18:03:20.226085: step 23773, loss 0.176044, acc 0.921875
2017-03-02T18:03:20.292996: step 23774, loss 0.0939213, acc 0.9375
2017-03-02T18:03:20.366566: step 23775, loss 0.193261, acc 0.9375
2017-03-02T18:03:20.440060: step 23776, loss 0.294911, acc 0.890625
2017-03-02T18:03:20.516882: step 23777, loss 0.0721499, acc 0.953125
2017-03-02T18:03:20.587789: step 23778, loss 0.123576, acc 0.921875
2017-03-02T18:03:20.659314: step 23779, loss 0.0780511, acc 0.96875
2017-03-02T18:03:20.730897: step 23780, loss 0.176649, acc 0.9375
2017-03-02T18:03:20.808530: step 23781, loss 0.152002, acc 0.890625
2017-03-02T18:03:20.874379: step 23782, loss 0.150932, acc 0.953125
2017-03-02T18:03:20.943764: step 23783, loss 0.177538, acc 0.890625
2017-03-02T18:03:21.015648: step 23784, loss 0.277712, acc 0.875
2017-03-02T18:03:21.087794: step 23785, loss 0.178284, acc 0.953125
2017-03-02T18:03:21.167379: step 23786, loss 0.16362, acc 0.9375
2017-03-02T18:03:21.241165: step 23787, loss 0.135149, acc 0.953125
2017-03-02T18:03:21.315326: step 23788, loss 0.0727226, acc 0.984375
2017-03-02T18:03:21.389832: step 23789, loss 0.129757, acc 0.96875
2017-03-02T18:03:21.467455: step 23790, loss 0.0726153, acc 0.96875
2017-03-02T18:03:21.540721: step 23791, loss 0.225894, acc 0.90625
2017-03-02T18:03:21.609099: step 23792, loss 0.0489218, acc 1
2017-03-02T18:03:21.678902: step 23793, loss 0.0819571, acc 0.984375
2017-03-02T18:03:21.751078: step 23794, loss 0.166941, acc 0.9375
2017-03-02T18:03:21.830264: step 23795, loss 0.12957, acc 0.9375
2017-03-02T18:03:21.909838: step 23796, loss 0.131117, acc 0.9375
2017-03-02T18:03:21.981548: step 23797, loss 0.0931963, acc 0.953125
2017-03-02T18:03:22.059030: step 23798, loss 0.0870688, acc 0.953125
2017-03-02T18:03:22.134701: step 23799, loss 0.141016, acc 0.921875
2017-03-02T18:03:22.203760: step 23800, loss 0.152485, acc 0.921875

Evaluation:
2017-03-02T18:03:22.233240: step 23800, loss 2.84107, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23800

2017-03-02T18:03:22.661758: step 23801, loss 0.0632901, acc 0.96875
2017-03-02T18:03:22.736151: step 23802, loss 0.15637, acc 0.9375
2017-03-02T18:03:22.824390: step 23803, loss 0.125098, acc 0.96875
2017-03-02T18:03:22.896073: step 23804, loss 0.0778914, acc 0.96875
2017-03-02T18:03:22.959263: step 23805, loss 0.249375, acc 0.890625
2017-03-02T18:03:23.033751: step 23806, loss 0.134037, acc 0.90625
2017-03-02T18:03:23.105634: step 23807, loss 0.10186, acc 0.96875
2017-03-02T18:03:23.178199: step 23808, loss 0.134798, acc 0.90625
2017-03-02T18:03:23.248349: step 23809, loss 0.128088, acc 0.90625
2017-03-02T18:03:23.319892: step 23810, loss 0.0722132, acc 0.984375
2017-03-02T18:03:23.410896: step 23811, loss 0.247021, acc 0.90625
2017-03-02T18:03:23.488452: step 23812, loss 0.110168, acc 0.96875
2017-03-02T18:03:23.561729: step 23813, loss 0.144194, acc 0.921875
2017-03-02T18:03:23.631596: step 23814, loss 0.201069, acc 0.890625
2017-03-02T18:03:23.708604: step 23815, loss 0.107378, acc 0.96875
2017-03-02T18:03:23.801904: step 23816, loss 0.0534458, acc 0.96875
2017-03-02T18:03:23.873345: step 23817, loss 0.20987, acc 0.921875
2017-03-02T18:03:23.945883: step 23818, loss 0.147022, acc 0.921875
2017-03-02T18:03:24.027040: step 23819, loss 0.1026, acc 0.96875
2017-03-02T18:03:24.107599: step 23820, loss 0.174956, acc 0.953125
2017-03-02T18:03:24.185822: step 23821, loss 0.073805, acc 0.953125
2017-03-02T18:03:24.259586: step 23822, loss 0.17018, acc 0.953125
2017-03-02T18:03:24.328581: step 23823, loss 0.33303, acc 0.859375
2017-03-02T18:03:24.403556: step 23824, loss 0.0736821, acc 0.96875
2017-03-02T18:03:24.473921: step 23825, loss 0.0909587, acc 0.96875
2017-03-02T18:03:24.545392: step 23826, loss 0.163147, acc 0.9375
2017-03-02T18:03:24.616511: step 23827, loss 0.221666, acc 0.90625
2017-03-02T18:03:24.692338: step 23828, loss 0.15803, acc 0.9375
2017-03-02T18:03:24.768356: step 23829, loss 0.0886963, acc 0.96875
2017-03-02T18:03:24.844804: step 23830, loss 0.167762, acc 0.921875
2017-03-02T18:03:24.923106: step 23831, loss 0.153642, acc 0.9375
2017-03-02T18:03:24.995329: step 23832, loss 0.117194, acc 0.9375
2017-03-02T18:03:25.064648: step 23833, loss 0.152941, acc 0.96875
2017-03-02T18:03:25.137536: step 23834, loss 0.282991, acc 0.90625
2017-03-02T18:03:25.219148: step 23835, loss 0.29251, acc 0.84375
2017-03-02T18:03:25.295501: step 23836, loss 0.146077, acc 0.921875
2017-03-02T18:03:25.377000: step 23837, loss 0.0508389, acc 0.984375
2017-03-02T18:03:25.460177: step 23838, loss 0.0573217, acc 0.984375
2017-03-02T18:03:25.531613: step 23839, loss 0.34906, acc 0.90625
2017-03-02T18:03:25.601921: step 23840, loss 0.0922495, acc 0.953125
2017-03-02T18:03:25.669647: step 23841, loss 0.199786, acc 0.890625
2017-03-02T18:03:25.740882: step 23842, loss 0.174022, acc 0.890625
2017-03-02T18:03:25.812986: step 23843, loss 0.134328, acc 0.96875
2017-03-02T18:03:25.880387: step 23844, loss 0.223535, acc 0.90625
2017-03-02T18:03:25.943920: step 23845, loss 0.126838, acc 0.9375
2017-03-02T18:03:26.017587: step 23846, loss 0.17271, acc 0.9375
2017-03-02T18:03:26.093589: step 23847, loss 0.107411, acc 0.96875
2017-03-02T18:03:26.167509: step 23848, loss 0.104389, acc 0.9375
2017-03-02T18:03:26.238388: step 23849, loss 0.118287, acc 0.953125
2017-03-02T18:03:26.311525: step 23850, loss 0.245542, acc 0.875
2017-03-02T18:03:26.379800: step 23851, loss 0.171576, acc 0.953125
2017-03-02T18:03:26.450693: step 23852, loss 0.132772, acc 0.921875
2017-03-02T18:03:26.524733: step 23853, loss 0.164827, acc 0.90625
2017-03-02T18:03:26.594513: step 23854, loss 0.170311, acc 0.875
2017-03-02T18:03:26.677908: step 23855, loss 0.17217, acc 0.875
2017-03-02T18:03:26.751947: step 23856, loss 0.0970904, acc 0.9375
2017-03-02T18:03:26.823456: step 23857, loss 0.225752, acc 0.890625
2017-03-02T18:03:26.890269: step 23858, loss 0.241665, acc 0.890625
2017-03-02T18:03:26.962818: step 23859, loss 0.143473, acc 0.921875
2017-03-02T18:03:27.026940: step 23860, loss 0.103859, acc 0.9375
2017-03-02T18:03:27.101921: step 23861, loss 0.103042, acc 0.96875
2017-03-02T18:03:27.174614: step 23862, loss 0.077275, acc 0.96875
2017-03-02T18:03:27.253572: step 23863, loss 0.195899, acc 0.921875
2017-03-02T18:03:27.323783: step 23864, loss 0.0853406, acc 0.953125
2017-03-02T18:03:27.399695: step 23865, loss 0.0581401, acc 0.984375
2017-03-02T18:03:27.473155: step 23866, loss 0.150579, acc 0.9375
2017-03-02T18:03:27.554903: step 23867, loss 0.16023, acc 0.9375
2017-03-02T18:03:27.627967: step 23868, loss 0.213623, acc 0.921875
2017-03-02T18:03:27.701415: step 23869, loss 0.119163, acc 0.921875
2017-03-02T18:03:27.771762: step 23870, loss 0.189902, acc 0.890625
2017-03-02T18:03:27.843950: step 23871, loss 0.0851912, acc 0.96875
2017-03-02T18:03:27.918898: step 23872, loss 0.178427, acc 0.890625
2017-03-02T18:03:27.991006: step 23873, loss 0.110326, acc 0.953125
2017-03-02T18:03:28.055416: step 23874, loss 0.200518, acc 0.9375
2017-03-02T18:03:28.132661: step 23875, loss 0.120992, acc 0.921875
2017-03-02T18:03:28.204627: step 23876, loss 0.256841, acc 0.90625
2017-03-02T18:03:28.277335: step 23877, loss 0.214759, acc 0.890625
2017-03-02T18:03:28.351984: step 23878, loss 0.31982, acc 0.890625
2017-03-02T18:03:28.444929: step 23879, loss 0.143833, acc 0.921875
2017-03-02T18:03:28.515559: step 23880, loss 0.190868, acc 0.9375
2017-03-02T18:03:28.589675: step 23881, loss 0.0980388, acc 0.96875
2017-03-02T18:03:28.666519: step 23882, loss 0.113579, acc 0.9375
2017-03-02T18:03:28.738628: step 23883, loss 0.141915, acc 0.9375
2017-03-02T18:03:28.817269: step 23884, loss 0.128031, acc 0.953125
2017-03-02T18:03:28.884028: step 23885, loss 0.142616, acc 0.9375
2017-03-02T18:03:28.961496: step 23886, loss 0.118056, acc 0.953125
2017-03-02T18:03:29.032482: step 23887, loss 0.182411, acc 0.921875
2017-03-02T18:03:29.112039: step 23888, loss 0.0926095, acc 0.953125
2017-03-02T18:03:29.185443: step 23889, loss 0.118164, acc 0.953125
2017-03-02T18:03:29.259738: step 23890, loss 0.107064, acc 0.953125
2017-03-02T18:03:29.333945: step 23891, loss 0.189695, acc 0.90625
2017-03-02T18:03:29.421456: step 23892, loss 0.201658, acc 0.90625
2017-03-02T18:03:29.494221: step 23893, loss 0.142616, acc 0.953125
2017-03-02T18:03:29.565889: step 23894, loss 0.273254, acc 0.890625
2017-03-02T18:03:29.647874: step 23895, loss 0.161849, acc 0.921875
2017-03-02T18:03:29.717511: step 23896, loss 0.0875383, acc 0.96875
2017-03-02T18:03:29.786547: step 23897, loss 0.175319, acc 0.921875
2017-03-02T18:03:29.852459: step 23898, loss 0.24487, acc 0.859375
2017-03-02T18:03:29.925422: step 23899, loss 0.0975308, acc 0.96875
2017-03-02T18:03:29.997497: step 23900, loss 0.143203, acc 0.953125

Evaluation:
2017-03-02T18:03:30.031783: step 23900, loss 2.8388, acc 0.635184

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-23900

2017-03-02T18:03:30.476344: step 23901, loss 0.230301, acc 0.875
2017-03-02T18:03:30.547861: step 23902, loss 0.0946615, acc 0.953125
2017-03-02T18:03:30.618177: step 23903, loss 0.111341, acc 0.9375
2017-03-02T18:03:30.688674: step 23904, loss 0.0959989, acc 0.96875
2017-03-02T18:03:30.762071: step 23905, loss 0.174931, acc 0.921875
2017-03-02T18:03:30.833591: step 23906, loss 0.306694, acc 0.90625
2017-03-02T18:03:30.913319: step 23907, loss 0.0847124, acc 0.984375
2017-03-02T18:03:30.988235: step 23908, loss 0.205615, acc 0.90625
2017-03-02T18:03:31.065377: step 23909, loss 0.169793, acc 0.90625
2017-03-02T18:03:31.130498: step 23910, loss 0.143687, acc 0.953125
2017-03-02T18:03:31.202018: step 23911, loss 0.247459, acc 0.90625
2017-03-02T18:03:31.287106: step 23912, loss 0.188472, acc 1
2017-03-02T18:03:31.361611: step 23913, loss 0.112284, acc 0.96875
2017-03-02T18:03:31.431847: step 23914, loss 0.145048, acc 0.921875
2017-03-02T18:03:31.507002: step 23915, loss 0.173667, acc 0.921875
2017-03-02T18:03:31.583463: step 23916, loss 0.0892049, acc 0.96875
2017-03-02T18:03:31.650912: step 23917, loss 0.190788, acc 0.90625
2017-03-02T18:03:31.714833: step 23918, loss 0.163754, acc 0.90625
2017-03-02T18:03:31.782506: step 23919, loss 0.126387, acc 0.9375
2017-03-02T18:03:31.854878: step 23920, loss 0.0805038, acc 0.984375
2017-03-02T18:03:31.926333: step 23921, loss 0.155653, acc 0.9375
2017-03-02T18:03:31.991486: step 23922, loss 0.143896, acc 0.96875
2017-03-02T18:03:32.076347: step 23923, loss 0.167518, acc 0.921875
2017-03-02T18:03:32.150739: step 23924, loss 0.224808, acc 0.875
2017-03-02T18:03:32.221170: step 23925, loss 0.176794, acc 0.9375
2017-03-02T18:03:32.288060: step 23926, loss 0.197443, acc 0.90625
2017-03-02T18:03:32.375194: step 23927, loss 0.149144, acc 0.9375
2017-03-02T18:03:32.450721: step 23928, loss 0.0822026, acc 0.96875
2017-03-02T18:03:32.521407: step 23929, loss 0.0963678, acc 0.96875
2017-03-02T18:03:32.600137: step 23930, loss 0.17223, acc 0.921875
2017-03-02T18:03:32.671728: step 23931, loss 0.166278, acc 0.90625
2017-03-02T18:03:32.739602: step 23932, loss 0.151372, acc 0.921875
2017-03-02T18:03:32.810880: step 23933, loss 0.176615, acc 0.921875
2017-03-02T18:03:32.876668: step 23934, loss 0.150486, acc 0.921875
2017-03-02T18:03:32.943318: step 23935, loss 0.137463, acc 0.953125
2017-03-02T18:03:33.020757: step 23936, loss 0.0571598, acc 0.984375
2017-03-02T18:03:33.091220: step 23937, loss 0.088167, acc 0.96875
2017-03-02T18:03:33.164699: step 23938, loss 0.134193, acc 0.921875
2017-03-02T18:03:33.229774: step 23939, loss 0.162533, acc 0.90625
2017-03-02T18:03:33.300582: step 23940, loss 0.0836676, acc 0.953125
2017-03-02T18:03:33.372620: step 23941, loss 0.119805, acc 0.984375
2017-03-02T18:03:33.449218: step 23942, loss 0.14033, acc 0.9375
2017-03-02T18:03:33.524769: step 23943, loss 0.159675, acc 0.953125
2017-03-02T18:03:33.597095: step 23944, loss 0.126481, acc 0.9375
2017-03-02T18:03:33.674960: step 23945, loss 0.0800088, acc 0.984375
2017-03-02T18:03:33.750585: step 23946, loss 0.265028, acc 0.84375
2017-03-02T18:03:33.826600: step 23947, loss 0.130544, acc 0.9375
2017-03-02T18:03:33.895104: step 23948, loss 0.186245, acc 0.921875
2017-03-02T18:03:33.965280: step 23949, loss 0.247881, acc 0.921875
2017-03-02T18:03:34.039456: step 23950, loss 0.14564, acc 0.921875
2017-03-02T18:03:34.111579: step 23951, loss 0.182674, acc 0.921875
2017-03-02T18:03:34.182086: step 23952, loss 0.173341, acc 0.90625
2017-03-02T18:03:34.252395: step 23953, loss 0.21093, acc 0.890625
2017-03-02T18:03:34.329211: step 23954, loss 0.148225, acc 0.90625
2017-03-02T18:03:34.398219: step 23955, loss 0.16238, acc 0.921875
2017-03-02T18:03:34.468576: step 23956, loss 0.173382, acc 0.921875
2017-03-02T18:03:34.543511: step 23957, loss 0.136939, acc 0.953125
2017-03-02T18:03:34.619698: step 23958, loss 0.148971, acc 0.921875
2017-03-02T18:03:34.689892: step 23959, loss 0.207027, acc 0.890625
2017-03-02T18:03:34.760355: step 23960, loss 0.265913, acc 0.84375
2017-03-02T18:03:34.828327: step 23961, loss 0.158552, acc 0.921875
2017-03-02T18:03:34.899819: step 23962, loss 0.243727, acc 0.90625
2017-03-02T18:03:34.975030: step 23963, loss 0.110188, acc 0.953125
2017-03-02T18:03:35.047979: step 23964, loss 0.168956, acc 0.9375
2017-03-02T18:03:35.118165: step 23965, loss 0.080228, acc 0.984375
2017-03-02T18:03:35.192539: step 23966, loss 0.117542, acc 0.953125
2017-03-02T18:03:35.265822: step 23967, loss 0.167329, acc 0.921875
2017-03-02T18:03:35.334585: step 23968, loss 0.100373, acc 0.953125
2017-03-02T18:03:35.412981: step 23969, loss 0.0879767, acc 0.953125
2017-03-02T18:03:35.482136: step 23970, loss 0.179407, acc 0.9375
2017-03-02T18:03:35.552079: step 23971, loss 0.163223, acc 0.921875
2017-03-02T18:03:35.628565: step 23972, loss 0.0588878, acc 0.984375
2017-03-02T18:03:35.700810: step 23973, loss 0.178817, acc 0.921875
2017-03-02T18:03:35.771021: step 23974, loss 0.0503701, acc 1
2017-03-02T18:03:35.845175: step 23975, loss 0.156553, acc 0.921875
2017-03-02T18:03:35.924547: step 23976, loss 0.0740151, acc 0.953125
2017-03-02T18:03:35.990421: step 23977, loss 0.0743619, acc 0.96875
2017-03-02T18:03:36.064660: step 23978, loss 0.160366, acc 0.9375
2017-03-02T18:03:36.136772: step 23979, loss 0.109149, acc 0.953125
2017-03-02T18:03:36.208415: step 23980, loss 0.088577, acc 0.953125
2017-03-02T18:03:36.274049: step 23981, loss 0.0870361, acc 0.96875
2017-03-02T18:03:36.341815: step 23982, loss 0.137706, acc 0.921875
2017-03-02T18:03:36.414553: step 23983, loss 0.0980417, acc 0.96875
2017-03-02T18:03:36.484872: step 23984, loss 0.0803829, acc 0.96875
2017-03-02T18:03:36.562497: step 23985, loss 0.0756319, acc 0.96875
2017-03-02T18:03:36.638151: step 23986, loss 0.104218, acc 0.9375
2017-03-02T18:03:36.707322: step 23987, loss 0.141626, acc 0.921875
2017-03-02T18:03:36.801562: step 23988, loss 0.132771, acc 0.953125
2017-03-02T18:03:36.880768: step 23989, loss 0.0413927, acc 0.984375
2017-03-02T18:03:36.944134: step 23990, loss 0.175377, acc 0.921875
2017-03-02T18:03:37.016554: step 23991, loss 0.207253, acc 0.921875
2017-03-02T18:03:37.086714: step 23992, loss 0.0909662, acc 0.96875
2017-03-02T18:03:37.155408: step 23993, loss 0.138837, acc 0.953125
2017-03-02T18:03:37.229528: step 23994, loss 0.0718992, acc 0.984375
2017-03-02T18:03:37.302242: step 23995, loss 0.174727, acc 0.921875
2017-03-02T18:03:37.371883: step 23996, loss 0.0632574, acc 0.984375
2017-03-02T18:03:37.461252: step 23997, loss 0.207002, acc 0.921875
2017-03-02T18:03:37.532509: step 23998, loss 0.0930733, acc 0.953125
2017-03-02T18:03:37.594790: step 23999, loss 0.160978, acc 0.9375
2017-03-02T18:03:37.670864: step 24000, loss 0.0698939, acc 0.984375

Evaluation:
2017-03-02T18:03:37.707094: step 24000, loss 2.97839, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24000

2017-03-02T18:03:38.187145: step 24001, loss 0.151146, acc 0.921875
2017-03-02T18:03:38.259300: step 24002, loss 0.28965, acc 0.90625
2017-03-02T18:03:38.331760: step 24003, loss 0.224562, acc 0.90625
2017-03-02T18:03:38.406295: step 24004, loss 0.212352, acc 0.9375
2017-03-02T18:03:38.479871: step 24005, loss 0.167601, acc 0.90625
2017-03-02T18:03:38.551120: step 24006, loss 0.121223, acc 0.921875
2017-03-02T18:03:38.633136: step 24007, loss 0.0800913, acc 0.96875
2017-03-02T18:03:38.700577: step 24008, loss 0.225045, acc 0.890625
2017-03-02T18:03:38.775499: step 24009, loss 0.202995, acc 0.875
2017-03-02T18:03:38.849756: step 24010, loss 0.169022, acc 0.921875
2017-03-02T18:03:38.918962: step 24011, loss 0.08682, acc 0.984375
2017-03-02T18:03:38.998672: step 24012, loss 0.0710828, acc 0.96875
2017-03-02T18:03:39.076646: step 24013, loss 0.177255, acc 0.9375
2017-03-02T18:03:39.146983: step 24014, loss 0.188528, acc 0.953125
2017-03-02T18:03:39.219436: step 24015, loss 0.106965, acc 0.9375
2017-03-02T18:03:39.291527: step 24016, loss 0.175207, acc 0.90625
2017-03-02T18:03:39.368452: step 24017, loss 0.0867289, acc 0.9375
2017-03-02T18:03:39.443642: step 24018, loss 0.208453, acc 0.921875
2017-03-02T18:03:39.513911: step 24019, loss 0.155079, acc 0.921875
2017-03-02T18:03:39.581331: step 24020, loss 0.115719, acc 0.9375
2017-03-02T18:03:39.652965: step 24021, loss 0.0660797, acc 0.96875
2017-03-02T18:03:39.730290: step 24022, loss 0.121935, acc 0.9375
2017-03-02T18:03:39.805155: step 24023, loss 0.193577, acc 0.921875
2017-03-02T18:03:39.882599: step 24024, loss 0.17799, acc 0.875
2017-03-02T18:03:39.954312: step 24025, loss 0.216614, acc 0.921875
2017-03-02T18:03:40.031248: step 24026, loss 0.132871, acc 0.9375
2017-03-02T18:03:40.109383: step 24027, loss 0.113793, acc 0.96875
2017-03-02T18:03:40.185106: step 24028, loss 0.123171, acc 0.96875
2017-03-02T18:03:40.260482: step 24029, loss 0.0865943, acc 0.96875
2017-03-02T18:03:40.335103: step 24030, loss 0.0925055, acc 0.953125
2017-03-02T18:03:40.408589: step 24031, loss 0.0918858, acc 0.9375
2017-03-02T18:03:40.482414: step 24032, loss 0.126948, acc 0.953125
2017-03-02T18:03:40.566858: step 24033, loss 0.171635, acc 0.921875
2017-03-02T18:03:40.640523: step 24034, loss 0.159129, acc 0.921875
2017-03-02T18:03:40.710496: step 24035, loss 0.167023, acc 0.9375
2017-03-02T18:03:40.781861: step 24036, loss 0.104707, acc 0.96875
2017-03-02T18:03:40.854212: step 24037, loss 0.0995824, acc 0.953125
2017-03-02T18:03:40.927186: step 24038, loss 0.165151, acc 0.921875
2017-03-02T18:03:40.998545: step 24039, loss 0.317427, acc 0.828125
2017-03-02T18:03:41.068490: step 24040, loss 0.138858, acc 0.9375
2017-03-02T18:03:41.141958: step 24041, loss 0.143519, acc 0.90625
2017-03-02T18:03:41.213071: step 24042, loss 0.114926, acc 0.9375
2017-03-02T18:03:41.291543: step 24043, loss 0.167901, acc 0.96875
2017-03-02T18:03:41.364338: step 24044, loss 0.146255, acc 0.921875
2017-03-02T18:03:41.434514: step 24045, loss 0.213241, acc 0.921875
2017-03-02T18:03:41.507098: step 24046, loss 0.163577, acc 0.9375
2017-03-02T18:03:41.581760: step 24047, loss 0.192077, acc 0.921875
2017-03-02T18:03:41.653256: step 24048, loss 0.115928, acc 0.9375
2017-03-02T18:03:41.719381: step 24049, loss 0.324828, acc 0.859375
2017-03-02T18:03:41.794171: step 24050, loss 0.122899, acc 0.953125
2017-03-02T18:03:41.862448: step 24051, loss 0.143998, acc 0.9375
2017-03-02T18:03:41.923707: step 24052, loss 0.121566, acc 0.953125
2017-03-02T18:03:41.988820: step 24053, loss 0.188245, acc 0.921875
2017-03-02T18:03:42.063877: step 24054, loss 0.236299, acc 0.921875
2017-03-02T18:03:42.136184: step 24055, loss 0.140316, acc 0.9375
2017-03-02T18:03:42.217129: step 24056, loss 0.0874009, acc 0.953125
2017-03-02T18:03:42.296023: step 24057, loss 0.228791, acc 0.859375
2017-03-02T18:03:42.363663: step 24058, loss 0.0813156, acc 0.953125
2017-03-02T18:03:42.440287: step 24059, loss 0.132998, acc 0.921875
2017-03-02T18:03:42.508380: step 24060, loss 0.167076, acc 0.90625
2017-03-02T18:03:42.579480: step 24061, loss 0.121997, acc 0.9375
2017-03-02T18:03:42.665555: step 24062, loss 0.133121, acc 0.9375
2017-03-02T18:03:42.735816: step 24063, loss 0.146723, acc 0.90625
2017-03-02T18:03:42.802727: step 24064, loss 0.126897, acc 0.921875
2017-03-02T18:03:42.872588: step 24065, loss 0.173257, acc 0.921875
2017-03-02T18:03:42.945123: step 24066, loss 0.04183, acc 0.984375
2017-03-02T18:03:43.013386: step 24067, loss 0.155471, acc 0.875
2017-03-02T18:03:43.083082: step 24068, loss 0.186418, acc 0.90625
2017-03-02T18:03:43.154556: step 24069, loss 0.144966, acc 0.9375
2017-03-02T18:03:43.230373: step 24070, loss 0.237524, acc 0.90625
2017-03-02T18:03:43.302714: step 24071, loss 0.103835, acc 0.96875
2017-03-02T18:03:43.369423: step 24072, loss 0.176372, acc 0.921875
2017-03-02T18:03:43.437921: step 24073, loss 0.141641, acc 0.9375
2017-03-02T18:03:43.507087: step 24074, loss 0.243352, acc 0.921875
2017-03-02T18:03:43.569705: step 24075, loss 0.17384, acc 0.921875
2017-03-02T18:03:43.630791: step 24076, loss 0.095028, acc 0.96875
2017-03-02T18:03:43.701769: step 24077, loss 0.21039, acc 0.90625
2017-03-02T18:03:43.772494: step 24078, loss 0.128178, acc 0.9375
2017-03-02T18:03:43.839461: step 24079, loss 0.113306, acc 0.9375
2017-03-02T18:03:43.914930: step 24080, loss 0.3411, acc 0.890625
2017-03-02T18:03:43.993118: step 24081, loss 0.117337, acc 0.9375
2017-03-02T18:03:44.066473: step 24082, loss 0.205232, acc 0.890625
2017-03-02T18:03:44.136700: step 24083, loss 0.155681, acc 0.921875
2017-03-02T18:03:44.208780: step 24084, loss 0.0320785, acc 1
2017-03-02T18:03:44.287671: step 24085, loss 0.16204, acc 0.90625
2017-03-02T18:03:44.357782: step 24086, loss 0.197901, acc 0.921875
2017-03-02T18:03:44.435725: step 24087, loss 0.270319, acc 0.90625
2017-03-02T18:03:44.508655: step 24088, loss 0.134516, acc 0.9375
2017-03-02T18:03:44.591244: step 24089, loss 0.115573, acc 0.9375
2017-03-02T18:03:44.662765: step 24090, loss 0.106657, acc 0.9375
2017-03-02T18:03:44.750187: step 24091, loss 0.266691, acc 0.875
2017-03-02T18:03:44.816978: step 24092, loss 0.114645, acc 0.953125
2017-03-02T18:03:44.894148: step 24093, loss 0.193838, acc 0.921875
2017-03-02T18:03:44.973142: step 24094, loss 0.0690448, acc 0.984375
2017-03-02T18:03:45.049480: step 24095, loss 0.0564362, acc 0.984375
2017-03-02T18:03:45.122714: step 24096, loss 0.15874, acc 0.9375
2017-03-02T18:03:45.195963: step 24097, loss 0.173354, acc 0.9375
2017-03-02T18:03:45.265453: step 24098, loss 0.189282, acc 0.9375
2017-03-02T18:03:45.341337: step 24099, loss 0.190293, acc 0.90625
2017-03-02T18:03:45.416349: step 24100, loss 0.101503, acc 0.953125

Evaluation:
2017-03-02T18:03:45.442814: step 24100, loss 2.87926, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24100

2017-03-02T18:03:45.882447: step 24101, loss 0.202483, acc 0.90625
2017-03-02T18:03:45.957258: step 24102, loss 0.211403, acc 0.90625
2017-03-02T18:03:46.024717: step 24103, loss 0.181997, acc 0.9375
2017-03-02T18:03:46.092840: step 24104, loss 0.130986, acc 0.953125
2017-03-02T18:03:46.177417: step 24105, loss 0.163971, acc 0.9375
2017-03-02T18:03:46.252683: step 24106, loss 0.157716, acc 0.9375
2017-03-02T18:03:46.325066: step 24107, loss 0.169418, acc 0.9375
2017-03-02T18:03:46.399215: step 24108, loss 1.81794e-06, acc 1
2017-03-02T18:03:46.462909: step 24109, loss 0.101244, acc 0.96875
2017-03-02T18:03:46.529489: step 24110, loss 0.135915, acc 0.9375
2017-03-02T18:03:46.601428: step 24111, loss 0.121479, acc 0.953125
2017-03-02T18:03:46.671884: step 24112, loss 0.17698, acc 0.953125
2017-03-02T18:03:46.747729: step 24113, loss 0.259597, acc 0.890625
2017-03-02T18:03:46.825758: step 24114, loss 0.113603, acc 0.9375
2017-03-02T18:03:46.902235: step 24115, loss 0.126687, acc 0.953125
2017-03-02T18:03:46.972950: step 24116, loss 0.189828, acc 0.921875
2017-03-02T18:03:47.045324: step 24117, loss 0.159128, acc 0.9375
2017-03-02T18:03:47.117663: step 24118, loss 0.167377, acc 0.921875
2017-03-02T18:03:47.189060: step 24119, loss 0.174052, acc 0.90625
2017-03-02T18:03:47.258185: step 24120, loss 0.155109, acc 0.921875
2017-03-02T18:03:47.333066: step 24121, loss 0.153396, acc 0.890625
2017-03-02T18:03:47.412942: step 24122, loss 0.0956297, acc 0.96875
2017-03-02T18:03:47.490018: step 24123, loss 0.145022, acc 0.953125
2017-03-02T18:03:47.564516: step 24124, loss 0.138773, acc 0.9375
2017-03-02T18:03:47.631240: step 24125, loss 0.0693969, acc 0.96875
2017-03-02T18:03:47.708689: step 24126, loss 0.218945, acc 0.859375
2017-03-02T18:03:47.777552: step 24127, loss 0.188324, acc 0.921875
2017-03-02T18:03:47.849129: step 24128, loss 0.112685, acc 0.953125
2017-03-02T18:03:47.919138: step 24129, loss 0.176802, acc 0.921875
2017-03-02T18:03:47.993871: step 24130, loss 0.139746, acc 0.9375
2017-03-02T18:03:48.068949: step 24131, loss 0.148564, acc 0.921875
2017-03-02T18:03:48.136658: step 24132, loss 0.137386, acc 0.9375
2017-03-02T18:03:48.208436: step 24133, loss 0.127451, acc 0.96875
2017-03-02T18:03:48.286872: step 24134, loss 0.218838, acc 0.90625
2017-03-02T18:03:48.352836: step 24135, loss 0.122359, acc 0.9375
2017-03-02T18:03:48.423108: step 24136, loss 0.119423, acc 0.953125
2017-03-02T18:03:48.499800: step 24137, loss 0.127126, acc 0.9375
2017-03-02T18:03:48.568943: step 24138, loss 0.0889198, acc 0.984375
2017-03-02T18:03:48.639190: step 24139, loss 0.137708, acc 0.9375
2017-03-02T18:03:48.711115: step 24140, loss 0.138162, acc 0.953125
2017-03-02T18:03:48.783477: step 24141, loss 0.209088, acc 0.921875
2017-03-02T18:03:48.858509: step 24142, loss 0.129075, acc 0.921875
2017-03-02T18:03:48.928746: step 24143, loss 0.180644, acc 0.90625
2017-03-02T18:03:49.000158: step 24144, loss 0.0530392, acc 0.953125
2017-03-02T18:03:49.073169: step 24145, loss 0.233197, acc 0.90625
2017-03-02T18:03:49.145990: step 24146, loss 0.243746, acc 0.875
2017-03-02T18:03:49.215827: step 24147, loss 0.126642, acc 0.953125
2017-03-02T18:03:49.287991: step 24148, loss 0.106769, acc 0.953125
2017-03-02T18:03:49.360450: step 24149, loss 0.0834929, acc 0.984375
2017-03-02T18:03:49.437119: step 24150, loss 0.0856346, acc 0.953125
2017-03-02T18:03:49.506953: step 24151, loss 0.129172, acc 0.953125
2017-03-02T18:03:49.579102: step 24152, loss 0.190549, acc 0.921875
2017-03-02T18:03:49.645150: step 24153, loss 0.170533, acc 0.9375
2017-03-02T18:03:49.720494: step 24154, loss 0.182222, acc 0.9375
2017-03-02T18:03:49.803397: step 24155, loss 0.0274618, acc 0.984375
2017-03-02T18:03:49.870475: step 24156, loss 0.0691345, acc 0.953125
2017-03-02T18:03:49.953140: step 24157, loss 0.131634, acc 0.921875
2017-03-02T18:03:50.031979: step 24158, loss 0.218145, acc 0.875
2017-03-02T18:03:50.107380: step 24159, loss 0.138209, acc 0.921875
2017-03-02T18:03:50.178580: step 24160, loss 0.225226, acc 0.90625
2017-03-02T18:03:50.252947: step 24161, loss 0.24933, acc 0.890625
2017-03-02T18:03:50.322389: step 24162, loss 0.217829, acc 0.890625
2017-03-02T18:03:50.397061: step 24163, loss 0.128443, acc 0.953125
2017-03-02T18:03:50.465985: step 24164, loss 0.101286, acc 0.953125
2017-03-02T18:03:50.556080: step 24165, loss 0.0966427, acc 0.96875
2017-03-02T18:03:50.634135: step 24166, loss 0.0859272, acc 0.96875
2017-03-02T18:03:50.702054: step 24167, loss 0.132704, acc 0.9375
2017-03-02T18:03:50.774974: step 24168, loss 0.153838, acc 0.9375
2017-03-02T18:03:50.847103: step 24169, loss 0.159697, acc 0.9375
2017-03-02T18:03:50.912564: step 24170, loss 0.107078, acc 0.953125
2017-03-02T18:03:50.977083: step 24171, loss 0.124666, acc 0.96875
2017-03-02T18:03:51.067071: step 24172, loss 0.167593, acc 0.90625
2017-03-02T18:03:51.145140: step 24173, loss 0.229013, acc 0.875
2017-03-02T18:03:51.209521: step 24174, loss 0.140741, acc 0.9375
2017-03-02T18:03:51.280418: step 24175, loss 0.156115, acc 0.90625
2017-03-02T18:03:51.348700: step 24176, loss 0.192503, acc 0.953125
2017-03-02T18:03:51.418054: step 24177, loss 0.148409, acc 0.90625
2017-03-02T18:03:51.488348: step 24178, loss 0.185318, acc 0.921875
2017-03-02T18:03:51.565192: step 24179, loss 0.179982, acc 0.890625
2017-03-02T18:03:51.639104: step 24180, loss 0.105345, acc 0.953125
2017-03-02T18:03:51.724127: step 24181, loss 0.181035, acc 0.921875
2017-03-02T18:03:51.797161: step 24182, loss 0.148432, acc 0.9375
2017-03-02T18:03:51.872344: step 24183, loss 0.211019, acc 0.90625
2017-03-02T18:03:51.943510: step 24184, loss 0.133456, acc 0.953125
2017-03-02T18:03:52.013166: step 24185, loss 0.19303, acc 0.890625
2017-03-02T18:03:52.088753: step 24186, loss 0.174565, acc 0.90625
2017-03-02T18:03:52.169157: step 24187, loss 0.106329, acc 0.96875
2017-03-02T18:03:52.243723: step 24188, loss 0.23119, acc 0.890625
2017-03-02T18:03:52.316766: step 24189, loss 0.0729058, acc 0.96875
2017-03-02T18:03:52.392802: step 24190, loss 0.110508, acc 0.953125
2017-03-02T18:03:52.464870: step 24191, loss 0.14874, acc 0.921875
2017-03-02T18:03:52.536136: step 24192, loss 0.094575, acc 0.96875
2017-03-02T18:03:52.601208: step 24193, loss 0.163384, acc 0.890625
2017-03-02T18:03:52.682721: step 24194, loss 0.0684748, acc 0.96875
2017-03-02T18:03:52.765462: step 24195, loss 0.0951198, acc 0.96875
2017-03-02T18:03:52.841622: step 24196, loss 0.164771, acc 0.9375
2017-03-02T18:03:52.910966: step 24197, loss 0.18047, acc 0.90625
2017-03-02T18:03:53.002665: step 24198, loss 0.197807, acc 0.90625
2017-03-02T18:03:53.081239: step 24199, loss 0.127646, acc 0.9375
2017-03-02T18:03:53.151025: step 24200, loss 0.168279, acc 0.90625

Evaluation:
2017-03-02T18:03:53.187254: step 24200, loss 2.90371, acc 0.636626

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24200

2017-03-02T18:03:53.665951: step 24201, loss 0.191893, acc 0.875
2017-03-02T18:03:53.743209: step 24202, loss 0.304023, acc 0.828125
2017-03-02T18:03:53.812430: step 24203, loss 0.298353, acc 0.890625
2017-03-02T18:03:53.884017: step 24204, loss 0.215757, acc 0.890625
2017-03-02T18:03:53.961126: step 24205, loss 0.19551, acc 0.890625
2017-03-02T18:03:54.031554: step 24206, loss 0.10892, acc 0.96875
2017-03-02T18:03:54.101961: step 24207, loss 0.0306095, acc 1
2017-03-02T18:03:54.193682: step 24208, loss 0.101879, acc 0.9375
2017-03-02T18:03:54.260108: step 24209, loss 0.0855794, acc 0.953125
2017-03-02T18:03:54.329995: step 24210, loss 0.0907113, acc 0.96875
2017-03-02T18:03:54.402326: step 24211, loss 0.193484, acc 0.921875
2017-03-02T18:03:54.477417: step 24212, loss 0.150528, acc 0.90625
2017-03-02T18:03:54.548594: step 24213, loss 0.229258, acc 0.859375
2017-03-02T18:03:54.615255: step 24214, loss 0.160685, acc 0.9375
2017-03-02T18:03:54.691368: step 24215, loss 0.0799011, acc 0.953125
2017-03-02T18:03:54.762235: step 24216, loss 0.0595831, acc 0.96875
2017-03-02T18:03:54.833536: step 24217, loss 0.0865181, acc 0.96875
2017-03-02T18:03:54.903215: step 24218, loss 0.107182, acc 0.921875
2017-03-02T18:03:54.984400: step 24219, loss 0.0956897, acc 0.96875
2017-03-02T18:03:55.054866: step 24220, loss 0.0721858, acc 0.96875
2017-03-02T18:03:55.129425: step 24221, loss 0.156404, acc 0.953125
2017-03-02T18:03:55.211697: step 24222, loss 0.182949, acc 0.9375
2017-03-02T18:03:55.277849: step 24223, loss 0.153599, acc 0.9375
2017-03-02T18:03:55.347723: step 24224, loss 0.202905, acc 0.90625
2017-03-02T18:03:55.421534: step 24225, loss 0.115255, acc 0.984375
2017-03-02T18:03:55.497997: step 24226, loss 0.14742, acc 0.9375
2017-03-02T18:03:55.581309: step 24227, loss 0.120003, acc 0.953125
2017-03-02T18:03:55.653593: step 24228, loss 0.123692, acc 0.9375
2017-03-02T18:03:55.725659: step 24229, loss 0.209501, acc 0.90625
2017-03-02T18:03:55.793976: step 24230, loss 0.123364, acc 0.9375
2017-03-02T18:03:55.864027: step 24231, loss 0.117171, acc 0.9375
2017-03-02T18:03:55.937888: step 24232, loss 0.17504, acc 0.90625
2017-03-02T18:03:56.012409: step 24233, loss 0.129479, acc 0.96875
2017-03-02T18:03:56.084574: step 24234, loss 0.22339, acc 0.90625
2017-03-02T18:03:56.153650: step 24235, loss 0.114507, acc 0.9375
2017-03-02T18:03:56.225136: step 24236, loss 0.178342, acc 0.9375
2017-03-02T18:03:56.296964: step 24237, loss 0.222963, acc 0.890625
2017-03-02T18:03:56.361156: step 24238, loss 0.181406, acc 0.921875
2017-03-02T18:03:56.431659: step 24239, loss 0.180792, acc 0.921875
2017-03-02T18:03:56.504197: step 24240, loss 0.173858, acc 0.90625
2017-03-02T18:03:56.577622: step 24241, loss 0.117914, acc 0.9375
2017-03-02T18:03:56.653657: step 24242, loss 0.216969, acc 0.890625
2017-03-02T18:03:56.737307: step 24243, loss 0.0860323, acc 0.96875
2017-03-02T18:03:56.804342: step 24244, loss 0.178411, acc 0.921875
2017-03-02T18:03:56.875251: step 24245, loss 0.0405099, acc 0.96875
2017-03-02T18:03:56.950613: step 24246, loss 0.131533, acc 0.96875
2017-03-02T18:03:57.020058: step 24247, loss 0.244821, acc 0.875
2017-03-02T18:03:57.089331: step 24248, loss 0.108576, acc 0.96875
2017-03-02T18:03:57.164706: step 24249, loss 0.127862, acc 0.9375
2017-03-02T18:03:57.243194: step 24250, loss 0.235932, acc 0.921875
2017-03-02T18:03:57.317202: step 24251, loss 0.115951, acc 0.96875
2017-03-02T18:03:57.388620: step 24252, loss 0.0678034, acc 0.96875
2017-03-02T18:03:57.455608: step 24253, loss 0.100014, acc 0.984375
2017-03-02T18:03:57.530153: step 24254, loss 0.156549, acc 0.921875
2017-03-02T18:03:57.604358: step 24255, loss 0.124542, acc 0.953125
2017-03-02T18:03:57.673231: step 24256, loss 0.113383, acc 0.953125
2017-03-02T18:03:57.746311: step 24257, loss 0.251092, acc 0.90625
2017-03-02T18:03:57.820578: step 24258, loss 0.129449, acc 0.953125
2017-03-02T18:03:57.896851: step 24259, loss 0.166433, acc 0.9375
2017-03-02T18:03:57.970856: step 24260, loss 0.107137, acc 0.96875
2017-03-02T18:03:58.045663: step 24261, loss 0.115269, acc 0.9375
2017-03-02T18:03:58.108173: step 24262, loss 0.154379, acc 0.921875
2017-03-02T18:03:58.180173: step 24263, loss 0.167044, acc 0.9375
2017-03-02T18:03:58.257378: step 24264, loss 0.126517, acc 0.9375
2017-03-02T18:03:58.330653: step 24265, loss 0.115178, acc 0.9375
2017-03-02T18:03:58.396155: step 24266, loss 0.162392, acc 0.90625
2017-03-02T18:03:58.474067: step 24267, loss 0.103868, acc 0.984375
2017-03-02T18:03:58.545647: step 24268, loss 0.141262, acc 0.953125
2017-03-02T18:03:58.615057: step 24269, loss 0.081584, acc 0.953125
2017-03-02T18:03:58.685838: step 24270, loss 0.209403, acc 0.890625
2017-03-02T18:03:58.766061: step 24271, loss 0.204154, acc 0.9375
2017-03-02T18:03:58.845330: step 24272, loss 0.192717, acc 0.9375
2017-03-02T18:03:58.921954: step 24273, loss 0.156158, acc 0.90625
2017-03-02T18:03:58.986836: step 24274, loss 0.116911, acc 0.953125
2017-03-02T18:03:59.056710: step 24275, loss 0.0717997, acc 0.984375
2017-03-02T18:03:59.129317: step 24276, loss 0.321351, acc 0.90625
2017-03-02T18:03:59.199385: step 24277, loss 0.128773, acc 0.9375
2017-03-02T18:03:59.269454: step 24278, loss 0.16904, acc 0.890625
2017-03-02T18:03:59.344560: step 24279, loss 0.11005, acc 0.9375
2017-03-02T18:03:59.417620: step 24280, loss 0.226116, acc 0.90625
2017-03-02T18:03:59.489109: step 24281, loss 0.1065, acc 0.921875
2017-03-02T18:03:59.563510: step 24282, loss 0.160863, acc 0.921875
2017-03-02T18:03:59.641355: step 24283, loss 0.190808, acc 0.921875
2017-03-02T18:03:59.714598: step 24284, loss 0.179581, acc 0.921875
2017-03-02T18:03:59.787050: step 24285, loss 0.0807865, acc 0.96875
2017-03-02T18:03:59.855304: step 24286, loss 0.0478136, acc 0.984375
2017-03-02T18:03:59.930291: step 24287, loss 0.117995, acc 0.96875
2017-03-02T18:04:00.002702: step 24288, loss 0.182226, acc 0.96875
2017-03-02T18:04:00.076147: step 24289, loss 0.123684, acc 0.9375
2017-03-02T18:04:00.153960: step 24290, loss 0.199404, acc 0.921875
2017-03-02T18:04:00.231796: step 24291, loss 0.279211, acc 0.859375
2017-03-02T18:04:00.307134: step 24292, loss 0.189065, acc 0.921875
2017-03-02T18:04:00.384172: step 24293, loss 0.200648, acc 0.90625
2017-03-02T18:04:00.458562: step 24294, loss 0.144857, acc 0.9375
2017-03-02T18:04:00.535378: step 24295, loss 0.126484, acc 0.953125
2017-03-02T18:04:00.616417: step 24296, loss 0.151181, acc 0.921875
2017-03-02T18:04:00.697806: step 24297, loss 0.162804, acc 0.921875
2017-03-02T18:04:00.797747: step 24298, loss 0.152881, acc 0.9375
2017-03-02T18:04:00.874685: step 24299, loss 0.0771612, acc 0.96875
2017-03-02T18:04:00.943874: step 24300, loss 0.136092, acc 0.921875

Evaluation:
2017-03-02T18:04:00.980310: step 24300, loss 2.90557, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24300

2017-03-02T18:04:01.490237: step 24301, loss 0.119719, acc 0.9375
2017-03-02T18:04:01.564829: step 24302, loss 0.140974, acc 0.96875
2017-03-02T18:04:01.638107: step 24303, loss 0.123057, acc 0.9375
2017-03-02T18:04:01.706333: step 24304, loss 0.10948, acc 1
2017-03-02T18:04:01.786483: step 24305, loss 0.0491851, acc 0.984375
2017-03-02T18:04:01.854460: step 24306, loss 0.136704, acc 0.9375
2017-03-02T18:04:01.925480: step 24307, loss 0.133191, acc 0.953125
2017-03-02T18:04:02.004288: step 24308, loss 0.108598, acc 0.9375
2017-03-02T18:04:02.071338: step 24309, loss 0.0988254, acc 0.96875
2017-03-02T18:04:02.142292: step 24310, loss 0.089116, acc 0.984375
2017-03-02T18:04:02.218882: step 24311, loss 0.149797, acc 0.953125
2017-03-02T18:04:02.288507: step 24312, loss 0.161278, acc 0.921875
2017-03-02T18:04:02.365595: step 24313, loss 0.224586, acc 0.921875
2017-03-02T18:04:02.440211: step 24314, loss 0.16269, acc 0.921875
2017-03-02T18:04:02.510021: step 24315, loss 0.131188, acc 0.9375
2017-03-02T18:04:02.585070: step 24316, loss 0.197824, acc 0.90625
2017-03-02T18:04:02.663168: step 24317, loss 0.141446, acc 0.9375
2017-03-02T18:04:02.731642: step 24318, loss 0.124183, acc 0.90625
2017-03-02T18:04:02.804396: step 24319, loss 0.138385, acc 0.90625
2017-03-02T18:04:02.885268: step 24320, loss 0.188143, acc 0.9375
2017-03-02T18:04:02.955048: step 24321, loss 0.185908, acc 0.921875
2017-03-02T18:04:03.028326: step 24322, loss 0.104836, acc 0.96875
2017-03-02T18:04:03.107585: step 24323, loss 0.243079, acc 0.890625
2017-03-02T18:04:03.172842: step 24324, loss 0.134231, acc 0.96875
2017-03-02T18:04:03.246335: step 24325, loss 0.220462, acc 0.90625
2017-03-02T18:04:03.318833: step 24326, loss 0.229708, acc 0.875
2017-03-02T18:04:03.386176: step 24327, loss 0.164718, acc 0.90625
2017-03-02T18:04:03.458271: step 24328, loss 0.180749, acc 0.921875
2017-03-02T18:04:03.529773: step 24329, loss 0.159425, acc 0.921875
2017-03-02T18:04:03.603617: step 24330, loss 0.088445, acc 0.953125
2017-03-02T18:04:03.678970: step 24331, loss 0.0981269, acc 0.953125
2017-03-02T18:04:03.756104: step 24332, loss 0.126023, acc 0.953125
2017-03-02T18:04:03.828916: step 24333, loss 0.102446, acc 0.96875
2017-03-02T18:04:03.908375: step 24334, loss 0.0875587, acc 0.953125
2017-03-02T18:04:03.996919: step 24335, loss 0.0942344, acc 0.96875
2017-03-02T18:04:04.070180: step 24336, loss 0.185957, acc 0.9375
2017-03-02T18:04:04.146181: step 24337, loss 0.150971, acc 0.9375
2017-03-02T18:04:04.216358: step 24338, loss 0.176451, acc 0.953125
2017-03-02T18:04:04.296639: step 24339, loss 0.11687, acc 0.9375
2017-03-02T18:04:04.370422: step 24340, loss 0.135541, acc 0.953125
2017-03-02T18:04:04.443271: step 24341, loss 0.220492, acc 0.90625
2017-03-02T18:04:04.519314: step 24342, loss 0.259465, acc 0.90625
2017-03-02T18:04:04.605867: step 24343, loss 0.112008, acc 0.96875
2017-03-02T18:04:04.672159: step 24344, loss 0.20135, acc 0.921875
2017-03-02T18:04:04.743745: step 24345, loss 0.0955957, acc 0.96875
2017-03-02T18:04:04.817537: step 24346, loss 0.282959, acc 0.875
2017-03-02T18:04:04.891854: step 24347, loss 0.155075, acc 0.9375
2017-03-02T18:04:04.966818: step 24348, loss 0.14894, acc 0.921875
2017-03-02T18:04:05.036425: step 24349, loss 0.144643, acc 0.921875
2017-03-02T18:04:05.116342: step 24350, loss 0.121405, acc 0.90625
2017-03-02T18:04:05.193790: step 24351, loss 0.0772148, acc 0.984375
2017-03-02T18:04:05.275463: step 24352, loss 0.19898, acc 0.9375
2017-03-02T18:04:05.350694: step 24353, loss 0.111129, acc 0.953125
2017-03-02T18:04:05.428478: step 24354, loss 0.111775, acc 0.953125
2017-03-02T18:04:05.505509: step 24355, loss 0.185384, acc 0.921875
2017-03-02T18:04:05.575253: step 24356, loss 0.139864, acc 0.9375
2017-03-02T18:04:05.644450: step 24357, loss 0.186419, acc 0.9375
2017-03-02T18:04:05.717615: step 24358, loss 0.0935076, acc 0.96875
2017-03-02T18:04:05.791798: step 24359, loss 0.188691, acc 0.921875
2017-03-02T18:04:05.863525: step 24360, loss 0.130312, acc 0.90625
2017-03-02T18:04:05.936713: step 24361, loss 0.13897, acc 0.9375
2017-03-02T18:04:06.004936: step 24362, loss 0.0843917, acc 0.953125
2017-03-02T18:04:06.079730: step 24363, loss 0.139525, acc 0.921875
2017-03-02T18:04:06.160404: step 24364, loss 0.169302, acc 0.875
2017-03-02T18:04:06.232601: step 24365, loss 0.266504, acc 0.9375
2017-03-02T18:04:06.308570: step 24366, loss 0.204334, acc 0.921875
2017-03-02T18:04:06.379260: step 24367, loss 0.124274, acc 0.9375
2017-03-02T18:04:06.449447: step 24368, loss 0.196455, acc 0.890625
2017-03-02T18:04:06.522078: step 24369, loss 0.102418, acc 0.96875
2017-03-02T18:04:06.596951: step 24370, loss 0.181736, acc 0.875
2017-03-02T18:04:06.655788: step 24371, loss 0.124936, acc 0.9375
2017-03-02T18:04:06.722708: step 24372, loss 0.155884, acc 0.921875
2017-03-02T18:04:06.787668: step 24373, loss 0.134575, acc 0.953125
2017-03-02T18:04:06.857559: step 24374, loss 0.159852, acc 0.9375
2017-03-02T18:04:06.927855: step 24375, loss 0.164564, acc 0.921875
2017-03-02T18:04:06.998658: step 24376, loss 0.0501027, acc 1
2017-03-02T18:04:07.072770: step 24377, loss 0.114187, acc 0.953125
2017-03-02T18:04:07.145392: step 24378, loss 0.110924, acc 0.9375
2017-03-02T18:04:07.214496: step 24379, loss 0.227234, acc 0.90625
2017-03-02T18:04:07.291342: step 24380, loss 0.112767, acc 0.9375
2017-03-02T18:04:07.372441: step 24381, loss 0.143632, acc 0.9375
2017-03-02T18:04:07.442028: step 24382, loss 0.0753883, acc 0.984375
2017-03-02T18:04:07.509680: step 24383, loss 0.196763, acc 0.90625
2017-03-02T18:04:07.580041: step 24384, loss 0.160918, acc 0.921875
2017-03-02T18:04:07.651640: step 24385, loss 0.115823, acc 0.953125
2017-03-02T18:04:07.719990: step 24386, loss 0.214706, acc 0.921875
2017-03-02T18:04:07.793824: step 24387, loss 0.0820531, acc 0.984375
2017-03-02T18:04:07.890568: step 24388, loss 0.0780615, acc 0.96875
2017-03-02T18:04:07.962101: step 24389, loss 0.123737, acc 0.953125
2017-03-02T18:04:08.030479: step 24390, loss 0.132227, acc 0.921875
2017-03-02T18:04:08.105206: step 24391, loss 0.0927207, acc 0.953125
2017-03-02T18:04:08.174443: step 24392, loss 0.082784, acc 0.96875
2017-03-02T18:04:08.250959: step 24393, loss 0.176584, acc 0.921875
2017-03-02T18:04:08.326527: step 24394, loss 0.0930072, acc 0.953125
2017-03-02T18:04:08.391179: step 24395, loss 0.268463, acc 0.90625
2017-03-02T18:04:08.461093: step 24396, loss 0.144822, acc 0.9375
2017-03-02T18:04:08.536923: step 24397, loss 0.144773, acc 0.921875
2017-03-02T18:04:08.605575: step 24398, loss 0.162785, acc 0.90625
2017-03-02T18:04:08.678681: step 24399, loss 0.192093, acc 0.90625
2017-03-02T18:04:08.751859: step 24400, loss 0.178283, acc 0.921875

Evaluation:
2017-03-02T18:04:08.781311: step 24400, loss 3.0193, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24400

2017-03-02T18:04:09.234280: step 24401, loss 0.15139, acc 0.90625
2017-03-02T18:04:09.305876: step 24402, loss 0.179078, acc 0.953125
2017-03-02T18:04:09.378451: step 24403, loss 0.177055, acc 0.9375
2017-03-02T18:04:09.452368: step 24404, loss 0.115835, acc 0.96875
2017-03-02T18:04:09.527981: step 24405, loss 0.116503, acc 0.921875
2017-03-02T18:04:09.605241: step 24406, loss 0.137252, acc 0.9375
2017-03-02T18:04:09.672200: step 24407, loss 0.145224, acc 0.9375
2017-03-02T18:04:09.742918: step 24408, loss 0.153735, acc 0.921875
2017-03-02T18:04:09.827222: step 24409, loss 0.15513, acc 0.9375
2017-03-02T18:04:09.890926: step 24410, loss 0.179446, acc 0.921875
2017-03-02T18:04:09.962331: step 24411, loss 0.120145, acc 0.9375
2017-03-02T18:04:10.030879: step 24412, loss 0.0714786, acc 0.96875
2017-03-02T18:04:10.098820: step 24413, loss 0.157166, acc 0.9375
2017-03-02T18:04:10.180026: step 24414, loss 0.15333, acc 0.9375
2017-03-02T18:04:10.253269: step 24415, loss 0.216777, acc 0.90625
2017-03-02T18:04:10.321976: step 24416, loss 0.101924, acc 0.953125
2017-03-02T18:04:10.395333: step 24417, loss 0.12942, acc 0.96875
2017-03-02T18:04:10.470433: step 24418, loss 0.169295, acc 0.90625
2017-03-02T18:04:10.542411: step 24419, loss 0.126435, acc 0.984375
2017-03-02T18:04:10.612965: step 24420, loss 0.225502, acc 0.9375
2017-03-02T18:04:10.679367: step 24421, loss 0.12263, acc 0.921875
2017-03-02T18:04:10.753458: step 24422, loss 0.220363, acc 0.90625
2017-03-02T18:04:10.829673: step 24423, loss 0.167992, acc 0.90625
2017-03-02T18:04:10.901665: step 24424, loss 0.146005, acc 0.921875
2017-03-02T18:04:10.972762: step 24425, loss 0.146646, acc 0.921875
2017-03-02T18:04:11.047188: step 24426, loss 0.164226, acc 0.921875
2017-03-02T18:04:11.113461: step 24427, loss 0.305165, acc 0.890625
2017-03-02T18:04:11.186322: step 24428, loss 0.0962422, acc 0.953125
2017-03-02T18:04:11.255936: step 24429, loss 0.245714, acc 0.875
2017-03-02T18:04:11.327414: step 24430, loss 0.107324, acc 0.984375
2017-03-02T18:04:11.399868: step 24431, loss 0.247964, acc 0.921875
2017-03-02T18:04:11.466779: step 24432, loss 0.140578, acc 0.953125
2017-03-02T18:04:11.539612: step 24433, loss 0.176159, acc 0.90625
2017-03-02T18:04:11.621714: step 24434, loss 0.132658, acc 0.921875
2017-03-02T18:04:11.712405: step 24435, loss 0.1047, acc 0.953125
2017-03-02T18:04:11.792136: step 24436, loss 0.0865238, acc 0.96875
2017-03-02T18:04:11.855812: step 24437, loss 0.0674976, acc 0.96875
2017-03-02T18:04:11.929932: step 24438, loss 0.226775, acc 0.90625
2017-03-02T18:04:12.010688: step 24439, loss 0.160786, acc 0.953125
2017-03-02T18:04:12.083349: step 24440, loss 0.161606, acc 0.921875
2017-03-02T18:04:12.155900: step 24441, loss 0.102768, acc 0.953125
2017-03-02T18:04:12.229310: step 24442, loss 0.122531, acc 0.9375
2017-03-02T18:04:12.309843: step 24443, loss 0.118186, acc 0.9375
2017-03-02T18:04:12.389077: step 24444, loss 0.0834585, acc 0.953125
2017-03-02T18:04:12.465481: step 24445, loss 0.242453, acc 0.890625
2017-03-02T18:04:12.533583: step 24446, loss 0.13954, acc 0.9375
2017-03-02T18:04:12.612854: step 24447, loss 0.0872733, acc 0.9375
2017-03-02T18:04:12.686443: step 24448, loss 0.131818, acc 0.953125
2017-03-02T18:04:12.762773: step 24449, loss 0.201026, acc 0.90625
2017-03-02T18:04:12.842452: step 24450, loss 0.161831, acc 0.921875
2017-03-02T18:04:12.926759: step 24451, loss 0.166523, acc 0.9375
2017-03-02T18:04:13.008572: step 24452, loss 0.162092, acc 0.890625
2017-03-02T18:04:13.084749: step 24453, loss 0.250124, acc 0.859375
2017-03-02T18:04:13.159780: step 24454, loss 0.25176, acc 0.90625
2017-03-02T18:04:13.234575: step 24455, loss 0.120923, acc 0.96875
2017-03-02T18:04:13.309081: step 24456, loss 0.16879, acc 0.953125
2017-03-02T18:04:13.392674: step 24457, loss 0.139335, acc 0.921875
2017-03-02T18:04:13.502077: step 24458, loss 0.0976193, acc 0.953125
2017-03-02T18:04:13.578746: step 24459, loss 0.197998, acc 0.890625
2017-03-02T18:04:13.649395: step 24460, loss 0.119824, acc 0.96875
2017-03-02T18:04:13.725087: step 24461, loss 0.068661, acc 0.984375
2017-03-02T18:04:13.795758: step 24462, loss 0.168672, acc 0.9375
2017-03-02T18:04:13.867929: step 24463, loss 0.114306, acc 0.9375
2017-03-02T18:04:13.938488: step 24464, loss 0.248563, acc 0.890625
2017-03-02T18:04:14.014762: step 24465, loss 0.153102, acc 0.9375
2017-03-02T18:04:14.083506: step 24466, loss 0.143255, acc 0.953125
2017-03-02T18:04:14.160242: step 24467, loss 0.130378, acc 0.9375
2017-03-02T18:04:14.239700: step 24468, loss 0.171656, acc 0.890625
2017-03-02T18:04:14.309546: step 24469, loss 0.0824929, acc 0.953125
2017-03-02T18:04:14.384083: step 24470, loss 0.0976658, acc 0.984375
2017-03-02T18:04:14.457014: step 24471, loss 0.163458, acc 0.953125
2017-03-02T18:04:14.530997: step 24472, loss 0.140417, acc 0.953125
2017-03-02T18:04:14.605480: step 24473, loss 0.170531, acc 0.90625
2017-03-02T18:04:14.671888: step 24474, loss 0.0612904, acc 0.96875
2017-03-02T18:04:14.742351: step 24475, loss 0.126782, acc 0.953125
2017-03-02T18:04:14.807278: step 24476, loss 0.0932837, acc 0.953125
2017-03-02T18:04:14.878817: step 24477, loss 0.0828149, acc 0.953125
2017-03-02T18:04:14.973233: step 24478, loss 0.124711, acc 0.953125
2017-03-02T18:04:15.044451: step 24479, loss 0.263654, acc 0.921875
2017-03-02T18:04:15.122119: step 24480, loss 0.237185, acc 0.890625
2017-03-02T18:04:15.193082: step 24481, loss 0.138211, acc 0.9375
2017-03-02T18:04:15.284674: step 24482, loss 0.197662, acc 0.9375
2017-03-02T18:04:15.348806: step 24483, loss 0.194163, acc 0.890625
2017-03-02T18:04:15.426409: step 24484, loss 0.0839794, acc 0.96875
2017-03-02T18:04:15.498728: step 24485, loss 0.220772, acc 0.890625
2017-03-02T18:04:15.567063: step 24486, loss 0.208191, acc 0.921875
2017-03-02T18:04:15.643413: step 24487, loss 0.0717131, acc 0.953125
2017-03-02T18:04:15.722063: step 24488, loss 0.145014, acc 0.921875
2017-03-02T18:04:15.789218: step 24489, loss 0.159994, acc 0.90625
2017-03-02T18:04:15.863198: step 24490, loss 0.101501, acc 0.96875
2017-03-02T18:04:15.938430: step 24491, loss 0.10499, acc 0.953125
2017-03-02T18:04:16.000699: step 24492, loss 0.263636, acc 0.859375
2017-03-02T18:04:16.076816: step 24493, loss 0.165459, acc 0.9375
2017-03-02T18:04:16.147590: step 24494, loss 0.189364, acc 0.90625
2017-03-02T18:04:16.215585: step 24495, loss 0.118298, acc 0.984375
2017-03-02T18:04:16.298880: step 24496, loss 0.19209, acc 0.921875
2017-03-02T18:04:16.374298: step 24497, loss 0.073733, acc 0.96875
2017-03-02T18:04:16.444024: step 24498, loss 0.204495, acc 0.921875
2017-03-02T18:04:16.523457: step 24499, loss 0.141178, acc 0.921875
2017-03-02T18:04:16.608316: step 24500, loss 0.0860088, acc 1

Evaluation:
2017-03-02T18:04:16.646341: step 24500, loss 2.97417, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24500

2017-03-02T18:04:17.101454: step 24501, loss 0.186082, acc 0.9375
2017-03-02T18:04:17.172630: step 24502, loss 0.0968228, acc 0.953125
2017-03-02T18:04:17.242115: step 24503, loss 0.18442, acc 0.90625
2017-03-02T18:04:17.313030: step 24504, loss 0.149199, acc 0.953125
2017-03-02T18:04:17.391515: step 24505, loss 0.0590719, acc 0.96875
2017-03-02T18:04:17.469992: step 24506, loss 0.152352, acc 0.96875
2017-03-02T18:04:17.552516: step 24507, loss 0.144045, acc 0.921875
2017-03-02T18:04:17.639807: step 24508, loss 0.164775, acc 0.9375
2017-03-02T18:04:17.706296: step 24509, loss 0.146514, acc 0.921875
2017-03-02T18:04:17.782655: step 24510, loss 0.254171, acc 0.875
2017-03-02T18:04:17.854100: step 24511, loss 0.129569, acc 0.90625
2017-03-02T18:04:17.930065: step 24512, loss 0.0759614, acc 0.984375
2017-03-02T18:04:18.003275: step 24513, loss 0.222989, acc 0.875
2017-03-02T18:04:18.082815: step 24514, loss 0.0890625, acc 0.96875
2017-03-02T18:04:18.159778: step 24515, loss 0.107717, acc 0.9375
2017-03-02T18:04:18.235980: step 24516, loss 0.0671228, acc 0.953125
2017-03-02T18:04:18.299973: step 24517, loss 0.262013, acc 0.875
2017-03-02T18:04:18.401705: step 24518, loss 0.158217, acc 0.90625
2017-03-02T18:04:18.468886: step 24519, loss 0.104533, acc 0.9375
2017-03-02T18:04:18.532087: step 24520, loss 0.161675, acc 0.90625
2017-03-02T18:04:18.615577: step 24521, loss 0.233575, acc 0.890625
2017-03-02T18:04:18.683922: step 24522, loss 0.132999, acc 0.9375
2017-03-02T18:04:18.749503: step 24523, loss 0.147623, acc 0.953125
2017-03-02T18:04:18.821369: step 24524, loss 0.160181, acc 0.9375
2017-03-02T18:04:18.896408: step 24525, loss 0.151378, acc 0.9375
2017-03-02T18:04:18.961078: step 24526, loss 0.0969401, acc 0.9375
2017-03-02T18:04:19.038009: step 24527, loss 0.117199, acc 0.953125
2017-03-02T18:04:19.109378: step 24528, loss 0.14436, acc 0.890625
2017-03-02T18:04:19.179521: step 24529, loss 0.217499, acc 0.90625
2017-03-02T18:04:19.252050: step 24530, loss 0.0755639, acc 1
2017-03-02T18:04:19.328778: step 24531, loss 0.119261, acc 0.984375
2017-03-02T18:04:19.399720: step 24532, loss 0.0831137, acc 0.984375
2017-03-02T18:04:19.473938: step 24533, loss 0.140914, acc 0.96875
2017-03-02T18:04:19.551029: step 24534, loss 0.208252, acc 0.921875
2017-03-02T18:04:19.630437: step 24535, loss 0.214265, acc 0.875
2017-03-02T18:04:19.708030: step 24536, loss 0.114468, acc 0.96875
2017-03-02T18:04:19.785326: step 24537, loss 0.150162, acc 0.9375
2017-03-02T18:04:19.850593: step 24538, loss 0.119046, acc 0.953125
2017-03-02T18:04:19.933489: step 24539, loss 0.0413099, acc 0.96875
2017-03-02T18:04:20.006088: step 24540, loss 0.180266, acc 0.921875
2017-03-02T18:04:20.072699: step 24541, loss 0.111201, acc 0.953125
2017-03-02T18:04:20.146927: step 24542, loss 0.11818, acc 0.953125
2017-03-02T18:04:20.224223: step 24543, loss 0.168339, acc 0.921875
2017-03-02T18:04:20.294754: step 24544, loss 0.074395, acc 1
2017-03-02T18:04:20.364967: step 24545, loss 0.128859, acc 0.921875
2017-03-02T18:04:20.452859: step 24546, loss 0.0612147, acc 0.953125
2017-03-02T18:04:20.518771: step 24547, loss 0.0881486, acc 0.96875
2017-03-02T18:04:20.594809: step 24548, loss 0.12553, acc 0.96875
2017-03-02T18:04:20.668699: step 24549, loss 0.172079, acc 0.9375
2017-03-02T18:04:20.743270: step 24550, loss 0.166118, acc 0.9375
2017-03-02T18:04:20.823178: step 24551, loss 0.199875, acc 0.890625
2017-03-02T18:04:20.904882: step 24552, loss 0.166627, acc 0.890625
2017-03-02T18:04:20.967839: step 24553, loss 0.103091, acc 0.984375
2017-03-02T18:04:21.042787: step 24554, loss 0.0477577, acc 0.984375
2017-03-02T18:04:21.112618: step 24555, loss 0.279121, acc 0.859375
2017-03-02T18:04:21.183115: step 24556, loss 0.10565, acc 0.96875
2017-03-02T18:04:21.258146: step 24557, loss 0.208009, acc 0.875
2017-03-02T18:04:21.332392: step 24558, loss 0.0859641, acc 0.96875
2017-03-02T18:04:21.397204: step 24559, loss 0.280697, acc 0.875
2017-03-02T18:04:21.472362: step 24560, loss 0.140794, acc 0.953125
2017-03-02T18:04:21.551319: step 24561, loss 0.19746, acc 0.921875
2017-03-02T18:04:21.616630: step 24562, loss 0.0845748, acc 0.953125
2017-03-02T18:04:21.697176: step 24563, loss 0.123606, acc 0.9375
2017-03-02T18:04:21.776309: step 24564, loss 0.187317, acc 0.890625
2017-03-02T18:04:21.850497: step 24565, loss 0.18762, acc 0.875
2017-03-02T18:04:21.926612: step 24566, loss 0.230432, acc 0.90625
2017-03-02T18:04:22.001284: step 24567, loss 0.074787, acc 0.953125
2017-03-02T18:04:22.074655: step 24568, loss 0.101204, acc 0.96875
2017-03-02T18:04:22.144835: step 24569, loss 0.12813, acc 0.953125
2017-03-02T18:04:22.227816: step 24570, loss 0.0799709, acc 0.96875
2017-03-02T18:04:22.298150: step 24571, loss 0.0490439, acc 0.96875
2017-03-02T18:04:22.371790: step 24572, loss 0.127942, acc 0.9375
2017-03-02T18:04:22.451070: step 24573, loss 0.259951, acc 0.90625
2017-03-02T18:04:22.521462: step 24574, loss 0.0515455, acc 0.984375
2017-03-02T18:04:22.592974: step 24575, loss 0.0938506, acc 0.9375
2017-03-02T18:04:22.669670: step 24576, loss 0.129375, acc 0.953125
2017-03-02T18:04:22.737962: step 24577, loss 0.176925, acc 0.90625
2017-03-02T18:04:22.806928: step 24578, loss 0.065495, acc 0.984375
2017-03-02T18:04:22.877139: step 24579, loss 0.127426, acc 0.921875
2017-03-02T18:04:22.945594: step 24580, loss 0.182467, acc 0.921875
2017-03-02T18:04:23.016999: step 24581, loss 0.158003, acc 0.90625
2017-03-02T18:04:23.096410: step 24582, loss 0.186195, acc 0.9375
2017-03-02T18:04:23.167557: step 24583, loss 0.219511, acc 0.90625
2017-03-02T18:04:23.238548: step 24584, loss 0.174748, acc 0.890625
2017-03-02T18:04:23.312651: step 24585, loss 0.305553, acc 0.859375
2017-03-02T18:04:23.379996: step 24586, loss 0.0755351, acc 0.953125
2017-03-02T18:04:23.451426: step 24587, loss 0.130812, acc 0.953125
2017-03-02T18:04:23.525649: step 24588, loss 0.0758478, acc 0.984375
2017-03-02T18:04:23.598127: step 24589, loss 0.203166, acc 0.921875
2017-03-02T18:04:23.670049: step 24590, loss 0.0962062, acc 0.9375
2017-03-02T18:04:23.741058: step 24591, loss 0.0787419, acc 0.96875
2017-03-02T18:04:23.817568: step 24592, loss 0.130635, acc 0.953125
2017-03-02T18:04:23.888708: step 24593, loss 0.158327, acc 0.9375
2017-03-02T18:04:23.956817: step 24594, loss 0.136633, acc 0.9375
2017-03-02T18:04:24.028347: step 24595, loss 0.0968766, acc 0.984375
2017-03-02T18:04:24.094878: step 24596, loss 0.196489, acc 0.890625
2017-03-02T18:04:24.161671: step 24597, loss 0.21662, acc 0.921875
2017-03-02T18:04:24.232999: step 24598, loss 0.0934601, acc 0.96875
2017-03-02T18:04:24.304542: step 24599, loss 0.30189, acc 0.84375
2017-03-02T18:04:24.378627: step 24600, loss 0.289072, acc 0.875

Evaluation:
2017-03-02T18:04:24.413833: step 24600, loss 3.02142, acc 0.630858

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24600

2017-03-02T18:04:24.859263: step 24601, loss 0.129286, acc 0.953125
2017-03-02T18:04:24.927492: step 24602, loss 0.206025, acc 0.890625
2017-03-02T18:04:25.003274: step 24603, loss 0.188433, acc 0.921875
2017-03-02T18:04:25.079042: step 24604, loss 0.0816342, acc 0.953125
2017-03-02T18:04:25.146682: step 24605, loss 0.131399, acc 0.953125
2017-03-02T18:04:25.218099: step 24606, loss 0.232299, acc 0.875
2017-03-02T18:04:25.289266: step 24607, loss 0.115867, acc 0.953125
2017-03-02T18:04:25.357345: step 24608, loss 0.175258, acc 0.890625
2017-03-02T18:04:25.444151: step 24609, loss 0.134215, acc 0.953125
2017-03-02T18:04:25.516940: step 24610, loss 0.103208, acc 0.953125
2017-03-02T18:04:25.592123: step 24611, loss 0.13642, acc 0.9375
2017-03-02T18:04:25.667034: step 24612, loss 0.155993, acc 0.921875
2017-03-02T18:04:25.748545: step 24613, loss 0.153709, acc 0.890625
2017-03-02T18:04:25.819216: step 24614, loss 0.165065, acc 0.9375
2017-03-02T18:04:25.892581: step 24615, loss 0.20701, acc 0.890625
2017-03-02T18:04:25.976276: step 24616, loss 0.171878, acc 0.890625
2017-03-02T18:04:26.048068: step 24617, loss 0.108027, acc 0.921875
2017-03-02T18:04:26.121309: step 24618, loss 0.180169, acc 0.90625
2017-03-02T18:04:26.201254: step 24619, loss 0.181302, acc 0.890625
2017-03-02T18:04:26.273771: step 24620, loss 0.104837, acc 0.953125
2017-03-02T18:04:26.347230: step 24621, loss 0.119861, acc 0.9375
2017-03-02T18:04:26.417875: step 24622, loss 0.211882, acc 0.890625
2017-03-02T18:04:26.498812: step 24623, loss 0.112876, acc 0.953125
2017-03-02T18:04:26.571845: step 24624, loss 0.116931, acc 0.953125
2017-03-02T18:04:26.648073: step 24625, loss 0.17733, acc 0.921875
2017-03-02T18:04:26.718888: step 24626, loss 0.202965, acc 0.90625
2017-03-02T18:04:26.789796: step 24627, loss 0.0822101, acc 0.96875
2017-03-02T18:04:26.858611: step 24628, loss 0.172772, acc 0.921875
2017-03-02T18:04:26.932335: step 24629, loss 0.0965075, acc 0.984375
2017-03-02T18:04:27.002786: step 24630, loss 0.236821, acc 0.921875
2017-03-02T18:04:27.068783: step 24631, loss 0.174592, acc 0.921875
2017-03-02T18:04:27.138233: step 24632, loss 0.0842717, acc 0.984375
2017-03-02T18:04:27.212527: step 24633, loss 0.122688, acc 0.953125
2017-03-02T18:04:27.294672: step 24634, loss 0.170165, acc 0.90625
2017-03-02T18:04:27.366236: step 24635, loss 0.132675, acc 0.953125
2017-03-02T18:04:27.440239: step 24636, loss 0.102642, acc 0.953125
2017-03-02T18:04:27.516539: step 24637, loss 0.0770362, acc 1
2017-03-02T18:04:27.602941: step 24638, loss 0.322013, acc 0.875
2017-03-02T18:04:27.680590: step 24639, loss 0.0517585, acc 0.96875
2017-03-02T18:04:27.747225: step 24640, loss 0.166057, acc 0.921875
2017-03-02T18:04:27.817977: step 24641, loss 0.115593, acc 0.921875
2017-03-02T18:04:27.895058: step 24642, loss 0.242167, acc 0.890625
2017-03-02T18:04:27.966502: step 24643, loss 0.362612, acc 0.859375
2017-03-02T18:04:28.035948: step 24644, loss 0.158184, acc 0.9375
2017-03-02T18:04:28.103722: step 24645, loss 0.249315, acc 0.890625
2017-03-02T18:04:28.165951: step 24646, loss 0.135828, acc 0.953125
2017-03-02T18:04:28.236323: step 24647, loss 0.117845, acc 0.9375
2017-03-02T18:04:28.318598: step 24648, loss 0.118959, acc 0.90625
2017-03-02T18:04:28.394864: step 24649, loss 0.047998, acc 1
2017-03-02T18:04:28.467474: step 24650, loss 0.174216, acc 0.9375
2017-03-02T18:04:28.539653: step 24651, loss 0.147707, acc 0.921875
2017-03-02T18:04:28.617257: step 24652, loss 0.231283, acc 0.84375
2017-03-02T18:04:28.691813: step 24653, loss 0.149422, acc 0.921875
2017-03-02T18:04:28.765299: step 24654, loss 0.140137, acc 0.9375
2017-03-02T18:04:28.847206: step 24655, loss 0.225398, acc 0.859375
2017-03-02T18:04:28.923154: step 24656, loss 0.101298, acc 0.9375
2017-03-02T18:04:29.004658: step 24657, loss 0.0797154, acc 0.96875
2017-03-02T18:04:29.083094: step 24658, loss 0.0784545, acc 0.984375
2017-03-02T18:04:29.155948: step 24659, loss 0.106115, acc 0.9375
2017-03-02T18:04:29.230554: step 24660, loss 0.140486, acc 0.921875
2017-03-02T18:04:29.294207: step 24661, loss 0.0991244, acc 0.9375
2017-03-02T18:04:29.373198: step 24662, loss 0.136866, acc 0.9375
2017-03-02T18:04:29.441916: step 24663, loss 0.262752, acc 0.875
2017-03-02T18:04:29.531141: step 24664, loss 0.13049, acc 0.9375
2017-03-02T18:04:29.606315: step 24665, loss 0.12175, acc 0.96875
2017-03-02T18:04:29.681900: step 24666, loss 0.0484615, acc 1
2017-03-02T18:04:29.763219: step 24667, loss 0.169575, acc 0.90625
2017-03-02T18:04:29.836591: step 24668, loss 0.131024, acc 0.921875
2017-03-02T18:04:29.910117: step 24669, loss 0.184776, acc 0.9375
2017-03-02T18:04:29.979029: step 24670, loss 0.122426, acc 0.953125
2017-03-02T18:04:30.051453: step 24671, loss 0.120377, acc 0.953125
2017-03-02T18:04:30.125652: step 24672, loss 0.207234, acc 0.90625
2017-03-02T18:04:30.193298: step 24673, loss 0.168269, acc 0.90625
2017-03-02T18:04:30.279892: step 24674, loss 0.139814, acc 0.9375
2017-03-02T18:04:30.354174: step 24675, loss 0.121147, acc 0.9375
2017-03-02T18:04:30.426523: step 24676, loss 0.0832524, acc 0.9375
2017-03-02T18:04:30.507698: step 24677, loss 0.160881, acc 0.921875
2017-03-02T18:04:30.578379: step 24678, loss 0.123736, acc 0.9375
2017-03-02T18:04:30.656656: step 24679, loss 0.166175, acc 0.9375
2017-03-02T18:04:30.724910: step 24680, loss 0.0970319, acc 0.953125
2017-03-02T18:04:30.797913: step 24681, loss 0.101846, acc 0.953125
2017-03-02T18:04:30.866533: step 24682, loss 0.179793, acc 0.90625
2017-03-02T18:04:30.934768: step 24683, loss 0.365403, acc 0.890625
2017-03-02T18:04:31.010645: step 24684, loss 0.0806584, acc 0.984375
2017-03-02T18:04:31.101837: step 24685, loss 0.151972, acc 0.921875
2017-03-02T18:04:31.171040: step 24686, loss 0.230989, acc 0.875
2017-03-02T18:04:31.244914: step 24687, loss 0.119852, acc 0.953125
2017-03-02T18:04:31.312522: step 24688, loss 0.178631, acc 0.90625
2017-03-02T18:04:31.387349: step 24689, loss 0.187818, acc 0.90625
2017-03-02T18:04:31.458811: step 24690, loss 0.189531, acc 0.90625
2017-03-02T18:04:31.529446: step 24691, loss 0.169388, acc 0.921875
2017-03-02T18:04:31.602212: step 24692, loss 0.14442, acc 0.9375
2017-03-02T18:04:31.677425: step 24693, loss 0.145241, acc 0.953125
2017-03-02T18:04:31.747079: step 24694, loss 0.123713, acc 0.921875
2017-03-02T18:04:31.820640: step 24695, loss 0.0754557, acc 0.984375
2017-03-02T18:04:31.887450: step 24696, loss 0.492063, acc 0.75
2017-03-02T18:04:31.967819: step 24697, loss 0.107212, acc 0.953125
2017-03-02T18:04:32.042929: step 24698, loss 0.119193, acc 0.921875
2017-03-02T18:04:32.116001: step 24699, loss 0.0157821, acc 1
2017-03-02T18:04:32.196053: step 24700, loss 0.22053, acc 0.90625

Evaluation:
2017-03-02T18:04:32.229641: step 24700, loss 3.01383, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24700

2017-03-02T18:04:32.685912: step 24701, loss 0.167243, acc 0.90625
2017-03-02T18:04:32.758490: step 24702, loss 0.110819, acc 0.921875
2017-03-02T18:04:32.833091: step 24703, loss 0.144275, acc 0.921875
2017-03-02T18:04:32.901720: step 24704, loss 0.130531, acc 0.9375
2017-03-02T18:04:32.978046: step 24705, loss 0.122258, acc 0.9375
2017-03-02T18:04:33.057147: step 24706, loss 0.286213, acc 0.875
2017-03-02T18:04:33.135051: step 24707, loss 0.0754997, acc 0.984375
2017-03-02T18:04:33.209510: step 24708, loss 0.112633, acc 0.9375
2017-03-02T18:04:33.276614: step 24709, loss 0.078085, acc 0.984375
2017-03-02T18:04:33.358677: step 24710, loss 0.152425, acc 0.9375
2017-03-02T18:04:33.439456: step 24711, loss 0.0738656, acc 1
2017-03-02T18:04:33.508121: step 24712, loss 0.342215, acc 0.90625
2017-03-02T18:04:33.582409: step 24713, loss 0.203861, acc 0.90625
2017-03-02T18:04:33.670562: step 24714, loss 0.0983158, acc 0.953125
2017-03-02T18:04:33.737897: step 24715, loss 0.256451, acc 0.921875
2017-03-02T18:04:33.816911: step 24716, loss 0.143289, acc 0.921875
2017-03-02T18:04:33.890891: step 24717, loss 0.113811, acc 0.96875
2017-03-02T18:04:33.960540: step 24718, loss 0.145693, acc 0.90625
2017-03-02T18:04:34.047232: step 24719, loss 0.217339, acc 0.90625
2017-03-02T18:04:34.119549: step 24720, loss 0.170326, acc 0.921875
2017-03-02T18:04:34.197502: step 24721, loss 0.181881, acc 0.9375
2017-03-02T18:04:34.270171: step 24722, loss 0.056227, acc 0.96875
2017-03-02T18:04:34.347725: step 24723, loss 0.148695, acc 0.9375
2017-03-02T18:04:34.420241: step 24724, loss 0.167518, acc 0.9375
2017-03-02T18:04:34.494138: step 24725, loss 0.112287, acc 0.953125
2017-03-02T18:04:34.559233: step 24726, loss 0.22166, acc 0.921875
2017-03-02T18:04:34.635369: step 24727, loss 0.0924154, acc 0.96875
2017-03-02T18:04:34.706911: step 24728, loss 0.115839, acc 0.921875
2017-03-02T18:04:34.779909: step 24729, loss 0.14317, acc 0.9375
2017-03-02T18:04:34.851319: step 24730, loss 0.0587064, acc 0.984375
2017-03-02T18:04:34.927429: step 24731, loss 0.13146, acc 0.921875
2017-03-02T18:04:34.997469: step 24732, loss 0.0793835, acc 0.953125
2017-03-02T18:04:35.070531: step 24733, loss 0.211841, acc 0.921875
2017-03-02T18:04:35.139109: step 24734, loss 0.108225, acc 0.921875
2017-03-02T18:04:35.214423: step 24735, loss 0.205678, acc 0.9375
2017-03-02T18:04:35.287298: step 24736, loss 0.0606229, acc 0.96875
2017-03-02T18:04:35.368495: step 24737, loss 0.177058, acc 0.921875
2017-03-02T18:04:35.437437: step 24738, loss 0.119076, acc 0.96875
2017-03-02T18:04:35.525403: step 24739, loss 0.107135, acc 0.953125
2017-03-02T18:04:35.609552: step 24740, loss 0.15633, acc 0.921875
2017-03-02T18:04:35.685775: step 24741, loss 0.072682, acc 0.96875
2017-03-02T18:04:35.765092: step 24742, loss 0.107759, acc 0.953125
2017-03-02T18:04:35.849212: step 24743, loss 0.217116, acc 0.921875
2017-03-02T18:04:35.920021: step 24744, loss 0.185702, acc 0.890625
2017-03-02T18:04:35.993537: step 24745, loss 0.108785, acc 0.9375
2017-03-02T18:04:36.065274: step 24746, loss 0.0645554, acc 0.984375
2017-03-02T18:04:36.136819: step 24747, loss 0.083535, acc 0.96875
2017-03-02T18:04:36.217419: step 24748, loss 0.0926752, acc 0.953125
2017-03-02T18:04:36.290628: step 24749, loss 0.0675383, acc 0.96875
2017-03-02T18:04:36.361211: step 24750, loss 0.146333, acc 0.953125
2017-03-02T18:04:36.434021: step 24751, loss 0.301845, acc 0.890625
2017-03-02T18:04:36.519735: step 24752, loss 0.123798, acc 0.9375
2017-03-02T18:04:36.597043: step 24753, loss 0.212138, acc 0.90625
2017-03-02T18:04:36.666368: step 24754, loss 0.113764, acc 0.96875
2017-03-02T18:04:36.733193: step 24755, loss 0.235903, acc 0.921875
2017-03-02T18:04:36.803698: step 24756, loss 0.124773, acc 0.9375
2017-03-02T18:04:36.878567: step 24757, loss 0.192087, acc 0.90625
2017-03-02T18:04:36.949690: step 24758, loss 0.170911, acc 0.90625
2017-03-02T18:04:37.026896: step 24759, loss 0.140684, acc 0.921875
2017-03-02T18:04:37.096057: step 24760, loss 0.135707, acc 0.953125
2017-03-02T18:04:37.166614: step 24761, loss 0.176891, acc 0.90625
2017-03-02T18:04:37.237675: step 24762, loss 0.157089, acc 0.9375
2017-03-02T18:04:37.313869: step 24763, loss 0.0567045, acc 0.96875
2017-03-02T18:04:37.383808: step 24764, loss 0.245072, acc 0.90625
2017-03-02T18:04:37.453500: step 24765, loss 0.145149, acc 0.9375
2017-03-02T18:04:37.527291: step 24766, loss 0.12683, acc 0.953125
2017-03-02T18:04:37.600377: step 24767, loss 0.150295, acc 0.90625
2017-03-02T18:04:37.678413: step 24768, loss 0.0821532, acc 0.984375
2017-03-02T18:04:37.753299: step 24769, loss 0.222628, acc 0.890625
2017-03-02T18:04:37.827092: step 24770, loss 0.125353, acc 0.90625
2017-03-02T18:04:37.915543: step 24771, loss 0.107846, acc 0.96875
2017-03-02T18:04:37.985170: step 24772, loss 0.103969, acc 0.96875
2017-03-02T18:04:38.047124: step 24773, loss 0.128862, acc 0.9375
2017-03-02T18:04:38.124300: step 24774, loss 0.130119, acc 0.9375
2017-03-02T18:04:38.205395: step 24775, loss 0.181457, acc 0.875
2017-03-02T18:04:38.272145: step 24776, loss 0.148064, acc 0.953125
2017-03-02T18:04:38.345492: step 24777, loss 0.241021, acc 0.890625
2017-03-02T18:04:38.417541: step 24778, loss 0.0819822, acc 0.96875
2017-03-02T18:04:38.483798: step 24779, loss 0.0990404, acc 0.953125
2017-03-02T18:04:38.561106: step 24780, loss 0.094746, acc 0.96875
2017-03-02T18:04:38.635443: step 24781, loss 0.139145, acc 0.9375
2017-03-02T18:04:38.707438: step 24782, loss 0.0171395, acc 1
2017-03-02T18:04:38.779309: step 24783, loss 0.185365, acc 0.90625
2017-03-02T18:04:38.853922: step 24784, loss 0.145537, acc 0.9375
2017-03-02T18:04:38.924738: step 24785, loss 0.116621, acc 0.9375
2017-03-02T18:04:38.997152: step 24786, loss 0.169848, acc 0.890625
2017-03-02T18:04:39.070675: step 24787, loss 0.246067, acc 0.90625
2017-03-02T18:04:39.146650: step 24788, loss 0.127766, acc 0.9375
2017-03-02T18:04:39.221302: step 24789, loss 0.248379, acc 0.859375
2017-03-02T18:04:39.293318: step 24790, loss 0.14256, acc 0.96875
2017-03-02T18:04:39.374408: step 24791, loss 0.179466, acc 0.921875
2017-03-02T18:04:39.450007: step 24792, loss 0.121551, acc 0.921875
2017-03-02T18:04:39.532733: step 24793, loss 0.246684, acc 0.921875
2017-03-02T18:04:39.596341: step 24794, loss 0.174647, acc 0.921875
2017-03-02T18:04:39.672416: step 24795, loss 0.227806, acc 0.90625
2017-03-02T18:04:39.747986: step 24796, loss 0.204662, acc 0.890625
2017-03-02T18:04:39.818756: step 24797, loss 0.332806, acc 0.84375
2017-03-02T18:04:39.892106: step 24798, loss 0.0808616, acc 0.9375
2017-03-02T18:04:39.976025: step 24799, loss 0.218517, acc 0.921875
2017-03-02T18:04:40.043389: step 24800, loss 0.33235, acc 0.875

Evaluation:
2017-03-02T18:04:40.076959: step 24800, loss 2.98414, acc 0.633742

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24800

2017-03-02T18:04:40.516869: step 24801, loss 0.136135, acc 0.90625
2017-03-02T18:04:40.586938: step 24802, loss 0.0998631, acc 0.953125
2017-03-02T18:04:40.655213: step 24803, loss 0.176795, acc 0.90625
2017-03-02T18:04:40.726840: step 24804, loss 0.125011, acc 0.90625
2017-03-02T18:04:40.798399: step 24805, loss 0.124517, acc 0.9375
2017-03-02T18:04:40.873964: step 24806, loss 0.0868585, acc 0.9375
2017-03-02T18:04:40.942774: step 24807, loss 0.20886, acc 0.90625
2017-03-02T18:04:41.021519: step 24808, loss 0.206931, acc 0.890625
2017-03-02T18:04:41.094968: step 24809, loss 0.140017, acc 0.9375
2017-03-02T18:04:41.165768: step 24810, loss 0.261235, acc 0.875
2017-03-02T18:04:41.235098: step 24811, loss 0.0765328, acc 0.984375
2017-03-02T18:04:41.330580: step 24812, loss 0.228129, acc 0.890625
2017-03-02T18:04:41.399366: step 24813, loss 0.121394, acc 0.9375
2017-03-02T18:04:41.492466: step 24814, loss 0.126108, acc 0.953125
2017-03-02T18:04:41.564010: step 24815, loss 0.117161, acc 0.9375
2017-03-02T18:04:41.635285: step 24816, loss 0.207486, acc 0.890625
2017-03-02T18:04:41.705699: step 24817, loss 0.189075, acc 0.921875
2017-03-02T18:04:41.773557: step 24818, loss 0.137862, acc 0.953125
2017-03-02T18:04:41.853487: step 24819, loss 0.133155, acc 0.953125
2017-03-02T18:04:41.929738: step 24820, loss 0.184908, acc 0.921875
2017-03-02T18:04:41.999106: step 24821, loss 0.15333, acc 0.9375
2017-03-02T18:04:42.083314: step 24822, loss 0.0705387, acc 0.96875
2017-03-02T18:04:42.163965: step 24823, loss 0.139884, acc 0.9375
2017-03-02T18:04:42.238136: step 24824, loss 0.182947, acc 0.9375
2017-03-02T18:04:42.315838: step 24825, loss 0.224312, acc 0.890625
2017-03-02T18:04:42.392032: step 24826, loss 0.118927, acc 0.953125
2017-03-02T18:04:42.461858: step 24827, loss 0.158135, acc 0.9375
2017-03-02T18:04:42.541345: step 24828, loss 0.191917, acc 0.90625
2017-03-02T18:04:42.612097: step 24829, loss 0.0815822, acc 0.96875
2017-03-02T18:04:42.684173: step 24830, loss 0.194598, acc 0.9375
2017-03-02T18:04:42.756944: step 24831, loss 0.204575, acc 0.90625
2017-03-02T18:04:42.831302: step 24832, loss 0.283696, acc 0.875
2017-03-02T18:04:42.900583: step 24833, loss 0.200518, acc 0.890625
2017-03-02T18:04:42.976749: step 24834, loss 0.115296, acc 0.953125
2017-03-02T18:04:43.055927: step 24835, loss 0.151515, acc 0.90625
2017-03-02T18:04:43.132712: step 24836, loss 0.166738, acc 0.9375
2017-03-02T18:04:43.203832: step 24837, loss 0.0935164, acc 0.96875
2017-03-02T18:04:43.270758: step 24838, loss 0.094615, acc 0.984375
2017-03-02T18:04:43.352580: step 24839, loss 0.148342, acc 0.953125
2017-03-02T18:04:43.421178: step 24840, loss 0.171816, acc 0.921875
2017-03-02T18:04:43.492736: step 24841, loss 0.212675, acc 0.875
2017-03-02T18:04:43.569292: step 24842, loss 0.213769, acc 0.921875
2017-03-02T18:04:43.639678: step 24843, loss 0.29545, acc 0.875
2017-03-02T18:04:43.709213: step 24844, loss 0.189272, acc 0.921875
2017-03-02T18:04:43.779755: step 24845, loss 0.0897038, acc 0.96875
2017-03-02T18:04:43.851817: step 24846, loss 0.157288, acc 0.90625
2017-03-02T18:04:43.936102: step 24847, loss 0.144498, acc 0.921875
2017-03-02T18:04:44.013758: step 24848, loss 0.202865, acc 0.921875
2017-03-02T18:04:44.080910: step 24849, loss 0.200121, acc 0.921875
2017-03-02T18:04:44.159689: step 24850, loss 0.208854, acc 0.90625
2017-03-02T18:04:44.229262: step 24851, loss 0.192584, acc 0.9375
2017-03-02T18:04:44.303414: step 24852, loss 0.148052, acc 0.953125
2017-03-02T18:04:44.368722: step 24853, loss 0.200683, acc 0.890625
2017-03-02T18:04:44.448321: step 24854, loss 0.167006, acc 0.9375
2017-03-02T18:04:44.526943: step 24855, loss 0.0852223, acc 0.984375
2017-03-02T18:04:44.598083: step 24856, loss 0.0269811, acc 1
2017-03-02T18:04:44.669882: step 24857, loss 0.145342, acc 0.921875
2017-03-02T18:04:44.744441: step 24858, loss 0.193497, acc 0.90625
2017-03-02T18:04:44.814308: step 24859, loss 0.123537, acc 0.921875
2017-03-02T18:04:44.887705: step 24860, loss 0.0993512, acc 0.953125
2017-03-02T18:04:44.961614: step 24861, loss 0.133249, acc 0.9375
2017-03-02T18:04:45.031986: step 24862, loss 0.101588, acc 0.953125
2017-03-02T18:04:45.102146: step 24863, loss 0.21154, acc 0.921875
2017-03-02T18:04:45.177995: step 24864, loss 0.138134, acc 0.9375
2017-03-02T18:04:45.254200: step 24865, loss 0.132302, acc 0.953125
2017-03-02T18:04:45.323567: step 24866, loss 0.149311, acc 0.9375
2017-03-02T18:04:45.401630: step 24867, loss 0.0828705, acc 0.96875
2017-03-02T18:04:45.477896: step 24868, loss 0.121232, acc 0.9375
2017-03-02T18:04:45.561641: step 24869, loss 0.190975, acc 0.90625
2017-03-02T18:04:45.641618: step 24870, loss 0.0984382, acc 0.96875
2017-03-02T18:04:45.714996: step 24871, loss 0.0516375, acc 0.984375
2017-03-02T18:04:45.787225: step 24872, loss 0.0901025, acc 0.984375
2017-03-02T18:04:45.860171: step 24873, loss 0.132004, acc 0.9375
2017-03-02T18:04:45.934200: step 24874, loss 0.12255, acc 0.953125
2017-03-02T18:04:46.014363: step 24875, loss 0.132929, acc 0.921875
2017-03-02T18:04:46.097290: step 24876, loss 0.204362, acc 0.90625
2017-03-02T18:04:46.166443: step 24877, loss 0.11778, acc 0.9375
2017-03-02T18:04:46.239523: step 24878, loss 0.136912, acc 0.9375
2017-03-02T18:04:46.328998: step 24879, loss 0.0876573, acc 0.953125
2017-03-02T18:04:46.401454: step 24880, loss 0.167842, acc 0.90625
2017-03-02T18:04:46.465212: step 24881, loss 0.151316, acc 0.953125
2017-03-02T18:04:46.533952: step 24882, loss 0.0739232, acc 0.984375
2017-03-02T18:04:46.602076: step 24883, loss 0.0937601, acc 0.9375
2017-03-02T18:04:46.675183: step 24884, loss 0.20054, acc 0.9375
2017-03-02T18:04:46.751946: step 24885, loss 0.123961, acc 0.953125
2017-03-02T18:04:46.819296: step 24886, loss 0.110325, acc 0.96875
2017-03-02T18:04:46.891641: step 24887, loss 0.163337, acc 0.9375
2017-03-02T18:04:46.963445: step 24888, loss 0.0786677, acc 0.9375
2017-03-02T18:04:47.046021: step 24889, loss 0.152157, acc 0.921875
2017-03-02T18:04:47.118582: step 24890, loss 0.312556, acc 0.890625
2017-03-02T18:04:47.195398: step 24891, loss 0.118497, acc 0.9375
2017-03-02T18:04:47.260763: step 24892, loss 0.565161, acc 0.75
2017-03-02T18:04:47.337310: step 24893, loss 0.123402, acc 0.9375
2017-03-02T18:04:47.407318: step 24894, loss 0.197443, acc 0.921875
2017-03-02T18:04:47.476566: step 24895, loss 0.0946874, acc 0.953125
2017-03-02T18:04:47.549917: step 24896, loss 0.0635066, acc 0.96875
2017-03-02T18:04:47.623117: step 24897, loss 0.159717, acc 0.9375
2017-03-02T18:04:47.695824: step 24898, loss 0.122137, acc 0.9375
2017-03-02T18:04:47.766590: step 24899, loss 0.0745086, acc 0.96875
2017-03-02T18:04:47.837473: step 24900, loss 0.0849587, acc 0.984375

Evaluation:
2017-03-02T18:04:47.862357: step 24900, loss 3.02059, acc 0.631579

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-24900

2017-03-02T18:04:48.306657: step 24901, loss 0.0996566, acc 0.96875
2017-03-02T18:04:48.376773: step 24902, loss 0.178038, acc 0.890625
2017-03-02T18:04:48.448806: step 24903, loss 0.113349, acc 0.953125
2017-03-02T18:04:48.510248: step 24904, loss 0.131607, acc 0.953125
2017-03-02T18:04:48.581829: step 24905, loss 0.127512, acc 0.953125
2017-03-02T18:04:48.645893: step 24906, loss 0.101447, acc 0.9375
2017-03-02T18:04:48.725988: step 24907, loss 0.149994, acc 0.921875
2017-03-02T18:04:48.804201: step 24908, loss 0.0535063, acc 0.984375
2017-03-02T18:04:48.880509: step 24909, loss 0.192547, acc 0.875
2017-03-02T18:04:48.950077: step 24910, loss 0.1714, acc 0.96875
2017-03-02T18:04:49.023318: step 24911, loss 0.0678361, acc 0.984375
2017-03-02T18:04:49.096978: step 24912, loss 0.120621, acc 0.921875
2017-03-02T18:04:49.171708: step 24913, loss 0.160305, acc 0.921875
2017-03-02T18:04:49.252473: step 24914, loss 0.138495, acc 0.9375
2017-03-02T18:04:49.328711: step 24915, loss 0.242804, acc 0.953125
2017-03-02T18:04:49.397492: step 24916, loss 0.127298, acc 0.96875
2017-03-02T18:04:49.469160: step 24917, loss 0.13047, acc 0.953125
2017-03-02T18:04:49.546453: step 24918, loss 0.0312422, acc 0.984375
2017-03-02T18:04:49.616975: step 24919, loss 0.0612497, acc 0.96875
2017-03-02T18:04:49.687973: step 24920, loss 0.0941585, acc 0.96875
2017-03-02T18:04:49.762439: step 24921, loss 0.0794319, acc 0.96875
2017-03-02T18:04:49.833341: step 24922, loss 0.245924, acc 0.921875
2017-03-02T18:04:49.918464: step 24923, loss 0.135972, acc 0.9375
2017-03-02T18:04:49.993276: step 24924, loss 0.0779062, acc 0.96875
2017-03-02T18:04:50.061696: step 24925, loss 0.134433, acc 0.921875
2017-03-02T18:04:50.130236: step 24926, loss 0.145572, acc 0.921875
2017-03-02T18:04:50.202364: step 24927, loss 0.176859, acc 0.921875
2017-03-02T18:04:50.274117: step 24928, loss 0.174699, acc 0.90625
2017-03-02T18:04:50.345247: step 24929, loss 0.0757681, acc 0.96875
2017-03-02T18:04:50.423992: step 24930, loss 0.0998177, acc 0.9375
2017-03-02T18:04:50.497996: step 24931, loss 0.154296, acc 0.96875
2017-03-02T18:04:50.565825: step 24932, loss 0.152256, acc 0.890625
2017-03-02T18:04:50.640156: step 24933, loss 0.112791, acc 0.96875
2017-03-02T18:04:50.709504: step 24934, loss 0.0766681, acc 0.9375
2017-03-02T18:04:50.793475: step 24935, loss 0.170975, acc 0.9375
2017-03-02T18:04:50.864351: step 24936, loss 0.18716, acc 0.90625
2017-03-02T18:04:50.941046: step 24937, loss 0.14986, acc 0.921875
2017-03-02T18:04:51.017844: step 24938, loss 0.13282, acc 0.921875
2017-03-02T18:04:51.089947: step 24939, loss 0.21559, acc 0.890625
2017-03-02T18:04:51.166676: step 24940, loss 0.203291, acc 0.875
2017-03-02T18:04:51.241953: step 24941, loss 0.162715, acc 0.9375
2017-03-02T18:04:51.316631: step 24942, loss 0.258425, acc 0.90625
2017-03-02T18:04:51.389375: step 24943, loss 0.176441, acc 0.921875
2017-03-02T18:04:51.462173: step 24944, loss 0.122107, acc 0.9375
2017-03-02T18:04:51.533806: step 24945, loss 0.167365, acc 0.921875
2017-03-02T18:04:51.603049: step 24946, loss 0.137153, acc 0.953125
2017-03-02T18:04:51.681769: step 24947, loss 0.105259, acc 0.953125
2017-03-02T18:04:51.751899: step 24948, loss 0.0961522, acc 0.96875
2017-03-02T18:04:51.821761: step 24949, loss 0.215467, acc 0.890625
2017-03-02T18:04:51.895117: step 24950, loss 0.186451, acc 0.921875
2017-03-02T18:04:51.970694: step 24951, loss 0.275125, acc 0.875
2017-03-02T18:04:52.034551: step 24952, loss 0.116618, acc 0.921875
2017-03-02T18:04:52.107758: step 24953, loss 0.160658, acc 0.90625
2017-03-02T18:04:52.184107: step 24954, loss 0.108714, acc 0.953125
2017-03-02T18:04:52.260984: step 24955, loss 0.113601, acc 0.96875
2017-03-02T18:04:52.331769: step 24956, loss 0.239776, acc 0.84375
2017-03-02T18:04:52.415190: step 24957, loss 0.183742, acc 0.921875
2017-03-02T18:04:52.480502: step 24958, loss 0.134622, acc 0.921875
2017-03-02T18:04:52.552655: step 24959, loss 0.097579, acc 0.953125
2017-03-02T18:04:52.626535: step 24960, loss 0.19044, acc 0.90625
2017-03-02T18:04:52.706845: step 24961, loss 0.13978, acc 0.9375
2017-03-02T18:04:52.780998: step 24962, loss 0.153329, acc 0.890625
2017-03-02T18:04:52.853097: step 24963, loss 0.165378, acc 0.921875
2017-03-02T18:04:52.931183: step 24964, loss 0.0629199, acc 0.96875
2017-03-02T18:04:53.005991: step 24965, loss 0.235972, acc 0.921875
2017-03-02T18:04:53.080852: step 24966, loss 0.192664, acc 0.890625
2017-03-02T18:04:53.149846: step 24967, loss 0.0537232, acc 0.96875
2017-03-02T18:04:53.223350: step 24968, loss 0.169046, acc 0.9375
2017-03-02T18:04:53.302529: step 24969, loss 0.206793, acc 0.9375
2017-03-02T18:04:53.376092: step 24970, loss 0.102786, acc 0.96875
2017-03-02T18:04:53.451463: step 24971, loss 0.109578, acc 0.9375
2017-03-02T18:04:53.520852: step 24972, loss 0.121656, acc 0.96875
2017-03-02T18:04:53.587572: step 24973, loss 0.0969955, acc 0.953125
2017-03-02T18:04:53.658187: step 24974, loss 0.0598354, acc 0.984375
2017-03-02T18:04:53.728701: step 24975, loss 0.143418, acc 0.921875
2017-03-02T18:04:53.804519: step 24976, loss 0.180523, acc 0.890625
2017-03-02T18:04:53.876020: step 24977, loss 0.134669, acc 0.921875
2017-03-02T18:04:53.944823: step 24978, loss 0.157211, acc 0.921875
2017-03-02T18:04:54.024667: step 24979, loss 0.124769, acc 0.953125
2017-03-02T18:04:54.107548: step 24980, loss 0.20769, acc 0.921875
2017-03-02T18:04:54.182523: step 24981, loss 0.195743, acc 0.9375
2017-03-02T18:04:54.255607: step 24982, loss 0.247859, acc 0.875
2017-03-02T18:04:54.328194: step 24983, loss 0.160392, acc 0.890625
2017-03-02T18:04:54.404158: step 24984, loss 0.257717, acc 0.875
2017-03-02T18:04:54.483729: step 24985, loss 0.0990964, acc 0.953125
2017-03-02T18:04:54.566634: step 24986, loss 0.203299, acc 0.921875
2017-03-02T18:04:54.639792: step 24987, loss 0.0280589, acc 0.984375
2017-03-02T18:04:54.706611: step 24988, loss 0.170794, acc 0.921875
2017-03-02T18:04:54.781473: step 24989, loss 0.254723, acc 0.890625
2017-03-02T18:04:54.854386: step 24990, loss 0.0639439, acc 1
2017-03-02T18:04:54.924717: step 24991, loss 0.0661925, acc 0.96875
2017-03-02T18:04:54.998045: step 24992, loss 0.281103, acc 0.875
2017-03-02T18:04:55.075663: step 24993, loss 0.155977, acc 0.90625
2017-03-02T18:04:55.149584: step 24994, loss 0.170776, acc 0.921875
2017-03-02T18:04:55.214365: step 24995, loss 0.143707, acc 0.921875
2017-03-02T18:04:55.289867: step 24996, loss 0.161594, acc 0.90625
2017-03-02T18:04:55.362621: step 24997, loss 0.19718, acc 0.90625
2017-03-02T18:04:55.434141: step 24998, loss 0.174826, acc 0.90625
2017-03-02T18:04:55.516782: step 24999, loss 0.0958843, acc 0.953125
2017-03-02T18:04:55.595644: step 25000, loss 0.169273, acc 0.921875

Evaluation:
2017-03-02T18:04:55.630347: step 25000, loss 3.02606, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25000

2017-03-02T18:04:56.093230: step 25001, loss 0.174733, acc 0.90625
2017-03-02T18:04:56.166586: step 25002, loss 0.163064, acc 0.953125
2017-03-02T18:04:56.238546: step 25003, loss 0.220867, acc 0.875
2017-03-02T18:04:56.310549: step 25004, loss 0.184834, acc 0.890625
2017-03-02T18:04:56.383747: step 25005, loss 0.137448, acc 0.9375
2017-03-02T18:04:56.452855: step 25006, loss 0.15721, acc 0.890625
2017-03-02T18:04:56.526051: step 25007, loss 0.140065, acc 0.9375
2017-03-02T18:04:56.597065: step 25008, loss 0.182871, acc 0.921875
2017-03-02T18:04:56.672627: step 25009, loss 0.0888008, acc 0.9375
2017-03-02T18:04:56.739953: step 25010, loss 0.151754, acc 0.921875
2017-03-02T18:04:56.819322: step 25011, loss 0.127878, acc 0.953125
2017-03-02T18:04:56.894015: step 25012, loss 0.229689, acc 0.890625
2017-03-02T18:04:56.972500: step 25013, loss 0.18775, acc 0.9375
2017-03-02T18:04:57.042244: step 25014, loss 0.154268, acc 0.9375
2017-03-02T18:04:57.111172: step 25015, loss 0.145436, acc 0.9375
2017-03-02T18:04:57.182608: step 25016, loss 0.017338, acc 1
2017-03-02T18:04:57.262420: step 25017, loss 0.0939614, acc 0.921875
2017-03-02T18:04:57.329918: step 25018, loss 0.158155, acc 0.921875
2017-03-02T18:04:57.399504: step 25019, loss 0.126623, acc 0.9375
2017-03-02T18:04:57.471685: step 25020, loss 0.219845, acc 0.953125
2017-03-02T18:04:57.538968: step 25021, loss 0.179784, acc 0.890625
2017-03-02T18:04:57.651628: step 25022, loss 0.276151, acc 0.859375
2017-03-02T18:04:57.738311: step 25023, loss 0.225296, acc 0.90625
2017-03-02T18:04:57.809874: step 25024, loss 0.112938, acc 0.921875
2017-03-02T18:04:57.887066: step 25025, loss 0.12065, acc 0.921875
2017-03-02T18:04:57.975007: step 25026, loss 0.211623, acc 0.890625
2017-03-02T18:04:58.046554: step 25027, loss 0.142484, acc 0.953125
2017-03-02T18:04:58.124770: step 25028, loss 0.110678, acc 0.953125
2017-03-02T18:04:58.191561: step 25029, loss 0.182589, acc 0.90625
2017-03-02T18:04:58.264519: step 25030, loss 0.163956, acc 0.90625
2017-03-02T18:04:58.342582: step 25031, loss 0.145011, acc 0.9375
2017-03-02T18:04:58.420921: step 25032, loss 0.113697, acc 0.953125
2017-03-02T18:04:58.506647: step 25033, loss 0.130529, acc 0.953125
2017-03-02T18:04:58.586637: step 25034, loss 0.199702, acc 0.875
2017-03-02T18:04:58.660563: step 25035, loss 0.114543, acc 0.921875
2017-03-02T18:04:58.732948: step 25036, loss 0.126536, acc 0.953125
2017-03-02T18:04:58.812195: step 25037, loss 0.127437, acc 0.96875
2017-03-02T18:04:58.885332: step 25038, loss 0.045278, acc 0.96875
2017-03-02T18:04:58.970415: step 25039, loss 0.0601374, acc 0.96875
2017-03-02T18:04:59.056111: step 25040, loss 0.0872161, acc 0.953125
2017-03-02T18:04:59.130890: step 25041, loss 0.0824277, acc 0.96875
2017-03-02T18:04:59.200309: step 25042, loss 0.0980048, acc 0.96875
2017-03-02T18:04:59.272903: step 25043, loss 0.174895, acc 0.9375
2017-03-02T18:04:59.344477: step 25044, loss 0.106263, acc 0.96875
2017-03-02T18:04:59.417039: step 25045, loss 0.174921, acc 0.96875
2017-03-02T18:04:59.486826: step 25046, loss 0.158661, acc 0.921875
2017-03-02T18:04:59.559158: step 25047, loss 0.129537, acc 0.90625
2017-03-02T18:04:59.635783: step 25048, loss 0.1143, acc 0.953125
2017-03-02T18:04:59.705487: step 25049, loss 0.105033, acc 0.9375
2017-03-02T18:04:59.773636: step 25050, loss 0.199703, acc 0.890625
2017-03-02T18:04:59.840968: step 25051, loss 0.228879, acc 0.921875
2017-03-02T18:04:59.915454: step 25052, loss 0.0958454, acc 0.953125
2017-03-02T18:04:59.987503: step 25053, loss 0.129066, acc 0.953125
2017-03-02T18:05:00.054759: step 25054, loss 0.233991, acc 0.859375
2017-03-02T18:05:00.123907: step 25055, loss 0.0989698, acc 0.96875
2017-03-02T18:05:00.194170: step 25056, loss 0.120935, acc 0.953125
2017-03-02T18:05:00.265699: step 25057, loss 0.132482, acc 0.953125
2017-03-02T18:05:00.340593: step 25058, loss 0.183551, acc 0.921875
2017-03-02T18:05:00.431606: step 25059, loss 0.149762, acc 0.953125
2017-03-02T18:05:00.505124: step 25060, loss 0.154778, acc 0.921875
2017-03-02T18:05:00.573732: step 25061, loss 0.134456, acc 0.953125
2017-03-02T18:05:00.642406: step 25062, loss 0.099146, acc 0.96875
2017-03-02T18:05:00.712904: step 25063, loss 0.145663, acc 0.953125
2017-03-02T18:05:00.786605: step 25064, loss 0.133269, acc 0.96875
2017-03-02T18:05:00.857118: step 25065, loss 0.225163, acc 0.921875
2017-03-02T18:05:00.930075: step 25066, loss 0.197033, acc 0.890625
2017-03-02T18:05:01.001522: step 25067, loss 0.219888, acc 0.9375
2017-03-02T18:05:01.069417: step 25068, loss 0.131732, acc 0.9375
2017-03-02T18:05:01.147721: step 25069, loss 0.155457, acc 0.921875
2017-03-02T18:05:01.219346: step 25070, loss 0.199625, acc 0.90625
2017-03-02T18:05:01.289121: step 25071, loss 0.175013, acc 0.921875
2017-03-02T18:05:01.366638: step 25072, loss 0.0812557, acc 0.96875
2017-03-02T18:05:01.441427: step 25073, loss 0.156809, acc 0.953125
2017-03-02T18:05:01.506787: step 25074, loss 0.129253, acc 0.9375
2017-03-02T18:05:01.580307: step 25075, loss 0.145255, acc 0.953125
2017-03-02T18:05:01.651493: step 25076, loss 0.171685, acc 0.890625
2017-03-02T18:05:01.719193: step 25077, loss 0.189799, acc 0.921875
2017-03-02T18:05:01.796032: step 25078, loss 0.168965, acc 0.890625
2017-03-02T18:05:01.873549: step 25079, loss 0.202813, acc 0.890625
2017-03-02T18:05:01.947991: step 25080, loss 0.0895944, acc 0.953125
2017-03-02T18:05:02.018610: step 25081, loss 0.184857, acc 0.921875
2017-03-02T18:05:02.088976: step 25082, loss 0.297126, acc 0.875
2017-03-02T18:05:02.154941: step 25083, loss 0.16945, acc 0.9375
2017-03-02T18:05:02.226220: step 25084, loss 0.135623, acc 0.921875
2017-03-02T18:05:02.298012: step 25085, loss 0.226548, acc 0.9375
2017-03-02T18:05:02.366261: step 25086, loss 0.145927, acc 0.90625
2017-03-02T18:05:02.439973: step 25087, loss 0.118643, acc 0.9375
2017-03-02T18:05:02.507595: step 25088, loss 0.138381, acc 1
2017-03-02T18:05:02.581323: step 25089, loss 0.183614, acc 0.9375
2017-03-02T18:05:02.647216: step 25090, loss 0.103959, acc 0.96875
2017-03-02T18:05:02.716848: step 25091, loss 0.186792, acc 0.90625
2017-03-02T18:05:02.784217: step 25092, loss 0.114409, acc 0.984375
2017-03-02T18:05:02.846134: step 25093, loss 0.127554, acc 0.953125
2017-03-02T18:05:02.910745: step 25094, loss 0.0875446, acc 0.9375
2017-03-02T18:05:02.980711: step 25095, loss 0.256144, acc 0.875
2017-03-02T18:05:03.051616: step 25096, loss 0.0986033, acc 0.96875
2017-03-02T18:05:03.121350: step 25097, loss 0.125113, acc 0.953125
2017-03-02T18:05:03.185963: step 25098, loss 0.0938076, acc 0.953125
2017-03-02T18:05:03.259395: step 25099, loss 0.153651, acc 0.921875
2017-03-02T18:05:03.333736: step 25100, loss 0.11206, acc 0.9375

Evaluation:
2017-03-02T18:05:03.361185: step 25100, loss 3.02025, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25100

2017-03-02T18:05:03.822148: step 25101, loss 0.184847, acc 0.953125
2017-03-02T18:05:03.890557: step 25102, loss 0.246918, acc 0.859375
2017-03-02T18:05:03.961424: step 25103, loss 0.196979, acc 0.90625
2017-03-02T18:05:04.030781: step 25104, loss 0.104338, acc 0.953125
2017-03-02T18:05:04.103869: step 25105, loss 0.256312, acc 0.921875
2017-03-02T18:05:04.168647: step 25106, loss 0.0988196, acc 0.96875
2017-03-02T18:05:04.231659: step 25107, loss 0.113499, acc 0.984375
2017-03-02T18:05:04.299649: step 25108, loss 0.112185, acc 0.953125
2017-03-02T18:05:04.361209: step 25109, loss 0.115304, acc 0.953125
2017-03-02T18:05:04.425553: step 25110, loss 0.0867618, acc 0.9375
2017-03-02T18:05:04.490878: step 25111, loss 0.187874, acc 0.921875
2017-03-02T18:05:04.558375: step 25112, loss 0.0697532, acc 0.96875
2017-03-02T18:05:04.619180: step 25113, loss 0.155905, acc 0.90625
2017-03-02T18:05:04.684798: step 25114, loss 0.090897, acc 0.953125
2017-03-02T18:05:04.748938: step 25115, loss 0.0744578, acc 0.96875
2017-03-02T18:05:04.823744: step 25116, loss 0.145545, acc 0.921875
2017-03-02T18:05:04.899088: step 25117, loss 0.188117, acc 0.90625
2017-03-02T18:05:04.969846: step 25118, loss 0.139394, acc 0.921875
2017-03-02T18:05:05.036670: step 25119, loss 0.095912, acc 0.984375
2017-03-02T18:05:05.108747: step 25120, loss 0.217942, acc 0.921875
2017-03-02T18:05:05.176006: step 25121, loss 0.103127, acc 0.9375
2017-03-02T18:05:05.241929: step 25122, loss 0.107839, acc 0.9375
2017-03-02T18:05:05.315951: step 25123, loss 0.159398, acc 0.9375
2017-03-02T18:05:05.381542: step 25124, loss 0.1074, acc 0.953125
2017-03-02T18:05:05.446160: step 25125, loss 0.185217, acc 0.90625
2017-03-02T18:05:05.514843: step 25126, loss 0.191872, acc 0.953125
2017-03-02T18:05:05.583790: step 25127, loss 0.0621515, acc 0.953125
2017-03-02T18:05:05.651066: step 25128, loss 0.136606, acc 0.9375
2017-03-02T18:05:05.714740: step 25129, loss 0.129141, acc 0.9375
2017-03-02T18:05:05.785173: step 25130, loss 0.178559, acc 0.921875
2017-03-02T18:05:05.850047: step 25131, loss 0.0702629, acc 0.96875
2017-03-02T18:05:05.913795: step 25132, loss 0.068944, acc 0.96875
2017-03-02T18:05:05.976795: step 25133, loss 0.217272, acc 0.921875
2017-03-02T18:05:06.043078: step 25134, loss 0.200211, acc 0.921875
2017-03-02T18:05:06.111001: step 25135, loss 0.1751, acc 0.921875
2017-03-02T18:05:06.179777: step 25136, loss 0.216979, acc 0.90625
2017-03-02T18:05:06.246153: step 25137, loss 0.0261529, acc 1
2017-03-02T18:05:06.310826: step 25138, loss 0.129453, acc 0.921875
2017-03-02T18:05:06.380413: step 25139, loss 0.0911222, acc 0.953125
2017-03-02T18:05:06.448791: step 25140, loss 0.0551043, acc 0.96875
2017-03-02T18:05:06.518001: step 25141, loss 0.136045, acc 0.9375
2017-03-02T18:05:06.581271: step 25142, loss 0.0491343, acc 0.984375
2017-03-02T18:05:06.646535: step 25143, loss 0.158502, acc 0.9375
2017-03-02T18:05:06.714157: step 25144, loss 0.0941405, acc 0.921875
2017-03-02T18:05:06.774804: step 25145, loss 0.105144, acc 0.953125
2017-03-02T18:05:06.843401: step 25146, loss 0.127119, acc 0.921875
2017-03-02T18:05:06.913362: step 25147, loss 0.171924, acc 0.953125
2017-03-02T18:05:06.978466: step 25148, loss 0.188745, acc 0.9375
2017-03-02T18:05:07.041141: step 25149, loss 0.0946009, acc 0.953125
2017-03-02T18:05:07.109155: step 25150, loss 0.23399, acc 0.859375
2017-03-02T18:05:07.174738: step 25151, loss 0.139575, acc 0.96875
2017-03-02T18:05:07.246261: step 25152, loss 0.0661781, acc 0.96875
2017-03-02T18:05:07.311258: step 25153, loss 0.170939, acc 0.921875
2017-03-02T18:05:07.375398: step 25154, loss 0.148386, acc 0.953125
2017-03-02T18:05:07.439466: step 25155, loss 0.258462, acc 0.921875
2017-03-02T18:05:07.506555: step 25156, loss 0.290015, acc 0.859375
2017-03-02T18:05:07.576780: step 25157, loss 0.120384, acc 0.9375
2017-03-02T18:05:07.642315: step 25158, loss 0.133186, acc 0.9375
2017-03-02T18:05:07.710952: step 25159, loss 0.139205, acc 0.921875
2017-03-02T18:05:07.777923: step 25160, loss 0.150465, acc 0.953125
2017-03-02T18:05:07.841548: step 25161, loss 0.110202, acc 0.953125
2017-03-02T18:05:07.908299: step 25162, loss 0.142364, acc 0.953125
2017-03-02T18:05:07.977560: step 25163, loss 0.168154, acc 0.921875
2017-03-02T18:05:08.042309: step 25164, loss 0.0501955, acc 0.96875
2017-03-02T18:05:08.112083: step 25165, loss 0.156192, acc 0.9375
2017-03-02T18:05:08.177690: step 25166, loss 0.149285, acc 0.9375
2017-03-02T18:05:08.246585: step 25167, loss 0.103451, acc 0.96875
2017-03-02T18:05:08.312529: step 25168, loss 0.115794, acc 0.953125
2017-03-02T18:05:08.379813: step 25169, loss 0.113054, acc 0.953125
2017-03-02T18:05:08.449971: step 25170, loss 0.112663, acc 0.9375
2017-03-02T18:05:08.516501: step 25171, loss 0.254329, acc 0.890625
2017-03-02T18:05:08.583460: step 25172, loss 0.230189, acc 0.90625
2017-03-02T18:05:08.643666: step 25173, loss 0.160742, acc 0.90625
2017-03-02T18:05:08.712689: step 25174, loss 0.128346, acc 0.921875
2017-03-02T18:05:08.775775: step 25175, loss 0.129479, acc 0.953125
2017-03-02T18:05:08.847796: step 25176, loss 0.0668296, acc 0.96875
2017-03-02T18:05:08.919308: step 25177, loss 0.151115, acc 0.9375
2017-03-02T18:05:08.990626: step 25178, loss 0.245525, acc 0.90625
2017-03-02T18:05:09.065359: step 25179, loss 0.139066, acc 0.9375
2017-03-02T18:05:09.136184: step 25180, loss 0.219037, acc 0.921875
2017-03-02T18:05:09.201385: step 25181, loss 0.135072, acc 0.90625
2017-03-02T18:05:09.267254: step 25182, loss 0.16296, acc 0.9375
2017-03-02T18:05:09.338664: step 25183, loss 0.0936741, acc 0.953125
2017-03-02T18:05:09.410737: step 25184, loss 0.13586, acc 0.921875
2017-03-02T18:05:09.476155: step 25185, loss 0.136488, acc 0.9375
2017-03-02T18:05:09.543594: step 25186, loss 0.111999, acc 0.953125
2017-03-02T18:05:09.613166: step 25187, loss 0.19403, acc 0.90625
2017-03-02T18:05:09.679436: step 25188, loss 0.287974, acc 0.875
2017-03-02T18:05:09.747674: step 25189, loss 0.175486, acc 0.9375
2017-03-02T18:05:09.813774: step 25190, loss 0.136959, acc 0.953125
2017-03-02T18:05:09.879769: step 25191, loss 0.176854, acc 0.921875
2017-03-02T18:05:09.948656: step 25192, loss 0.150783, acc 0.9375
2017-03-02T18:05:10.012727: step 25193, loss 0.197804, acc 0.90625
2017-03-02T18:05:10.077405: step 25194, loss 0.0603019, acc 0.96875
2017-03-02T18:05:10.139490: step 25195, loss 0.151185, acc 0.890625
2017-03-02T18:05:10.210781: step 25196, loss 0.119713, acc 0.9375
2017-03-02T18:05:10.280415: step 25197, loss 0.240007, acc 0.875
2017-03-02T18:05:10.347524: step 25198, loss 0.159715, acc 0.890625
2017-03-02T18:05:10.410382: step 25199, loss 0.0918124, acc 0.953125
2017-03-02T18:05:10.477882: step 25200, loss 0.115408, acc 0.921875

Evaluation:
2017-03-02T18:05:10.505826: step 25200, loss 3.13367, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25200

2017-03-02T18:05:11.224634: step 25201, loss 0.131439, acc 0.953125
2017-03-02T18:05:11.299900: step 25202, loss 0.127779, acc 0.96875
2017-03-02T18:05:11.368059: step 25203, loss 0.143341, acc 0.90625
2017-03-02T18:05:11.437930: step 25204, loss 0.0916767, acc 0.96875
2017-03-02T18:05:11.507263: step 25205, loss 0.105269, acc 0.953125
2017-03-02T18:05:11.571582: step 25206, loss 0.107033, acc 0.921875
2017-03-02T18:05:11.642422: step 25207, loss 0.0561939, acc 0.96875
2017-03-02T18:05:11.706195: step 25208, loss 0.211885, acc 0.90625
2017-03-02T18:05:11.774809: step 25209, loss 0.108293, acc 0.921875
2017-03-02T18:05:11.837445: step 25210, loss 0.188126, acc 0.921875
2017-03-02T18:05:11.905036: step 25211, loss 0.233952, acc 0.890625
2017-03-02T18:05:11.969522: step 25212, loss 0.166038, acc 0.921875
2017-03-02T18:05:12.046077: step 25213, loss 0.243327, acc 0.890625
2017-03-02T18:05:12.112095: step 25214, loss 0.278066, acc 0.90625
2017-03-02T18:05:12.181599: step 25215, loss 0.104096, acc 0.921875
2017-03-02T18:05:12.246980: step 25216, loss 0.128292, acc 0.96875
2017-03-02T18:05:12.317158: step 25217, loss 0.166829, acc 0.921875
2017-03-02T18:05:12.399643: step 25218, loss 0.13864, acc 0.953125
2017-03-02T18:05:12.469470: step 25219, loss 0.128791, acc 0.9375
2017-03-02T18:05:12.535508: step 25220, loss 0.137478, acc 0.9375
2017-03-02T18:05:12.601586: step 25221, loss 0.077764, acc 0.984375
2017-03-02T18:05:12.667875: step 25222, loss 0.163135, acc 0.921875
2017-03-02T18:05:12.750879: step 25223, loss 0.320692, acc 0.890625
2017-03-02T18:05:12.832672: step 25224, loss 0.210248, acc 0.875
2017-03-02T18:05:12.903691: step 25225, loss 0.14173, acc 0.90625
2017-03-02T18:05:12.974824: step 25226, loss 0.199729, acc 0.890625
2017-03-02T18:05:13.033768: step 25227, loss 0.099865, acc 0.953125
2017-03-02T18:05:13.107497: step 25228, loss 0.161925, acc 0.9375
2017-03-02T18:05:13.175196: step 25229, loss 0.119625, acc 0.96875
2017-03-02T18:05:13.238299: step 25230, loss 0.181879, acc 0.90625
2017-03-02T18:05:13.303624: step 25231, loss 0.11354, acc 0.953125
2017-03-02T18:05:13.369624: step 25232, loss 0.136429, acc 0.9375
2017-03-02T18:05:13.435348: step 25233, loss 0.0810565, acc 0.96875
2017-03-02T18:05:13.500701: step 25234, loss 0.163067, acc 0.890625
2017-03-02T18:05:13.556106: step 25235, loss 0.159175, acc 0.90625
2017-03-02T18:05:13.621287: step 25236, loss 0.225631, acc 0.921875
2017-03-02T18:05:13.688934: step 25237, loss 0.125942, acc 0.9375
2017-03-02T18:05:13.756145: step 25238, loss 0.136651, acc 0.9375
2017-03-02T18:05:13.834469: step 25239, loss 0.146191, acc 0.9375
2017-03-02T18:05:13.904118: step 25240, loss 0.25658, acc 0.875
2017-03-02T18:05:13.972517: step 25241, loss 0.123678, acc 0.9375
2017-03-02T18:05:14.039072: step 25242, loss 0.179943, acc 0.890625
2017-03-02T18:05:14.105891: step 25243, loss 0.0940444, acc 0.96875
2017-03-02T18:05:14.177404: step 25244, loss 0.0957688, acc 0.96875
2017-03-02T18:05:14.247172: step 25245, loss 0.125224, acc 0.953125
2017-03-02T18:05:14.326945: step 25246, loss 0.0953272, acc 0.96875
2017-03-02T18:05:14.385206: step 25247, loss 0.172313, acc 0.953125
2017-03-02T18:05:14.451682: step 25248, loss 0.112085, acc 0.984375
2017-03-02T18:05:14.522417: step 25249, loss 0.29885, acc 0.859375
2017-03-02T18:05:14.588646: step 25250, loss 0.156308, acc 0.9375
2017-03-02T18:05:14.659277: step 25251, loss 0.102227, acc 0.953125
2017-03-02T18:05:14.721256: step 25252, loss 0.118584, acc 0.9375
2017-03-02T18:05:14.788831: step 25253, loss 0.105598, acc 0.984375
2017-03-02T18:05:14.857635: step 25254, loss 0.155119, acc 0.921875
2017-03-02T18:05:14.926432: step 25255, loss 0.190515, acc 0.890625
2017-03-02T18:05:14.992975: step 25256, loss 0.0521206, acc 0.984375
2017-03-02T18:05:15.060711: step 25257, loss 0.0955195, acc 0.9375
2017-03-02T18:05:15.129156: step 25258, loss 0.196049, acc 0.921875
2017-03-02T18:05:15.193753: step 25259, loss 0.238852, acc 0.890625
2017-03-02T18:05:15.278441: step 25260, loss 0.176873, acc 0.921875
2017-03-02T18:05:15.348509: step 25261, loss 0.138683, acc 0.90625
2017-03-02T18:05:15.422227: step 25262, loss 0.248269, acc 0.890625
2017-03-02T18:05:15.492621: step 25263, loss 0.0936396, acc 0.953125
2017-03-02T18:05:15.559156: step 25264, loss 0.156871, acc 0.921875
2017-03-02T18:05:15.623693: step 25265, loss 0.0604765, acc 0.96875
2017-03-02T18:05:15.691052: step 25266, loss 0.247418, acc 0.890625
2017-03-02T18:05:15.766493: step 25267, loss 0.153151, acc 0.921875
2017-03-02T18:05:15.833293: step 25268, loss 0.1493, acc 0.9375
2017-03-02T18:05:15.904414: step 25269, loss 0.14442, acc 0.9375
2017-03-02T18:05:15.975036: step 25270, loss 0.102024, acc 0.96875
2017-03-02T18:05:16.047305: step 25271, loss 0.213249, acc 0.875
2017-03-02T18:05:16.111380: step 25272, loss 0.244459, acc 0.921875
2017-03-02T18:05:16.177073: step 25273, loss 0.11392, acc 0.953125
2017-03-02T18:05:16.243449: step 25274, loss 0.179066, acc 0.921875
2017-03-02T18:05:16.313323: step 25275, loss 0.12888, acc 0.9375
2017-03-02T18:05:16.377486: step 25276, loss 0.161368, acc 0.953125
2017-03-02T18:05:16.450355: step 25277, loss 0.0769453, acc 0.96875
2017-03-02T18:05:16.514177: step 25278, loss 0.104456, acc 0.96875
2017-03-02T18:05:16.580417: step 25279, loss 0.126166, acc 0.9375
2017-03-02T18:05:16.648786: step 25280, loss 0.220772, acc 0.90625
2017-03-02T18:05:16.712683: step 25281, loss 0.107246, acc 0.953125
2017-03-02T18:05:16.780768: step 25282, loss 0.107869, acc 0.953125
2017-03-02T18:05:16.850495: step 25283, loss 0.215413, acc 0.9375
2017-03-02T18:05:16.911648: step 25284, loss 0.694356, acc 0.75
2017-03-02T18:05:16.984342: step 25285, loss 0.100118, acc 0.953125
2017-03-02T18:05:17.054357: step 25286, loss 0.201843, acc 0.921875
2017-03-02T18:05:17.124450: step 25287, loss 0.108437, acc 0.96875
2017-03-02T18:05:17.194795: step 25288, loss 0.180268, acc 0.921875
2017-03-02T18:05:17.262925: step 25289, loss 0.305159, acc 0.84375
2017-03-02T18:05:17.331050: step 25290, loss 0.0947457, acc 0.953125
2017-03-02T18:05:17.401259: step 25291, loss 0.207623, acc 0.9375
2017-03-02T18:05:17.466014: step 25292, loss 0.282596, acc 0.84375
2017-03-02T18:05:17.531346: step 25293, loss 0.0791004, acc 0.96875
2017-03-02T18:05:17.596706: step 25294, loss 0.131693, acc 0.9375
2017-03-02T18:05:17.664157: step 25295, loss 0.161158, acc 0.90625
2017-03-02T18:05:17.730058: step 25296, loss 0.0672383, acc 0.984375
2017-03-02T18:05:17.792763: step 25297, loss 0.0767369, acc 0.953125
2017-03-02T18:05:17.861065: step 25298, loss 0.211092, acc 0.90625
2017-03-02T18:05:17.946097: step 25299, loss 0.0586847, acc 0.96875
2017-03-02T18:05:18.017440: step 25300, loss 0.20382, acc 0.921875

Evaluation:
2017-03-02T18:05:18.045226: step 25300, loss 3.11641, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25300

2017-03-02T18:05:18.508854: step 25301, loss 0.119631, acc 0.9375
2017-03-02T18:05:18.579552: step 25302, loss 0.109488, acc 0.9375
2017-03-02T18:05:18.654079: step 25303, loss 0.148021, acc 0.90625
2017-03-02T18:05:18.720424: step 25304, loss 0.0816629, acc 0.9375
2017-03-02T18:05:18.784728: step 25305, loss 0.203025, acc 0.921875
2017-03-02T18:05:18.859599: step 25306, loss 0.211948, acc 0.875
2017-03-02T18:05:18.928174: step 25307, loss 0.162034, acc 0.90625
2017-03-02T18:05:19.002560: step 25308, loss 0.199903, acc 0.890625
2017-03-02T18:05:19.067843: step 25309, loss 0.117897, acc 0.953125
2017-03-02T18:05:19.138111: step 25310, loss 0.164695, acc 0.921875
2017-03-02T18:05:19.206571: step 25311, loss 0.0886276, acc 0.96875
2017-03-02T18:05:19.277644: step 25312, loss 0.11132, acc 0.96875
2017-03-02T18:05:19.346790: step 25313, loss 0.127298, acc 0.953125
2017-03-02T18:05:19.417490: step 25314, loss 0.103919, acc 0.96875
2017-03-02T18:05:19.484948: step 25315, loss 0.141597, acc 0.9375
2017-03-02T18:05:19.555556: step 25316, loss 0.213274, acc 0.890625
2017-03-02T18:05:19.632309: step 25317, loss 0.17067, acc 0.90625
2017-03-02T18:05:19.696331: step 25318, loss 0.126954, acc 0.921875
2017-03-02T18:05:19.764862: step 25319, loss 0.0996203, acc 0.96875
2017-03-02T18:05:19.833590: step 25320, loss 0.166224, acc 0.9375
2017-03-02T18:05:19.908823: step 25321, loss 0.121128, acc 0.921875
2017-03-02T18:05:19.974274: step 25322, loss 0.137369, acc 0.921875
2017-03-02T18:05:20.044764: step 25323, loss 0.20368, acc 0.90625
2017-03-02T18:05:20.116085: step 25324, loss 0.255535, acc 0.921875
2017-03-02T18:05:20.202313: step 25325, loss 0.0883795, acc 0.984375
2017-03-02T18:05:20.270823: step 25326, loss 0.0591141, acc 0.984375
2017-03-02T18:05:20.342799: step 25327, loss 0.141058, acc 0.90625
2017-03-02T18:05:20.420577: step 25328, loss 0.128029, acc 0.96875
2017-03-02T18:05:20.489217: step 25329, loss 0.30579, acc 0.90625
2017-03-02T18:05:20.559676: step 25330, loss 0.142268, acc 0.953125
2017-03-02T18:05:20.631603: step 25331, loss 0.217959, acc 0.875
2017-03-02T18:05:20.700594: step 25332, loss 0.232516, acc 0.890625
2017-03-02T18:05:20.766407: step 25333, loss 0.106409, acc 0.96875
2017-03-02T18:05:20.833741: step 25334, loss 0.247177, acc 0.859375
2017-03-02T18:05:20.899749: step 25335, loss 0.211066, acc 0.890625
2017-03-02T18:05:20.966789: step 25336, loss 0.0947114, acc 0.984375
2017-03-02T18:05:21.039600: step 25337, loss 0.0620309, acc 0.96875
2017-03-02T18:05:21.110711: step 25338, loss 0.0783766, acc 0.984375
2017-03-02T18:05:21.188198: step 25339, loss 0.171388, acc 0.90625
2017-03-02T18:05:21.258839: step 25340, loss 0.167913, acc 0.9375
2017-03-02T18:05:21.329527: step 25341, loss 0.129624, acc 0.953125
2017-03-02T18:05:21.400486: step 25342, loss 0.144403, acc 0.921875
2017-03-02T18:05:21.470647: step 25343, loss 0.238081, acc 0.9375
2017-03-02T18:05:21.534274: step 25344, loss 0.179825, acc 0.90625
2017-03-02T18:05:21.602691: step 25345, loss 0.231989, acc 0.890625
2017-03-02T18:05:21.667928: step 25346, loss 0.131584, acc 0.9375
2017-03-02T18:05:21.737167: step 25347, loss 0.129401, acc 0.96875
2017-03-02T18:05:21.805447: step 25348, loss 0.197923, acc 0.90625
2017-03-02T18:05:21.872530: step 25349, loss 0.102301, acc 0.984375
2017-03-02T18:05:21.942262: step 25350, loss 0.0881785, acc 0.953125
2017-03-02T18:05:22.012243: step 25351, loss 0.159815, acc 0.890625
2017-03-02T18:05:22.075765: step 25352, loss 0.196259, acc 0.921875
2017-03-02T18:05:22.144918: step 25353, loss 0.174829, acc 0.921875
2017-03-02T18:05:22.210809: step 25354, loss 0.185524, acc 0.921875
2017-03-02T18:05:22.280146: step 25355, loss 0.112864, acc 0.953125
2017-03-02T18:05:22.348707: step 25356, loss 0.0855949, acc 0.953125
2017-03-02T18:05:22.422190: step 25357, loss 0.155862, acc 0.9375
2017-03-02T18:05:22.487943: step 25358, loss 0.134177, acc 0.9375
2017-03-02T18:05:22.555578: step 25359, loss 0.118568, acc 0.9375
2017-03-02T18:05:22.623799: step 25360, loss 0.149669, acc 0.953125
2017-03-02T18:05:22.690098: step 25361, loss 0.193551, acc 0.90625
2017-03-02T18:05:22.757261: step 25362, loss 0.168103, acc 0.890625
2017-03-02T18:05:22.827700: step 25363, loss 0.241223, acc 0.890625
2017-03-02T18:05:22.899090: step 25364, loss 0.169463, acc 0.921875
2017-03-02T18:05:22.973777: step 25365, loss 0.186083, acc 0.90625
2017-03-02T18:05:23.041412: step 25366, loss 0.0532682, acc 0.984375
2017-03-02T18:05:23.107427: step 25367, loss 0.116829, acc 0.9375
2017-03-02T18:05:23.174016: step 25368, loss 0.193666, acc 0.90625
2017-03-02T18:05:23.228180: step 25369, loss 0.136563, acc 0.953125
2017-03-02T18:05:23.295631: step 25370, loss 0.0605165, acc 0.984375
2017-03-02T18:05:23.362876: step 25371, loss 0.111821, acc 0.9375
2017-03-02T18:05:23.428755: step 25372, loss 0.142058, acc 0.921875
2017-03-02T18:05:23.498361: step 25373, loss 0.218195, acc 0.921875
2017-03-02T18:05:23.571015: step 25374, loss 0.168173, acc 0.90625
2017-03-02T18:05:23.634507: step 25375, loss 0.0986454, acc 0.953125
2017-03-02T18:05:23.700912: step 25376, loss 0.125711, acc 0.953125
2017-03-02T18:05:23.764832: step 25377, loss 0.121019, acc 0.984375
2017-03-02T18:05:23.831840: step 25378, loss 0.179253, acc 0.921875
2017-03-02T18:05:23.896727: step 25379, loss 0.158088, acc 0.921875
2017-03-02T18:05:23.967878: step 25380, loss 0.181044, acc 0.890625
2017-03-02T18:05:24.035805: step 25381, loss 0.169928, acc 0.90625
2017-03-02T18:05:24.101196: step 25382, loss 0.0833446, acc 0.953125
2017-03-02T18:05:24.178577: step 25383, loss 0.133984, acc 0.890625
2017-03-02T18:05:24.245041: step 25384, loss 0.191768, acc 0.921875
2017-03-02T18:05:24.308039: step 25385, loss 0.0886121, acc 0.96875
2017-03-02T18:05:24.368597: step 25386, loss 0.127945, acc 0.921875
2017-03-02T18:05:24.440682: step 25387, loss 0.157713, acc 0.921875
2017-03-02T18:05:24.507560: step 25388, loss 0.108561, acc 0.96875
2017-03-02T18:05:24.579250: step 25389, loss 0.0992988, acc 0.96875
2017-03-02T18:05:24.645482: step 25390, loss 0.211326, acc 0.890625
2017-03-02T18:05:24.717198: step 25391, loss 0.0774259, acc 0.96875
2017-03-02T18:05:24.788064: step 25392, loss 0.262129, acc 0.875
2017-03-02T18:05:24.856893: step 25393, loss 0.164089, acc 0.9375
2017-03-02T18:05:24.926234: step 25394, loss 0.202601, acc 0.90625
2017-03-02T18:05:24.992015: step 25395, loss 0.198335, acc 0.90625
2017-03-02T18:05:25.059379: step 25396, loss 0.0684182, acc 0.984375
2017-03-02T18:05:25.129130: step 25397, loss 0.0715236, acc 0.984375
2017-03-02T18:05:25.202984: step 25398, loss 0.161783, acc 0.953125
2017-03-02T18:05:25.274633: step 25399, loss 0.14659, acc 0.921875
2017-03-02T18:05:25.350175: step 25400, loss 0.176661, acc 0.921875

Evaluation:
2017-03-02T18:05:25.379970: step 25400, loss 3.15869, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25400

2017-03-02T18:05:25.841843: step 25401, loss 0.146362, acc 0.921875
2017-03-02T18:05:25.909308: step 25402, loss 0.19171, acc 0.90625
2017-03-02T18:05:25.965844: step 25403, loss 0.182032, acc 0.921875
2017-03-02T18:05:26.032379: step 25404, loss 0.166035, acc 0.921875
2017-03-02T18:05:26.101298: step 25405, loss 0.136829, acc 0.953125
2017-03-02T18:05:26.167863: step 25406, loss 0.136203, acc 0.921875
2017-03-02T18:05:26.230596: step 25407, loss 0.0970385, acc 0.96875
2017-03-02T18:05:26.297507: step 25408, loss 0.135538, acc 0.953125
2017-03-02T18:05:26.364061: step 25409, loss 0.194771, acc 0.890625
2017-03-02T18:05:26.429091: step 25410, loss 0.219695, acc 0.921875
2017-03-02T18:05:26.496373: step 25411, loss 0.107443, acc 0.953125
2017-03-02T18:05:26.562382: step 25412, loss 0.103346, acc 0.953125
2017-03-02T18:05:26.629794: step 25413, loss 0.0886079, acc 0.96875
2017-03-02T18:05:26.693901: step 25414, loss 0.111854, acc 0.96875
2017-03-02T18:05:26.765873: step 25415, loss 0.0932237, acc 0.984375
2017-03-02T18:05:26.832617: step 25416, loss 0.173241, acc 0.9375
2017-03-02T18:05:26.896926: step 25417, loss 0.213399, acc 0.875
2017-03-02T18:05:26.966732: step 25418, loss 0.201468, acc 0.890625
2017-03-02T18:05:27.032323: step 25419, loss 0.0953152, acc 0.9375
2017-03-02T18:05:27.099641: step 25420, loss 0.157223, acc 0.921875
2017-03-02T18:05:27.169569: step 25421, loss 0.124452, acc 0.9375
2017-03-02T18:05:27.238664: step 25422, loss 0.237111, acc 0.875
2017-03-02T18:05:27.302514: step 25423, loss 0.137283, acc 0.953125
2017-03-02T18:05:27.369509: step 25424, loss 0.191035, acc 0.890625
2017-03-02T18:05:27.437801: step 25425, loss 0.0966073, acc 0.96875
2017-03-02T18:05:27.500854: step 25426, loss 0.10798, acc 0.9375
2017-03-02T18:05:27.566216: step 25427, loss 0.15614, acc 0.921875
2017-03-02T18:05:27.629470: step 25428, loss 0.234177, acc 0.921875
2017-03-02T18:05:27.698287: step 25429, loss 0.288844, acc 0.90625
2017-03-02T18:05:27.768361: step 25430, loss 0.146296, acc 0.921875
2017-03-02T18:05:27.839163: step 25431, loss 0.166701, acc 0.890625
2017-03-02T18:05:27.913564: step 25432, loss 0.227835, acc 0.9375
2017-03-02T18:05:27.986002: step 25433, loss 0.199111, acc 0.921875
2017-03-02T18:05:28.051965: step 25434, loss 0.202487, acc 0.953125
2017-03-02T18:05:28.128634: step 25435, loss 0.15239, acc 0.9375
2017-03-02T18:05:28.194946: step 25436, loss 0.246107, acc 0.875
2017-03-02T18:05:28.268521: step 25437, loss 0.156361, acc 0.9375
2017-03-02T18:05:28.339879: step 25438, loss 0.110715, acc 0.953125
2017-03-02T18:05:28.412972: step 25439, loss 0.151428, acc 0.921875
2017-03-02T18:05:28.488353: step 25440, loss 0.175677, acc 0.921875
2017-03-02T18:05:28.554292: step 25441, loss 0.0499638, acc 0.96875
2017-03-02T18:05:28.623151: step 25442, loss 0.0900579, acc 0.953125
2017-03-02T18:05:28.696696: step 25443, loss 0.107223, acc 0.9375
2017-03-02T18:05:28.763764: step 25444, loss 0.222132, acc 0.953125
2017-03-02T18:05:28.830783: step 25445, loss 0.129946, acc 0.9375
2017-03-02T18:05:28.900457: step 25446, loss 0.065622, acc 0.984375
2017-03-02T18:05:28.969751: step 25447, loss 0.0597996, acc 0.984375
2017-03-02T18:05:29.035440: step 25448, loss 0.124944, acc 0.953125
2017-03-02T18:05:29.103616: step 25449, loss 0.127849, acc 0.96875
2017-03-02T18:05:29.170194: step 25450, loss 0.155417, acc 0.921875
2017-03-02T18:05:29.236157: step 25451, loss 0.10099, acc 0.96875
2017-03-02T18:05:29.306998: step 25452, loss 0.179565, acc 0.890625
2017-03-02T18:05:29.377921: step 25453, loss 0.148103, acc 0.96875
2017-03-02T18:05:29.444683: step 25454, loss 0.160519, acc 0.921875
2017-03-02T18:05:29.513648: step 25455, loss 0.120229, acc 0.953125
2017-03-02T18:05:29.582428: step 25456, loss 0.0932247, acc 0.9375
2017-03-02T18:05:29.648025: step 25457, loss 0.239734, acc 0.90625
2017-03-02T18:05:29.714471: step 25458, loss 0.147098, acc 0.921875
2017-03-02T18:05:29.785506: step 25459, loss 0.186501, acc 0.921875
2017-03-02T18:05:29.851728: step 25460, loss 0.114103, acc 0.953125
2017-03-02T18:05:29.930751: step 25461, loss 0.0609181, acc 0.96875
2017-03-02T18:05:30.005880: step 25462, loss 0.149687, acc 0.9375
2017-03-02T18:05:30.076578: step 25463, loss 0.17137, acc 0.921875
2017-03-02T18:05:30.147834: step 25464, loss 0.399482, acc 0.859375
2017-03-02T18:05:30.218957: step 25465, loss 0.16529, acc 0.9375
2017-03-02T18:05:30.288068: step 25466, loss 0.129755, acc 0.921875
2017-03-02T18:05:30.364796: step 25467, loss 0.177283, acc 0.9375
2017-03-02T18:05:30.436154: step 25468, loss 0.0595546, acc 1
2017-03-02T18:05:30.509805: step 25469, loss 0.167986, acc 0.921875
2017-03-02T18:05:30.580110: step 25470, loss 0.112013, acc 0.9375
2017-03-02T18:05:30.652002: step 25471, loss 0.219509, acc 0.921875
2017-03-02T18:05:30.727304: step 25472, loss 0.0981875, acc 0.953125
2017-03-02T18:05:30.798845: step 25473, loss 0.151476, acc 0.9375
2017-03-02T18:05:30.869870: step 25474, loss 0.150943, acc 0.9375
2017-03-02T18:05:30.936307: step 25475, loss 0.0620249, acc 0.96875
2017-03-02T18:05:31.008060: step 25476, loss 0.100815, acc 0.96875
2017-03-02T18:05:31.077881: step 25477, loss 0.135287, acc 0.921875
2017-03-02T18:05:31.145895: step 25478, loss 0.106408, acc 0.953125
2017-03-02T18:05:31.215901: step 25479, loss 0.162795, acc 0.890625
2017-03-02T18:05:31.278428: step 25480, loss 0, acc 1
2017-03-02T18:05:31.352653: step 25481, loss 0.115038, acc 0.96875
2017-03-02T18:05:31.421054: step 25482, loss 0.17341, acc 0.921875
2017-03-02T18:05:31.490296: step 25483, loss 0.129477, acc 0.953125
2017-03-02T18:05:31.558143: step 25484, loss 0.174317, acc 0.921875
2017-03-02T18:05:31.632529: step 25485, loss 0.121075, acc 0.921875
2017-03-02T18:05:31.698083: step 25486, loss 0.200428, acc 0.90625
2017-03-02T18:05:31.767440: step 25487, loss 0.141258, acc 0.9375
2017-03-02T18:05:31.836576: step 25488, loss 0.298681, acc 0.84375
2017-03-02T18:05:31.904415: step 25489, loss 0.240692, acc 0.90625
2017-03-02T18:05:31.973704: step 25490, loss 0.21479, acc 0.921875
2017-03-02T18:05:32.050380: step 25491, loss 0.103015, acc 0.953125
2017-03-02T18:05:32.117568: step 25492, loss 0.13568, acc 0.921875
2017-03-02T18:05:32.192693: step 25493, loss 0.106264, acc 0.953125
2017-03-02T18:05:32.263437: step 25494, loss 0.0328559, acc 1
2017-03-02T18:05:32.332945: step 25495, loss 0.085512, acc 0.96875
2017-03-02T18:05:32.411762: step 25496, loss 0.0879695, acc 0.96875
2017-03-02T18:05:32.476611: step 25497, loss 0.22204, acc 0.859375
2017-03-02T18:05:32.550051: step 25498, loss 0.0495777, acc 0.984375
2017-03-02T18:05:32.613756: step 25499, loss 0.170976, acc 0.921875
2017-03-02T18:05:32.681771: step 25500, loss 0.19805, acc 0.875

Evaluation:
2017-03-02T18:05:32.705546: step 25500, loss 3.14768, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25500

2017-03-02T18:05:33.129411: step 25501, loss 0.0834778, acc 0.953125
2017-03-02T18:05:33.196231: step 25502, loss 0.135623, acc 0.953125
2017-03-02T18:05:33.263071: step 25503, loss 0.170507, acc 0.890625
2017-03-02T18:05:33.344315: step 25504, loss 0.196628, acc 0.90625
2017-03-02T18:05:33.411768: step 25505, loss 0.0828266, acc 0.96875
2017-03-02T18:05:33.484490: step 25506, loss 0.0943866, acc 0.96875
2017-03-02T18:05:33.554753: step 25507, loss 0.193518, acc 0.90625
2017-03-02T18:05:33.625443: step 25508, loss 0.145492, acc 0.890625
2017-03-02T18:05:33.691776: step 25509, loss 0.130049, acc 0.921875
2017-03-02T18:05:33.756313: step 25510, loss 0.104759, acc 0.9375
2017-03-02T18:05:33.821403: step 25511, loss 0.0927561, acc 0.96875
2017-03-02T18:05:33.890354: step 25512, loss 0.139772, acc 0.9375
2017-03-02T18:05:33.956574: step 25513, loss 0.0869151, acc 0.96875
2017-03-02T18:05:34.022646: step 25514, loss 0.0995655, acc 1
2017-03-02T18:05:34.092844: step 25515, loss 0.0988552, acc 0.96875
2017-03-02T18:05:34.162865: step 25516, loss 0.0880642, acc 0.96875
2017-03-02T18:05:34.224983: step 25517, loss 0.196392, acc 0.90625
2017-03-02T18:05:34.291506: step 25518, loss 0.200783, acc 0.96875
2017-03-02T18:05:34.355554: step 25519, loss 0.151995, acc 0.9375
2017-03-02T18:05:34.413753: step 25520, loss 0.045769, acc 0.984375
2017-03-02T18:05:34.483435: step 25521, loss 0.194669, acc 0.890625
2017-03-02T18:05:34.545972: step 25522, loss 0.125672, acc 0.96875
2017-03-02T18:05:34.612165: step 25523, loss 0.205579, acc 0.890625
2017-03-02T18:05:34.680748: step 25524, loss 0.0832713, acc 0.984375
2017-03-02T18:05:34.754375: step 25525, loss 0.149881, acc 0.921875
2017-03-02T18:05:34.822036: step 25526, loss 0.097413, acc 0.953125
2017-03-02T18:05:34.890156: step 25527, loss 0.139521, acc 0.953125
2017-03-02T18:05:34.960006: step 25528, loss 0.1577, acc 0.921875
2017-03-02T18:05:35.029646: step 25529, loss 0.0941511, acc 0.953125
2017-03-02T18:05:35.095515: step 25530, loss 0.119318, acc 0.9375
2017-03-02T18:05:35.167253: step 25531, loss 0.200295, acc 0.96875
2017-03-02T18:05:35.236482: step 25532, loss 0.110225, acc 0.9375
2017-03-02T18:05:35.305495: step 25533, loss 0.0527194, acc 0.984375
2017-03-02T18:05:35.370027: step 25534, loss 0.194849, acc 0.921875
2017-03-02T18:05:35.438957: step 25535, loss 0.185209, acc 0.953125
2017-03-02T18:05:35.507227: step 25536, loss 0.117765, acc 0.953125
2017-03-02T18:05:35.575472: step 25537, loss 0.0795534, acc 0.984375
2017-03-02T18:05:35.646852: step 25538, loss 0.16384, acc 0.953125
2017-03-02T18:05:35.718693: step 25539, loss 0.166284, acc 0.890625
2017-03-02T18:05:35.783758: step 25540, loss 0.148951, acc 0.9375
2017-03-02T18:05:35.849308: step 25541, loss 0.102472, acc 0.953125
2017-03-02T18:05:35.921234: step 25542, loss 0.131679, acc 0.96875
2017-03-02T18:05:35.987770: step 25543, loss 0.103624, acc 0.9375
2017-03-02T18:05:36.043812: step 25544, loss 0.0768988, acc 0.96875
2017-03-02T18:05:36.099205: step 25545, loss 0.105837, acc 0.984375
2017-03-02T18:05:36.169988: step 25546, loss 0.105436, acc 0.953125
2017-03-02T18:05:36.240328: step 25547, loss 0.108643, acc 0.9375
2017-03-02T18:05:36.299789: step 25548, loss 0.216888, acc 0.890625
2017-03-02T18:05:36.362985: step 25549, loss 0.323991, acc 0.828125
2017-03-02T18:05:36.430753: step 25550, loss 0.202715, acc 0.890625
2017-03-02T18:05:36.515190: step 25551, loss 0.132721, acc 0.90625
2017-03-02T18:05:36.574370: step 25552, loss 0.136221, acc 0.921875
2017-03-02T18:05:36.635186: step 25553, loss 0.149633, acc 0.953125
2017-03-02T18:05:36.700871: step 25554, loss 0.125711, acc 0.96875
2017-03-02T18:05:36.770805: step 25555, loss 0.135172, acc 0.9375
2017-03-02T18:05:36.838944: step 25556, loss 0.206619, acc 0.9375
2017-03-02T18:05:36.905946: step 25557, loss 0.10253, acc 0.953125
2017-03-02T18:05:36.970768: step 25558, loss 0.165111, acc 0.90625
2017-03-02T18:05:37.039616: step 25559, loss 0.0673083, acc 0.96875
2017-03-02T18:05:37.112840: step 25560, loss 0.182839, acc 0.9375
2017-03-02T18:05:37.178193: step 25561, loss 0.099948, acc 0.953125
2017-03-02T18:05:37.245429: step 25562, loss 0.175344, acc 0.875
2017-03-02T18:05:37.307910: step 25563, loss 0.125482, acc 0.9375
2017-03-02T18:05:37.374320: step 25564, loss 0.132848, acc 0.921875
2017-03-02T18:05:37.440206: step 25565, loss 0.152854, acc 0.9375
2017-03-02T18:05:37.503664: step 25566, loss 0.222721, acc 0.90625
2017-03-02T18:05:37.567979: step 25567, loss 0.223044, acc 0.875
2017-03-02T18:05:37.640932: step 25568, loss 0.18296, acc 0.9375
2017-03-02T18:05:37.708846: step 25569, loss 0.120966, acc 0.921875
2017-03-02T18:05:37.777479: step 25570, loss 0.140215, acc 0.953125
2017-03-02T18:05:37.843106: step 25571, loss 0.107282, acc 0.953125
2017-03-02T18:05:37.913661: step 25572, loss 0.181514, acc 0.921875
2017-03-02T18:05:37.984665: step 25573, loss 0.0863679, acc 0.96875
2017-03-02T18:05:38.056429: step 25574, loss 0.125821, acc 0.921875
2017-03-02T18:05:38.129819: step 25575, loss 0.0944204, acc 0.953125
2017-03-02T18:05:38.199794: step 25576, loss 0.213342, acc 0.890625
2017-03-02T18:05:38.267206: step 25577, loss 0.266456, acc 0.890625
2017-03-02T18:05:38.334512: step 25578, loss 0.0667335, acc 0.96875
2017-03-02T18:05:38.405602: step 25579, loss 0.20088, acc 0.890625
2017-03-02T18:05:38.474665: step 25580, loss 0.264422, acc 0.875
2017-03-02T18:05:38.542390: step 25581, loss 0.210024, acc 0.90625
2017-03-02T18:05:38.608720: step 25582, loss 0.106691, acc 0.953125
2017-03-02T18:05:38.679968: step 25583, loss 0.14567, acc 0.953125
2017-03-02T18:05:38.750306: step 25584, loss 0.183951, acc 0.90625
2017-03-02T18:05:38.819848: step 25585, loss 0.127778, acc 0.9375
2017-03-02T18:05:38.886992: step 25586, loss 0.210412, acc 0.921875
2017-03-02T18:05:38.954810: step 25587, loss 0.0480688, acc 0.96875
2017-03-02T18:05:39.018279: step 25588, loss 0.157174, acc 0.953125
2017-03-02T18:05:39.082519: step 25589, loss 0.106438, acc 0.953125
2017-03-02T18:05:39.148750: step 25590, loss 0.251358, acc 0.90625
2017-03-02T18:05:39.215791: step 25591, loss 0.112984, acc 0.9375
2017-03-02T18:05:39.280410: step 25592, loss 0.160889, acc 0.921875
2017-03-02T18:05:39.350205: step 25593, loss 0.158831, acc 0.90625
2017-03-02T18:05:39.419996: step 25594, loss 0.160563, acc 0.890625
2017-03-02T18:05:39.486248: step 25595, loss 0.127044, acc 0.9375
2017-03-02T18:05:39.553357: step 25596, loss 0.127515, acc 0.953125
2017-03-02T18:05:39.619817: step 25597, loss 0.0830517, acc 0.96875
2017-03-02T18:05:39.684889: step 25598, loss 0.228059, acc 0.890625
2017-03-02T18:05:39.749428: step 25599, loss 0.148757, acc 0.953125
2017-03-02T18:05:39.816536: step 25600, loss 0.106726, acc 0.96875

Evaluation:
2017-03-02T18:05:39.842774: step 25600, loss 3.16947, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25600

2017-03-02T18:05:40.330610: step 25601, loss 0.122198, acc 0.953125
2017-03-02T18:05:40.397381: step 25602, loss 0.137755, acc 0.9375
2017-03-02T18:05:40.465579: step 25603, loss 0.166695, acc 0.90625
2017-03-02T18:05:40.529462: step 25604, loss 0.154538, acc 0.921875
2017-03-02T18:05:40.596606: step 25605, loss 0.0879468, acc 0.953125
2017-03-02T18:05:40.665626: step 25606, loss 0.136399, acc 0.953125
2017-03-02T18:05:40.737155: step 25607, loss 0.12212, acc 0.921875
2017-03-02T18:05:40.804033: step 25608, loss 0.143273, acc 0.90625
2017-03-02T18:05:40.869941: step 25609, loss 0.294805, acc 0.890625
2017-03-02T18:05:40.939596: step 25610, loss 0.150441, acc 0.953125
2017-03-02T18:05:41.008594: step 25611, loss 0.223781, acc 0.890625
2017-03-02T18:05:41.086533: step 25612, loss 0.0662568, acc 0.96875
2017-03-02T18:05:41.155490: step 25613, loss 0.176365, acc 0.9375
2017-03-02T18:05:41.224753: step 25614, loss 0.155409, acc 0.9375
2017-03-02T18:05:41.294698: step 25615, loss 0.0925039, acc 0.953125
2017-03-02T18:05:41.366244: step 25616, loss 0.200236, acc 0.90625
2017-03-02T18:05:41.434438: step 25617, loss 0.116563, acc 0.9375
2017-03-02T18:05:41.503022: step 25618, loss 0.144337, acc 0.921875
2017-03-02T18:05:41.577329: step 25619, loss 0.112894, acc 0.984375
2017-03-02T18:05:41.647471: step 25620, loss 0.374627, acc 0.84375
2017-03-02T18:05:41.715955: step 25621, loss 0.136019, acc 0.921875
2017-03-02T18:05:41.779965: step 25622, loss 0.10215, acc 0.9375
2017-03-02T18:05:41.849356: step 25623, loss 0.150019, acc 0.9375
2017-03-02T18:05:41.918944: step 25624, loss 0.0750177, acc 0.96875
2017-03-02T18:05:41.991149: step 25625, loss 0.156006, acc 0.953125
2017-03-02T18:05:42.054729: step 25626, loss 0.199969, acc 0.921875
2017-03-02T18:05:42.119543: step 25627, loss 0.131108, acc 0.9375
2017-03-02T18:05:42.186678: step 25628, loss 0.19884, acc 0.921875
2017-03-02T18:05:42.253403: step 25629, loss 0.100009, acc 0.96875
2017-03-02T18:05:42.325766: step 25630, loss 0.143985, acc 0.921875
2017-03-02T18:05:42.399034: step 25631, loss 0.0415619, acc 0.984375
2017-03-02T18:05:42.466159: step 25632, loss 0.145929, acc 0.9375
2017-03-02T18:05:42.530954: step 25633, loss 0.0729477, acc 0.96875
2017-03-02T18:05:42.621813: step 25634, loss 0.242271, acc 0.90625
2017-03-02T18:05:42.685615: step 25635, loss 0.0965127, acc 0.953125
2017-03-02T18:05:42.749924: step 25636, loss 0.162202, acc 0.9375
2017-03-02T18:05:42.816395: step 25637, loss 0.272728, acc 0.9375
2017-03-02T18:05:42.886229: step 25638, loss 0.0459932, acc 0.984375
2017-03-02T18:05:42.948941: step 25639, loss 0.176593, acc 0.9375
2017-03-02T18:05:43.019571: step 25640, loss 0.26009, acc 0.921875
2017-03-02T18:05:43.088324: step 25641, loss 0.164843, acc 0.90625
2017-03-02T18:05:43.159005: step 25642, loss 0.154326, acc 0.921875
2017-03-02T18:05:43.231101: step 25643, loss 0.18032, acc 0.921875
2017-03-02T18:05:43.307524: step 25644, loss 0.11234, acc 0.96875
2017-03-02T18:05:43.375258: step 25645, loss 0.0973022, acc 0.96875
2017-03-02T18:05:43.444332: step 25646, loss 0.167534, acc 0.921875
2017-03-02T18:05:43.514517: step 25647, loss 0.286794, acc 0.859375
2017-03-02T18:05:43.585910: step 25648, loss 0.139617, acc 0.921875
2017-03-02T18:05:43.652579: step 25649, loss 0.220369, acc 0.90625
2017-03-02T18:05:43.715482: step 25650, loss 0.129482, acc 0.921875
2017-03-02T18:05:43.782285: step 25651, loss 0.216244, acc 0.921875
2017-03-02T18:05:43.852223: step 25652, loss 0.10454, acc 0.9375
2017-03-02T18:05:43.929123: step 25653, loss 0.12722, acc 0.9375
2017-03-02T18:05:43.991178: step 25654, loss 0.153152, acc 0.890625
2017-03-02T18:05:44.058642: step 25655, loss 0.213204, acc 0.90625
2017-03-02T18:05:44.129483: step 25656, loss 0.0885686, acc 1
2017-03-02T18:05:44.199620: step 25657, loss 0.126572, acc 0.9375
2017-03-02T18:05:44.272558: step 25658, loss 0.273314, acc 0.84375
2017-03-02T18:05:44.343871: step 25659, loss 0.167271, acc 0.953125
2017-03-02T18:05:44.415430: step 25660, loss 0.189962, acc 0.90625
2017-03-02T18:05:44.484705: step 25661, loss 0.176151, acc 0.9375
2017-03-02T18:05:44.559865: step 25662, loss 0.102159, acc 0.9375
2017-03-02T18:05:44.630229: step 25663, loss 0.161167, acc 0.90625
2017-03-02T18:05:44.705036: step 25664, loss 0.0605771, acc 0.984375
2017-03-02T18:05:44.778289: step 25665, loss 0.125257, acc 0.953125
2017-03-02T18:05:44.846750: step 25666, loss 0.200619, acc 0.921875
2017-03-02T18:05:44.919159: step 25667, loss 0.122829, acc 0.953125
2017-03-02T18:05:44.989374: step 25668, loss 0.132729, acc 0.921875
2017-03-02T18:05:45.062680: step 25669, loss 0.188842, acc 0.90625
2017-03-02T18:05:45.132396: step 25670, loss 0.158109, acc 0.90625
2017-03-02T18:05:45.202525: step 25671, loss 0.136115, acc 0.953125
2017-03-02T18:05:45.273874: step 25672, loss 0.221212, acc 0.953125
2017-03-02T18:05:45.342019: step 25673, loss 0.185413, acc 0.890625
2017-03-02T18:05:45.407666: step 25674, loss 0.101134, acc 0.96875
2017-03-02T18:05:45.482478: step 25675, loss 0.169575, acc 0.90625
2017-03-02T18:05:45.545242: step 25676, loss 0.0799168, acc 1
2017-03-02T18:05:45.619693: step 25677, loss 0.205376, acc 0.921875
2017-03-02T18:05:45.682131: step 25678, loss 0.122545, acc 0.953125
2017-03-02T18:05:45.754815: step 25679, loss 0.116109, acc 0.96875
2017-03-02T18:05:45.821748: step 25680, loss 0.100363, acc 0.953125
2017-03-02T18:05:45.892220: step 25681, loss 0.146717, acc 0.9375
2017-03-02T18:05:45.961845: step 25682, loss 0.0717434, acc 0.96875
2017-03-02T18:05:46.029052: step 25683, loss 0.119739, acc 0.9375
2017-03-02T18:05:46.092693: step 25684, loss 0.132967, acc 0.921875
2017-03-02T18:05:46.185880: step 25685, loss 0.0952697, acc 0.984375
2017-03-02T18:05:46.252115: step 25686, loss 0.105819, acc 0.96875
2017-03-02T18:05:46.324820: step 25687, loss 0.0565169, acc 1
2017-03-02T18:05:46.395768: step 25688, loss 0.160415, acc 0.953125
2017-03-02T18:05:46.467106: step 25689, loss 0.165005, acc 0.9375
2017-03-02T18:05:46.537667: step 25690, loss 0.157052, acc 0.921875
2017-03-02T18:05:46.609480: step 25691, loss 0.115405, acc 0.953125
2017-03-02T18:05:46.674429: step 25692, loss 0.163907, acc 0.90625
2017-03-02T18:05:46.743525: step 25693, loss 0.105194, acc 0.9375
2017-03-02T18:05:46.813431: step 25694, loss 0.0733552, acc 0.96875
2017-03-02T18:05:46.880924: step 25695, loss 0.228766, acc 0.859375
2017-03-02T18:05:46.950171: step 25696, loss 0.0985272, acc 0.9375
2017-03-02T18:05:47.022294: step 25697, loss 0.172333, acc 0.90625
2017-03-02T18:05:47.089386: step 25698, loss 0.138428, acc 0.9375
2017-03-02T18:05:47.157144: step 25699, loss 0.0478999, acc 1
2017-03-02T18:05:47.226985: step 25700, loss 0.105213, acc 0.9375

Evaluation:
2017-03-02T18:05:47.261252: step 25700, loss 3.17505, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25700

2017-03-02T18:05:47.716782: step 25701, loss 0.141304, acc 0.9375
2017-03-02T18:05:47.784413: step 25702, loss 0.164199, acc 0.921875
2017-03-02T18:05:47.855016: step 25703, loss 0.185987, acc 0.90625
2017-03-02T18:05:47.921929: step 25704, loss 0.257785, acc 0.9375
2017-03-02T18:05:47.992699: step 25705, loss 0.154807, acc 0.921875
2017-03-02T18:05:48.059172: step 25706, loss 0.149264, acc 0.90625
2017-03-02T18:05:48.126195: step 25707, loss 0.139533, acc 0.953125
2017-03-02T18:05:48.195731: step 25708, loss 0.137007, acc 0.921875
2017-03-02T18:05:48.272623: step 25709, loss 0.223682, acc 0.90625
2017-03-02T18:05:48.348700: step 25710, loss 0.180765, acc 0.90625
2017-03-02T18:05:48.414610: step 25711, loss 0.0513267, acc 0.984375
2017-03-02T18:05:48.484632: step 25712, loss 0.227081, acc 0.9375
2017-03-02T18:05:48.551615: step 25713, loss 0.197134, acc 0.90625
2017-03-02T18:05:48.619660: step 25714, loss 0.170546, acc 0.9375
2017-03-02T18:05:48.690438: step 25715, loss 0.179735, acc 0.90625
2017-03-02T18:05:48.761004: step 25716, loss 0.184125, acc 0.9375
2017-03-02T18:05:48.829438: step 25717, loss 0.193274, acc 0.90625
2017-03-02T18:05:48.886622: step 25718, loss 0.136519, acc 0.953125
2017-03-02T18:05:48.952202: step 25719, loss 0.188302, acc 0.921875
2017-03-02T18:05:49.018760: step 25720, loss 0.0650705, acc 0.96875
2017-03-02T18:05:49.087675: step 25721, loss 0.0995167, acc 0.96875
2017-03-02T18:05:49.159446: step 25722, loss 0.110161, acc 0.953125
2017-03-02T18:05:49.228192: step 25723, loss 0.0941786, acc 0.96875
2017-03-02T18:05:49.300502: step 25724, loss 0.158037, acc 0.90625
2017-03-02T18:05:49.368096: step 25725, loss 0.242763, acc 0.90625
2017-03-02T18:05:49.436314: step 25726, loss 0.122804, acc 0.953125
2017-03-02T18:05:49.507432: step 25727, loss 0.195294, acc 0.90625
2017-03-02T18:05:49.572710: step 25728, loss 0.124657, acc 0.921875
2017-03-02T18:05:49.644042: step 25729, loss 0.103912, acc 0.953125
2017-03-02T18:05:49.708773: step 25730, loss 0.0622603, acc 0.984375
2017-03-02T18:05:49.778523: step 25731, loss 0.185079, acc 0.9375
2017-03-02T18:05:49.847146: step 25732, loss 0.117078, acc 0.953125
2017-03-02T18:05:49.911428: step 25733, loss 0.125128, acc 0.9375
2017-03-02T18:05:49.981954: step 25734, loss 0.17486, acc 0.9375
2017-03-02T18:05:50.051955: step 25735, loss 0.176181, acc 0.9375
2017-03-02T18:05:50.117610: step 25736, loss 0.0771076, acc 0.984375
2017-03-02T18:05:50.186834: step 25737, loss 0.155452, acc 0.921875
2017-03-02T18:05:50.259546: step 25738, loss 0.191152, acc 0.90625
2017-03-02T18:05:50.331909: step 25739, loss 0.109054, acc 0.9375
2017-03-02T18:05:50.401094: step 25740, loss 0.130221, acc 0.953125
2017-03-02T18:05:50.471766: step 25741, loss 0.120265, acc 0.9375
2017-03-02T18:05:50.539813: step 25742, loss 0.0789732, acc 0.96875
2017-03-02T18:05:50.610850: step 25743, loss 0.255283, acc 0.890625
2017-03-02T18:05:50.680246: step 25744, loss 0.306841, acc 0.828125
2017-03-02T18:05:50.758806: step 25745, loss 0.173816, acc 0.921875
2017-03-02T18:05:50.822809: step 25746, loss 0.0841658, acc 0.96875
2017-03-02T18:05:50.883614: step 25747, loss 0.224171, acc 0.90625
2017-03-02T18:05:50.949647: step 25748, loss 0.231605, acc 0.875
2017-03-02T18:05:51.022388: step 25749, loss 0.25972, acc 0.890625
2017-03-02T18:05:51.080445: step 25750, loss 0.227899, acc 0.890625
2017-03-02T18:05:51.147606: step 25751, loss 0.296004, acc 0.859375
2017-03-02T18:05:51.216182: step 25752, loss 0.128438, acc 0.90625
2017-03-02T18:05:51.297228: step 25753, loss 0.187619, acc 0.953125
2017-03-02T18:05:51.372866: step 25754, loss 0.234076, acc 0.953125
2017-03-02T18:05:51.438612: step 25755, loss 0.0895991, acc 0.953125
2017-03-02T18:05:51.506629: step 25756, loss 0.104552, acc 0.953125
2017-03-02T18:05:51.571673: step 25757, loss 0.154966, acc 0.953125
2017-03-02T18:05:51.640082: step 25758, loss 0.227536, acc 0.875
2017-03-02T18:05:51.708516: step 25759, loss 0.103589, acc 0.953125
2017-03-02T18:05:51.775624: step 25760, loss 0.0948633, acc 0.953125
2017-03-02T18:05:51.845911: step 25761, loss 0.229984, acc 0.90625
2017-03-02T18:05:51.911241: step 25762, loss 0.251774, acc 0.90625
2017-03-02T18:05:51.978752: step 25763, loss 0.141268, acc 0.953125
2017-03-02T18:05:52.047940: step 25764, loss 0.0587425, acc 0.96875
2017-03-02T18:05:52.119579: step 25765, loss 0.0672661, acc 0.96875
2017-03-02T18:05:52.188413: step 25766, loss 0.127351, acc 0.96875
2017-03-02T18:05:52.254824: step 25767, loss 0.342796, acc 0.859375
2017-03-02T18:05:52.323628: step 25768, loss 0.151007, acc 0.921875
2017-03-02T18:05:52.394270: step 25769, loss 0.103514, acc 0.953125
2017-03-02T18:05:52.461093: step 25770, loss 0.111619, acc 0.9375
2017-03-02T18:05:52.527428: step 25771, loss 0.163835, acc 0.890625
2017-03-02T18:05:52.597323: step 25772, loss 0.182365, acc 0.921875
2017-03-02T18:05:52.664695: step 25773, loss 0.146312, acc 0.9375
2017-03-02T18:05:52.730291: step 25774, loss 0.190203, acc 0.921875
2017-03-02T18:05:52.794106: step 25775, loss 0.232202, acc 0.875
2017-03-02T18:05:52.859439: step 25776, loss 0.131972, acc 0.9375
2017-03-02T18:05:52.924600: step 25777, loss 0.125573, acc 0.921875
2017-03-02T18:05:52.994530: step 25778, loss 0.133525, acc 0.9375
2017-03-02T18:05:53.059242: step 25779, loss 0.154921, acc 0.875
2017-03-02T18:05:53.122644: step 25780, loss 0.163072, acc 0.90625
2017-03-02T18:05:53.187640: step 25781, loss 0.164582, acc 0.921875
2017-03-02T18:05:53.257808: step 25782, loss 0.15461, acc 0.921875
2017-03-02T18:05:53.324616: step 25783, loss 0.217152, acc 0.90625
2017-03-02T18:05:53.392531: step 25784, loss 0.146428, acc 0.953125
2017-03-02T18:05:53.468369: step 25785, loss 0.123857, acc 0.9375
2017-03-02T18:05:53.535016: step 25786, loss 0.167447, acc 0.90625
2017-03-02T18:05:53.604462: step 25787, loss 0.118044, acc 0.9375
2017-03-02T18:05:53.669770: step 25788, loss 0.160852, acc 0.90625
2017-03-02T18:05:53.735108: step 25789, loss 0.109023, acc 0.9375
2017-03-02T18:05:53.826999: step 25790, loss 0.148825, acc 0.9375
2017-03-02T18:05:53.899709: step 25791, loss 0.165008, acc 0.90625
2017-03-02T18:05:53.976314: step 25792, loss 0.146175, acc 0.953125
2017-03-02T18:05:54.046455: step 25793, loss 0.0639585, acc 0.953125
2017-03-02T18:05:54.105241: step 25794, loss 0.171201, acc 0.9375
2017-03-02T18:05:54.165726: step 25795, loss 0.0526145, acc 0.96875
2017-03-02T18:05:54.234927: step 25796, loss 0.123223, acc 0.921875
2017-03-02T18:05:54.303523: step 25797, loss 0.13004, acc 0.90625
2017-03-02T18:05:54.375377: step 25798, loss 0.152025, acc 0.90625
2017-03-02T18:05:54.449284: step 25799, loss 0.198856, acc 0.890625
2017-03-02T18:05:54.517562: step 25800, loss 0.195424, acc 0.9375

Evaluation:
2017-03-02T18:05:54.541976: step 25800, loss 3.27864, acc 0.651045

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25800

2017-03-02T18:05:55.009053: step 25801, loss 0.22993, acc 0.875
2017-03-02T18:05:55.074971: step 25802, loss 0.158257, acc 0.921875
2017-03-02T18:05:55.148878: step 25803, loss 0.151446, acc 0.953125
2017-03-02T18:05:55.221101: step 25804, loss 0.162889, acc 0.953125
2017-03-02T18:05:55.283704: step 25805, loss 0.152859, acc 0.921875
2017-03-02T18:05:55.357825: step 25806, loss 0.300996, acc 0.859375
2017-03-02T18:05:55.423622: step 25807, loss 0.264792, acc 0.921875
2017-03-02T18:05:55.492728: step 25808, loss 0.125299, acc 0.9375
2017-03-02T18:05:55.559352: step 25809, loss 0.230252, acc 0.921875
2017-03-02T18:05:55.627433: step 25810, loss 0.261953, acc 0.890625
2017-03-02T18:05:55.691444: step 25811, loss 0.227535, acc 0.890625
2017-03-02T18:05:55.763062: step 25812, loss 0.119884, acc 0.96875
2017-03-02T18:05:55.829127: step 25813, loss 0.197553, acc 0.875
2017-03-02T18:05:55.900763: step 25814, loss 0.169632, acc 0.90625
2017-03-02T18:05:55.963173: step 25815, loss 0.157808, acc 0.90625
2017-03-02T18:05:56.030931: step 25816, loss 0.163141, acc 0.921875
2017-03-02T18:05:56.097861: step 25817, loss 0.124145, acc 0.921875
2017-03-02T18:05:56.169351: step 25818, loss 0.213611, acc 0.890625
2017-03-02T18:05:56.233460: step 25819, loss 0.0620836, acc 0.96875
2017-03-02T18:05:56.303586: step 25820, loss 0.202278, acc 0.875
2017-03-02T18:05:56.369013: step 25821, loss 0.0742366, acc 0.96875
2017-03-02T18:05:56.435139: step 25822, loss 0.179227, acc 0.890625
2017-03-02T18:05:56.504097: step 25823, loss 0.161971, acc 0.90625
2017-03-02T18:05:56.574442: step 25824, loss 0.12918, acc 0.9375
2017-03-02T18:05:56.644669: step 25825, loss 0.209563, acc 0.921875
2017-03-02T18:05:56.719713: step 25826, loss 0.11818, acc 0.9375
2017-03-02T18:05:56.785953: step 25827, loss 0.0564595, acc 1
2017-03-02T18:05:56.858990: step 25828, loss 0.0846401, acc 0.96875
2017-03-02T18:05:56.923217: step 25829, loss 0.0846458, acc 0.984375
2017-03-02T18:05:56.989446: step 25830, loss 0.166613, acc 0.90625
2017-03-02T18:05:57.045402: step 25831, loss 0.250732, acc 0.90625
2017-03-02T18:05:57.110038: step 25832, loss 0.230063, acc 0.90625
2017-03-02T18:05:57.174846: step 25833, loss 0.0826255, acc 0.96875
2017-03-02T18:05:57.239720: step 25834, loss 0.166191, acc 0.921875
2017-03-02T18:05:57.306857: step 25835, loss 0.138595, acc 0.921875
2017-03-02T18:05:57.374342: step 25836, loss 0.0752093, acc 0.96875
2017-03-02T18:05:57.448764: step 25837, loss 0.0627355, acc 0.984375
2017-03-02T18:05:57.514588: step 25838, loss 0.16077, acc 0.921875
2017-03-02T18:05:57.584525: step 25839, loss 0.0596797, acc 0.96875
2017-03-02T18:05:57.655668: step 25840, loss 0.0716597, acc 0.984375
2017-03-02T18:05:57.732517: step 25841, loss 0.224361, acc 0.90625
2017-03-02T18:05:57.802955: step 25842, loss 0.136084, acc 0.953125
2017-03-02T18:05:57.875376: step 25843, loss 0.0862795, acc 0.96875
2017-03-02T18:05:57.941140: step 25844, loss 0.206611, acc 0.953125
2017-03-02T18:05:58.013251: step 25845, loss 0.199439, acc 0.890625
2017-03-02T18:05:58.085026: step 25846, loss 0.254587, acc 0.890625
2017-03-02T18:05:58.157352: step 25847, loss 0.12665, acc 0.921875
2017-03-02T18:05:58.226009: step 25848, loss 0.11405, acc 0.96875
2017-03-02T18:05:58.299612: step 25849, loss 0.0650302, acc 0.984375
2017-03-02T18:05:58.374209: step 25850, loss 0.065713, acc 0.984375
2017-03-02T18:05:58.442413: step 25851, loss 0.111716, acc 0.953125
2017-03-02T18:05:58.517525: step 25852, loss 0.147075, acc 0.921875
2017-03-02T18:05:58.588634: step 25853, loss 0.141929, acc 0.921875
2017-03-02T18:05:58.659043: step 25854, loss 0.0886565, acc 0.953125
2017-03-02T18:05:58.728458: step 25855, loss 0.191219, acc 0.90625
2017-03-02T18:05:58.796456: step 25856, loss 0.158651, acc 0.9375
2017-03-02T18:05:58.877043: step 25857, loss 0.148995, acc 0.921875
2017-03-02T18:05:58.942498: step 25858, loss 0.0843584, acc 0.9375
2017-03-02T18:05:59.005280: step 25859, loss 0.112287, acc 0.9375
2017-03-02T18:05:59.083239: step 25860, loss 0.100786, acc 0.9375
2017-03-02T18:05:59.161775: step 25861, loss 0.176694, acc 0.953125
2017-03-02T18:05:59.233695: step 25862, loss 0.186351, acc 0.9375
2017-03-02T18:05:59.301254: step 25863, loss 0.135357, acc 0.984375
2017-03-02T18:05:59.368530: step 25864, loss 0.132819, acc 0.9375
2017-03-02T18:05:59.431421: step 25865, loss 0.12129, acc 0.9375
2017-03-02T18:05:59.501539: step 25866, loss 0.164752, acc 0.921875
2017-03-02T18:05:59.571612: step 25867, loss 0.123489, acc 0.9375
2017-03-02T18:05:59.650293: step 25868, loss 0.0803789, acc 0.984375
2017-03-02T18:05:59.715911: step 25869, loss 0.163987, acc 0.9375
2017-03-02T18:05:59.782234: step 25870, loss 0.0983102, acc 0.9375
2017-03-02T18:05:59.849174: step 25871, loss 0.203961, acc 0.921875
2017-03-02T18:05:59.911716: step 25872, loss 0.0472849, acc 1
2017-03-02T18:05:59.982126: step 25873, loss 0.108996, acc 0.921875
2017-03-02T18:06:00.050153: step 25874, loss 0.121581, acc 0.9375
2017-03-02T18:06:00.129765: step 25875, loss 0.197128, acc 0.90625
2017-03-02T18:06:00.211213: step 25876, loss 0.100923, acc 0.9375
2017-03-02T18:06:00.280104: step 25877, loss 0.141878, acc 0.921875
2017-03-02T18:06:00.352921: step 25878, loss 0.178591, acc 0.90625
2017-03-02T18:06:00.417148: step 25879, loss 0.108728, acc 0.9375
2017-03-02T18:06:00.488230: step 25880, loss 0.311596, acc 0.875
2017-03-02T18:06:00.555730: step 25881, loss 0.127931, acc 0.9375
2017-03-02T18:06:00.620459: step 25882, loss 0.31308, acc 0.84375
2017-03-02T18:06:00.684663: step 25883, loss 0.184965, acc 0.90625
2017-03-02T18:06:00.755967: step 25884, loss 0.160156, acc 0.90625
2017-03-02T18:06:00.821994: step 25885, loss 0.266779, acc 0.90625
2017-03-02T18:06:00.891997: step 25886, loss 0.103616, acc 0.953125
2017-03-02T18:06:00.959153: step 25887, loss 0.215772, acc 0.9375
2017-03-02T18:06:01.026241: step 25888, loss 0.0425173, acc 0.984375
2017-03-02T18:06:01.094129: step 25889, loss 0.11771, acc 0.953125
2017-03-02T18:06:01.162180: step 25890, loss 0.186895, acc 0.921875
2017-03-02T18:06:01.232990: step 25891, loss 0.176576, acc 0.90625
2017-03-02T18:06:01.333413: step 25892, loss 0.0547131, acc 0.96875
2017-03-02T18:06:01.407005: step 25893, loss 0.259657, acc 0.875
2017-03-02T18:06:01.477134: step 25894, loss 0.113614, acc 0.921875
2017-03-02T18:06:01.543545: step 25895, loss 0.168227, acc 0.921875
2017-03-02T18:06:01.611085: step 25896, loss 0.316322, acc 0.890625
2017-03-02T18:06:01.679543: step 25897, loss 0.186546, acc 0.921875
2017-03-02T18:06:01.746477: step 25898, loss 0.156098, acc 0.90625
2017-03-02T18:06:01.815381: step 25899, loss 0.1537, acc 0.90625
2017-03-02T18:06:01.885436: step 25900, loss 0.128197, acc 0.9375

Evaluation:
2017-03-02T18:06:01.914154: step 25900, loss 3.18341, acc 0.636626

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-25900

2017-03-02T18:06:02.347426: step 25901, loss 0.14816, acc 0.921875
2017-03-02T18:06:02.418593: step 25902, loss 0.206636, acc 0.890625
2017-03-02T18:06:02.493157: step 25903, loss 0.102771, acc 0.9375
2017-03-02T18:06:02.591966: step 25904, loss 0.1755, acc 0.90625
2017-03-02T18:06:02.649751: step 25905, loss 0.100779, acc 0.953125
2017-03-02T18:06:02.721527: step 25906, loss 0.154271, acc 0.953125
2017-03-02T18:06:02.788341: step 25907, loss 0.146431, acc 0.9375
2017-03-02T18:06:02.857045: step 25908, loss 0.136937, acc 0.9375
2017-03-02T18:06:02.928511: step 25909, loss 0.0857161, acc 0.953125
2017-03-02T18:06:02.994016: step 25910, loss 0.130921, acc 0.9375
2017-03-02T18:06:03.062430: step 25911, loss 0.135768, acc 0.921875
2017-03-02T18:06:03.129246: step 25912, loss 0.0536848, acc 0.96875
2017-03-02T18:06:03.198920: step 25913, loss 0.116355, acc 0.953125
2017-03-02T18:06:03.267292: step 25914, loss 0.113111, acc 0.9375
2017-03-02T18:06:03.340088: step 25915, loss 0.203868, acc 0.90625
2017-03-02T18:06:03.410575: step 25916, loss 0.075966, acc 0.984375
2017-03-02T18:06:03.480527: step 25917, loss 0.126849, acc 0.953125
2017-03-02T18:06:03.544758: step 25918, loss 0.134114, acc 0.9375
2017-03-02T18:06:03.609785: step 25919, loss 0.0549961, acc 0.984375
2017-03-02T18:06:03.676345: step 25920, loss 0.0409491, acc 0.96875
2017-03-02T18:06:03.740103: step 25921, loss 0.206517, acc 0.921875
2017-03-02T18:06:03.807983: step 25922, loss 0.226853, acc 0.90625
2017-03-02T18:06:03.881504: step 25923, loss 0.0777305, acc 0.9375
2017-03-02T18:06:03.951210: step 25924, loss 0.154324, acc 0.90625
2017-03-02T18:06:04.018129: step 25925, loss 0.17018, acc 0.90625
2017-03-02T18:06:04.087769: step 25926, loss 0.134237, acc 0.9375
2017-03-02T18:06:04.165484: step 25927, loss 0.0965291, acc 0.953125
2017-03-02T18:06:04.226955: step 25928, loss 0.117881, acc 0.953125
2017-03-02T18:06:04.299788: step 25929, loss 0.15775, acc 0.90625
2017-03-02T18:06:04.380630: step 25930, loss 0.151532, acc 0.9375
2017-03-02T18:06:04.447712: step 25931, loss 0.187125, acc 0.953125
2017-03-02T18:06:04.514121: step 25932, loss 0.258903, acc 0.90625
2017-03-02T18:06:04.580425: step 25933, loss 0.0644747, acc 0.96875
2017-03-02T18:06:04.649764: step 25934, loss 0.239487, acc 0.90625
2017-03-02T18:06:04.713866: step 25935, loss 0.313589, acc 0.890625
2017-03-02T18:06:04.778012: step 25936, loss 0.132115, acc 0.9375
2017-03-02T18:06:04.842610: step 25937, loss 0.202934, acc 0.90625
2017-03-02T18:06:04.914260: step 25938, loss 0.182098, acc 0.890625
2017-03-02T18:06:04.982663: step 25939, loss 0.163749, acc 0.921875
2017-03-02T18:06:05.047220: step 25940, loss 0.25019, acc 0.859375
2017-03-02T18:06:05.116468: step 25941, loss 0.120676, acc 0.921875
2017-03-02T18:06:05.186564: step 25942, loss 0.103981, acc 0.9375
2017-03-02T18:06:05.247495: step 25943, loss 0.0777994, acc 0.96875
2017-03-02T18:06:05.305448: step 25944, loss 0.127295, acc 0.96875
2017-03-02T18:06:05.378856: step 25945, loss 0.101392, acc 0.9375
2017-03-02T18:06:05.443065: step 25946, loss 0.122311, acc 0.9375
2017-03-02T18:06:05.512776: step 25947, loss 0.0627993, acc 0.96875
2017-03-02T18:06:05.580551: step 25948, loss 0.0763147, acc 0.96875
2017-03-02T18:06:05.647615: step 25949, loss 0.134495, acc 0.921875
2017-03-02T18:06:05.709061: step 25950, loss 0.13128, acc 0.953125
2017-03-02T18:06:05.776834: step 25951, loss 0.122719, acc 0.9375
2017-03-02T18:06:05.850998: step 25952, loss 0.203967, acc 0.890625
2017-03-02T18:06:05.915907: step 25953, loss 0.115604, acc 0.9375
2017-03-02T18:06:05.982403: step 25954, loss 0.331774, acc 0.84375
2017-03-02T18:06:06.048029: step 25955, loss 0.13911, acc 0.9375
2017-03-02T18:06:06.118310: step 25956, loss 0.0905706, acc 0.96875
2017-03-02T18:06:06.181685: step 25957, loss 0.0995875, acc 0.953125
2017-03-02T18:06:06.245703: step 25958, loss 0.150529, acc 0.9375
2017-03-02T18:06:06.309514: step 25959, loss 0.0100733, acc 1
2017-03-02T18:06:06.381894: step 25960, loss 0.154436, acc 0.9375
2017-03-02T18:06:06.447822: step 25961, loss 0.106893, acc 0.953125
2017-03-02T18:06:06.509814: step 25962, loss 0.136646, acc 0.921875
2017-03-02T18:06:06.572941: step 25963, loss 0.225935, acc 0.890625
2017-03-02T18:06:06.630331: step 25964, loss 0.109982, acc 0.953125
2017-03-02T18:06:06.698130: step 25965, loss 0.172355, acc 0.90625
2017-03-02T18:06:06.762937: step 25966, loss 0.244462, acc 0.890625
2017-03-02T18:06:06.833288: step 25967, loss 0.110526, acc 0.9375
2017-03-02T18:06:06.902257: step 25968, loss 0.133924, acc 0.9375
2017-03-02T18:06:06.969685: step 25969, loss 0.136044, acc 0.9375
2017-03-02T18:06:07.034183: step 25970, loss 0.0589187, acc 0.96875
2017-03-02T18:06:07.099902: step 25971, loss 0.127373, acc 0.921875
2017-03-02T18:06:07.166475: step 25972, loss 0.153821, acc 0.921875
2017-03-02T18:06:07.237124: step 25973, loss 0.136245, acc 0.9375
2017-03-02T18:06:07.304100: step 25974, loss 0.110734, acc 0.953125
2017-03-02T18:06:07.372501: step 25975, loss 0.250793, acc 0.90625
2017-03-02T18:06:07.442694: step 25976, loss 0.194577, acc 0.921875
2017-03-02T18:06:07.516740: step 25977, loss 0.115674, acc 0.96875
2017-03-02T18:06:07.583948: step 25978, loss 0.0624521, acc 0.96875
2017-03-02T18:06:07.655711: step 25979, loss 0.187005, acc 0.921875
2017-03-02T18:06:07.719137: step 25980, loss 0.109645, acc 0.9375
2017-03-02T18:06:07.782278: step 25981, loss 0.159394, acc 0.9375
2017-03-02T18:06:07.853048: step 25982, loss 0.211689, acc 0.9375
2017-03-02T18:06:07.923980: step 25983, loss 0.0723301, acc 0.984375
2017-03-02T18:06:07.995592: step 25984, loss 0.164636, acc 0.9375
2017-03-02T18:06:08.064209: step 25985, loss 0.299447, acc 0.875
2017-03-02T18:06:08.131352: step 25986, loss 0.163144, acc 0.890625
2017-03-02T18:06:08.200171: step 25987, loss 0.209768, acc 0.921875
2017-03-02T18:06:08.271381: step 25988, loss 0.199083, acc 0.90625
2017-03-02T18:06:08.337061: step 25989, loss 0.177659, acc 0.9375
2017-03-02T18:06:08.403548: step 25990, loss 0.088509, acc 0.96875
2017-03-02T18:06:08.474319: step 25991, loss 0.165543, acc 0.921875
2017-03-02T18:06:08.538485: step 25992, loss 0.103681, acc 0.953125
2017-03-02T18:06:08.602870: step 25993, loss 0.0813606, acc 0.953125
2017-03-02T18:06:08.672830: step 25994, loss 0.226509, acc 0.921875
2017-03-02T18:06:08.736975: step 25995, loss 0.126641, acc 0.953125
2017-03-02T18:06:08.808566: step 25996, loss 0.13007, acc 0.953125
2017-03-02T18:06:08.876797: step 25997, loss 0.139374, acc 0.9375
2017-03-02T18:06:08.946736: step 25998, loss 0.0928335, acc 0.953125
2017-03-02T18:06:09.007403: step 25999, loss 0.208506, acc 0.875
2017-03-02T18:06:09.077510: step 26000, loss 0.0499096, acc 0.984375

Evaluation:
2017-03-02T18:06:09.107584: step 26000, loss 3.22445, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26000

2017-03-02T18:06:09.536481: step 26001, loss 0.0650337, acc 0.984375
2017-03-02T18:06:09.603667: step 26002, loss 0.208942, acc 0.90625
2017-03-02T18:06:09.672842: step 26003, loss 0.156972, acc 0.96875
2017-03-02T18:06:09.746825: step 26004, loss 0.169587, acc 0.90625
2017-03-02T18:06:09.810203: step 26005, loss 0.153689, acc 0.921875
2017-03-02T18:06:09.872922: step 26006, loss 0.167881, acc 0.953125
2017-03-02T18:06:09.942217: step 26007, loss 0.151516, acc 0.921875
2017-03-02T18:06:10.010751: step 26008, loss 0.117112, acc 0.96875
2017-03-02T18:06:10.081017: step 26009, loss 0.0417294, acc 0.984375
2017-03-02T18:06:10.146462: step 26010, loss 0.079915, acc 0.96875
2017-03-02T18:06:10.223331: step 26011, loss 0.120471, acc 0.953125
2017-03-02T18:06:10.291378: step 26012, loss 0.153083, acc 0.9375
2017-03-02T18:06:10.361807: step 26013, loss 0.12906, acc 0.9375
2017-03-02T18:06:10.432420: step 26014, loss 0.123184, acc 0.9375
2017-03-02T18:06:10.492192: step 26015, loss 0.185591, acc 0.890625
2017-03-02T18:06:10.549871: step 26016, loss 0.14828, acc 0.90625
2017-03-02T18:06:10.615162: step 26017, loss 0.0977814, acc 0.953125
2017-03-02T18:06:10.681178: step 26018, loss 0.15657, acc 0.90625
2017-03-02T18:06:10.749150: step 26019, loss 0.147124, acc 0.921875
2017-03-02T18:06:10.818733: step 26020, loss 0.214523, acc 0.921875
2017-03-02T18:06:10.887652: step 26021, loss 0.248869, acc 0.859375
2017-03-02T18:06:10.958801: step 26022, loss 0.0364364, acc 0.984375
2017-03-02T18:06:11.027982: step 26023, loss 0.193065, acc 0.921875
2017-03-02T18:06:11.099204: step 26024, loss 0.168284, acc 0.90625
2017-03-02T18:06:11.166220: step 26025, loss 0.148188, acc 0.890625
2017-03-02T18:06:11.231100: step 26026, loss 0.0797895, acc 0.96875
2017-03-02T18:06:11.296219: step 26027, loss 0.100256, acc 0.96875
2017-03-02T18:06:11.366699: step 26028, loss 0.159311, acc 0.953125
2017-03-02T18:06:11.437436: step 26029, loss 0.310407, acc 0.859375
2017-03-02T18:06:11.502844: step 26030, loss 0.130149, acc 0.9375
2017-03-02T18:06:11.569550: step 26031, loss 0.284521, acc 0.921875
2017-03-02T18:06:11.634305: step 26032, loss 0.0450588, acc 1
2017-03-02T18:06:11.702964: step 26033, loss 0.115248, acc 0.921875
2017-03-02T18:06:11.770228: step 26034, loss 0.182789, acc 0.890625
2017-03-02T18:06:11.834710: step 26035, loss 0.117284, acc 0.921875
2017-03-02T18:06:11.903001: step 26036, loss 0.12749, acc 0.953125
2017-03-02T18:06:11.969982: step 26037, loss 0.220213, acc 0.90625
2017-03-02T18:06:12.039502: step 26038, loss 0.289544, acc 0.90625
2017-03-02T18:06:12.109096: step 26039, loss 0.131648, acc 0.921875
2017-03-02T18:06:12.175031: step 26040, loss 0.155537, acc 0.921875
2017-03-02T18:06:12.245333: step 26041, loss 0.172477, acc 0.921875
2017-03-02T18:06:12.316346: step 26042, loss 0.160189, acc 0.921875
2017-03-02T18:06:12.386604: step 26043, loss 0.121707, acc 0.9375
2017-03-02T18:06:12.449757: step 26044, loss 0.13367, acc 0.9375
2017-03-02T18:06:12.517170: step 26045, loss 0.0934652, acc 0.96875
2017-03-02T18:06:12.581211: step 26046, loss 0.117839, acc 0.953125
2017-03-02T18:06:12.652477: step 26047, loss 0.062133, acc 0.96875
2017-03-02T18:06:12.722023: step 26048, loss 0.255894, acc 0.875
2017-03-02T18:06:12.777293: step 26049, loss 0.0928712, acc 0.953125
2017-03-02T18:06:12.868665: step 26050, loss 0.153316, acc 0.890625
2017-03-02T18:06:12.935278: step 26051, loss 0.0897438, acc 0.984375
2017-03-02T18:06:13.002191: step 26052, loss 0.180247, acc 0.890625
2017-03-02T18:06:13.072741: step 26053, loss 0.128463, acc 0.9375
2017-03-02T18:06:13.137665: step 26054, loss 0.309967, acc 0.828125
2017-03-02T18:06:13.198653: step 26055, loss 0.169007, acc 0.953125
2017-03-02T18:06:13.265611: step 26056, loss 0.110599, acc 0.953125
2017-03-02T18:06:13.337345: step 26057, loss 0.0799581, acc 0.96875
2017-03-02T18:06:13.407301: step 26058, loss 0.0645681, acc 0.96875
2017-03-02T18:06:13.475745: step 26059, loss 0.170196, acc 0.953125
2017-03-02T18:06:13.546710: step 26060, loss 0.214247, acc 0.921875
2017-03-02T18:06:13.613852: step 26061, loss 0.200397, acc 0.921875
2017-03-02T18:06:13.682911: step 26062, loss 0.0969326, acc 0.96875
2017-03-02T18:06:13.751313: step 26063, loss 0.0464887, acc 0.984375
2017-03-02T18:06:13.815607: step 26064, loss 0.169763, acc 0.921875
2017-03-02T18:06:13.885541: step 26065, loss 0.115235, acc 0.953125
2017-03-02T18:06:13.951217: step 26066, loss 0.0836349, acc 0.96875
2017-03-02T18:06:14.020133: step 26067, loss 0.0886937, acc 0.9375
2017-03-02T18:06:14.083413: step 26068, loss 0.0326617, acc 1
2017-03-02T18:06:14.153045: step 26069, loss 0.132144, acc 0.9375
2017-03-02T18:06:14.219012: step 26070, loss 0.100594, acc 0.953125
2017-03-02T18:06:14.292886: step 26071, loss 0.193723, acc 0.921875
2017-03-02T18:06:14.358957: step 26072, loss 0.178817, acc 0.9375
2017-03-02T18:06:14.428440: step 26073, loss 0.137177, acc 0.96875
2017-03-02T18:06:14.502813: step 26074, loss 0.276567, acc 0.890625
2017-03-02T18:06:14.570347: step 26075, loss 0.171405, acc 0.921875
2017-03-02T18:06:14.636410: step 26076, loss 0.0614122, acc 0.984375
2017-03-02T18:06:14.699898: step 26077, loss 0.180986, acc 0.953125
2017-03-02T18:06:14.769425: step 26078, loss 0.130453, acc 0.9375
2017-03-02T18:06:14.836138: step 26079, loss 0.17379, acc 0.890625
2017-03-02T18:06:14.904972: step 26080, loss 0.176003, acc 0.9375
2017-03-02T18:06:14.969130: step 26081, loss 0.119645, acc 0.96875
2017-03-02T18:06:15.036856: step 26082, loss 0.0956471, acc 0.953125
2017-03-02T18:06:15.104629: step 26083, loss 0.177313, acc 0.921875
2017-03-02T18:06:15.169914: step 26084, loss 0.221191, acc 0.890625
2017-03-02T18:06:15.234677: step 26085, loss 0.132769, acc 0.9375
2017-03-02T18:06:15.305312: step 26086, loss 0.116333, acc 0.96875
2017-03-02T18:06:15.373598: step 26087, loss 0.183531, acc 0.9375
2017-03-02T18:06:15.432076: step 26088, loss 0.137817, acc 0.953125
2017-03-02T18:06:15.497299: step 26089, loss 0.134722, acc 0.9375
2017-03-02T18:06:15.559822: step 26090, loss 0.0575134, acc 0.96875
2017-03-02T18:06:15.641287: step 26091, loss 0.210392, acc 0.9375
2017-03-02T18:06:15.703722: step 26092, loss 0.0448975, acc 0.984375
2017-03-02T18:06:15.766782: step 26093, loss 0.0817207, acc 0.953125
2017-03-02T18:06:15.828686: step 26094, loss 0.147476, acc 0.921875
2017-03-02T18:06:15.894240: step 26095, loss 0.098501, acc 0.953125
2017-03-02T18:06:15.964564: step 26096, loss 0.0754016, acc 0.953125
2017-03-02T18:06:16.034050: step 26097, loss 0.306178, acc 0.84375
2017-03-02T18:06:16.100867: step 26098, loss 0.161636, acc 0.90625
2017-03-02T18:06:16.168012: step 26099, loss 0.234568, acc 0.890625
2017-03-02T18:06:16.237042: step 26100, loss 0.179784, acc 0.90625

Evaluation:
2017-03-02T18:06:16.262468: step 26100, loss 3.29403, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26100

2017-03-02T18:06:16.692538: step 26101, loss 0.109843, acc 0.9375
2017-03-02T18:06:16.758457: step 26102, loss 0.106713, acc 0.96875
2017-03-02T18:06:16.823843: step 26103, loss 0.160411, acc 0.9375
2017-03-02T18:06:16.887886: step 26104, loss 0.193025, acc 0.921875
2017-03-02T18:06:16.956053: step 26105, loss 0.199494, acc 0.890625
2017-03-02T18:06:17.021708: step 26106, loss 0.118556, acc 0.96875
2017-03-02T18:06:17.091035: step 26107, loss 0.0999236, acc 0.953125
2017-03-02T18:06:17.157579: step 26108, loss 0.150361, acc 0.9375
2017-03-02T18:06:17.224821: step 26109, loss 0.204368, acc 0.890625
2017-03-02T18:06:17.289756: step 26110, loss 0.0555156, acc 0.96875
2017-03-02T18:06:17.357758: step 26111, loss 0.156105, acc 0.921875
2017-03-02T18:06:17.429907: step 26112, loss 0.0951759, acc 0.953125
2017-03-02T18:06:17.499819: step 26113, loss 0.158273, acc 0.90625
2017-03-02T18:06:17.571272: step 26114, loss 0.138058, acc 0.953125
2017-03-02T18:06:17.641091: step 26115, loss 0.124006, acc 0.9375
2017-03-02T18:06:17.710729: step 26116, loss 0.181477, acc 0.921875
2017-03-02T18:06:17.778736: step 26117, loss 0.207801, acc 0.921875
2017-03-02T18:06:17.848038: step 26118, loss 0.0901577, acc 0.953125
2017-03-02T18:06:17.919752: step 26119, loss 0.140741, acc 0.9375
2017-03-02T18:06:17.992893: step 26120, loss 0.217281, acc 0.859375
2017-03-02T18:06:18.064368: step 26121, loss 0.097416, acc 0.953125
2017-03-02T18:06:18.136456: step 26122, loss 0.131691, acc 0.9375
2017-03-02T18:06:18.206737: step 26123, loss 0.136849, acc 0.953125
2017-03-02T18:06:18.272144: step 26124, loss 0.25179, acc 0.875
2017-03-02T18:06:18.337713: step 26125, loss 0.107104, acc 0.9375
2017-03-02T18:06:18.408869: step 26126, loss 0.164822, acc 0.953125
2017-03-02T18:06:18.468902: step 26127, loss 0.0883583, acc 0.953125
2017-03-02T18:06:18.538798: step 26128, loss 0.221805, acc 0.90625
2017-03-02T18:06:18.605166: step 26129, loss 0.104025, acc 0.984375
2017-03-02T18:06:18.664902: step 26130, loss 0.0961141, acc 0.96875
2017-03-02T18:06:18.732872: step 26131, loss 0.137151, acc 0.9375
2017-03-02T18:06:18.799237: step 26132, loss 0.0725608, acc 0.984375
2017-03-02T18:06:18.868097: step 26133, loss 0.170206, acc 0.921875
2017-03-02T18:06:18.935805: step 26134, loss 0.0977002, acc 0.953125
2017-03-02T18:06:19.005141: step 26135, loss 0.134321, acc 0.953125
2017-03-02T18:06:19.072580: step 26136, loss 0.180183, acc 0.9375
2017-03-02T18:06:19.144957: step 26137, loss 0.149417, acc 0.890625
2017-03-02T18:06:19.214967: step 26138, loss 0.185287, acc 0.921875
2017-03-02T18:06:19.274132: step 26139, loss 0.216321, acc 0.890625
2017-03-02T18:06:19.338480: step 26140, loss 0.17402, acc 0.90625
2017-03-02T18:06:19.406338: step 26141, loss 0.160373, acc 0.90625
2017-03-02T18:06:19.471825: step 26142, loss 0.236833, acc 0.921875
2017-03-02T18:06:19.538300: step 26143, loss 0.181267, acc 0.90625
2017-03-02T18:06:19.598584: step 26144, loss 0.143089, acc 0.953125
2017-03-02T18:06:19.669606: step 26145, loss 0.18658, acc 0.9375
2017-03-02T18:06:19.730743: step 26146, loss 0.230277, acc 0.921875
2017-03-02T18:06:19.800562: step 26147, loss 0.0702685, acc 0.96875
2017-03-02T18:06:19.863679: step 26148, loss 0.152723, acc 0.921875
2017-03-02T18:06:19.933278: step 26149, loss 0.145348, acc 0.921875
2017-03-02T18:06:20.014866: step 26150, loss 0.124445, acc 0.921875
2017-03-02T18:06:20.077577: step 26151, loss 0.203063, acc 0.921875
2017-03-02T18:06:20.142715: step 26152, loss 0.1542, acc 0.921875
2017-03-02T18:06:20.208870: step 26153, loss 0.169183, acc 0.921875
2017-03-02T18:06:20.274491: step 26154, loss 0.102175, acc 0.953125
2017-03-02T18:06:20.341165: step 26155, loss 0.169367, acc 0.921875
2017-03-02T18:06:20.407489: step 26156, loss 0.15541, acc 0.90625
2017-03-02T18:06:20.470810: step 26157, loss 0.0786008, acc 0.96875
2017-03-02T18:06:20.563197: step 26158, loss 0.0928471, acc 0.9375
2017-03-02T18:06:20.632730: step 26159, loss 0.111815, acc 0.953125
2017-03-02T18:06:20.703819: step 26160, loss 0.140586, acc 0.953125
2017-03-02T18:06:20.776351: step 26161, loss 0.286445, acc 0.875
2017-03-02T18:06:20.854377: step 26162, loss 0.0951226, acc 0.9375
2017-03-02T18:06:20.920157: step 26163, loss 0.0653575, acc 0.984375
2017-03-02T18:06:20.980888: step 26164, loss 0.159842, acc 0.953125
2017-03-02T18:06:21.050657: step 26165, loss 0.0846866, acc 0.96875
2017-03-02T18:06:21.116524: step 26166, loss 0.0800642, acc 0.984375
2017-03-02T18:06:21.175750: step 26167, loss 0.199598, acc 0.921875
2017-03-02T18:06:21.241638: step 26168, loss 0.0473382, acc 1
2017-03-02T18:06:21.306232: step 26169, loss 0.336514, acc 0.875
2017-03-02T18:06:21.383517: step 26170, loss 0.195417, acc 0.921875
2017-03-02T18:06:21.449107: step 26171, loss 0.0864776, acc 0.96875
2017-03-02T18:06:21.513820: step 26172, loss 0.1261, acc 0.953125
2017-03-02T18:06:21.579070: step 26173, loss 0.144196, acc 0.9375
2017-03-02T18:06:21.634903: step 26174, loss 0.119108, acc 0.921875
2017-03-02T18:06:21.701548: step 26175, loss 0.149444, acc 0.953125
2017-03-02T18:06:21.765918: step 26176, loss 0.153392, acc 0.921875
2017-03-02T18:06:21.830890: step 26177, loss 0.13447, acc 0.9375
2017-03-02T18:06:21.896768: step 26178, loss 0.0983838, acc 0.953125
2017-03-02T18:06:21.963875: step 26179, loss 0.166549, acc 0.953125
2017-03-02T18:06:22.031804: step 26180, loss 0.119874, acc 0.953125
2017-03-02T18:06:22.100092: step 26181, loss 0.13923, acc 0.9375
2017-03-02T18:06:22.167801: step 26182, loss 0.0857818, acc 0.953125
2017-03-02T18:06:22.239243: step 26183, loss 0.0815638, acc 0.96875
2017-03-02T18:06:22.310338: step 26184, loss 0.143031, acc 0.9375
2017-03-02T18:06:22.378750: step 26185, loss 0.101283, acc 0.953125
2017-03-02T18:06:22.445491: step 26186, loss 0.0679297, acc 0.96875
2017-03-02T18:06:22.506147: step 26187, loss 0.246258, acc 0.890625
2017-03-02T18:06:22.572053: step 26188, loss 0.191772, acc 0.921875
2017-03-02T18:06:22.638201: step 26189, loss 0.114079, acc 0.953125
2017-03-02T18:06:22.706764: step 26190, loss 0.0738363, acc 0.96875
2017-03-02T18:06:22.775391: step 26191, loss 0.178454, acc 0.9375
2017-03-02T18:06:22.842610: step 26192, loss 0.106478, acc 0.953125
2017-03-02T18:06:22.912765: step 26193, loss 0.189896, acc 0.921875
2017-03-02T18:06:22.982721: step 26194, loss 0.196456, acc 0.90625
2017-03-02T18:06:23.052202: step 26195, loss 0.157358, acc 0.9375
2017-03-02T18:06:23.128341: step 26196, loss 0.0434517, acc 0.984375
2017-03-02T18:06:23.199950: step 26197, loss 0.0950256, acc 0.953125
2017-03-02T18:06:23.267271: step 26198, loss 0.173737, acc 0.9375
2017-03-02T18:06:23.333367: step 26199, loss 0.107342, acc 0.953125
2017-03-02T18:06:23.402134: step 26200, loss 0.106986, acc 0.953125

Evaluation:
2017-03-02T18:06:23.431501: step 26200, loss 3.26854, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26200

2017-03-02T18:06:23.874436: step 26201, loss 0.0974348, acc 0.984375
2017-03-02T18:06:23.945544: step 26202, loss 0.0913441, acc 0.953125
2017-03-02T18:06:24.016649: step 26203, loss 0.118491, acc 0.921875
2017-03-02T18:06:24.092667: step 26204, loss 0.0933012, acc 0.96875
2017-03-02T18:06:24.160784: step 26205, loss 0.251923, acc 0.859375
2017-03-02T18:06:24.232293: step 26206, loss 0.0662472, acc 0.96875
2017-03-02T18:06:24.302888: step 26207, loss 0.112076, acc 0.9375
2017-03-02T18:06:24.379208: step 26208, loss 0.0681503, acc 0.96875
2017-03-02T18:06:24.447742: step 26209, loss 0.0747584, acc 0.953125
2017-03-02T18:06:24.519472: step 26210, loss 0.169783, acc 0.9375
2017-03-02T18:06:24.590241: step 26211, loss 0.160891, acc 0.90625
2017-03-02T18:06:24.654650: step 26212, loss 0.13355, acc 0.953125
2017-03-02T18:06:24.724620: step 26213, loss 0.181832, acc 0.921875
2017-03-02T18:06:24.794020: step 26214, loss 0.237871, acc 0.890625
2017-03-02T18:06:24.864065: step 26215, loss 0.106719, acc 0.9375
2017-03-02T18:06:24.932117: step 26216, loss 0.0830256, acc 0.96875
2017-03-02T18:06:24.998026: step 26217, loss 0.103983, acc 0.953125
2017-03-02T18:06:25.063173: step 26218, loss 0.117764, acc 0.9375
2017-03-02T18:06:25.132106: step 26219, loss 0.179611, acc 0.921875
2017-03-02T18:06:25.204225: step 26220, loss 0.0932263, acc 0.953125
2017-03-02T18:06:25.271761: step 26221, loss 0.122565, acc 0.9375
2017-03-02T18:06:25.335009: step 26222, loss 0.0824142, acc 0.953125
2017-03-02T18:06:25.404400: step 26223, loss 0.164234, acc 0.890625
2017-03-02T18:06:25.474460: step 26224, loss 0.250728, acc 0.890625
2017-03-02T18:06:25.550913: step 26225, loss 0.146769, acc 0.921875
2017-03-02T18:06:25.617278: step 26226, loss 0.315535, acc 0.875
2017-03-02T18:06:25.692930: step 26227, loss 0.279711, acc 0.953125
2017-03-02T18:06:25.760622: step 26228, loss 0.242752, acc 0.875
2017-03-02T18:06:25.827619: step 26229, loss 0.217283, acc 0.90625
2017-03-02T18:06:25.893610: step 26230, loss 0.138609, acc 0.9375
2017-03-02T18:06:25.961232: step 26231, loss 0.105209, acc 0.953125
2017-03-02T18:06:26.031960: step 26232, loss 0.194009, acc 0.9375
2017-03-02T18:06:26.097490: step 26233, loss 0.159317, acc 0.90625
2017-03-02T18:06:26.166233: step 26234, loss 0.197214, acc 0.890625
2017-03-02T18:06:26.237430: step 26235, loss 0.137462, acc 0.9375
2017-03-02T18:06:26.307949: step 26236, loss 0.12968, acc 0.921875
2017-03-02T18:06:26.378455: step 26237, loss 0.244203, acc 0.890625
2017-03-02T18:06:26.452680: step 26238, loss 0.0793691, acc 0.96875
2017-03-02T18:06:26.523126: step 26239, loss 0.143471, acc 0.890625
2017-03-02T18:06:26.594292: step 26240, loss 0.256513, acc 0.890625
2017-03-02T18:06:26.660215: step 26241, loss 0.0757608, acc 0.984375
2017-03-02T18:06:26.725520: step 26242, loss 0.070489, acc 0.984375
2017-03-02T18:06:26.794030: step 26243, loss 0.106882, acc 0.9375
2017-03-02T18:06:26.870001: step 26244, loss 0.092346, acc 0.953125
2017-03-02T18:06:26.938952: step 26245, loss 0.141359, acc 0.90625
2017-03-02T18:06:27.006906: step 26246, loss 0.16913, acc 0.921875
2017-03-02T18:06:27.080407: step 26247, loss 0.207695, acc 0.90625
2017-03-02T18:06:27.148087: step 26248, loss 0.135095, acc 0.921875
2017-03-02T18:06:27.216856: step 26249, loss 0.183883, acc 0.953125
2017-03-02T18:06:27.285754: step 26250, loss 0.214991, acc 0.921875
2017-03-02T18:06:27.366266: step 26251, loss 0.161279, acc 0.921875
2017-03-02T18:06:27.437595: step 26252, loss 0.1424, acc 0.90625
2017-03-02T18:06:27.508729: step 26253, loss 0.196306, acc 0.921875
2017-03-02T18:06:27.577783: step 26254, loss 0.250252, acc 0.890625
2017-03-02T18:06:27.651711: step 26255, loss 0.118209, acc 0.921875
2017-03-02T18:06:27.719686: step 26256, loss 0.247322, acc 0.890625
2017-03-02T18:06:27.784559: step 26257, loss 0.19144, acc 0.953125
2017-03-02T18:06:27.854371: step 26258, loss 0.077283, acc 0.984375
2017-03-02T18:06:27.927493: step 26259, loss 0.117911, acc 0.9375
2017-03-02T18:06:27.991337: step 26260, loss 0.157792, acc 0.9375
2017-03-02T18:06:28.060163: step 26261, loss 0.234077, acc 0.875
2017-03-02T18:06:28.129832: step 26262, loss 0.120541, acc 0.921875
2017-03-02T18:06:28.201806: step 26263, loss 0.15932, acc 0.90625
2017-03-02T18:06:28.259168: step 26264, loss 0.131862, acc 1
2017-03-02T18:06:28.332439: step 26265, loss 0.144522, acc 0.921875
2017-03-02T18:06:28.398481: step 26266, loss 0.167547, acc 0.890625
2017-03-02T18:06:28.467455: step 26267, loss 0.0992312, acc 0.96875
2017-03-02T18:06:28.544643: step 26268, loss 0.238441, acc 0.875
2017-03-02T18:06:28.606828: step 26269, loss 0.156723, acc 0.9375
2017-03-02T18:06:28.688371: step 26270, loss 0.0729893, acc 0.96875
2017-03-02T18:06:28.758376: step 26271, loss 0.0710031, acc 0.984375
2017-03-02T18:06:28.827545: step 26272, loss 0.151586, acc 0.890625
2017-03-02T18:06:28.893807: step 26273, loss 0.186261, acc 0.90625
2017-03-02T18:06:28.960164: step 26274, loss 0.0730873, acc 0.953125
2017-03-02T18:06:29.026546: step 26275, loss 0.148791, acc 0.9375
2017-03-02T18:06:29.092544: step 26276, loss 0.0609448, acc 0.984375
2017-03-02T18:06:29.161768: step 26277, loss 0.145048, acc 0.921875
2017-03-02T18:06:29.234284: step 26278, loss 0.172335, acc 0.9375
2017-03-02T18:06:29.298945: step 26279, loss 0.17238, acc 0.90625
2017-03-02T18:06:29.368121: step 26280, loss 0.241256, acc 0.890625
2017-03-02T18:06:29.437264: step 26281, loss 0.126808, acc 0.96875
2017-03-02T18:06:29.512317: step 26282, loss 0.157048, acc 0.921875
2017-03-02T18:06:29.580299: step 26283, loss 0.105689, acc 0.984375
2017-03-02T18:06:29.645493: step 26284, loss 0.0465741, acc 0.984375
2017-03-02T18:06:29.718967: step 26285, loss 0.143972, acc 0.921875
2017-03-02T18:06:29.784848: step 26286, loss 0.213457, acc 0.90625
2017-03-02T18:06:29.850536: step 26287, loss 0.0740976, acc 0.96875
2017-03-02T18:06:29.919695: step 26288, loss 0.286282, acc 0.875
2017-03-02T18:06:29.990728: step 26289, loss 0.140184, acc 0.90625
2017-03-02T18:06:30.063308: step 26290, loss 0.161993, acc 0.9375
2017-03-02T18:06:30.143649: step 26291, loss 0.117463, acc 0.921875
2017-03-02T18:06:30.210104: step 26292, loss 0.126855, acc 0.953125
2017-03-02T18:06:30.280155: step 26293, loss 0.0971198, acc 0.9375
2017-03-02T18:06:30.348570: step 26294, loss 0.24511, acc 0.9375
2017-03-02T18:06:30.415766: step 26295, loss 0.129304, acc 0.953125
2017-03-02T18:06:30.487743: step 26296, loss 0.140556, acc 0.9375
2017-03-02T18:06:30.554892: step 26297, loss 0.197416, acc 0.921875
2017-03-02T18:06:30.624877: step 26298, loss 0.198703, acc 0.921875
2017-03-02T18:06:30.690977: step 26299, loss 0.143646, acc 0.984375
2017-03-02T18:06:30.760430: step 26300, loss 0.281057, acc 0.890625

Evaluation:
2017-03-02T18:06:30.790299: step 26300, loss 3.35459, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26300

2017-03-02T18:06:32.895404: step 26301, loss 0.188506, acc 0.890625
2017-03-02T18:06:32.967620: step 26302, loss 0.128353, acc 0.921875
2017-03-02T18:06:33.038996: step 26303, loss 0.300644, acc 0.890625
2017-03-02T18:06:33.137846: step 26304, loss 0.175753, acc 0.953125
2017-03-02T18:06:33.209067: step 26305, loss 0.145885, acc 0.9375
2017-03-02T18:06:33.277820: step 26306, loss 0.216074, acc 0.859375
2017-03-02T18:06:33.346173: step 26307, loss 0.115687, acc 0.96875
2017-03-02T18:06:33.415316: step 26308, loss 0.0288962, acc 1
2017-03-02T18:06:33.482043: step 26309, loss 0.182374, acc 0.90625
2017-03-02T18:06:33.552871: step 26310, loss 0.168817, acc 0.859375
2017-03-02T18:06:33.617578: step 26311, loss 0.20536, acc 0.90625
2017-03-02T18:06:33.684554: step 26312, loss 0.133064, acc 0.953125
2017-03-02T18:06:33.753148: step 26313, loss 0.0789768, acc 0.96875
2017-03-02T18:06:33.819557: step 26314, loss 0.0902872, acc 0.96875
2017-03-02T18:06:33.887773: step 26315, loss 0.145221, acc 0.953125
2017-03-02T18:06:33.956809: step 26316, loss 0.124555, acc 0.921875
2017-03-02T18:06:34.026079: step 26317, loss 0.120218, acc 0.953125
2017-03-02T18:06:34.095074: step 26318, loss 0.124435, acc 0.9375
2017-03-02T18:06:34.162856: step 26319, loss 0.0733029, acc 0.953125
2017-03-02T18:06:34.229878: step 26320, loss 0.161666, acc 0.90625
2017-03-02T18:06:34.298799: step 26321, loss 0.110167, acc 0.9375
2017-03-02T18:06:34.368750: step 26322, loss 0.13048, acc 0.9375
2017-03-02T18:06:34.423741: step 26323, loss 0.106863, acc 0.9375
2017-03-02T18:06:34.490977: step 26324, loss 0.193262, acc 0.9375
2017-03-02T18:06:34.562281: step 26325, loss 0.128606, acc 0.9375
2017-03-02T18:06:34.625275: step 26326, loss 0.32304, acc 0.84375
2017-03-02T18:06:34.694294: step 26327, loss 0.103805, acc 0.953125
2017-03-02T18:06:34.759253: step 26328, loss 0.157357, acc 0.9375
2017-03-02T18:06:34.823411: step 26329, loss 0.23451, acc 0.875
2017-03-02T18:06:34.891719: step 26330, loss 0.12442, acc 0.9375
2017-03-02T18:06:34.960294: step 26331, loss 0.105272, acc 0.953125
2017-03-02T18:06:35.029618: step 26332, loss 0.140016, acc 0.9375
2017-03-02T18:06:35.095211: step 26333, loss 0.149, acc 0.953125
2017-03-02T18:06:35.161756: step 26334, loss 0.150893, acc 0.921875
2017-03-02T18:06:35.226743: step 26335, loss 0.0598947, acc 0.96875
2017-03-02T18:06:35.295450: step 26336, loss 0.223191, acc 0.921875
2017-03-02T18:06:35.365414: step 26337, loss 0.064103, acc 0.984375
2017-03-02T18:06:35.431437: step 26338, loss 0.119006, acc 0.9375
2017-03-02T18:06:35.498644: step 26339, loss 0.253625, acc 0.921875
2017-03-02T18:06:35.563705: step 26340, loss 0.188511, acc 0.9375
2017-03-02T18:06:35.632226: step 26341, loss 0.0801719, acc 0.953125
2017-03-02T18:06:35.698568: step 26342, loss 0.0632141, acc 0.953125
2017-03-02T18:06:35.765695: step 26343, loss 0.226561, acc 0.890625
2017-03-02T18:06:35.836661: step 26344, loss 0.23021, acc 0.890625
2017-03-02T18:06:35.901394: step 26345, loss 0.174334, acc 0.921875
2017-03-02T18:06:35.968613: step 26346, loss 0.2184, acc 0.9375
2017-03-02T18:06:36.038460: step 26347, loss 0.181215, acc 0.921875
2017-03-02T18:06:36.107083: step 26348, loss 0.176111, acc 0.90625
2017-03-02T18:06:36.174255: step 26349, loss 0.187339, acc 0.90625
2017-03-02T18:06:36.244236: step 26350, loss 0.216461, acc 0.90625
2017-03-02T18:06:36.316682: step 26351, loss 0.0861438, acc 0.953125
2017-03-02T18:06:36.387559: step 26352, loss 0.180154, acc 0.90625
2017-03-02T18:06:36.453858: step 26353, loss 0.0542105, acc 0.984375
2017-03-02T18:06:36.524555: step 26354, loss 0.187302, acc 0.953125
2017-03-02T18:06:36.591558: step 26355, loss 0.15991, acc 0.890625
2017-03-02T18:06:36.655962: step 26356, loss 0.201299, acc 0.90625
2017-03-02T18:06:36.725297: step 26357, loss 0.179625, acc 0.90625
2017-03-02T18:06:36.798437: step 26358, loss 0.0810172, acc 0.96875
2017-03-02T18:06:36.864227: step 26359, loss 0.103418, acc 0.953125
2017-03-02T18:06:36.930328: step 26360, loss 0.15674, acc 0.953125
2017-03-02T18:06:36.999698: step 26361, loss 0.0870131, acc 0.953125
2017-03-02T18:06:37.070227: step 26362, loss 0.0949886, acc 0.953125
2017-03-02T18:06:37.138897: step 26363, loss 0.164796, acc 0.921875
2017-03-02T18:06:37.203432: step 26364, loss 0.25901, acc 0.890625
2017-03-02T18:06:37.274919: step 26365, loss 0.0722705, acc 0.96875
2017-03-02T18:06:37.345734: step 26366, loss 0.0986171, acc 0.96875
2017-03-02T18:06:37.419521: step 26367, loss 0.136535, acc 0.953125
2017-03-02T18:06:37.485881: step 26368, loss 0.261789, acc 0.859375
2017-03-02T18:06:37.551618: step 26369, loss 0.060417, acc 0.96875
2017-03-02T18:06:37.622816: step 26370, loss 0.0574324, acc 0.96875
2017-03-02T18:06:37.697305: step 26371, loss 0.234616, acc 0.921875
2017-03-02T18:06:37.770807: step 26372, loss 0.0654714, acc 0.984375
2017-03-02T18:06:37.841180: step 26373, loss 0.129851, acc 0.9375
2017-03-02T18:06:37.910059: step 26374, loss 0.134043, acc 0.953125
2017-03-02T18:06:37.976894: step 26375, loss 0.12499, acc 0.953125
2017-03-02T18:06:38.043657: step 26376, loss 0.210517, acc 0.90625
2017-03-02T18:06:38.112887: step 26377, loss 0.216503, acc 0.890625
2017-03-02T18:06:38.178478: step 26378, loss 0.153998, acc 0.921875
2017-03-02T18:06:38.246595: step 26379, loss 0.109926, acc 0.9375
2017-03-02T18:06:38.317777: step 26380, loss 0.243128, acc 0.875
2017-03-02T18:06:38.382959: step 26381, loss 0.131896, acc 0.9375
2017-03-02T18:06:38.454217: step 26382, loss 0.112691, acc 0.953125
2017-03-02T18:06:38.521564: step 26383, loss 0.0844361, acc 0.984375
2017-03-02T18:06:38.592131: step 26384, loss 0.135441, acc 0.953125
2017-03-02T18:06:38.660971: step 26385, loss 0.0456453, acc 0.984375
2017-03-02T18:06:38.728315: step 26386, loss 0.249617, acc 0.90625
2017-03-02T18:06:38.797505: step 26387, loss 0.107011, acc 0.921875
2017-03-02T18:06:38.864380: step 26388, loss 0.041636, acc 1
2017-03-02T18:06:38.931001: step 26389, loss 0.0517694, acc 0.96875
2017-03-02T18:06:39.003121: step 26390, loss 0.18531, acc 0.921875
2017-03-02T18:06:39.072166: step 26391, loss 0.112688, acc 0.953125
2017-03-02T18:06:39.138517: step 26392, loss 0.195071, acc 0.90625
2017-03-02T18:06:39.208867: step 26393, loss 0.0982789, acc 0.96875
2017-03-02T18:06:39.279805: step 26394, loss 0.151609, acc 0.9375
2017-03-02T18:06:39.350072: step 26395, loss 0.166397, acc 0.890625
2017-03-02T18:06:39.421973: step 26396, loss 0.1542, acc 0.921875
2017-03-02T18:06:39.485418: step 26397, loss 0.30756, acc 0.859375
2017-03-02T18:06:39.550642: step 26398, loss 0.072172, acc 0.984375
2017-03-02T18:06:39.617725: step 26399, loss 0.240014, acc 0.921875
2017-03-02T18:06:39.684946: step 26400, loss 0.147616, acc 0.921875

Evaluation:
2017-03-02T18:06:39.710515: step 26400, loss 3.31747, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26400

2017-03-02T18:06:40.193474: step 26401, loss 0.0583738, acc 1
2017-03-02T18:06:40.263341: step 26402, loss 0.120183, acc 0.9375
2017-03-02T18:06:40.328754: step 26403, loss 0.120989, acc 0.9375
2017-03-02T18:06:40.400216: step 26404, loss 0.0772369, acc 0.984375
2017-03-02T18:06:40.479898: step 26405, loss 0.0759241, acc 0.984375
2017-03-02T18:06:40.550235: step 26406, loss 0.163494, acc 0.921875
2017-03-02T18:06:40.617838: step 26407, loss 0.170856, acc 0.890625
2017-03-02T18:06:40.686507: step 26408, loss 0.0593485, acc 0.984375
2017-03-02T18:06:40.754448: step 26409, loss 0.0783058, acc 0.953125
2017-03-02T18:06:40.826416: step 26410, loss 0.130999, acc 0.953125
2017-03-02T18:06:40.894376: step 26411, loss 0.118534, acc 0.9375
2017-03-02T18:06:40.962836: step 26412, loss 0.0584798, acc 0.96875
2017-03-02T18:06:41.032807: step 26413, loss 0.250354, acc 0.859375
2017-03-02T18:06:41.108332: step 26414, loss 0.055426, acc 0.984375
2017-03-02T18:06:41.176095: step 26415, loss 0.164965, acc 0.9375
2017-03-02T18:06:41.247892: step 26416, loss 0.288932, acc 0.875
2017-03-02T18:06:41.319511: step 26417, loss 0.11661, acc 0.9375
2017-03-02T18:06:41.392548: step 26418, loss 0.199447, acc 0.953125
2017-03-02T18:06:41.461586: step 26419, loss 0.182554, acc 0.90625
2017-03-02T18:06:41.529294: step 26420, loss 0.189907, acc 0.9375
2017-03-02T18:06:41.600741: step 26421, loss 0.150957, acc 0.921875
2017-03-02T18:06:41.672979: step 26422, loss 0.0920122, acc 0.953125
2017-03-02T18:06:41.742993: step 26423, loss 0.124118, acc 0.9375
2017-03-02T18:06:41.813203: step 26424, loss 0.0940762, acc 0.96875
2017-03-02T18:06:41.879302: step 26425, loss 0.102574, acc 0.96875
2017-03-02T18:06:41.945568: step 26426, loss 0.152036, acc 0.90625
2017-03-02T18:06:42.013697: step 26427, loss 0.211788, acc 0.890625
2017-03-02T18:06:42.077690: step 26428, loss 0.308217, acc 0.859375
2017-03-02T18:06:42.143368: step 26429, loss 0.290929, acc 0.875
2017-03-02T18:06:42.210384: step 26430, loss 0.107246, acc 0.953125
2017-03-02T18:06:42.281576: step 26431, loss 0.192925, acc 0.921875
2017-03-02T18:06:42.351462: step 26432, loss 0.158738, acc 0.921875
2017-03-02T18:06:42.426498: step 26433, loss 0.117672, acc 0.9375
2017-03-02T18:06:42.493678: step 26434, loss 0.224118, acc 0.875
2017-03-02T18:06:42.562404: step 26435, loss 0.0992603, acc 0.96875
2017-03-02T18:06:42.627866: step 26436, loss 0.131434, acc 0.90625
2017-03-02T18:06:42.696405: step 26437, loss 0.0345655, acc 0.984375
2017-03-02T18:06:42.764122: step 26438, loss 0.192316, acc 0.90625
2017-03-02T18:06:42.832012: step 26439, loss 0.0537404, acc 0.984375
2017-03-02T18:06:42.904239: step 26440, loss 0.173469, acc 0.953125
2017-03-02T18:06:42.970046: step 26441, loss 0.148304, acc 0.921875
2017-03-02T18:06:43.034356: step 26442, loss 0.111978, acc 0.9375
2017-03-02T18:06:43.105204: step 26443, loss 0.180596, acc 0.921875
2017-03-02T18:06:43.177668: step 26444, loss 0.111787, acc 0.96875
2017-03-02T18:06:43.241750: step 26445, loss 0.124375, acc 0.9375
2017-03-02T18:06:43.313308: step 26446, loss 0.179543, acc 0.9375
2017-03-02T18:06:43.383455: step 26447, loss 0.227536, acc 0.90625
2017-03-02T18:06:43.455480: step 26448, loss 0.137133, acc 0.984375
2017-03-02T18:06:43.529661: step 26449, loss 0.145977, acc 0.953125
2017-03-02T18:06:43.596805: step 26450, loss 0.0918178, acc 0.953125
2017-03-02T18:06:43.664101: step 26451, loss 0.106518, acc 0.9375
2017-03-02T18:06:43.736711: step 26452, loss 0.238608, acc 0.859375
2017-03-02T18:06:43.803630: step 26453, loss 0.141961, acc 0.921875
2017-03-02T18:06:43.871135: step 26454, loss 0.0559433, acc 0.96875
2017-03-02T18:06:43.940108: step 26455, loss 0.246883, acc 0.890625
2017-03-02T18:06:44.007955: step 26456, loss 0.099732, acc 0.953125
2017-03-02T18:06:44.077281: step 26457, loss 0.288726, acc 0.875
2017-03-02T18:06:44.149487: step 26458, loss 0.202139, acc 0.921875
2017-03-02T18:06:44.223189: step 26459, loss 0.127714, acc 0.921875
2017-03-02T18:06:44.295021: step 26460, loss 0.0762336, acc 1
2017-03-02T18:06:44.363629: step 26461, loss 0.111417, acc 0.953125
2017-03-02T18:06:44.432207: step 26462, loss 0.0524467, acc 0.984375
2017-03-02T18:06:44.500674: step 26463, loss 0.0907985, acc 0.96875
2017-03-02T18:06:44.569953: step 26464, loss 0.14288, acc 0.90625
2017-03-02T18:06:44.637226: step 26465, loss 0.225687, acc 0.9375
2017-03-02T18:06:44.704423: step 26466, loss 0.0865535, acc 0.953125
2017-03-02T18:06:44.765353: step 26467, loss 0.0779309, acc 0.953125
2017-03-02T18:06:44.825056: step 26468, loss 0.19629, acc 0.90625
2017-03-02T18:06:44.896175: step 26469, loss 0.131456, acc 0.9375
2017-03-02T18:06:44.963480: step 26470, loss 0.0898253, acc 0.96875
2017-03-02T18:06:45.029558: step 26471, loss 0.147364, acc 0.921875
2017-03-02T18:06:45.095533: step 26472, loss 0.140638, acc 0.953125
2017-03-02T18:06:45.160284: step 26473, loss 0.110132, acc 0.9375
2017-03-02T18:06:45.232745: step 26474, loss 0.235062, acc 0.90625
2017-03-02T18:06:45.307094: step 26475, loss 0.106842, acc 0.953125
2017-03-02T18:06:45.377721: step 26476, loss 0.128611, acc 0.9375
2017-03-02T18:06:45.448891: step 26477, loss 0.0506916, acc 0.96875
2017-03-02T18:06:45.517773: step 26478, loss 0.14532, acc 0.921875
2017-03-02T18:06:45.590743: step 26479, loss 0.148605, acc 0.90625
2017-03-02T18:06:45.659415: step 26480, loss 0.181739, acc 0.921875
2017-03-02T18:06:45.725659: step 26481, loss 0.122362, acc 0.921875
2017-03-02T18:06:45.784675: step 26482, loss 0.12707, acc 0.921875
2017-03-02T18:06:45.853252: step 26483, loss 0.129745, acc 0.953125
2017-03-02T18:06:45.920993: step 26484, loss 0.178776, acc 0.890625
2017-03-02T18:06:45.989235: step 26485, loss 0.160447, acc 0.921875
2017-03-02T18:06:46.058560: step 26486, loss 0.15242, acc 0.953125
2017-03-02T18:06:46.130010: step 26487, loss 0.158552, acc 0.9375
2017-03-02T18:06:46.198282: step 26488, loss 0.0972537, acc 0.953125
2017-03-02T18:06:46.266222: step 26489, loss 0.110097, acc 0.953125
2017-03-02T18:06:46.334559: step 26490, loss 0.0920993, acc 0.96875
2017-03-02T18:06:46.397975: step 26491, loss 0.174879, acc 0.9375
2017-03-02T18:06:46.467474: step 26492, loss 0.125121, acc 0.953125
2017-03-02T18:06:46.539895: step 26493, loss 0.0942842, acc 0.96875
2017-03-02T18:06:46.612941: step 26494, loss 0.104638, acc 0.9375
2017-03-02T18:06:46.681374: step 26495, loss 0.256127, acc 0.90625
2017-03-02T18:06:46.753628: step 26496, loss 0.163101, acc 0.9375
2017-03-02T18:06:46.820606: step 26497, loss 0.0985321, acc 0.953125
2017-03-02T18:06:46.896105: step 26498, loss 0.13029, acc 0.9375
2017-03-02T18:06:46.966300: step 26499, loss 0.178473, acc 0.921875
2017-03-02T18:06:47.037785: step 26500, loss 0.151709, acc 0.953125

Evaluation:
2017-03-02T18:06:47.067463: step 26500, loss 3.31929, acc 0.655371

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26500

2017-03-02T18:06:47.504413: step 26501, loss 0.190177, acc 0.9375
2017-03-02T18:06:47.588577: step 26502, loss 0.0785211, acc 0.96875
2017-03-02T18:06:47.660128: step 26503, loss 0.122713, acc 0.953125
2017-03-02T18:06:47.727753: step 26504, loss 0.0407273, acc 0.984375
2017-03-02T18:06:47.807646: step 26505, loss 0.0994118, acc 0.96875
2017-03-02T18:06:47.878406: step 26506, loss 0.215066, acc 0.890625
2017-03-02T18:06:47.945371: step 26507, loss 0.12067, acc 0.9375
2017-03-02T18:06:48.012110: step 26508, loss 0.047564, acc 0.984375
2017-03-02T18:06:48.078404: step 26509, loss 0.0627104, acc 0.953125
2017-03-02T18:06:48.148491: step 26510, loss 0.145882, acc 0.9375
2017-03-02T18:06:48.220758: step 26511, loss 0.131074, acc 0.953125
2017-03-02T18:06:48.287094: step 26512, loss 0.13784, acc 0.921875
2017-03-02T18:06:48.355604: step 26513, loss 0.16685, acc 0.90625
2017-03-02T18:06:48.428186: step 26514, loss 0.112728, acc 0.9375
2017-03-02T18:06:48.505837: step 26515, loss 0.166285, acc 0.921875
2017-03-02T18:06:48.574326: step 26516, loss 0.139252, acc 0.953125
2017-03-02T18:06:48.636201: step 26517, loss 0.111238, acc 0.953125
2017-03-02T18:06:48.717254: step 26518, loss 0.163927, acc 0.9375
2017-03-02T18:06:48.789103: step 26519, loss 0.188186, acc 0.875
2017-03-02T18:06:48.864732: step 26520, loss 0.169708, acc 0.890625
2017-03-02T18:06:48.939304: step 26521, loss 0.15561, acc 0.90625
2017-03-02T18:06:49.009552: step 26522, loss 0.167796, acc 0.921875
2017-03-02T18:06:49.082399: step 26523, loss 0.0927863, acc 0.96875
2017-03-02T18:06:49.157587: step 26524, loss 0.247111, acc 0.90625
2017-03-02T18:06:49.234451: step 26525, loss 0.158643, acc 0.90625
2017-03-02T18:06:49.306954: step 26526, loss 0.0711931, acc 0.953125
2017-03-02T18:06:49.375229: step 26527, loss 0.00973913, acc 1
2017-03-02T18:06:49.446136: step 26528, loss 0.223528, acc 0.890625
2017-03-02T18:06:49.522356: step 26529, loss 0.158566, acc 0.90625
2017-03-02T18:06:49.598100: step 26530, loss 0.0771965, acc 0.984375
2017-03-02T18:06:49.673478: step 26531, loss 0.102017, acc 0.984375
2017-03-02T18:06:49.741031: step 26532, loss 0.107265, acc 0.96875
2017-03-02T18:06:49.828636: step 26533, loss 0.124578, acc 0.96875
2017-03-02T18:06:49.916859: step 26534, loss 0.198643, acc 0.90625
2017-03-02T18:06:49.986156: step 26535, loss 0.0632499, acc 0.96875
2017-03-02T18:06:50.058179: step 26536, loss 0.129809, acc 0.953125
2017-03-02T18:06:50.132798: step 26537, loss 0.13541, acc 0.921875
2017-03-02T18:06:50.201956: step 26538, loss 0.0980264, acc 0.9375
2017-03-02T18:06:50.279446: step 26539, loss 0.0999731, acc 0.953125
2017-03-02T18:06:50.361782: step 26540, loss 0.198577, acc 0.921875
2017-03-02T18:06:50.429379: step 26541, loss 0.203105, acc 0.890625
2017-03-02T18:06:50.499890: step 26542, loss 0.17482, acc 0.90625
2017-03-02T18:06:50.572907: step 26543, loss 0.251935, acc 0.90625
2017-03-02T18:06:50.643058: step 26544, loss 0.145471, acc 0.921875
2017-03-02T18:06:50.717861: step 26545, loss 0.15247, acc 0.9375
2017-03-02T18:06:50.791754: step 26546, loss 0.0796411, acc 0.96875
2017-03-02T18:06:50.862853: step 26547, loss 0.143351, acc 0.953125
2017-03-02T18:06:50.935994: step 26548, loss 0.183425, acc 0.9375
2017-03-02T18:06:51.007475: step 26549, loss 0.200227, acc 0.9375
2017-03-02T18:06:51.078418: step 26550, loss 0.22587, acc 0.875
2017-03-02T18:06:51.152363: step 26551, loss 0.164306, acc 0.921875
2017-03-02T18:06:51.227719: step 26552, loss 0.101965, acc 0.96875
2017-03-02T18:06:51.303247: step 26553, loss 0.177924, acc 0.890625
2017-03-02T18:06:51.381902: step 26554, loss 0.164457, acc 0.890625
2017-03-02T18:06:51.460970: step 26555, loss 0.199917, acc 0.90625
2017-03-02T18:06:51.531938: step 26556, loss 0.214005, acc 0.890625
2017-03-02T18:06:51.606034: step 26557, loss 0.191696, acc 0.9375
2017-03-02T18:06:51.681820: step 26558, loss 0.113332, acc 0.9375
2017-03-02T18:06:51.755491: step 26559, loss 0.0711427, acc 0.96875
2017-03-02T18:06:51.832469: step 26560, loss 0.134569, acc 0.9375
2017-03-02T18:06:51.906613: step 26561, loss 0.236254, acc 0.9375
2017-03-02T18:06:51.978313: step 26562, loss 0.160967, acc 0.921875
2017-03-02T18:06:52.051601: step 26563, loss 0.132135, acc 0.90625
2017-03-02T18:06:52.129531: step 26564, loss 0.0700564, acc 0.96875
2017-03-02T18:06:52.214355: step 26565, loss 0.16619, acc 0.921875
2017-03-02T18:06:52.305477: step 26566, loss 0.100842, acc 0.953125
2017-03-02T18:06:52.377022: step 26567, loss 0.320533, acc 0.90625
2017-03-02T18:06:52.444762: step 26568, loss 0.117938, acc 0.96875
2017-03-02T18:06:52.515423: step 26569, loss 0.0923776, acc 0.96875
2017-03-02T18:06:52.587313: step 26570, loss 0.189384, acc 0.921875
2017-03-02T18:06:52.674255: step 26571, loss 0.0732576, acc 0.96875
2017-03-02T18:06:52.744361: step 26572, loss 0.112553, acc 0.921875
2017-03-02T18:06:52.817296: step 26573, loss 0.103794, acc 0.9375
2017-03-02T18:06:52.891569: step 26574, loss 0.145281, acc 0.953125
2017-03-02T18:06:52.962086: step 26575, loss 0.247938, acc 0.875
2017-03-02T18:06:53.030805: step 26576, loss 0.167379, acc 0.921875
2017-03-02T18:06:53.108197: step 26577, loss 0.100331, acc 0.9375
2017-03-02T18:06:53.177417: step 26578, loss 0.100309, acc 0.9375
2017-03-02T18:06:53.249373: step 26579, loss 0.214886, acc 0.90625
2017-03-02T18:06:53.326616: step 26580, loss 0.189759, acc 0.921875
2017-03-02T18:06:53.393923: step 26581, loss 0.158706, acc 0.9375
2017-03-02T18:06:53.463442: step 26582, loss 0.100284, acc 0.953125
2017-03-02T18:06:53.535174: step 26583, loss 0.218762, acc 0.90625
2017-03-02T18:06:53.604248: step 26584, loss 0.202434, acc 0.9375
2017-03-02T18:06:53.692425: step 26585, loss 0.118826, acc 0.984375
2017-03-02T18:06:53.769169: step 26586, loss 0.203436, acc 0.890625
2017-03-02T18:06:53.849751: step 26587, loss 0.241906, acc 0.890625
2017-03-02T18:06:53.922106: step 26588, loss 0.149678, acc 0.953125
2017-03-02T18:06:54.002418: step 26589, loss 0.203629, acc 0.921875
2017-03-02T18:06:54.074123: step 26590, loss 0.155315, acc 0.90625
2017-03-02T18:06:54.149669: step 26591, loss 0.13897, acc 0.953125
2017-03-02T18:06:54.229521: step 26592, loss 0.308381, acc 0.859375
2017-03-02T18:06:54.295987: step 26593, loss 0.117074, acc 0.921875
2017-03-02T18:06:54.372708: step 26594, loss 0.142033, acc 0.9375
2017-03-02T18:06:54.452747: step 26595, loss 0.153468, acc 0.9375
2017-03-02T18:06:54.522326: step 26596, loss 0.121391, acc 0.9375
2017-03-02T18:06:54.602669: step 26597, loss 0.148335, acc 0.9375
2017-03-02T18:06:54.662157: step 26598, loss 0.110299, acc 0.96875
2017-03-02T18:06:54.746022: step 26599, loss 0.136072, acc 0.953125
2017-03-02T18:06:54.819310: step 26600, loss 0.134727, acc 0.9375

Evaluation:
2017-03-02T18:06:54.851496: step 26600, loss 3.23991, acc 0.64672

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26600

2017-03-02T18:06:55.292196: step 26601, loss 0.178119, acc 0.9375
2017-03-02T18:06:55.360252: step 26602, loss 0.152819, acc 0.90625
2017-03-02T18:06:55.428834: step 26603, loss 0.105917, acc 0.953125
2017-03-02T18:06:55.507175: step 26604, loss 0.122429, acc 0.953125
2017-03-02T18:06:55.585921: step 26605, loss 0.132848, acc 0.984375
2017-03-02T18:06:55.678750: step 26606, loss 0.132622, acc 0.953125
2017-03-02T18:06:55.753644: step 26607, loss 0.112419, acc 0.953125
2017-03-02T18:06:55.825171: step 26608, loss 0.254256, acc 0.90625
2017-03-02T18:06:55.888907: step 26609, loss 0.17536, acc 0.890625
2017-03-02T18:06:55.960777: step 26610, loss 0.116782, acc 0.9375
2017-03-02T18:06:56.052198: step 26611, loss 0.126648, acc 0.984375
2017-03-02T18:06:56.124628: step 26612, loss 0.195357, acc 0.96875
2017-03-02T18:06:56.197984: step 26613, loss 0.215192, acc 0.90625
2017-03-02T18:06:56.277286: step 26614, loss 0.134318, acc 0.953125
2017-03-02T18:06:56.346300: step 26615, loss 0.0586674, acc 0.984375
2017-03-02T18:06:56.416652: step 26616, loss 0.143293, acc 0.921875
2017-03-02T18:06:56.500795: step 26617, loss 0.13981, acc 0.9375
2017-03-02T18:06:56.571917: step 26618, loss 0.0695595, acc 0.96875
2017-03-02T18:06:56.646878: step 26619, loss 0.145658, acc 0.9375
2017-03-02T18:06:56.724702: step 26620, loss 0.0966564, acc 0.953125
2017-03-02T18:06:56.788977: step 26621, loss 0.109033, acc 0.953125
2017-03-02T18:06:56.862499: step 26622, loss 0.197712, acc 0.953125
2017-03-02T18:06:56.934838: step 26623, loss 0.225361, acc 0.921875
2017-03-02T18:06:57.004312: step 26624, loss 0.201934, acc 0.890625
2017-03-02T18:06:57.077187: step 26625, loss 0.106979, acc 0.984375
2017-03-02T18:06:57.149864: step 26626, loss 0.104836, acc 0.953125
2017-03-02T18:06:57.222081: step 26627, loss 0.246495, acc 0.875
2017-03-02T18:06:57.301969: step 26628, loss 0.0716564, acc 0.96875
2017-03-02T18:06:57.372019: step 26629, loss 0.223629, acc 0.90625
2017-03-02T18:06:57.446902: step 26630, loss 0.209847, acc 0.890625
2017-03-02T18:06:57.517512: step 26631, loss 0.152516, acc 0.90625
2017-03-02T18:06:57.594914: step 26632, loss 0.106867, acc 0.9375
2017-03-02T18:06:57.663869: step 26633, loss 0.185251, acc 0.90625
2017-03-02T18:06:57.733771: step 26634, loss 0.269102, acc 0.9375
2017-03-02T18:06:57.803746: step 26635, loss 0.314927, acc 0.859375
2017-03-02T18:06:57.880984: step 26636, loss 0.270447, acc 0.921875
2017-03-02T18:06:57.940743: step 26637, loss 0.172745, acc 0.90625
2017-03-02T18:06:58.018454: step 26638, loss 0.11324, acc 0.9375
2017-03-02T18:06:58.097929: step 26639, loss 0.0863204, acc 0.953125
2017-03-02T18:06:58.167404: step 26640, loss 0.0772897, acc 0.953125
2017-03-02T18:06:58.240348: step 26641, loss 0.198133, acc 0.9375
2017-03-02T18:06:58.310165: step 26642, loss 0.14892, acc 0.953125
2017-03-02T18:06:58.389038: step 26643, loss 0.0864434, acc 0.953125
2017-03-02T18:06:58.459107: step 26644, loss 0.229576, acc 0.90625
2017-03-02T18:06:58.524655: step 26645, loss 0.0847747, acc 0.953125
2017-03-02T18:06:58.598018: step 26646, loss 0.0950692, acc 0.96875
2017-03-02T18:06:58.660673: step 26647, loss 0.0695505, acc 0.984375
2017-03-02T18:06:58.730304: step 26648, loss 0.14998, acc 0.921875
2017-03-02T18:06:58.805296: step 26649, loss 0.144413, acc 0.921875
2017-03-02T18:06:58.873949: step 26650, loss 0.151256, acc 0.921875
2017-03-02T18:06:58.946770: step 26651, loss 0.144897, acc 0.9375
2017-03-02T18:06:59.016554: step 26652, loss 0.188869, acc 0.890625
2017-03-02T18:06:59.082449: step 26653, loss 0.152416, acc 0.953125
2017-03-02T18:06:59.152092: step 26654, loss 0.100299, acc 0.953125
2017-03-02T18:06:59.234475: step 26655, loss 0.193644, acc 0.9375
2017-03-02T18:06:59.303779: step 26656, loss 2.68221e-07, acc 1
2017-03-02T18:06:59.382206: step 26657, loss 0.199488, acc 0.953125
2017-03-02T18:06:59.453370: step 26658, loss 0.201667, acc 0.90625
2017-03-02T18:06:59.522781: step 26659, loss 0.105105, acc 0.953125
2017-03-02T18:06:59.598942: step 26660, loss 0.0878632, acc 0.96875
2017-03-02T18:06:59.676984: step 26661, loss 0.0850962, acc 0.953125
2017-03-02T18:06:59.749507: step 26662, loss 0.141914, acc 0.9375
2017-03-02T18:06:59.821092: step 26663, loss 0.12826, acc 0.953125
2017-03-02T18:06:59.892633: step 26664, loss 0.21532, acc 0.90625
2017-03-02T18:06:59.967308: step 26665, loss 0.101188, acc 0.953125
2017-03-02T18:07:00.037453: step 26666, loss 0.201513, acc 0.921875
2017-03-02T18:07:00.111191: step 26667, loss 0.23646, acc 0.875
2017-03-02T18:07:00.190031: step 26668, loss 0.0906221, acc 0.9375
2017-03-02T18:07:00.259094: step 26669, loss 0.128701, acc 0.9375
2017-03-02T18:07:00.331998: step 26670, loss 0.143267, acc 0.921875
2017-03-02T18:07:00.405050: step 26671, loss 0.171534, acc 0.890625
2017-03-02T18:07:00.475999: step 26672, loss 0.177862, acc 0.90625
2017-03-02T18:07:00.553840: step 26673, loss 0.064531, acc 0.984375
2017-03-02T18:07:00.627678: step 26674, loss 0.080648, acc 0.921875
2017-03-02T18:07:00.699234: step 26675, loss 0.0632784, acc 0.984375
2017-03-02T18:07:00.772819: step 26676, loss 0.158936, acc 0.921875
2017-03-02T18:07:00.845199: step 26677, loss 0.0886528, acc 0.953125
2017-03-02T18:07:00.915694: step 26678, loss 0.0788081, acc 0.96875
2017-03-02T18:07:00.982991: step 26679, loss 0.148603, acc 0.921875
2017-03-02T18:07:01.058272: step 26680, loss 0.21005, acc 0.875
2017-03-02T18:07:01.132580: step 26681, loss 0.157518, acc 0.90625
2017-03-02T18:07:01.204868: step 26682, loss 0.078843, acc 0.953125
2017-03-02T18:07:01.276424: step 26683, loss 0.127154, acc 0.9375
2017-03-02T18:07:01.349922: step 26684, loss 0.118176, acc 0.953125
2017-03-02T18:07:01.424357: step 26685, loss 0.084968, acc 0.984375
2017-03-02T18:07:01.498389: step 26686, loss 0.102412, acc 0.953125
2017-03-02T18:07:01.590825: step 26687, loss 0.143239, acc 0.9375
2017-03-02T18:07:01.662434: step 26688, loss 0.0776645, acc 0.953125
2017-03-02T18:07:01.733389: step 26689, loss 0.148467, acc 0.921875
2017-03-02T18:07:01.810800: step 26690, loss 0.0664204, acc 0.96875
2017-03-02T18:07:01.878424: step 26691, loss 0.118386, acc 0.96875
2017-03-02T18:07:01.950809: step 26692, loss 0.101838, acc 0.96875
2017-03-02T18:07:02.029070: step 26693, loss 0.128051, acc 0.953125
2017-03-02T18:07:02.098344: step 26694, loss 0.13738, acc 0.921875
2017-03-02T18:07:02.173247: step 26695, loss 0.177313, acc 0.90625
2017-03-02T18:07:02.245227: step 26696, loss 0.127777, acc 0.921875
2017-03-02T18:07:02.317122: step 26697, loss 0.335567, acc 0.84375
2017-03-02T18:07:02.380364: step 26698, loss 0.204245, acc 0.921875
2017-03-02T18:07:02.452058: step 26699, loss 0.154519, acc 0.90625
2017-03-02T18:07:02.523823: step 26700, loss 0.126103, acc 0.953125

Evaluation:
2017-03-02T18:07:02.553643: step 26700, loss 3.39086, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26700

2017-03-02T18:07:02.998637: step 26701, loss 0.211178, acc 0.90625
2017-03-02T18:07:03.080022: step 26702, loss 0.0616033, acc 0.984375
2017-03-02T18:07:03.160927: step 26703, loss 0.191629, acc 0.875
2017-03-02T18:07:03.232984: step 26704, loss 0.20516, acc 0.9375
2017-03-02T18:07:03.298744: step 26705, loss 0.278824, acc 0.890625
2017-03-02T18:07:03.379217: step 26706, loss 0.169579, acc 0.90625
2017-03-02T18:07:03.448999: step 26707, loss 0.0713817, acc 0.96875
2017-03-02T18:07:03.524898: step 26708, loss 0.0754251, acc 0.96875
2017-03-02T18:07:03.600812: step 26709, loss 0.245386, acc 0.90625
2017-03-02T18:07:03.663823: step 26710, loss 0.101559, acc 0.96875
2017-03-02T18:07:03.737577: step 26711, loss 0.130978, acc 0.921875
2017-03-02T18:07:03.809865: step 26712, loss 0.167974, acc 0.9375
2017-03-02T18:07:03.879303: step 26713, loss 0.174959, acc 0.9375
2017-03-02T18:07:03.951682: step 26714, loss 0.151743, acc 0.90625
2017-03-02T18:07:04.023567: step 26715, loss 0.108216, acc 0.9375
2017-03-02T18:07:04.099185: step 26716, loss 0.118624, acc 0.921875
2017-03-02T18:07:04.169850: step 26717, loss 0.113709, acc 0.9375
2017-03-02T18:07:04.249658: step 26718, loss 0.127343, acc 0.953125
2017-03-02T18:07:04.323930: step 26719, loss 0.0980662, acc 0.953125
2017-03-02T18:07:04.393789: step 26720, loss 0.12605, acc 0.953125
2017-03-02T18:07:04.467894: step 26721, loss 0.0400118, acc 1
2017-03-02T18:07:04.543384: step 26722, loss 0.115909, acc 0.953125
2017-03-02T18:07:04.613009: step 26723, loss 0.0777721, acc 0.96875
2017-03-02T18:07:04.686977: step 26724, loss 0.170767, acc 0.90625
2017-03-02T18:07:04.758111: step 26725, loss 0.0783931, acc 0.96875
2017-03-02T18:07:04.842426: step 26726, loss 0.13716, acc 0.96875
2017-03-02T18:07:04.921960: step 26727, loss 0.0475854, acc 0.984375
2017-03-02T18:07:04.995971: step 26728, loss 0.170182, acc 0.90625
2017-03-02T18:07:05.069448: step 26729, loss 0.209712, acc 0.890625
2017-03-02T18:07:05.135204: step 26730, loss 0.237346, acc 0.890625
2017-03-02T18:07:05.208738: step 26731, loss 0.251102, acc 0.90625
2017-03-02T18:07:05.284581: step 26732, loss 0.154563, acc 0.921875
2017-03-02T18:07:05.355512: step 26733, loss 0.132974, acc 0.921875
2017-03-02T18:07:05.430843: step 26734, loss 0.115291, acc 0.9375
2017-03-02T18:07:05.504495: step 26735, loss 0.0732498, acc 0.96875
2017-03-02T18:07:05.570385: step 26736, loss 0.145297, acc 0.921875
2017-03-02T18:07:05.638198: step 26737, loss 0.0573901, acc 0.984375
2017-03-02T18:07:05.709265: step 26738, loss 0.153036, acc 0.9375
2017-03-02T18:07:05.778443: step 26739, loss 0.0747938, acc 0.96875
2017-03-02T18:07:05.859291: step 26740, loss 0.155432, acc 0.921875
2017-03-02T18:07:05.930676: step 26741, loss 0.115375, acc 0.953125
2017-03-02T18:07:06.011049: step 26742, loss 0.152879, acc 0.9375
2017-03-02T18:07:06.080799: step 26743, loss 0.152102, acc 0.9375
2017-03-02T18:07:06.148798: step 26744, loss 0.184348, acc 0.90625
2017-03-02T18:07:06.222170: step 26745, loss 0.10948, acc 0.984375
2017-03-02T18:07:06.300984: step 26746, loss 0.0495525, acc 1
2017-03-02T18:07:06.374038: step 26747, loss 0.13391, acc 0.9375
2017-03-02T18:07:06.437422: step 26748, loss 0.163402, acc 0.9375
2017-03-02T18:07:06.510209: step 26749, loss 0.238172, acc 0.921875
2017-03-02T18:07:06.584406: step 26750, loss 0.0982725, acc 0.9375
2017-03-02T18:07:06.656091: step 26751, loss 0.122724, acc 0.96875
2017-03-02T18:07:06.723582: step 26752, loss 0.069179, acc 0.984375
2017-03-02T18:07:06.795275: step 26753, loss 0.123029, acc 0.9375
2017-03-02T18:07:06.865944: step 26754, loss 0.210507, acc 0.9375
2017-03-02T18:07:06.931322: step 26755, loss 0.223774, acc 0.875
2017-03-02T18:07:06.996123: step 26756, loss 0.33228, acc 0.890625
2017-03-02T18:07:07.071931: step 26757, loss 0.16621, acc 0.921875
2017-03-02T18:07:07.138858: step 26758, loss 0.226577, acc 0.890625
2017-03-02T18:07:07.212590: step 26759, loss 0.154628, acc 0.90625
2017-03-02T18:07:07.293242: step 26760, loss 0.108801, acc 0.9375
2017-03-02T18:07:07.356009: step 26761, loss 0.176903, acc 0.921875
2017-03-02T18:07:07.427224: step 26762, loss 0.114011, acc 0.9375
2017-03-02T18:07:07.498468: step 26763, loss 0.136975, acc 0.9375
2017-03-02T18:07:07.572670: step 26764, loss 0.148226, acc 0.90625
2017-03-02T18:07:07.638946: step 26765, loss 0.0694777, acc 0.96875
2017-03-02T18:07:07.712510: step 26766, loss 0.0998083, acc 0.984375
2017-03-02T18:07:07.785912: step 26767, loss 0.1531, acc 0.90625
2017-03-02T18:07:07.853614: step 26768, loss 0.119356, acc 0.96875
2017-03-02T18:07:07.926940: step 26769, loss 0.166382, acc 0.921875
2017-03-02T18:07:08.006899: step 26770, loss 0.138718, acc 0.9375
2017-03-02T18:07:08.075466: step 26771, loss 0.158348, acc 0.9375
2017-03-02T18:07:08.148705: step 26772, loss 0.198281, acc 0.90625
2017-03-02T18:07:08.223163: step 26773, loss 0.204333, acc 0.90625
2017-03-02T18:07:08.301242: step 26774, loss 0.17766, acc 0.90625
2017-03-02T18:07:08.379615: step 26775, loss 0.120513, acc 0.96875
2017-03-02T18:07:08.461546: step 26776, loss 0.144021, acc 0.921875
2017-03-02T18:07:08.544379: step 26777, loss 0.0999106, acc 0.953125
2017-03-02T18:07:08.618349: step 26778, loss 0.135035, acc 0.953125
2017-03-02T18:07:08.697258: step 26779, loss 0.185312, acc 0.890625
2017-03-02T18:07:08.762484: step 26780, loss 0.313977, acc 0.859375
2017-03-02T18:07:08.841605: step 26781, loss 0.282425, acc 0.90625
2017-03-02T18:07:08.913555: step 26782, loss 0.227532, acc 0.890625
2017-03-02T18:07:08.978328: step 26783, loss 0.145148, acc 0.953125
2017-03-02T18:07:09.058096: step 26784, loss 0.11344, acc 0.953125
2017-03-02T18:07:09.129074: step 26785, loss 0.079652, acc 0.96875
2017-03-02T18:07:09.199111: step 26786, loss 0.105616, acc 0.96875
2017-03-02T18:07:09.271103: step 26787, loss 0.132452, acc 0.953125
2017-03-02T18:07:09.345601: step 26788, loss 0.231348, acc 0.890625
2017-03-02T18:07:09.413189: step 26789, loss 0.262842, acc 0.875
2017-03-02T18:07:09.482564: step 26790, loss 0.111569, acc 0.9375
2017-03-02T18:07:09.557800: step 26791, loss 0.0826895, acc 0.953125
2017-03-02T18:07:09.633304: step 26792, loss 0.112241, acc 0.953125
2017-03-02T18:07:09.701713: step 26793, loss 0.0969232, acc 0.96875
2017-03-02T18:07:09.775968: step 26794, loss 0.216362, acc 0.90625
2017-03-02T18:07:09.857426: step 26795, loss 0.18439, acc 0.921875
2017-03-02T18:07:09.927381: step 26796, loss 0.125526, acc 0.953125
2017-03-02T18:07:10.002028: step 26797, loss 0.142971, acc 0.9375
2017-03-02T18:07:10.079359: step 26798, loss 0.151834, acc 0.921875
2017-03-02T18:07:10.150001: step 26799, loss 0.355909, acc 0.84375
2017-03-02T18:07:10.225342: step 26800, loss 0.200065, acc 0.90625

Evaluation:
2017-03-02T18:07:10.260164: step 26800, loss 3.40489, acc 0.650324

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26800

2017-03-02T18:07:10.704400: step 26801, loss 0.228321, acc 0.859375
2017-03-02T18:07:10.780013: step 26802, loss 0.232048, acc 0.890625
2017-03-02T18:07:10.849227: step 26803, loss 0.132656, acc 0.921875
2017-03-02T18:07:10.939284: step 26804, loss 0.0265736, acc 1
2017-03-02T18:07:11.004147: step 26805, loss 0.122136, acc 0.953125
2017-03-02T18:07:11.076635: step 26806, loss 0.194282, acc 0.921875
2017-03-02T18:07:11.151276: step 26807, loss 0.131949, acc 0.921875
2017-03-02T18:07:11.223436: step 26808, loss 0.196441, acc 0.9375
2017-03-02T18:07:11.290986: step 26809, loss 0.21398, acc 0.90625
2017-03-02T18:07:11.368155: step 26810, loss 0.122769, acc 0.96875
2017-03-02T18:07:11.443218: step 26811, loss 0.160923, acc 0.921875
2017-03-02T18:07:11.514743: step 26812, loss 0.103889, acc 0.96875
2017-03-02T18:07:11.588197: step 26813, loss 0.162609, acc 0.9375
2017-03-02T18:07:11.680614: step 26814, loss 0.0967938, acc 0.96875
2017-03-02T18:07:11.752192: step 26815, loss 0.0778585, acc 0.984375
2017-03-02T18:07:11.825286: step 26816, loss 0.163631, acc 0.90625
2017-03-02T18:07:11.901151: step 26817, loss 0.196787, acc 0.890625
2017-03-02T18:07:11.969412: step 26818, loss 0.176252, acc 0.921875
2017-03-02T18:07:12.043585: step 26819, loss 0.0712353, acc 0.953125
2017-03-02T18:07:12.115153: step 26820, loss 0.133934, acc 0.921875
2017-03-02T18:07:12.184956: step 26821, loss 0.200269, acc 0.890625
2017-03-02T18:07:12.259819: step 26822, loss 0.117806, acc 0.953125
2017-03-02T18:07:12.335227: step 26823, loss 0.186449, acc 0.90625
2017-03-02T18:07:12.406632: step 26824, loss 0.192135, acc 0.9375
2017-03-02T18:07:12.482544: step 26825, loss 0.084748, acc 0.9375
2017-03-02T18:07:12.558788: step 26826, loss 0.122182, acc 0.953125
2017-03-02T18:07:12.627968: step 26827, loss 0.310167, acc 0.921875
2017-03-02T18:07:12.700018: step 26828, loss 0.148382, acc 0.9375
2017-03-02T18:07:12.773565: step 26829, loss 0.105795, acc 0.953125
2017-03-02T18:07:12.843620: step 26830, loss 0.0580928, acc 0.984375
2017-03-02T18:07:12.915570: step 26831, loss 0.19173, acc 0.921875
2017-03-02T18:07:12.993755: step 26832, loss 0.221306, acc 0.921875
2017-03-02T18:07:13.065690: step 26833, loss 0.128408, acc 0.953125
2017-03-02T18:07:13.137351: step 26834, loss 0.175475, acc 0.921875
2017-03-02T18:07:13.211848: step 26835, loss 0.114472, acc 0.96875
2017-03-02T18:07:13.287994: step 26836, loss 0.146335, acc 0.921875
2017-03-02T18:07:13.356700: step 26837, loss 0.229188, acc 0.921875
2017-03-02T18:07:13.431211: step 26838, loss 0.200257, acc 0.921875
2017-03-02T18:07:13.511356: step 26839, loss 0.214304, acc 0.9375
2017-03-02T18:07:13.579603: step 26840, loss 0.128577, acc 0.921875
2017-03-02T18:07:13.659680: step 26841, loss 0.0676937, acc 0.96875
2017-03-02T18:07:13.732198: step 26842, loss 0.150134, acc 0.921875
2017-03-02T18:07:13.813805: step 26843, loss 0.21631, acc 0.9375
2017-03-02T18:07:13.889623: step 26844, loss 0.120723, acc 0.953125
2017-03-02T18:07:13.962135: step 26845, loss 0.142309, acc 0.921875
2017-03-02T18:07:14.046930: step 26846, loss 0.05163, acc 0.984375
2017-03-02T18:07:14.116061: step 26847, loss 0.170999, acc 0.90625
2017-03-02T18:07:14.202530: step 26848, loss 0.159502, acc 0.9375
2017-03-02T18:07:14.266185: step 26849, loss 0.0851286, acc 0.96875
2017-03-02T18:07:14.341814: step 26850, loss 0.14897, acc 0.921875
2017-03-02T18:07:14.422269: step 26851, loss 0.241306, acc 0.890625
2017-03-02T18:07:14.486126: step 26852, loss 2.38419e-07, acc 1
2017-03-02T18:07:14.559462: step 26853, loss 0.117828, acc 0.953125
2017-03-02T18:07:14.634028: step 26854, loss 0.202114, acc 0.921875
2017-03-02T18:07:14.717986: step 26855, loss 0.169643, acc 0.90625
2017-03-02T18:07:14.793768: step 26856, loss 0.112716, acc 0.953125
2017-03-02T18:07:14.870981: step 26857, loss 0.0982405, acc 0.96875
2017-03-02T18:07:14.936363: step 26858, loss 0.121507, acc 0.953125
2017-03-02T18:07:15.009857: step 26859, loss 0.106676, acc 0.953125
2017-03-02T18:07:15.084652: step 26860, loss 0.190523, acc 0.90625
2017-03-02T18:07:15.149612: step 26861, loss 0.0976703, acc 0.953125
2017-03-02T18:07:15.235762: step 26862, loss 0.0927328, acc 0.96875
2017-03-02T18:07:15.313595: step 26863, loss 0.144012, acc 0.90625
2017-03-02T18:07:15.384922: step 26864, loss 0.196678, acc 0.921875
2017-03-02T18:07:15.453189: step 26865, loss 0.124981, acc 0.921875
2017-03-02T18:07:15.522188: step 26866, loss 0.1169, acc 0.953125
2017-03-02T18:07:15.594655: step 26867, loss 0.125108, acc 0.921875
2017-03-02T18:07:15.663364: step 26868, loss 0.223, acc 0.921875
2017-03-02T18:07:15.735032: step 26869, loss 0.189228, acc 0.90625
2017-03-02T18:07:15.816730: step 26870, loss 0.231522, acc 0.890625
2017-03-02T18:07:15.893270: step 26871, loss 0.155395, acc 0.9375
2017-03-02T18:07:15.964445: step 26872, loss 0.177454, acc 0.890625
2017-03-02T18:07:16.042521: step 26873, loss 0.160977, acc 0.953125
2017-03-02T18:07:16.111677: step 26874, loss 0.103909, acc 0.953125
2017-03-02T18:07:16.181941: step 26875, loss 0.0723812, acc 0.984375
2017-03-02T18:07:16.254755: step 26876, loss 0.167873, acc 0.921875
2017-03-02T18:07:16.319613: step 26877, loss 0.104649, acc 0.96875
2017-03-02T18:07:16.393851: step 26878, loss 0.0889514, acc 0.96875
2017-03-02T18:07:16.466723: step 26879, loss 0.086045, acc 0.984375
2017-03-02T18:07:16.529927: step 26880, loss 0.189985, acc 0.90625
2017-03-02T18:07:16.599707: step 26881, loss 0.146507, acc 0.921875
2017-03-02T18:07:16.672219: step 26882, loss 0.231086, acc 0.890625
2017-03-02T18:07:16.746480: step 26883, loss 0.225211, acc 0.890625
2017-03-02T18:07:16.816588: step 26884, loss 0.21412, acc 0.875
2017-03-02T18:07:16.891591: step 26885, loss 0.103156, acc 0.9375
2017-03-02T18:07:16.965513: step 26886, loss 0.11583, acc 0.9375
2017-03-02T18:07:17.032792: step 26887, loss 0.0835489, acc 0.96875
2017-03-02T18:07:17.111067: step 26888, loss 0.112385, acc 0.953125
2017-03-02T18:07:17.192778: step 26889, loss 0.0921609, acc 0.9375
2017-03-02T18:07:17.260359: step 26890, loss 0.178141, acc 0.921875
2017-03-02T18:07:17.335111: step 26891, loss 0.129514, acc 0.90625
2017-03-02T18:07:17.417829: step 26892, loss 0.15443, acc 0.921875
2017-03-02T18:07:17.486460: step 26893, loss 0.114529, acc 0.96875
2017-03-02T18:07:17.560325: step 26894, loss 0.136806, acc 0.9375
2017-03-02T18:07:17.634463: step 26895, loss 0.116149, acc 0.953125
2017-03-02T18:07:17.699038: step 26896, loss 0.184593, acc 0.890625
2017-03-02T18:07:17.775318: step 26897, loss 0.129697, acc 0.953125
2017-03-02T18:07:17.854576: step 26898, loss 0.117163, acc 0.953125
2017-03-02T18:07:17.931783: step 26899, loss 0.108242, acc 0.96875
2017-03-02T18:07:18.011861: step 26900, loss 0.297343, acc 0.921875

Evaluation:
2017-03-02T18:07:18.047603: step 26900, loss 3.43268, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-26900

2017-03-02T18:07:18.522211: step 26901, loss 0.188323, acc 0.921875
2017-03-02T18:07:18.595955: step 26902, loss 0.0941587, acc 0.953125
2017-03-02T18:07:18.674288: step 26903, loss 0.100942, acc 0.96875
2017-03-02T18:07:18.749109: step 26904, loss 0.121767, acc 0.953125
2017-03-02T18:07:18.814411: step 26905, loss 0.114301, acc 0.953125
2017-03-02T18:07:18.888420: step 26906, loss 0.186417, acc 0.890625
2017-03-02T18:07:18.965178: step 26907, loss 0.121968, acc 0.953125
2017-03-02T18:07:19.035141: step 26908, loss 0.107783, acc 0.953125
2017-03-02T18:07:19.105930: step 26909, loss 0.104135, acc 0.953125
2017-03-02T18:07:19.178884: step 26910, loss 0.143915, acc 0.953125
2017-03-02T18:07:19.244911: step 26911, loss 0.143013, acc 0.9375
2017-03-02T18:07:19.316563: step 26912, loss 0.0932078, acc 0.9375
2017-03-02T18:07:19.392903: step 26913, loss 0.122753, acc 0.953125
2017-03-02T18:07:19.466893: step 26914, loss 0.157676, acc 0.9375
2017-03-02T18:07:19.535643: step 26915, loss 0.14137, acc 0.96875
2017-03-02T18:07:19.614178: step 26916, loss 0.0869848, acc 0.953125
2017-03-02T18:07:19.693518: step 26917, loss 0.124514, acc 0.96875
2017-03-02T18:07:19.764107: step 26918, loss 0.15109, acc 0.953125
2017-03-02T18:07:19.840340: step 26919, loss 0.188477, acc 0.9375
2017-03-02T18:07:19.917119: step 26920, loss 0.194697, acc 0.90625
2017-03-02T18:07:19.985658: step 26921, loss 0.174511, acc 0.921875
2017-03-02T18:07:20.057611: step 26922, loss 0.272839, acc 0.890625
2017-03-02T18:07:20.135136: step 26923, loss 0.150452, acc 0.90625
2017-03-02T18:07:20.211207: step 26924, loss 0.327333, acc 0.84375
2017-03-02T18:07:20.285502: step 26925, loss 0.0778339, acc 0.96875
2017-03-02T18:07:20.357793: step 26926, loss 0.204231, acc 0.9375
2017-03-02T18:07:20.433904: step 26927, loss 0.267207, acc 0.875
2017-03-02T18:07:20.503803: step 26928, loss 0.139545, acc 0.953125
2017-03-02T18:07:20.575786: step 26929, loss 0.0885788, acc 0.953125
2017-03-02T18:07:20.642086: step 26930, loss 0.136248, acc 0.953125
2017-03-02T18:07:20.709786: step 26931, loss 0.0324279, acc 1
2017-03-02T18:07:20.778740: step 26932, loss 0.101602, acc 0.9375
2017-03-02T18:07:20.858204: step 26933, loss 0.0755931, acc 0.953125
2017-03-02T18:07:20.928907: step 26934, loss 0.12132, acc 0.953125
2017-03-02T18:07:21.001863: step 26935, loss 0.103606, acc 0.953125
2017-03-02T18:07:21.075254: step 26936, loss 0.417689, acc 0.796875
2017-03-02T18:07:21.146499: step 26937, loss 0.0706974, acc 0.953125
2017-03-02T18:07:21.220182: step 26938, loss 0.169823, acc 0.921875
2017-03-02T18:07:21.293280: step 26939, loss 0.158058, acc 0.921875
2017-03-02T18:07:21.367846: step 26940, loss 0.108702, acc 0.96875
2017-03-02T18:07:21.440124: step 26941, loss 0.183952, acc 0.921875
2017-03-02T18:07:21.521423: step 26942, loss 0.1822, acc 0.90625
2017-03-02T18:07:21.588016: step 26943, loss 0.108639, acc 0.953125
2017-03-02T18:07:21.657317: step 26944, loss 0.151083, acc 0.9375
2017-03-02T18:07:21.728614: step 26945, loss 0.147955, acc 0.9375
2017-03-02T18:07:21.803163: step 26946, loss 0.159702, acc 0.9375
2017-03-02T18:07:21.870471: step 26947, loss 0.18078, acc 0.953125
2017-03-02T18:07:21.944884: step 26948, loss 0.181639, acc 0.9375
2017-03-02T18:07:22.019318: step 26949, loss 0.086552, acc 0.96875
2017-03-02T18:07:22.088217: step 26950, loss 0.115749, acc 0.96875
2017-03-02T18:07:22.149961: step 26951, loss 0.132284, acc 0.921875
2017-03-02T18:07:22.219113: step 26952, loss 0.215786, acc 0.90625
2017-03-02T18:07:22.285400: step 26953, loss 0.156518, acc 0.953125
2017-03-02T18:07:22.352169: step 26954, loss 0.133238, acc 0.9375
2017-03-02T18:07:22.424406: step 26955, loss 0.184964, acc 0.921875
2017-03-02T18:07:22.503987: step 26956, loss 0.184289, acc 0.890625
2017-03-02T18:07:22.576608: step 26957, loss 0.124919, acc 0.953125
2017-03-02T18:07:22.649427: step 26958, loss 0.130259, acc 0.90625
2017-03-02T18:07:22.735786: step 26959, loss 0.108448, acc 0.953125
2017-03-02T18:07:22.800842: step 26960, loss 0.138495, acc 0.9375
2017-03-02T18:07:22.876995: step 26961, loss 0.140662, acc 0.9375
2017-03-02T18:07:22.954209: step 26962, loss 0.169814, acc 0.921875
2017-03-02T18:07:23.024578: step 26963, loss 0.147883, acc 0.921875
2017-03-02T18:07:23.102801: step 26964, loss 0.188097, acc 0.9375
2017-03-02T18:07:23.187345: step 26965, loss 0.143799, acc 0.953125
2017-03-02T18:07:23.260127: step 26966, loss 0.106677, acc 0.953125
2017-03-02T18:07:23.332410: step 26967, loss 0.153606, acc 0.9375
2017-03-02T18:07:23.416779: step 26968, loss 0.206305, acc 0.890625
2017-03-02T18:07:23.504882: step 26969, loss 0.107753, acc 0.921875
2017-03-02T18:07:23.580761: step 26970, loss 0.187006, acc 0.890625
2017-03-02T18:07:23.663862: step 26971, loss 0.0940449, acc 0.953125
2017-03-02T18:07:23.729831: step 26972, loss 0.0818644, acc 0.96875
2017-03-02T18:07:23.804015: step 26973, loss 0.145651, acc 0.921875
2017-03-02T18:07:23.878662: step 26974, loss 0.163608, acc 0.90625
2017-03-02T18:07:23.944812: step 26975, loss 0.10349, acc 0.9375
2017-03-02T18:07:24.015822: step 26976, loss 0.114256, acc 0.921875
2017-03-02T18:07:24.089702: step 26977, loss 0.114638, acc 0.953125
2017-03-02T18:07:24.154892: step 26978, loss 0.208337, acc 0.921875
2017-03-02T18:07:24.229063: step 26979, loss 0.0812642, acc 0.96875
2017-03-02T18:07:24.305331: step 26980, loss 0.13387, acc 0.921875
2017-03-02T18:07:24.373638: step 26981, loss 0.189426, acc 0.875
2017-03-02T18:07:24.445706: step 26982, loss 0.130077, acc 0.921875
2017-03-02T18:07:24.518252: step 26983, loss 0.124948, acc 0.953125
2017-03-02T18:07:24.600624: step 26984, loss 0.159198, acc 0.96875
2017-03-02T18:07:24.672358: step 26985, loss 0.115457, acc 0.953125
2017-03-02T18:07:24.752023: step 26986, loss 0.127042, acc 0.9375
2017-03-02T18:07:24.843205: step 26987, loss 0.0916678, acc 0.953125
2017-03-02T18:07:24.917324: step 26988, loss 0.132612, acc 0.953125
2017-03-02T18:07:24.991999: step 26989, loss 0.152397, acc 0.9375
2017-03-02T18:07:25.061534: step 26990, loss 0.203004, acc 0.9375
2017-03-02T18:07:25.136187: step 26991, loss 0.152442, acc 0.953125
2017-03-02T18:07:25.212658: step 26992, loss 0.151587, acc 0.921875
2017-03-02T18:07:25.285257: step 26993, loss 0.109719, acc 0.953125
2017-03-02T18:07:25.352396: step 26994, loss 0.162466, acc 0.9375
2017-03-02T18:07:25.429060: step 26995, loss 0.138954, acc 0.953125
2017-03-02T18:07:25.498892: step 26996, loss 0.213687, acc 0.890625
2017-03-02T18:07:25.573536: step 26997, loss 0.108417, acc 0.96875
2017-03-02T18:07:25.649073: step 26998, loss 0.133891, acc 0.921875
2017-03-02T18:07:25.723744: step 26999, loss 0.154516, acc 0.9375
2017-03-02T18:07:25.797378: step 27000, loss 0.26392, acc 0.890625

Evaluation:
2017-03-02T18:07:25.831158: step 27000, loss 3.47825, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27000

2017-03-02T18:07:26.274302: step 27001, loss 0.114746, acc 0.953125
2017-03-02T18:07:26.345335: step 27002, loss 0.118327, acc 0.953125
2017-03-02T18:07:26.410667: step 27003, loss 0.202639, acc 0.921875
2017-03-02T18:07:26.486335: step 27004, loss 0.157122, acc 0.921875
2017-03-02T18:07:26.561325: step 27005, loss 0.156078, acc 0.921875
2017-03-02T18:07:26.634347: step 27006, loss 0.153102, acc 0.953125
2017-03-02T18:07:26.704978: step 27007, loss 0.167388, acc 0.9375
2017-03-02T18:07:26.775930: step 27008, loss 0.120575, acc 0.9375
2017-03-02T18:07:26.859845: step 27009, loss 0.284427, acc 0.859375
2017-03-02T18:07:26.932874: step 27010, loss 0.0911519, acc 0.96875
2017-03-02T18:07:27.002903: step 27011, loss 0.112411, acc 0.953125
2017-03-02T18:07:27.074857: step 27012, loss 0.18075, acc 0.90625
2017-03-02T18:07:27.142238: step 27013, loss 0.116712, acc 0.921875
2017-03-02T18:07:27.217799: step 27014, loss 0.136669, acc 0.9375
2017-03-02T18:07:27.291428: step 27015, loss 0.164712, acc 0.890625
2017-03-02T18:07:27.353703: step 27016, loss 0.235178, acc 0.828125
2017-03-02T18:07:27.427058: step 27017, loss 0.10382, acc 0.953125
2017-03-02T18:07:27.503186: step 27018, loss 0.0515884, acc 0.984375
2017-03-02T18:07:27.572345: step 27019, loss 0.0916094, acc 0.953125
2017-03-02T18:07:27.655504: step 27020, loss 0.170985, acc 0.921875
2017-03-02T18:07:27.728723: step 27021, loss 0.233391, acc 0.90625
2017-03-02T18:07:27.798946: step 27022, loss 0.0970163, acc 0.96875
2017-03-02T18:07:27.867534: step 27023, loss 0.16025, acc 0.96875
2017-03-02T18:07:27.951880: step 27024, loss 0.271879, acc 0.828125
2017-03-02T18:07:28.024829: step 27025, loss 0.0701781, acc 0.953125
2017-03-02T18:07:28.102566: step 27026, loss 0.109407, acc 0.953125
2017-03-02T18:07:28.180915: step 27027, loss 0.0863115, acc 0.96875
2017-03-02T18:07:28.249411: step 27028, loss 0.171904, acc 0.9375
2017-03-02T18:07:28.322308: step 27029, loss 0.107351, acc 0.953125
2017-03-02T18:07:28.395783: step 27030, loss 0.256162, acc 0.875
2017-03-02T18:07:28.472006: step 27031, loss 0.116575, acc 0.953125
2017-03-02T18:07:28.543469: step 27032, loss 0.141643, acc 0.9375
2017-03-02T18:07:28.617747: step 27033, loss 0.0912903, acc 0.9375
2017-03-02T18:07:28.693350: step 27034, loss 0.156957, acc 0.90625
2017-03-02T18:07:28.776563: step 27035, loss 0.177049, acc 0.9375
2017-03-02T18:07:28.849800: step 27036, loss 0.0825864, acc 0.953125
2017-03-02T18:07:28.922581: step 27037, loss 0.215984, acc 0.921875
2017-03-02T18:07:28.997523: step 27038, loss 0.256714, acc 0.859375
2017-03-02T18:07:29.069385: step 27039, loss 0.129414, acc 0.953125
2017-03-02T18:07:29.141689: step 27040, loss 0.171127, acc 0.921875
2017-03-02T18:07:29.211615: step 27041, loss 0.209475, acc 0.875
2017-03-02T18:07:29.290402: step 27042, loss 0.124374, acc 0.921875
2017-03-02T18:07:29.371252: step 27043, loss 0.0985238, acc 0.96875
2017-03-02T18:07:29.442775: step 27044, loss 0.183236, acc 0.9375
2017-03-02T18:07:29.514673: step 27045, loss 0.158091, acc 0.9375
2017-03-02T18:07:29.587409: step 27046, loss 0.155697, acc 0.953125
2017-03-02T18:07:29.659474: step 27047, loss 0.189802, acc 0.921875
2017-03-02T18:07:29.735120: step 27048, loss 7.65908e-06, acc 1
2017-03-02T18:07:29.820895: step 27049, loss 0.0897988, acc 0.953125
2017-03-02T18:07:29.890728: step 27050, loss 0.0672842, acc 0.96875
2017-03-02T18:07:29.999495: step 27051, loss 0.0972406, acc 0.984375
2017-03-02T18:07:30.082922: step 27052, loss 0.199591, acc 0.9375
2017-03-02T18:07:30.151675: step 27053, loss 0.0925906, acc 0.96875
2017-03-02T18:07:30.226782: step 27054, loss 0.149814, acc 0.921875
2017-03-02T18:07:30.300344: step 27055, loss 0.158743, acc 0.921875
2017-03-02T18:07:30.360077: step 27056, loss 0.137976, acc 0.90625
2017-03-02T18:07:30.429136: step 27057, loss 0.113925, acc 0.921875
2017-03-02T18:07:30.511630: step 27058, loss 0.101703, acc 0.96875
2017-03-02T18:07:30.579031: step 27059, loss 0.115427, acc 0.9375
2017-03-02T18:07:30.651785: step 27060, loss 0.211518, acc 0.921875
2017-03-02T18:07:30.732113: step 27061, loss 0.166972, acc 0.9375
2017-03-02T18:07:30.805743: step 27062, loss 0.0781357, acc 0.984375
2017-03-02T18:07:30.878671: step 27063, loss 0.0827756, acc 0.96875
2017-03-02T18:07:30.952632: step 27064, loss 0.150802, acc 0.953125
2017-03-02T18:07:31.022733: step 27065, loss 0.110845, acc 0.953125
2017-03-02T18:07:31.096030: step 27066, loss 0.172809, acc 0.921875
2017-03-02T18:07:31.168005: step 27067, loss 0.0939127, acc 0.953125
2017-03-02T18:07:31.238276: step 27068, loss 0.118394, acc 0.953125
2017-03-02T18:07:31.312934: step 27069, loss 0.156247, acc 0.9375
2017-03-02T18:07:31.389867: step 27070, loss 0.118199, acc 0.953125
2017-03-02T18:07:31.460930: step 27071, loss 0.165493, acc 0.90625
2017-03-02T18:07:31.560025: step 27072, loss 0.0841322, acc 0.96875
2017-03-02T18:07:31.634968: step 27073, loss 0.107635, acc 0.984375
2017-03-02T18:07:31.702420: step 27074, loss 0.124011, acc 0.9375
2017-03-02T18:07:31.777529: step 27075, loss 0.104675, acc 0.953125
2017-03-02T18:07:31.852573: step 27076, loss 0.193614, acc 0.890625
2017-03-02T18:07:31.924671: step 27077, loss 0.0432729, acc 0.984375
2017-03-02T18:07:31.996343: step 27078, loss 0.159016, acc 0.921875
2017-03-02T18:07:32.069721: step 27079, loss 0.165014, acc 0.890625
2017-03-02T18:07:32.147353: step 27080, loss 0.0740252, acc 0.96875
2017-03-02T18:07:32.219719: step 27081, loss 0.209907, acc 0.859375
2017-03-02T18:07:32.304069: step 27082, loss 0.0860638, acc 0.96875
2017-03-02T18:07:32.378161: step 27083, loss 0.114756, acc 0.953125
2017-03-02T18:07:32.483026: step 27084, loss 0.195731, acc 0.90625
2017-03-02T18:07:32.551847: step 27085, loss 0.0937659, acc 0.96875
2017-03-02T18:07:32.628755: step 27086, loss 0.257095, acc 0.921875
2017-03-02T18:07:32.735239: step 27087, loss 0.0898081, acc 0.96875
2017-03-02T18:07:32.810700: step 27088, loss 0.138972, acc 0.90625
2017-03-02T18:07:32.877895: step 27089, loss 0.144831, acc 0.9375
2017-03-02T18:07:32.952211: step 27090, loss 0.230891, acc 0.90625
2017-03-02T18:07:33.031344: step 27091, loss 0.102391, acc 0.953125
2017-03-02T18:07:33.102933: step 27092, loss 0.124277, acc 0.9375
2017-03-02T18:07:33.174670: step 27093, loss 0.0808376, acc 0.96875
2017-03-02T18:07:33.246822: step 27094, loss 0.0915344, acc 0.953125
2017-03-02T18:07:33.327715: step 27095, loss 0.15952, acc 0.9375
2017-03-02T18:07:33.400638: step 27096, loss 0.182456, acc 0.921875
2017-03-02T18:07:33.473582: step 27097, loss 0.0552047, acc 0.96875
2017-03-02T18:07:33.542930: step 27098, loss 0.234257, acc 0.890625
2017-03-02T18:07:33.608761: step 27099, loss 0.172952, acc 0.921875
2017-03-02T18:07:33.689138: step 27100, loss 0.105693, acc 0.953125

Evaluation:
2017-03-02T18:07:33.718883: step 27100, loss 3.47699, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27100

2017-03-02T18:07:34.165026: step 27101, loss 0.0908005, acc 0.984375
2017-03-02T18:07:34.237457: step 27102, loss 0.150399, acc 0.9375
2017-03-02T18:07:34.312070: step 27103, loss 0.0993398, acc 0.96875
2017-03-02T18:07:34.387175: step 27104, loss 0.235587, acc 0.90625
2017-03-02T18:07:34.459135: step 27105, loss 0.151896, acc 0.953125
2017-03-02T18:07:34.530589: step 27106, loss 0.144517, acc 0.921875
2017-03-02T18:07:34.602554: step 27107, loss 0.116832, acc 0.9375
2017-03-02T18:07:34.677175: step 27108, loss 0.155303, acc 0.953125
2017-03-02T18:07:34.748722: step 27109, loss 0.108727, acc 0.96875
2017-03-02T18:07:34.820223: step 27110, loss 0.153404, acc 0.921875
2017-03-02T18:07:34.890339: step 27111, loss 0.108662, acc 0.953125
2017-03-02T18:07:34.963094: step 27112, loss 0.293407, acc 0.875
2017-03-02T18:07:35.033562: step 27113, loss 0.106362, acc 0.984375
2017-03-02T18:07:35.107552: step 27114, loss 0.0726025, acc 0.953125
2017-03-02T18:07:35.179506: step 27115, loss 0.0835358, acc 0.96875
2017-03-02T18:07:35.257630: step 27116, loss 0.20324, acc 0.90625
2017-03-02T18:07:35.330191: step 27117, loss 0.042132, acc 1
2017-03-02T18:07:35.401783: step 27118, loss 0.183916, acc 0.90625
2017-03-02T18:07:35.475257: step 27119, loss 0.198837, acc 0.90625
2017-03-02T18:07:35.539407: step 27120, loss 0.136484, acc 0.9375
2017-03-02T18:07:35.608429: step 27121, loss 0.0904293, acc 0.96875
2017-03-02T18:07:35.682052: step 27122, loss 0.104016, acc 0.96875
2017-03-02T18:07:35.753351: step 27123, loss 0.171917, acc 0.90625
2017-03-02T18:07:35.821811: step 27124, loss 0.139304, acc 0.953125
2017-03-02T18:07:35.896590: step 27125, loss 0.0920212, acc 0.9375
2017-03-02T18:07:35.969955: step 27126, loss 0.180442, acc 0.9375
2017-03-02T18:07:36.043511: step 27127, loss 0.0982734, acc 0.9375
2017-03-02T18:07:36.110133: step 27128, loss 0.103837, acc 0.96875
2017-03-02T18:07:36.207955: step 27129, loss 0.141828, acc 0.9375
2017-03-02T18:07:36.275583: step 27130, loss 0.0511931, acc 0.96875
2017-03-02T18:07:36.349240: step 27131, loss 0.177198, acc 0.9375
2017-03-02T18:07:36.424736: step 27132, loss 0.121818, acc 0.984375
2017-03-02T18:07:36.510432: step 27133, loss 0.165263, acc 0.921875
2017-03-02T18:07:36.591586: step 27134, loss 0.0735763, acc 0.984375
2017-03-02T18:07:36.657615: step 27135, loss 0.22707, acc 0.859375
2017-03-02T18:07:36.724287: step 27136, loss 0.124193, acc 0.9375
2017-03-02T18:07:36.795332: step 27137, loss 0.131293, acc 0.953125
2017-03-02T18:07:36.855668: step 27138, loss 0.143708, acc 0.921875
2017-03-02T18:07:36.934866: step 27139, loss 0.166522, acc 0.90625
2017-03-02T18:07:37.012784: step 27140, loss 0.291561, acc 0.890625
2017-03-02T18:07:37.087291: step 27141, loss 0.124778, acc 0.953125
2017-03-02T18:07:37.160858: step 27142, loss 0.125154, acc 0.90625
2017-03-02T18:07:37.227954: step 27143, loss 0.141177, acc 0.921875
2017-03-02T18:07:37.295802: step 27144, loss 0.0928534, acc 0.953125
2017-03-02T18:07:37.373638: step 27145, loss 0.0955558, acc 0.96875
2017-03-02T18:07:37.441589: step 27146, loss 0.210282, acc 0.90625
2017-03-02T18:07:37.519809: step 27147, loss 0.09317, acc 0.953125
2017-03-02T18:07:37.607295: step 27148, loss 0.217923, acc 0.9375
2017-03-02T18:07:37.678606: step 27149, loss 0.13681, acc 0.9375
2017-03-02T18:07:37.754929: step 27150, loss 0.168778, acc 0.90625
2017-03-02T18:07:37.832048: step 27151, loss 0.192319, acc 0.90625
2017-03-02T18:07:37.898076: step 27152, loss 0.192669, acc 0.921875
2017-03-02T18:07:37.971270: step 27153, loss 0.247558, acc 0.859375
2017-03-02T18:07:38.043445: step 27154, loss 0.0774595, acc 0.96875
2017-03-02T18:07:38.108936: step 27155, loss 0.0767633, acc 0.984375
2017-03-02T18:07:38.176788: step 27156, loss 0.205092, acc 0.875
2017-03-02T18:07:38.247460: step 27157, loss 0.166257, acc 0.9375
2017-03-02T18:07:38.333926: step 27158, loss 0.0833192, acc 0.984375
2017-03-02T18:07:38.403307: step 27159, loss 0.208887, acc 0.890625
2017-03-02T18:07:38.494453: step 27160, loss 0.0966004, acc 0.96875
2017-03-02T18:07:38.569081: step 27161, loss 0.111798, acc 0.9375
2017-03-02T18:07:38.639297: step 27162, loss 0.193202, acc 0.875
2017-03-02T18:07:38.712819: step 27163, loss 0.166509, acc 0.90625
2017-03-02T18:07:38.784794: step 27164, loss 0.166356, acc 0.9375
2017-03-02T18:07:38.850092: step 27165, loss 0.0587257, acc 0.984375
2017-03-02T18:07:38.923994: step 27166, loss 0.0725601, acc 0.96875
2017-03-02T18:07:38.993771: step 27167, loss 0.109706, acc 0.9375
2017-03-02T18:07:39.066796: step 27168, loss 0.105501, acc 0.953125
2017-03-02T18:07:39.139763: step 27169, loss 0.125584, acc 0.9375
2017-03-02T18:07:39.214834: step 27170, loss 0.124455, acc 0.9375
2017-03-02T18:07:39.280438: step 27171, loss 0.229574, acc 0.890625
2017-03-02T18:07:39.357818: step 27172, loss 0.146325, acc 0.9375
2017-03-02T18:07:39.428490: step 27173, loss 0.150527, acc 0.921875
2017-03-02T18:07:39.498046: step 27174, loss 0.281337, acc 0.859375
2017-03-02T18:07:39.568068: step 27175, loss 0.150477, acc 0.9375
2017-03-02T18:07:39.638790: step 27176, loss 0.0722374, acc 0.984375
2017-03-02T18:07:39.723798: step 27177, loss 0.103486, acc 0.9375
2017-03-02T18:07:39.793437: step 27178, loss 0.134368, acc 0.953125
2017-03-02T18:07:39.876174: step 27179, loss 0.209477, acc 0.921875
2017-03-02T18:07:39.948058: step 27180, loss 0.113693, acc 0.953125
2017-03-02T18:07:40.007860: step 27181, loss 0.0665073, acc 0.984375
2017-03-02T18:07:40.075124: step 27182, loss 0.174354, acc 0.90625
2017-03-02T18:07:40.145465: step 27183, loss 0.138579, acc 0.90625
2017-03-02T18:07:40.222785: step 27184, loss 0.117779, acc 0.9375
2017-03-02T18:07:40.296078: step 27185, loss 0.246389, acc 0.953125
2017-03-02T18:07:40.362798: step 27186, loss 0.112798, acc 0.9375
2017-03-02T18:07:40.437194: step 27187, loss 0.213634, acc 0.921875
2017-03-02T18:07:40.503627: step 27188, loss 0.194587, acc 0.90625
2017-03-02T18:07:40.591151: step 27189, loss 0.142966, acc 0.953125
2017-03-02T18:07:40.671166: step 27190, loss 0.216282, acc 0.890625
2017-03-02T18:07:40.743024: step 27191, loss 0.091764, acc 0.953125
2017-03-02T18:07:40.814659: step 27192, loss 0.171888, acc 0.9375
2017-03-02T18:07:40.892427: step 27193, loss 0.114639, acc 0.953125
2017-03-02T18:07:40.986064: step 27194, loss 0.11483, acc 0.953125
2017-03-02T18:07:41.057393: step 27195, loss 0.160885, acc 0.90625
2017-03-02T18:07:41.131535: step 27196, loss 0.243612, acc 0.921875
2017-03-02T18:07:41.200843: step 27197, loss 0.121301, acc 0.9375
2017-03-02T18:07:41.270915: step 27198, loss 0.239023, acc 0.921875
2017-03-02T18:07:41.356744: step 27199, loss 0.123433, acc 0.953125
2017-03-02T18:07:41.429465: step 27200, loss 0.137648, acc 0.9375

Evaluation:
2017-03-02T18:07:41.465719: step 27200, loss 3.42472, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27200

2017-03-02T18:07:41.916719: step 27201, loss 0.0800671, acc 0.9375
2017-03-02T18:07:41.988601: step 27202, loss 0.137311, acc 0.9375
2017-03-02T18:07:42.063331: step 27203, loss 0.284235, acc 0.828125
2017-03-02T18:07:42.134683: step 27204, loss 0.161486, acc 0.953125
2017-03-02T18:07:42.211951: step 27205, loss 0.164786, acc 0.921875
2017-03-02T18:07:42.278608: step 27206, loss 0.182272, acc 0.921875
2017-03-02T18:07:42.360327: step 27207, loss 0.159807, acc 0.875
2017-03-02T18:07:42.450145: step 27208, loss 0.122794, acc 0.921875
2017-03-02T18:07:42.516781: step 27209, loss 0.158359, acc 0.90625
2017-03-02T18:07:42.585743: step 27210, loss 0.145026, acc 0.9375
2017-03-02T18:07:42.677686: step 27211, loss 0.167338, acc 0.953125
2017-03-02T18:07:42.748091: step 27212, loss 0.178328, acc 0.90625
2017-03-02T18:07:42.818286: step 27213, loss 0.192123, acc 0.921875
2017-03-02T18:07:42.885230: step 27214, loss 0.281668, acc 0.921875
2017-03-02T18:07:42.952059: step 27215, loss 0.122733, acc 0.953125
2017-03-02T18:07:43.023347: step 27216, loss 0.160655, acc 0.953125
2017-03-02T18:07:43.105545: step 27217, loss 0.143789, acc 0.9375
2017-03-02T18:07:43.172217: step 27218, loss 0.196022, acc 0.90625
2017-03-02T18:07:43.243548: step 27219, loss 0.13485, acc 0.9375
2017-03-02T18:07:43.313547: step 27220, loss 0.228221, acc 0.890625
2017-03-02T18:07:43.393133: step 27221, loss 0.131624, acc 0.9375
2017-03-02T18:07:43.460564: step 27222, loss 0.230285, acc 0.921875
2017-03-02T18:07:43.529412: step 27223, loss 0.15519, acc 0.921875
2017-03-02T18:07:43.604027: step 27224, loss 0.16308, acc 0.953125
2017-03-02T18:07:43.675929: step 27225, loss 0.134919, acc 0.96875
2017-03-02T18:07:43.746484: step 27226, loss 0.200136, acc 0.890625
2017-03-02T18:07:43.822687: step 27227, loss 0.156014, acc 0.921875
2017-03-02T18:07:43.888008: step 27228, loss 0.178064, acc 0.90625
2017-03-02T18:07:43.959949: step 27229, loss 0.269789, acc 0.875
2017-03-02T18:07:44.032907: step 27230, loss 0.139706, acc 0.9375
2017-03-02T18:07:44.099115: step 27231, loss 0.160251, acc 0.9375
2017-03-02T18:07:44.170863: step 27232, loss 0.134813, acc 0.96875
2017-03-02T18:07:44.247587: step 27233, loss 0.156106, acc 0.921875
2017-03-02T18:07:44.323048: step 27234, loss 0.112898, acc 0.984375
2017-03-02T18:07:44.399602: step 27235, loss 0.0962548, acc 0.953125
2017-03-02T18:07:44.465534: step 27236, loss 0.207559, acc 0.890625
2017-03-02T18:07:44.535872: step 27237, loss 0.187178, acc 0.921875
2017-03-02T18:07:44.610643: step 27238, loss 0.145865, acc 0.9375
2017-03-02T18:07:44.678622: step 27239, loss 0.298232, acc 0.8125
2017-03-02T18:07:44.756294: step 27240, loss 0.21584, acc 0.9375
2017-03-02T18:07:44.823224: step 27241, loss 0.186254, acc 0.90625
2017-03-02T18:07:44.896450: step 27242, loss 0.180185, acc 0.90625
2017-03-02T18:07:44.969422: step 27243, loss 0.139229, acc 0.953125
2017-03-02T18:07:45.038632: step 27244, loss 0.489302, acc 0.75
2017-03-02T18:07:45.113592: step 27245, loss 0.0586813, acc 0.984375
2017-03-02T18:07:45.187630: step 27246, loss 0.145257, acc 0.9375
2017-03-02T18:07:45.250737: step 27247, loss 0.258927, acc 0.875
2017-03-02T18:07:45.319412: step 27248, loss 0.116117, acc 0.9375
2017-03-02T18:07:45.391730: step 27249, loss 0.206919, acc 0.890625
2017-03-02T18:07:45.459255: step 27250, loss 0.180071, acc 0.9375
2017-03-02T18:07:45.528250: step 27251, loss 0.127422, acc 0.9375
2017-03-02T18:07:45.601664: step 27252, loss 0.0790123, acc 0.96875
2017-03-02T18:07:45.673499: step 27253, loss 0.24733, acc 0.890625
2017-03-02T18:07:45.732011: step 27254, loss 0.104973, acc 0.96875
2017-03-02T18:07:45.798387: step 27255, loss 0.213587, acc 0.9375
2017-03-02T18:07:45.861958: step 27256, loss 0.119346, acc 0.9375
2017-03-02T18:07:45.928542: step 27257, loss 0.116309, acc 0.9375
2017-03-02T18:07:45.997637: step 27258, loss 0.116039, acc 0.9375
2017-03-02T18:07:46.071036: step 27259, loss 0.159977, acc 0.9375
2017-03-02T18:07:46.146452: step 27260, loss 0.167966, acc 0.921875
2017-03-02T18:07:46.216015: step 27261, loss 0.110241, acc 0.9375
2017-03-02T18:07:46.294713: step 27262, loss 0.100824, acc 0.953125
2017-03-02T18:07:46.363708: step 27263, loss 0.171495, acc 0.921875
2017-03-02T18:07:46.434643: step 27264, loss 0.117114, acc 0.953125
2017-03-02T18:07:46.507183: step 27265, loss 0.109754, acc 0.953125
2017-03-02T18:07:46.577608: step 27266, loss 0.102114, acc 0.953125
2017-03-02T18:07:46.645930: step 27267, loss 0.0956775, acc 0.953125
2017-03-02T18:07:46.719707: step 27268, loss 0.132789, acc 0.984375
2017-03-02T18:07:46.801437: step 27269, loss 0.188251, acc 0.890625
2017-03-02T18:07:46.867296: step 27270, loss 0.130938, acc 0.953125
2017-03-02T18:07:46.936686: step 27271, loss 0.189678, acc 0.90625
2017-03-02T18:07:47.009252: step 27272, loss 0.128838, acc 0.96875
2017-03-02T18:07:47.076508: step 27273, loss 0.188115, acc 0.90625
2017-03-02T18:07:47.150199: step 27274, loss 0.132732, acc 0.9375
2017-03-02T18:07:47.230361: step 27275, loss 0.154497, acc 0.921875
2017-03-02T18:07:47.300031: step 27276, loss 0.166253, acc 0.90625
2017-03-02T18:07:47.367283: step 27277, loss 0.176435, acc 0.9375
2017-03-02T18:07:47.439103: step 27278, loss 0.178064, acc 0.90625
2017-03-02T18:07:47.526487: step 27279, loss 0.168698, acc 0.90625
2017-03-02T18:07:47.592416: step 27280, loss 0.100642, acc 0.96875
2017-03-02T18:07:47.667705: step 27281, loss 0.0803607, acc 0.96875
2017-03-02T18:07:47.744126: step 27282, loss 0.136809, acc 0.921875
2017-03-02T18:07:47.811603: step 27283, loss 0.132124, acc 0.921875
2017-03-02T18:07:47.883496: step 27284, loss 0.0796492, acc 0.984375
2017-03-02T18:07:47.955722: step 27285, loss 0.117267, acc 0.9375
2017-03-02T18:07:48.021846: step 27286, loss 0.159341, acc 0.9375
2017-03-02T18:07:48.091094: step 27287, loss 0.15493, acc 0.921875
2017-03-02T18:07:48.160182: step 27288, loss 0.140621, acc 0.953125
2017-03-02T18:07:48.234427: step 27289, loss 0.113653, acc 0.953125
2017-03-02T18:07:48.305599: step 27290, loss 0.127534, acc 0.953125
2017-03-02T18:07:48.375442: step 27291, loss 0.138175, acc 0.9375
2017-03-02T18:07:48.447342: step 27292, loss 0.180635, acc 0.921875
2017-03-02T18:07:48.519026: step 27293, loss 0.0974867, acc 0.953125
2017-03-02T18:07:48.589723: step 27294, loss 0.110389, acc 0.953125
2017-03-02T18:07:48.667327: step 27295, loss 0.0873196, acc 0.984375
2017-03-02T18:07:48.734785: step 27296, loss 0.0578613, acc 0.984375
2017-03-02T18:07:48.807341: step 27297, loss 0.0590459, acc 0.984375
2017-03-02T18:07:48.880114: step 27298, loss 0.151951, acc 0.953125
2017-03-02T18:07:48.945713: step 27299, loss 0.0702372, acc 0.953125
2017-03-02T18:07:49.014959: step 27300, loss 0.216086, acc 0.890625

Evaluation:
2017-03-02T18:07:49.051562: step 27300, loss 3.45138, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27300

2017-03-02T18:07:49.493531: step 27301, loss 0.18249, acc 0.921875
2017-03-02T18:07:49.570107: step 27302, loss 0.265816, acc 0.90625
2017-03-02T18:07:49.642347: step 27303, loss 0.13, acc 0.921875
2017-03-02T18:07:49.711074: step 27304, loss 0.117874, acc 0.9375
2017-03-02T18:07:49.780019: step 27305, loss 0.126711, acc 0.921875
2017-03-02T18:07:49.856807: step 27306, loss 0.206872, acc 0.9375
2017-03-02T18:07:49.946663: step 27307, loss 0.0497984, acc 0.984375
2017-03-02T18:07:50.024068: step 27308, loss 0.223316, acc 0.890625
2017-03-02T18:07:50.093895: step 27309, loss 0.317732, acc 0.859375
2017-03-02T18:07:50.165547: step 27310, loss 0.10981, acc 0.953125
2017-03-02T18:07:50.238655: step 27311, loss 0.157875, acc 0.921875
2017-03-02T18:07:50.310830: step 27312, loss 0.0810112, acc 0.953125
2017-03-02T18:07:50.386398: step 27313, loss 0.282031, acc 0.890625
2017-03-02T18:07:50.461249: step 27314, loss 0.193547, acc 0.90625
2017-03-02T18:07:50.526256: step 27315, loss 0.141691, acc 0.953125
2017-03-02T18:07:50.606592: step 27316, loss 0.0960668, acc 0.9375
2017-03-02T18:07:50.678196: step 27317, loss 0.1307, acc 0.9375
2017-03-02T18:07:50.742183: step 27318, loss 0.0867282, acc 0.953125
2017-03-02T18:07:50.814485: step 27319, loss 0.200662, acc 0.921875
2017-03-02T18:07:50.885826: step 27320, loss 0.174986, acc 0.90625
2017-03-02T18:07:50.957005: step 27321, loss 0.12204, acc 0.9375
2017-03-02T18:07:51.029150: step 27322, loss 0.21186, acc 0.9375
2017-03-02T18:07:51.101540: step 27323, loss 0.118193, acc 0.9375
2017-03-02T18:07:51.173786: step 27324, loss 0.129272, acc 0.921875
2017-03-02T18:07:51.251713: step 27325, loss 0.166609, acc 0.9375
2017-03-02T18:07:51.329319: step 27326, loss 0.257242, acc 0.875
2017-03-02T18:07:51.403149: step 27327, loss 0.109298, acc 0.96875
2017-03-02T18:07:51.468622: step 27328, loss 0.150845, acc 0.9375
2017-03-02T18:07:51.536186: step 27329, loss 0.323896, acc 0.890625
2017-03-02T18:07:51.608929: step 27330, loss 0.165222, acc 0.921875
2017-03-02T18:07:51.675567: step 27331, loss 0.159695, acc 0.90625
2017-03-02T18:07:51.746599: step 27332, loss 0.0997062, acc 0.953125
2017-03-02T18:07:51.821427: step 27333, loss 0.112241, acc 0.96875
2017-03-02T18:07:51.896277: step 27334, loss 0.0585576, acc 0.984375
2017-03-02T18:07:51.969491: step 27335, loss 0.219285, acc 0.921875
2017-03-02T18:07:52.043316: step 27336, loss 0.159248, acc 0.921875
2017-03-02T18:07:52.120459: step 27337, loss 0.118299, acc 0.953125
2017-03-02T18:07:52.188781: step 27338, loss 0.182934, acc 0.921875
2017-03-02T18:07:52.257686: step 27339, loss 0.109033, acc 0.9375
2017-03-02T18:07:52.332731: step 27340, loss 0.152029, acc 0.921875
2017-03-02T18:07:52.411647: step 27341, loss 0.0809773, acc 0.96875
2017-03-02T18:07:52.479187: step 27342, loss 0.105425, acc 0.9375
2017-03-02T18:07:52.552744: step 27343, loss 0.159347, acc 0.9375
2017-03-02T18:07:52.621885: step 27344, loss 0.220437, acc 0.890625
2017-03-02T18:07:52.708556: step 27345, loss 0.150944, acc 0.9375
2017-03-02T18:07:52.777283: step 27346, loss 0.146401, acc 0.9375
2017-03-02T18:07:52.845732: step 27347, loss 0.261721, acc 0.859375
2017-03-02T18:07:52.914434: step 27348, loss 0.0718955, acc 0.984375
2017-03-02T18:07:52.985517: step 27349, loss 0.0456807, acc 0.984375
2017-03-02T18:07:53.059966: step 27350, loss 0.0676062, acc 0.984375
2017-03-02T18:07:53.133313: step 27351, loss 0.210619, acc 0.9375
2017-03-02T18:07:53.212388: step 27352, loss 0.121948, acc 0.953125
2017-03-02T18:07:53.284683: step 27353, loss 0.24389, acc 0.921875
2017-03-02T18:07:53.353822: step 27354, loss 0.072409, acc 0.96875
2017-03-02T18:07:53.426185: step 27355, loss 0.1132, acc 0.9375
2017-03-02T18:07:53.504891: step 27356, loss 0.184949, acc 0.921875
2017-03-02T18:07:53.570485: step 27357, loss 0.147074, acc 0.9375
2017-03-02T18:07:53.650003: step 27358, loss 0.0574274, acc 0.96875
2017-03-02T18:07:53.727299: step 27359, loss 0.0857673, acc 0.96875
2017-03-02T18:07:53.797699: step 27360, loss 0.0709043, acc 0.984375
2017-03-02T18:07:53.871691: step 27361, loss 0.269855, acc 0.90625
2017-03-02T18:07:53.943692: step 27362, loss 0.142703, acc 0.921875
2017-03-02T18:07:54.019207: step 27363, loss 0.187499, acc 0.90625
2017-03-02T18:07:54.090781: step 27364, loss 0.256143, acc 0.90625
2017-03-02T18:07:54.160090: step 27365, loss 0.0700661, acc 0.96875
2017-03-02T18:07:54.229851: step 27366, loss 0.0849565, acc 0.984375
2017-03-02T18:07:54.296914: step 27367, loss 0.112353, acc 0.921875
2017-03-02T18:07:54.366049: step 27368, loss 0.20424, acc 0.9375
2017-03-02T18:07:54.452745: step 27369, loss 0.0562087, acc 0.96875
2017-03-02T18:07:54.535195: step 27370, loss 0.15624, acc 0.921875
2017-03-02T18:07:54.610230: step 27371, loss 0.143775, acc 0.921875
2017-03-02T18:07:54.681882: step 27372, loss 0.108992, acc 0.96875
2017-03-02T18:07:54.756887: step 27373, loss 0.227736, acc 0.875
2017-03-02T18:07:54.833334: step 27374, loss 0.133226, acc 0.953125
2017-03-02T18:07:54.908568: step 27375, loss 0.191081, acc 0.90625
2017-03-02T18:07:54.984301: step 27376, loss 0.200493, acc 0.9375
2017-03-02T18:07:55.064213: step 27377, loss 0.103158, acc 0.953125
2017-03-02T18:07:55.134331: step 27378, loss 0.238488, acc 0.921875
2017-03-02T18:07:55.208457: step 27379, loss 0.178428, acc 0.90625
2017-03-02T18:07:55.279048: step 27380, loss 0.15339, acc 0.953125
2017-03-02T18:07:55.349978: step 27381, loss 0.192423, acc 0.921875
2017-03-02T18:07:55.420245: step 27382, loss 0.206111, acc 0.921875
2017-03-02T18:07:55.494349: step 27383, loss 0.158657, acc 0.921875
2017-03-02T18:07:55.576534: step 27384, loss 0.30917, acc 0.84375
2017-03-02T18:07:55.651605: step 27385, loss 0.278102, acc 0.875
2017-03-02T18:07:55.722083: step 27386, loss 0.143234, acc 0.96875
2017-03-02T18:07:55.797850: step 27387, loss 0.158043, acc 0.9375
2017-03-02T18:07:55.866997: step 27388, loss 0.178645, acc 0.890625
2017-03-02T18:07:55.937339: step 27389, loss 0.123413, acc 0.921875
2017-03-02T18:07:56.013492: step 27390, loss 0.0738108, acc 0.96875
2017-03-02T18:07:56.082857: step 27391, loss 0.248827, acc 0.90625
2017-03-02T18:07:56.151630: step 27392, loss 0.158738, acc 0.90625
2017-03-02T18:07:56.225515: step 27393, loss 0.111695, acc 0.921875
2017-03-02T18:07:56.316041: step 27394, loss 0.168624, acc 0.921875
2017-03-02T18:07:56.384791: step 27395, loss 0.187924, acc 0.875
2017-03-02T18:07:56.455721: step 27396, loss 0.103232, acc 0.9375
2017-03-02T18:07:56.526751: step 27397, loss 0.228477, acc 0.875
2017-03-02T18:07:56.596938: step 27398, loss 0.130888, acc 0.9375
2017-03-02T18:07:56.670045: step 27399, loss 0.135472, acc 0.953125
2017-03-02T18:07:56.738990: step 27400, loss 0.181265, acc 0.921875

Evaluation:
2017-03-02T18:07:56.770294: step 27400, loss 3.44243, acc 0.633742

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27400

2017-03-02T18:07:57.208786: step 27401, loss 0.0923182, acc 0.953125
2017-03-02T18:07:57.279823: step 27402, loss 0.137258, acc 0.953125
2017-03-02T18:07:57.355475: step 27403, loss 0.262072, acc 0.875
2017-03-02T18:07:57.425825: step 27404, loss 0.162318, acc 0.921875
2017-03-02T18:07:57.501987: step 27405, loss 0.198961, acc 0.90625
2017-03-02T18:07:57.585260: step 27406, loss 0.122572, acc 0.96875
2017-03-02T18:07:57.662714: step 27407, loss 0.215519, acc 0.90625
2017-03-02T18:07:57.732880: step 27408, loss 0.124493, acc 0.953125
2017-03-02T18:07:57.793547: step 27409, loss 0.24151, acc 0.875
2017-03-02T18:07:57.857488: step 27410, loss 0.133469, acc 0.953125
2017-03-02T18:07:57.924237: step 27411, loss 0.147869, acc 0.921875
2017-03-02T18:07:57.997544: step 27412, loss 0.25006, acc 0.875
2017-03-02T18:07:58.069935: step 27413, loss 0.138468, acc 0.96875
2017-03-02T18:07:58.135962: step 27414, loss 0.172136, acc 0.921875
2017-03-02T18:07:58.210697: step 27415, loss 0.0617369, acc 0.984375
2017-03-02T18:07:58.277248: step 27416, loss 0.0659749, acc 0.96875
2017-03-02T18:07:58.347530: step 27417, loss 0.151867, acc 0.921875
2017-03-02T18:07:58.408715: step 27418, loss 0.158588, acc 0.90625
2017-03-02T18:07:58.486519: step 27419, loss 0.118067, acc 0.921875
2017-03-02T18:07:58.569670: step 27420, loss 0.184484, acc 0.921875
2017-03-02T18:07:58.641658: step 27421, loss 0.169561, acc 0.90625
2017-03-02T18:07:58.714337: step 27422, loss 0.223344, acc 0.859375
2017-03-02T18:07:58.798173: step 27423, loss 0.235724, acc 0.90625
2017-03-02T18:07:58.862691: step 27424, loss 0.11275, acc 0.9375
2017-03-02T18:07:58.935569: step 27425, loss 0.135278, acc 0.9375
2017-03-02T18:07:59.009787: step 27426, loss 0.211248, acc 0.921875
2017-03-02T18:07:59.092429: step 27427, loss 0.229531, acc 0.90625
2017-03-02T18:07:59.164565: step 27428, loss 0.142276, acc 0.921875
2017-03-02T18:07:59.242003: step 27429, loss 0.110201, acc 0.96875
2017-03-02T18:07:59.319086: step 27430, loss 0.103315, acc 0.953125
2017-03-02T18:07:59.394092: step 27431, loss 0.110079, acc 0.9375
2017-03-02T18:07:59.469114: step 27432, loss 0.102889, acc 0.96875
2017-03-02T18:07:59.537144: step 27433, loss 0.196769, acc 0.890625
2017-03-02T18:07:59.615650: step 27434, loss 0.0998371, acc 0.953125
2017-03-02T18:07:59.682022: step 27435, loss 0.134586, acc 0.921875
2017-03-02T18:07:59.750309: step 27436, loss 0.121487, acc 0.921875
2017-03-02T18:07:59.823911: step 27437, loss 0.0964511, acc 0.953125
2017-03-02T18:07:59.896756: step 27438, loss 0.14422, acc 0.9375
2017-03-02T18:07:59.970278: step 27439, loss 0.0628286, acc 0.96875
2017-03-02T18:08:00.031396: step 27440, loss 0, acc 1
2017-03-02T18:08:00.102687: step 27441, loss 0.181933, acc 0.84375
2017-03-02T18:08:00.181617: step 27442, loss 0.12377, acc 0.9375
2017-03-02T18:08:00.261567: step 27443, loss 0.19104, acc 0.921875
2017-03-02T18:08:00.341275: step 27444, loss 0.121181, acc 0.96875
2017-03-02T18:08:00.419348: step 27445, loss 0.111211, acc 0.96875
2017-03-02T18:08:00.489949: step 27446, loss 0.111663, acc 0.9375
2017-03-02T18:08:00.561984: step 27447, loss 0.19967, acc 0.890625
2017-03-02T18:08:00.644415: step 27448, loss 0.188832, acc 0.875
2017-03-02T18:08:00.713139: step 27449, loss 0.128988, acc 0.9375
2017-03-02T18:08:00.785065: step 27450, loss 0.163205, acc 0.9375
2017-03-02T18:08:00.866068: step 27451, loss 0.141456, acc 0.9375
2017-03-02T18:08:00.930928: step 27452, loss 0.158989, acc 0.90625
2017-03-02T18:08:01.002069: step 27453, loss 0.0918353, acc 0.96875
2017-03-02T18:08:01.085763: step 27454, loss 0.0579122, acc 0.984375
2017-03-02T18:08:01.158153: step 27455, loss 0.147379, acc 0.921875
2017-03-02T18:08:01.233646: step 27456, loss 0.130638, acc 0.921875
2017-03-02T18:08:01.504089: step 27457, loss 0.0857658, acc 0.96875
2017-03-02T18:08:01.579024: step 27458, loss 0.151663, acc 0.9375
2017-03-02T18:08:01.661501: step 27459, loss 0.125425, acc 0.953125
2017-03-02T18:08:01.731321: step 27460, loss 0.097851, acc 0.96875
2017-03-02T18:08:01.802982: step 27461, loss 0.207746, acc 0.9375
2017-03-02T18:08:01.878274: step 27462, loss 0.118464, acc 0.9375
2017-03-02T18:08:01.950284: step 27463, loss 0.137175, acc 0.9375
2017-03-02T18:08:02.021954: step 27464, loss 0.10979, acc 0.953125
2017-03-02T18:08:02.090741: step 27465, loss 0.150975, acc 0.921875
2017-03-02T18:08:02.164988: step 27466, loss 0.18628, acc 0.9375
2017-03-02T18:08:02.240055: step 27467, loss 0.111347, acc 0.953125
2017-03-02T18:08:02.309002: step 27468, loss 0.142966, acc 0.9375
2017-03-02T18:08:02.380459: step 27469, loss 0.147286, acc 0.9375
2017-03-02T18:08:02.456430: step 27470, loss 0.0659536, acc 0.96875
2017-03-02T18:08:02.524737: step 27471, loss 0.0532707, acc 0.96875
2017-03-02T18:08:02.592987: step 27472, loss 0.172762, acc 0.9375
2017-03-02T18:08:02.668554: step 27473, loss 0.128632, acc 0.953125
2017-03-02T18:08:02.742066: step 27474, loss 0.205702, acc 0.890625
2017-03-02T18:08:02.816809: step 27475, loss 0.104632, acc 0.9375
2017-03-02T18:08:02.892505: step 27476, loss 0.22563, acc 0.90625
2017-03-02T18:08:02.956306: step 27477, loss 0.154917, acc 0.875
2017-03-02T18:08:03.028232: step 27478, loss 0.17506, acc 0.90625
2017-03-02T18:08:03.100535: step 27479, loss 0.0903742, acc 0.984375
2017-03-02T18:08:03.170930: step 27480, loss 0.142487, acc 0.953125
2017-03-02T18:08:03.243635: step 27481, loss 0.0718772, acc 0.96875
2017-03-02T18:08:03.309046: step 27482, loss 0.0717371, acc 0.984375
2017-03-02T18:08:03.382508: step 27483, loss 0.121644, acc 0.96875
2017-03-02T18:08:03.449771: step 27484, loss 0.172263, acc 0.9375
2017-03-02T18:08:03.521992: step 27485, loss 0.170291, acc 0.9375
2017-03-02T18:08:03.610379: step 27486, loss 0.272265, acc 0.859375
2017-03-02T18:08:03.679623: step 27487, loss 0.174922, acc 0.921875
2017-03-02T18:08:03.774238: step 27488, loss 0.130424, acc 0.9375
2017-03-02T18:08:03.845482: step 27489, loss 0.164038, acc 0.9375
2017-03-02T18:08:03.918037: step 27490, loss 0.121602, acc 0.953125
2017-03-02T18:08:03.994612: step 27491, loss 0.131235, acc 0.96875
2017-03-02T18:08:04.074659: step 27492, loss 0.160987, acc 0.921875
2017-03-02T18:08:04.145644: step 27493, loss 0.0779809, acc 0.96875
2017-03-02T18:08:04.216972: step 27494, loss 0.233703, acc 0.890625
2017-03-02T18:08:04.286095: step 27495, loss 0.110101, acc 0.96875
2017-03-02T18:08:04.353585: step 27496, loss 0.0460258, acc 1
2017-03-02T18:08:04.425958: step 27497, loss 0.0879318, acc 0.96875
2017-03-02T18:08:04.494831: step 27498, loss 0.0458481, acc 0.984375
2017-03-02T18:08:04.562670: step 27499, loss 0.288787, acc 0.890625
2017-03-02T18:08:04.634772: step 27500, loss 0.150656, acc 0.9375

Evaluation:
2017-03-02T18:08:04.671356: step 27500, loss 3.40819, acc 0.648162

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27500

2017-03-02T18:08:05.108428: step 27501, loss 0.0938022, acc 0.96875
2017-03-02T18:08:05.177490: step 27502, loss 0.280215, acc 0.875
2017-03-02T18:08:05.257737: step 27503, loss 0.196193, acc 0.90625
2017-03-02T18:08:05.330917: step 27504, loss 0.19023, acc 0.890625
2017-03-02T18:08:05.405016: step 27505, loss 0.163413, acc 0.90625
2017-03-02T18:08:05.474576: step 27506, loss 0.185068, acc 0.921875
2017-03-02T18:08:05.541398: step 27507, loss 0.11759, acc 0.953125
2017-03-02T18:08:05.617754: step 27508, loss 0.214468, acc 0.890625
2017-03-02T18:08:05.689774: step 27509, loss 0.144493, acc 0.953125
2017-03-02T18:08:05.759292: step 27510, loss 0.127384, acc 0.953125
2017-03-02T18:08:05.833885: step 27511, loss 0.102929, acc 0.96875
2017-03-02T18:08:05.914288: step 27512, loss 0.169376, acc 0.890625
2017-03-02T18:08:05.979626: step 27513, loss 0.265828, acc 0.90625
2017-03-02T18:08:06.054193: step 27514, loss 0.0657064, acc 0.96875
2017-03-02T18:08:06.125637: step 27515, loss 0.148374, acc 0.921875
2017-03-02T18:08:06.194229: step 27516, loss 0.0402227, acc 1
2017-03-02T18:08:06.273233: step 27517, loss 0.105731, acc 0.953125
2017-03-02T18:08:06.343187: step 27518, loss 0.124497, acc 0.9375
2017-03-02T18:08:06.409477: step 27519, loss 0.175749, acc 0.9375
2017-03-02T18:08:06.474432: step 27520, loss 0.237722, acc 0.875
2017-03-02T18:08:06.545019: step 27521, loss 0.246711, acc 0.875
2017-03-02T18:08:06.622940: step 27522, loss 0.188589, acc 0.921875
2017-03-02T18:08:06.691203: step 27523, loss 0.254567, acc 0.859375
2017-03-02T18:08:06.763106: step 27524, loss 0.0877398, acc 0.9375
2017-03-02T18:08:06.834564: step 27525, loss 0.204877, acc 0.90625
2017-03-02T18:08:06.904645: step 27526, loss 0.113598, acc 0.953125
2017-03-02T18:08:06.978310: step 27527, loss 0.206901, acc 0.921875
2017-03-02T18:08:07.051294: step 27528, loss 0.0787344, acc 0.96875
2017-03-02T18:08:07.120901: step 27529, loss 0.106582, acc 0.953125
2017-03-02T18:08:07.204543: step 27530, loss 0.217454, acc 0.890625
2017-03-02T18:08:07.280098: step 27531, loss 0.124255, acc 0.953125
2017-03-02T18:08:07.348281: step 27532, loss 0.131574, acc 0.984375
2017-03-02T18:08:07.423698: step 27533, loss 0.110673, acc 0.953125
2017-03-02T18:08:07.494405: step 27534, loss 0.271351, acc 0.921875
2017-03-02T18:08:07.565546: step 27535, loss 0.105647, acc 0.96875
2017-03-02T18:08:07.632782: step 27536, loss 0.152689, acc 0.90625
2017-03-02T18:08:07.699754: step 27537, loss 0.158057, acc 0.9375
2017-03-02T18:08:07.768522: step 27538, loss 0.120716, acc 0.953125
2017-03-02T18:08:07.853102: step 27539, loss 0.0898706, acc 0.96875
2017-03-02T18:08:07.927940: step 27540, loss 0.279634, acc 0.859375
2017-03-02T18:08:08.001658: step 27541, loss 0.150669, acc 0.90625
2017-03-02T18:08:08.068622: step 27542, loss 0.0681078, acc 0.984375
2017-03-02T18:08:08.150506: step 27543, loss 0.210139, acc 0.90625
2017-03-02T18:08:08.224825: step 27544, loss 0.205678, acc 0.859375
2017-03-02T18:08:08.292479: step 27545, loss 0.116801, acc 0.953125
2017-03-02T18:08:08.377982: step 27546, loss 0.234224, acc 0.9375
2017-03-02T18:08:08.455396: step 27547, loss 0.259282, acc 0.859375
2017-03-02T18:08:08.523667: step 27548, loss 0.185098, acc 0.921875
2017-03-02T18:08:08.603790: step 27549, loss 0.208948, acc 0.90625
2017-03-02T18:08:08.676359: step 27550, loss 0.128611, acc 0.96875
2017-03-02T18:08:08.741886: step 27551, loss 0.0938195, acc 0.96875
2017-03-02T18:08:08.813037: step 27552, loss 0.18091, acc 0.921875
2017-03-02T18:08:08.891107: step 27553, loss 0.149235, acc 0.890625
2017-03-02T18:08:08.954664: step 27554, loss 0.141807, acc 0.921875
2017-03-02T18:08:09.026571: step 27555, loss 0.220467, acc 0.921875
2017-03-02T18:08:09.090407: step 27556, loss 0.219752, acc 0.890625
2017-03-02T18:08:09.156965: step 27557, loss 0.255118, acc 0.890625
2017-03-02T18:08:09.224024: step 27558, loss 0.0901055, acc 0.921875
2017-03-02T18:08:09.298782: step 27559, loss 0.169982, acc 0.921875
2017-03-02T18:08:09.371464: step 27560, loss 0.111089, acc 0.953125
2017-03-02T18:08:09.451857: step 27561, loss 0.194547, acc 0.921875
2017-03-02T18:08:09.521531: step 27562, loss 0.148508, acc 0.9375
2017-03-02T18:08:09.592645: step 27563, loss 0.234712, acc 0.890625
2017-03-02T18:08:09.667630: step 27564, loss 0.185139, acc 0.9375
2017-03-02T18:08:09.746722: step 27565, loss 0.206238, acc 0.90625
2017-03-02T18:08:09.817296: step 27566, loss 0.0609634, acc 0.984375
2017-03-02T18:08:09.893345: step 27567, loss 0.195984, acc 0.90625
2017-03-02T18:08:09.971722: step 27568, loss 0.144708, acc 0.921875
2017-03-02T18:08:10.043787: step 27569, loss 0.164614, acc 0.9375
2017-03-02T18:08:10.120282: step 27570, loss 0.0720801, acc 0.96875
2017-03-02T18:08:10.192887: step 27571, loss 0.0982148, acc 0.96875
2017-03-02T18:08:10.261729: step 27572, loss 0.165899, acc 0.90625
2017-03-02T18:08:10.336813: step 27573, loss 0.160742, acc 0.921875
2017-03-02T18:08:10.407695: step 27574, loss 0.107979, acc 0.9375
2017-03-02T18:08:10.479617: step 27575, loss 0.165932, acc 0.921875
2017-03-02T18:08:10.550859: step 27576, loss 0.181431, acc 0.90625
2017-03-02T18:08:10.620178: step 27577, loss 0.0956539, acc 0.953125
2017-03-02T18:08:10.694307: step 27578, loss 0.145729, acc 0.90625
2017-03-02T18:08:10.768367: step 27579, loss 0.21615, acc 0.921875
2017-03-02T18:08:10.840790: step 27580, loss 0.165968, acc 0.890625
2017-03-02T18:08:10.911485: step 27581, loss 0.177908, acc 0.921875
2017-03-02T18:08:10.985335: step 27582, loss 0.146103, acc 0.96875
2017-03-02T18:08:11.055236: step 27583, loss 0.198798, acc 0.921875
2017-03-02T18:08:11.131236: step 27584, loss 0.200762, acc 0.890625
2017-03-02T18:08:11.209187: step 27585, loss 0.197765, acc 0.921875
2017-03-02T18:08:11.281690: step 27586, loss 0.180207, acc 0.890625
2017-03-02T18:08:11.355127: step 27587, loss 0.156535, acc 0.921875
2017-03-02T18:08:11.426400: step 27588, loss 0.105737, acc 0.9375
2017-03-02T18:08:11.502144: step 27589, loss 0.234535, acc 0.90625
2017-03-02T18:08:11.572031: step 27590, loss 0.127047, acc 0.953125
2017-03-02T18:08:11.643533: step 27591, loss 0.212564, acc 0.875
2017-03-02T18:08:11.719584: step 27592, loss 0.125371, acc 0.96875
2017-03-02T18:08:11.788248: step 27593, loss 0.162335, acc 0.953125
2017-03-02T18:08:11.865554: step 27594, loss 0.0637862, acc 0.96875
2017-03-02T18:08:11.942545: step 27595, loss 0.176688, acc 0.90625
2017-03-02T18:08:12.008458: step 27596, loss 0.0816338, acc 0.953125
2017-03-02T18:08:12.079757: step 27597, loss 0.121141, acc 0.921875
2017-03-02T18:08:12.149128: step 27598, loss 0.0829923, acc 0.96875
2017-03-02T18:08:12.216791: step 27599, loss 0.108629, acc 0.96875
2017-03-02T18:08:12.289390: step 27600, loss 0.162761, acc 0.890625

Evaluation:
2017-03-02T18:08:12.328998: step 27600, loss 3.37394, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27600

2017-03-02T18:08:12.801348: step 27601, loss 0.0571059, acc 0.96875
2017-03-02T18:08:12.870372: step 27602, loss 0.0740537, acc 0.96875
2017-03-02T18:08:12.954077: step 27603, loss 0.116504, acc 0.953125
2017-03-02T18:08:13.022837: step 27604, loss 0.115114, acc 0.96875
2017-03-02T18:08:13.099922: step 27605, loss 0.102291, acc 0.953125
2017-03-02T18:08:13.166499: step 27606, loss 0.123408, acc 0.953125
2017-03-02T18:08:13.239911: step 27607, loss 0.23605, acc 0.921875
2017-03-02T18:08:13.312362: step 27608, loss 0.140502, acc 0.921875
2017-03-02T18:08:13.381909: step 27609, loss 0.131679, acc 0.953125
2017-03-02T18:08:13.454511: step 27610, loss 0.15567, acc 0.90625
2017-03-02T18:08:13.527864: step 27611, loss 0.126962, acc 0.9375
2017-03-02T18:08:13.602776: step 27612, loss 0.150246, acc 0.9375
2017-03-02T18:08:13.670692: step 27613, loss 0.068792, acc 0.96875
2017-03-02T18:08:13.747277: step 27614, loss 0.0905844, acc 0.9375
2017-03-02T18:08:13.815611: step 27615, loss 0.149299, acc 0.921875
2017-03-02T18:08:13.890152: step 27616, loss 0.114484, acc 0.953125
2017-03-02T18:08:13.962315: step 27617, loss 0.193369, acc 0.921875
2017-03-02T18:08:14.036969: step 27618, loss 0.123607, acc 0.953125
2017-03-02T18:08:14.110875: step 27619, loss 0.233392, acc 0.9375
2017-03-02T18:08:14.184570: step 27620, loss 0.12431, acc 0.953125
2017-03-02T18:08:14.254849: step 27621, loss 0.067802, acc 0.96875
2017-03-02T18:08:14.341655: step 27622, loss 0.105571, acc 0.96875
2017-03-02T18:08:14.415056: step 27623, loss 0.0813047, acc 0.984375
2017-03-02T18:08:14.486100: step 27624, loss 0.11729, acc 0.921875
2017-03-02T18:08:14.551266: step 27625, loss 0.132038, acc 0.953125
2017-03-02T18:08:14.629193: step 27626, loss 0.312211, acc 0.859375
2017-03-02T18:08:14.713404: step 27627, loss 0.185832, acc 0.921875
2017-03-02T18:08:14.780562: step 27628, loss 0.0698927, acc 0.96875
2017-03-02T18:08:14.847563: step 27629, loss 0.114933, acc 0.9375
2017-03-02T18:08:14.920001: step 27630, loss 0.0978167, acc 0.96875
2017-03-02T18:08:14.989787: step 27631, loss 0.0877976, acc 0.953125
2017-03-02T18:08:15.063647: step 27632, loss 0.100519, acc 0.953125
2017-03-02T18:08:15.134868: step 27633, loss 0.248101, acc 0.875
2017-03-02T18:08:15.211060: step 27634, loss 0.149747, acc 0.921875
2017-03-02T18:08:15.281372: step 27635, loss 0.188511, acc 0.90625
2017-03-02T18:08:15.352798: step 27636, loss 0.0392988, acc 1
2017-03-02T18:08:15.431780: step 27637, loss 0.0406072, acc 0.984375
2017-03-02T18:08:15.505248: step 27638, loss 0.137225, acc 0.9375
2017-03-02T18:08:15.611882: step 27639, loss 0.118663, acc 0.9375
2017-03-02T18:08:15.678407: step 27640, loss 0.227316, acc 0.875
2017-03-02T18:08:15.749584: step 27641, loss 0.115224, acc 0.96875
2017-03-02T18:08:15.824413: step 27642, loss 0.150169, acc 0.953125
2017-03-02T18:08:15.892114: step 27643, loss 0.143373, acc 0.953125
2017-03-02T18:08:15.961655: step 27644, loss 0.14898, acc 0.90625
2017-03-02T18:08:16.037456: step 27645, loss 0.232864, acc 0.890625
2017-03-02T18:08:16.109968: step 27646, loss 0.0862815, acc 0.96875
2017-03-02T18:08:16.176587: step 27647, loss 0.133466, acc 0.90625
2017-03-02T18:08:16.251766: step 27648, loss 0.132895, acc 0.953125
2017-03-02T18:08:16.330918: step 27649, loss 0.195377, acc 0.921875
2017-03-02T18:08:16.407049: step 27650, loss 0.126448, acc 0.953125
2017-03-02T18:08:16.483983: step 27651, loss 0.19802, acc 0.90625
2017-03-02T18:08:16.557974: step 27652, loss 0.121987, acc 0.96875
2017-03-02T18:08:16.628476: step 27653, loss 0.139394, acc 0.9375
2017-03-02T18:08:16.704910: step 27654, loss 0.0959949, acc 0.984375
2017-03-02T18:08:16.780162: step 27655, loss 0.13808, acc 0.953125
2017-03-02T18:08:16.853846: step 27656, loss 0.179084, acc 0.921875
2017-03-02T18:08:16.927380: step 27657, loss 0.140569, acc 0.953125
2017-03-02T18:08:17.003853: step 27658, loss 0.127636, acc 0.953125
2017-03-02T18:08:17.074599: step 27659, loss 0.0515694, acc 0.96875
2017-03-02T18:08:17.142998: step 27660, loss 0.121382, acc 0.953125
2017-03-02T18:08:17.213221: step 27661, loss 0.174471, acc 0.921875
2017-03-02T18:08:17.286114: step 27662, loss 0.132808, acc 0.96875
2017-03-02T18:08:17.358053: step 27663, loss 0.182412, acc 0.90625
2017-03-02T18:08:17.430609: step 27664, loss 0.135143, acc 0.953125
2017-03-02T18:08:17.503440: step 27665, loss 0.0464179, acc 1
2017-03-02T18:08:17.570543: step 27666, loss 0.193909, acc 0.875
2017-03-02T18:08:17.644112: step 27667, loss 0.140137, acc 0.9375
2017-03-02T18:08:17.722054: step 27668, loss 0.141587, acc 0.9375
2017-03-02T18:08:17.794629: step 27669, loss 0.142151, acc 0.953125
2017-03-02T18:08:17.867240: step 27670, loss 0.207444, acc 0.890625
2017-03-02T18:08:17.940963: step 27671, loss 0.105724, acc 0.953125
2017-03-02T18:08:18.007881: step 27672, loss 0.098883, acc 0.9375
2017-03-02T18:08:18.078286: step 27673, loss 0.149902, acc 0.953125
2017-03-02T18:08:18.158399: step 27674, loss 0.112195, acc 0.953125
2017-03-02T18:08:18.225766: step 27675, loss 0.251711, acc 0.890625
2017-03-02T18:08:18.294075: step 27676, loss 0.115868, acc 0.953125
2017-03-02T18:08:18.371690: step 27677, loss 0.118764, acc 0.921875
2017-03-02T18:08:18.437790: step 27678, loss 0.123495, acc 0.9375
2017-03-02T18:08:18.510007: step 27679, loss 0.0954139, acc 0.953125
2017-03-02T18:08:18.590584: step 27680, loss 0.287315, acc 0.90625
2017-03-02T18:08:18.668198: step 27681, loss 0.121659, acc 0.96875
2017-03-02T18:08:18.748147: step 27682, loss 0.252197, acc 0.90625
2017-03-02T18:08:18.824786: step 27683, loss 0.167641, acc 0.921875
2017-03-02T18:08:18.892890: step 27684, loss 0.116207, acc 0.921875
2017-03-02T18:08:18.967587: step 27685, loss 0.102263, acc 0.9375
2017-03-02T18:08:19.031702: step 27686, loss 0.148673, acc 0.921875
2017-03-02T18:08:19.106164: step 27687, loss 0.130039, acc 0.953125
2017-03-02T18:08:19.177192: step 27688, loss 0.154643, acc 0.9375
2017-03-02T18:08:19.253416: step 27689, loss 0.160561, acc 0.921875
2017-03-02T18:08:19.324895: step 27690, loss 0.26883, acc 0.859375
2017-03-02T18:08:19.406060: step 27691, loss 0.211863, acc 0.890625
2017-03-02T18:08:19.472141: step 27692, loss 0.154205, acc 0.90625
2017-03-02T18:08:19.546188: step 27693, loss 0.12926, acc 0.90625
2017-03-02T18:08:19.610540: step 27694, loss 0.223519, acc 0.84375
2017-03-02T18:08:19.685282: step 27695, loss 0.16153, acc 0.96875
2017-03-02T18:08:19.756779: step 27696, loss 0.110634, acc 0.96875
2017-03-02T18:08:19.825475: step 27697, loss 0.130438, acc 0.921875
2017-03-02T18:08:19.904090: step 27698, loss 0.0840099, acc 0.953125
2017-03-02T18:08:19.964464: step 27699, loss 0.212257, acc 0.890625
2017-03-02T18:08:20.035674: step 27700, loss 0.14152, acc 0.9375

Evaluation:
2017-03-02T18:08:20.075112: step 27700, loss 3.43935, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27700

2017-03-02T18:08:20.530663: step 27701, loss 0.0718025, acc 0.96875
2017-03-02T18:08:20.601630: step 27702, loss 0.146344, acc 0.90625
2017-03-02T18:08:20.676919: step 27703, loss 0.159274, acc 0.9375
2017-03-02T18:08:20.749484: step 27704, loss 0.200845, acc 0.875
2017-03-02T18:08:20.826263: step 27705, loss 0.101274, acc 0.9375
2017-03-02T18:08:20.899375: step 27706, loss 0.206856, acc 0.875
2017-03-02T18:08:20.966910: step 27707, loss 0.204017, acc 0.90625
2017-03-02T18:08:21.035967: step 27708, loss 0.243297, acc 0.90625
2017-03-02T18:08:21.115605: step 27709, loss 0.0728455, acc 0.96875
2017-03-02T18:08:21.190006: step 27710, loss 0.0691901, acc 0.96875
2017-03-02T18:08:21.251730: step 27711, loss 0.130312, acc 0.953125
2017-03-02T18:08:21.324514: step 27712, loss 0.157644, acc 0.90625
2017-03-02T18:08:21.398526: step 27713, loss 0.227923, acc 0.90625
2017-03-02T18:08:21.469246: step 27714, loss 0.108224, acc 0.96875
2017-03-02T18:08:21.535806: step 27715, loss 0.0940942, acc 0.9375
2017-03-02T18:08:21.605633: step 27716, loss 0.137201, acc 0.9375
2017-03-02T18:08:21.674893: step 27717, loss 0.230802, acc 0.90625
2017-03-02T18:08:21.744466: step 27718, loss 0.150225, acc 0.9375
2017-03-02T18:08:21.818541: step 27719, loss 0.15488, acc 0.921875
2017-03-02T18:08:21.897877: step 27720, loss 0.161567, acc 0.921875
2017-03-02T18:08:21.969709: step 27721, loss 0.139734, acc 0.921875
2017-03-02T18:08:22.038143: step 27722, loss 0.224091, acc 0.890625
2017-03-02T18:08:22.118561: step 27723, loss 0.169223, acc 0.953125
2017-03-02T18:08:22.184313: step 27724, loss 0.0517454, acc 1
2017-03-02T18:08:22.255869: step 27725, loss 0.164786, acc 0.921875
2017-03-02T18:08:22.326244: step 27726, loss 0.159969, acc 0.921875
2017-03-02T18:08:22.402904: step 27727, loss 0.204011, acc 0.921875
2017-03-02T18:08:22.476640: step 27728, loss 0.130444, acc 0.9375
2017-03-02T18:08:22.550172: step 27729, loss 0.128061, acc 0.96875
2017-03-02T18:08:22.620621: step 27730, loss 0.239592, acc 0.921875
2017-03-02T18:08:22.695054: step 27731, loss 0.0682165, acc 0.96875
2017-03-02T18:08:22.768314: step 27732, loss 0.216939, acc 0.921875
2017-03-02T18:08:22.837152: step 27733, loss 0.259507, acc 0.875
2017-03-02T18:08:22.911940: step 27734, loss 0.214454, acc 0.90625
2017-03-02T18:08:22.991010: step 27735, loss 0.126893, acc 0.953125
2017-03-02T18:08:23.058853: step 27736, loss 0.134266, acc 0.9375
2017-03-02T18:08:23.130712: step 27737, loss 0.182133, acc 0.9375
2017-03-02T18:08:23.208117: step 27738, loss 0.163393, acc 0.90625
2017-03-02T18:08:23.276825: step 27739, loss 0.168245, acc 0.921875
2017-03-02T18:08:23.353188: step 27740, loss 0.0809584, acc 0.953125
2017-03-02T18:08:23.428130: step 27741, loss 0.202317, acc 0.890625
2017-03-02T18:08:23.501637: step 27742, loss 0.140767, acc 0.9375
2017-03-02T18:08:23.571785: step 27743, loss 0.118453, acc 0.9375
2017-03-02T18:08:23.650351: step 27744, loss 0.153461, acc 0.90625
2017-03-02T18:08:23.726125: step 27745, loss 0.0593771, acc 0.96875
2017-03-02T18:08:23.792900: step 27746, loss 0.141509, acc 0.921875
2017-03-02T18:08:23.870331: step 27747, loss 0.0865652, acc 0.984375
2017-03-02T18:08:23.935328: step 27748, loss 0.0898882, acc 0.953125
2017-03-02T18:08:24.003570: step 27749, loss 0.0955134, acc 0.953125
2017-03-02T18:08:24.078582: step 27750, loss 0.11307, acc 0.9375
2017-03-02T18:08:24.164920: step 27751, loss 0.226195, acc 0.90625
2017-03-02T18:08:24.237795: step 27752, loss 0.22755, acc 0.859375
2017-03-02T18:08:24.316496: step 27753, loss 0.199698, acc 0.890625
2017-03-02T18:08:24.390968: step 27754, loss 0.16205, acc 0.90625
2017-03-02T18:08:24.453414: step 27755, loss 0.183946, acc 0.90625
2017-03-02T18:08:24.527466: step 27756, loss 0.09313, acc 0.953125
2017-03-02T18:08:24.595698: step 27757, loss 0.0742884, acc 0.96875
2017-03-02T18:08:24.666947: step 27758, loss 0.0903405, acc 0.953125
2017-03-02T18:08:24.737143: step 27759, loss 0.115554, acc 0.953125
2017-03-02T18:08:24.815206: step 27760, loss 0.0659306, acc 0.984375
2017-03-02T18:08:24.895799: step 27761, loss 0.0922284, acc 0.953125
2017-03-02T18:08:24.964145: step 27762, loss 0.0885474, acc 0.984375
2017-03-02T18:08:25.037348: step 27763, loss 0.110934, acc 0.953125
2017-03-02T18:08:25.109759: step 27764, loss 0.0924828, acc 0.9375
2017-03-02T18:08:25.191164: step 27765, loss 0.198108, acc 0.90625
2017-03-02T18:08:25.262782: step 27766, loss 0.152237, acc 0.9375
2017-03-02T18:08:25.335855: step 27767, loss 0.088192, acc 0.984375
2017-03-02T18:08:25.414279: step 27768, loss 0.0659762, acc 0.953125
2017-03-02T18:08:25.483022: step 27769, loss 0.0765381, acc 0.953125
2017-03-02T18:08:25.565487: step 27770, loss 0.13012, acc 0.9375
2017-03-02T18:08:25.634402: step 27771, loss 0.138474, acc 0.953125
2017-03-02T18:08:25.708733: step 27772, loss 0.118959, acc 0.96875
2017-03-02T18:08:25.782683: step 27773, loss 0.135733, acc 0.96875
2017-03-02T18:08:25.852886: step 27774, loss 0.0601738, acc 0.96875
2017-03-02T18:08:25.919790: step 27775, loss 0.140217, acc 0.9375
2017-03-02T18:08:25.986303: step 27776, loss 0.102388, acc 0.96875
2017-03-02T18:08:26.056386: step 27777, loss 0.0974676, acc 0.953125
2017-03-02T18:08:26.125424: step 27778, loss 0.185396, acc 0.90625
2017-03-02T18:08:26.198123: step 27779, loss 0.163048, acc 0.953125
2017-03-02T18:08:26.273576: step 27780, loss 0.133082, acc 0.921875
2017-03-02T18:08:26.340309: step 27781, loss 0.285225, acc 0.859375
2017-03-02T18:08:26.433923: step 27782, loss 0.136943, acc 0.953125
2017-03-02T18:08:26.512732: step 27783, loss 0.0777172, acc 0.953125
2017-03-02T18:08:26.591520: step 27784, loss 0.142597, acc 0.9375
2017-03-02T18:08:26.666969: step 27785, loss 0.172123, acc 0.9375
2017-03-02T18:08:26.746137: step 27786, loss 0.278269, acc 0.921875
2017-03-02T18:08:26.817949: step 27787, loss 0.136071, acc 0.9375
2017-03-02T18:08:26.893763: step 27788, loss 0.117303, acc 0.953125
2017-03-02T18:08:26.966111: step 27789, loss 0.149121, acc 0.9375
2017-03-02T18:08:27.037543: step 27790, loss 0.198638, acc 0.890625
2017-03-02T18:08:27.111568: step 27791, loss 0.192325, acc 0.921875
2017-03-02T18:08:27.184163: step 27792, loss 0.151701, acc 0.9375
2017-03-02T18:08:27.258845: step 27793, loss 0.151206, acc 0.921875
2017-03-02T18:08:27.329880: step 27794, loss 0.160941, acc 0.9375
2017-03-02T18:08:27.398839: step 27795, loss 0.102955, acc 0.9375
2017-03-02T18:08:27.472571: step 27796, loss 0.214978, acc 0.890625
2017-03-02T18:08:27.547309: step 27797, loss 0.242579, acc 0.859375
2017-03-02T18:08:27.652996: step 27798, loss 0.161195, acc 0.9375
2017-03-02T18:08:27.729216: step 27799, loss 0.0625802, acc 0.953125
2017-03-02T18:08:27.797786: step 27800, loss 0.148985, acc 0.953125

Evaluation:
2017-03-02T18:08:27.834342: step 27800, loss 3.40764, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27800

2017-03-02T18:08:28.277355: step 27801, loss 0.081079, acc 0.984375
2017-03-02T18:08:28.347326: step 27802, loss 0.155428, acc 0.9375
2017-03-02T18:08:28.417190: step 27803, loss 0.135789, acc 0.9375
2017-03-02T18:08:28.490503: step 27804, loss 0.172066, acc 0.921875
2017-03-02T18:08:28.563889: step 27805, loss 0.10527, acc 0.984375
2017-03-02T18:08:28.633750: step 27806, loss 0.20884, acc 0.90625
2017-03-02T18:08:28.705416: step 27807, loss 0.138484, acc 0.90625
2017-03-02T18:08:28.779799: step 27808, loss 0.119776, acc 0.921875
2017-03-02T18:08:28.847372: step 27809, loss 0.0459349, acc 0.984375
2017-03-02T18:08:28.912841: step 27810, loss 0.0887664, acc 0.953125
2017-03-02T18:08:28.980243: step 27811, loss 0.153355, acc 0.921875
2017-03-02T18:08:29.055271: step 27812, loss 0.108109, acc 0.9375
2017-03-02T18:08:29.124953: step 27813, loss 0.111325, acc 0.96875
2017-03-02T18:08:29.200152: step 27814, loss 0.11983, acc 0.953125
2017-03-02T18:08:29.273787: step 27815, loss 0.108548, acc 0.953125
2017-03-02T18:08:29.354079: step 27816, loss 0.165181, acc 0.921875
2017-03-02T18:08:29.461485: step 27817, loss 0.124994, acc 0.953125
2017-03-02T18:08:29.544158: step 27818, loss 0.14201, acc 0.9375
2017-03-02T18:08:29.613102: step 27819, loss 0.100898, acc 0.984375
2017-03-02T18:08:29.681561: step 27820, loss 0.284341, acc 0.921875
2017-03-02T18:08:29.767004: step 27821, loss 0.21754, acc 0.90625
2017-03-02T18:08:29.838709: step 27822, loss 0.184167, acc 0.921875
2017-03-02T18:08:29.917446: step 27823, loss 0.142515, acc 0.90625
2017-03-02T18:08:29.987250: step 27824, loss 0.083478, acc 0.953125
2017-03-02T18:08:30.061928: step 27825, loss 0.169891, acc 0.90625
2017-03-02T18:08:30.134385: step 27826, loss 0.225096, acc 0.890625
2017-03-02T18:08:30.208791: step 27827, loss 0.115162, acc 0.921875
2017-03-02T18:08:30.280230: step 27828, loss 0.224253, acc 0.921875
2017-03-02T18:08:30.351870: step 27829, loss 0.103033, acc 0.9375
2017-03-02T18:08:30.429155: step 27830, loss 0.176098, acc 0.90625
2017-03-02T18:08:30.495673: step 27831, loss 0.253362, acc 0.890625
2017-03-02T18:08:30.598821: step 27832, loss 0.00158912, acc 1
2017-03-02T18:08:30.681506: step 27833, loss 0.105601, acc 0.953125
2017-03-02T18:08:30.748393: step 27834, loss 0.125349, acc 0.9375
2017-03-02T18:08:30.822667: step 27835, loss 0.294809, acc 0.84375
2017-03-02T18:08:30.896526: step 27836, loss 0.135372, acc 0.9375
2017-03-02T18:08:30.968309: step 27837, loss 0.133632, acc 0.921875
2017-03-02T18:08:31.042387: step 27838, loss 0.0794393, acc 0.96875
2017-03-02T18:08:31.119246: step 27839, loss 0.149817, acc 0.9375
2017-03-02T18:08:31.194938: step 27840, loss 0.0976927, acc 0.96875
2017-03-02T18:08:31.267905: step 27841, loss 0.131936, acc 0.9375
2017-03-02T18:08:31.339101: step 27842, loss 0.130474, acc 0.921875
2017-03-02T18:08:31.409138: step 27843, loss 0.108512, acc 0.96875
2017-03-02T18:08:31.477999: step 27844, loss 0.11149, acc 0.96875
2017-03-02T18:08:31.554007: step 27845, loss 0.0895286, acc 0.96875
2017-03-02T18:08:31.626483: step 27846, loss 0.118356, acc 0.96875
2017-03-02T18:08:31.698106: step 27847, loss 0.143481, acc 0.9375
2017-03-02T18:08:31.770388: step 27848, loss 0.130832, acc 0.953125
2017-03-02T18:08:31.840628: step 27849, loss 0.238382, acc 0.875
2017-03-02T18:08:31.905569: step 27850, loss 0.133097, acc 0.9375
2017-03-02T18:08:31.973604: step 27851, loss 0.128477, acc 0.96875
2017-03-02T18:08:32.042335: step 27852, loss 0.0273105, acc 0.984375
2017-03-02T18:08:32.111646: step 27853, loss 0.131843, acc 0.96875
2017-03-02T18:08:32.176768: step 27854, loss 0.17056, acc 0.921875
2017-03-02T18:08:32.242728: step 27855, loss 0.113609, acc 0.984375
2017-03-02T18:08:32.325211: step 27856, loss 0.131202, acc 0.9375
2017-03-02T18:08:32.391709: step 27857, loss 0.174364, acc 0.9375
2017-03-02T18:08:32.463513: step 27858, loss 0.157818, acc 0.9375
2017-03-02T18:08:32.538098: step 27859, loss 0.153274, acc 0.90625
2017-03-02T18:08:32.612940: step 27860, loss 0.185795, acc 0.921875
2017-03-02T18:08:32.682702: step 27861, loss 0.182996, acc 0.9375
2017-03-02T18:08:32.754459: step 27862, loss 0.078468, acc 0.96875
2017-03-02T18:08:32.823483: step 27863, loss 0.0979087, acc 0.9375
2017-03-02T18:08:32.893039: step 27864, loss 0.178355, acc 0.875
2017-03-02T18:08:32.966933: step 27865, loss 0.18144, acc 0.875
2017-03-02T18:08:33.032223: step 27866, loss 0.264116, acc 0.890625
2017-03-02T18:08:33.102046: step 27867, loss 0.136261, acc 0.9375
2017-03-02T18:08:33.170993: step 27868, loss 0.0807793, acc 0.984375
2017-03-02T18:08:33.247711: step 27869, loss 0.061295, acc 1
2017-03-02T18:08:33.317404: step 27870, loss 0.166093, acc 0.9375
2017-03-02T18:08:33.388352: step 27871, loss 0.178261, acc 0.890625
2017-03-02T18:08:33.468439: step 27872, loss 0.11202, acc 0.9375
2017-03-02T18:08:33.534089: step 27873, loss 0.167957, acc 0.921875
2017-03-02T18:08:33.611841: step 27874, loss 0.135145, acc 0.953125
2017-03-02T18:08:33.685964: step 27875, loss 0.213208, acc 0.890625
2017-03-02T18:08:33.757359: step 27876, loss 0.0866343, acc 0.96875
2017-03-02T18:08:33.832462: step 27877, loss 0.308202, acc 0.90625
2017-03-02T18:08:33.901658: step 27878, loss 0.199053, acc 0.921875
2017-03-02T18:08:33.976084: step 27879, loss 0.125706, acc 0.953125
2017-03-02T18:08:34.046118: step 27880, loss 0.171173, acc 0.9375
2017-03-02T18:08:34.121801: step 27881, loss 0.245806, acc 0.90625
2017-03-02T18:08:34.193988: step 27882, loss 0.101908, acc 0.953125
2017-03-02T18:08:34.267251: step 27883, loss 0.216472, acc 0.875
2017-03-02T18:08:34.346941: step 27884, loss 0.110659, acc 0.9375
2017-03-02T18:08:34.423451: step 27885, loss 0.123541, acc 0.96875
2017-03-02T18:08:34.493611: step 27886, loss 0.242939, acc 0.859375
2017-03-02T18:08:34.575714: step 27887, loss 0.170186, acc 0.921875
2017-03-02T18:08:34.646776: step 27888, loss 0.177321, acc 0.90625
2017-03-02T18:08:34.718185: step 27889, loss 0.165185, acc 0.921875
2017-03-02T18:08:34.787118: step 27890, loss 0.194562, acc 0.90625
2017-03-02T18:08:34.862815: step 27891, loss 0.10264, acc 0.9375
2017-03-02T18:08:34.926376: step 27892, loss 0.240183, acc 0.921875
2017-03-02T18:08:35.005295: step 27893, loss 0.182033, acc 0.921875
2017-03-02T18:08:35.078752: step 27894, loss 0.0638884, acc 0.984375
2017-03-02T18:08:35.156072: step 27895, loss 0.137431, acc 0.890625
2017-03-02T18:08:35.230081: step 27896, loss 0.185575, acc 0.921875
2017-03-02T18:08:35.297993: step 27897, loss 0.0957578, acc 0.96875
2017-03-02T18:08:35.371456: step 27898, loss 0.119381, acc 0.921875
2017-03-02T18:08:35.444596: step 27899, loss 0.08613, acc 0.96875
2017-03-02T18:08:35.514312: step 27900, loss 0.26157, acc 0.90625

Evaluation:
2017-03-02T18:08:35.538965: step 27900, loss 3.48242, acc 0.633742

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-27900

2017-03-02T18:08:35.986279: step 27901, loss 0.21681, acc 0.9375
2017-03-02T18:08:36.055373: step 27902, loss 0.199736, acc 0.9375
2017-03-02T18:08:36.120046: step 27903, loss 0.218597, acc 0.859375
2017-03-02T18:08:36.198949: step 27904, loss 0.0781456, acc 0.984375
2017-03-02T18:08:36.273710: step 27905, loss 0.140963, acc 0.953125
2017-03-02T18:08:36.338636: step 27906, loss 0.17758, acc 0.921875
2017-03-02T18:08:36.424162: step 27907, loss 0.107867, acc 0.921875
2017-03-02T18:08:36.493288: step 27908, loss 0.1542, acc 0.90625
2017-03-02T18:08:36.559337: step 27909, loss 0.0615842, acc 0.96875
2017-03-02T18:08:36.636156: step 27910, loss 0.105432, acc 0.953125
2017-03-02T18:08:36.710709: step 27911, loss 0.181101, acc 0.921875
2017-03-02T18:08:36.779214: step 27912, loss 0.146231, acc 0.921875
2017-03-02T18:08:36.854380: step 27913, loss 0.0527773, acc 1
2017-03-02T18:08:36.928343: step 27914, loss 0.0777931, acc 0.953125
2017-03-02T18:08:36.993772: step 27915, loss 0.112053, acc 0.96875
2017-03-02T18:08:37.074226: step 27916, loss 0.144928, acc 0.921875
2017-03-02T18:08:37.145626: step 27917, loss 0.112879, acc 0.9375
2017-03-02T18:08:37.230896: step 27918, loss 0.231826, acc 0.9375
2017-03-02T18:08:37.309127: step 27919, loss 0.10523, acc 0.9375
2017-03-02T18:08:37.384742: step 27920, loss 0.131342, acc 0.921875
2017-03-02T18:08:37.457047: step 27921, loss 0.101175, acc 0.953125
2017-03-02T18:08:37.537470: step 27922, loss 0.140492, acc 0.9375
2017-03-02T18:08:37.606738: step 27923, loss 0.168323, acc 0.953125
2017-03-02T18:08:37.676648: step 27924, loss 0.162874, acc 0.921875
2017-03-02T18:08:37.755360: step 27925, loss 0.272799, acc 0.890625
2017-03-02T18:08:37.826817: step 27926, loss 0.144533, acc 0.9375
2017-03-02T18:08:37.899612: step 27927, loss 0.27968, acc 0.828125
2017-03-02T18:08:37.972353: step 27928, loss 0.0438359, acc 0.984375
2017-03-02T18:08:38.044384: step 27929, loss 0.0663746, acc 0.984375
2017-03-02T18:08:38.111197: step 27930, loss 0.367065, acc 0.859375
2017-03-02T18:08:38.176342: step 27931, loss 0.169287, acc 0.953125
2017-03-02T18:08:38.257858: step 27932, loss 0.163275, acc 0.921875
2017-03-02T18:08:38.332068: step 27933, loss 0.190264, acc 0.921875
2017-03-02T18:08:38.395925: step 27934, loss 0.092633, acc 0.96875
2017-03-02T18:08:38.464571: step 27935, loss 0.136514, acc 0.9375
2017-03-02T18:08:38.539560: step 27936, loss 0.1554, acc 0.921875
2017-03-02T18:08:38.606317: step 27937, loss 0.241243, acc 0.890625
2017-03-02T18:08:38.680773: step 27938, loss 0.212524, acc 0.90625
2017-03-02T18:08:38.753577: step 27939, loss 0.195065, acc 0.921875
2017-03-02T18:08:38.822013: step 27940, loss 0.167191, acc 0.90625
2017-03-02T18:08:38.898003: step 27941, loss 0.0686101, acc 0.984375
2017-03-02T18:08:38.969954: step 27942, loss 0.108123, acc 0.9375
2017-03-02T18:08:39.037176: step 27943, loss 0.0797331, acc 1
2017-03-02T18:08:39.113800: step 27944, loss 0.0471523, acc 1
2017-03-02T18:08:39.195008: step 27945, loss 0.109732, acc 0.9375
2017-03-02T18:08:39.273505: step 27946, loss 0.0748019, acc 0.96875
2017-03-02T18:08:39.342460: step 27947, loss 0.221908, acc 0.90625
2017-03-02T18:08:39.413374: step 27948, loss 0.237619, acc 0.921875
2017-03-02T18:08:39.482695: step 27949, loss 0.111172, acc 0.9375
2017-03-02T18:08:39.546240: step 27950, loss 0.104517, acc 0.96875
2017-03-02T18:08:39.620152: step 27951, loss 0.186156, acc 0.9375
2017-03-02T18:08:39.692837: step 27952, loss 0.151987, acc 0.953125
2017-03-02T18:08:39.761519: step 27953, loss 0.207008, acc 0.90625
2017-03-02T18:08:39.825080: step 27954, loss 0.202797, acc 0.890625
2017-03-02T18:08:39.897203: step 27955, loss 0.110528, acc 0.921875
2017-03-02T18:08:39.969811: step 27956, loss 0.125917, acc 0.96875
2017-03-02T18:08:40.045685: step 27957, loss 0.185251, acc 0.890625
2017-03-02T18:08:40.123282: step 27958, loss 0.211681, acc 0.90625
2017-03-02T18:08:40.192089: step 27959, loss 0.102144, acc 0.953125
2017-03-02T18:08:40.263237: step 27960, loss 0.200609, acc 0.921875
2017-03-02T18:08:40.336045: step 27961, loss 0.115159, acc 0.953125
2017-03-02T18:08:40.401568: step 27962, loss 0.162968, acc 0.921875
2017-03-02T18:08:40.472980: step 27963, loss 0.0792552, acc 0.9375
2017-03-02T18:08:40.542212: step 27964, loss 0.0888121, acc 0.984375
2017-03-02T18:08:40.627492: step 27965, loss 0.191679, acc 0.921875
2017-03-02T18:08:40.697437: step 27966, loss 0.317138, acc 0.796875
2017-03-02T18:08:40.771512: step 27967, loss 0.179188, acc 0.90625
2017-03-02T18:08:40.850665: step 27968, loss 0.0817873, acc 0.96875
2017-03-02T18:08:40.922954: step 27969, loss 0.125129, acc 0.984375
2017-03-02T18:08:40.997682: step 27970, loss 0.153377, acc 0.9375
2017-03-02T18:08:41.070855: step 27971, loss 0.108178, acc 0.9375
2017-03-02T18:08:41.133688: step 27972, loss 0.111767, acc 0.9375
2017-03-02T18:08:41.202470: step 27973, loss 0.182815, acc 0.96875
2017-03-02T18:08:41.271622: step 27974, loss 0.222844, acc 0.890625
2017-03-02T18:08:41.346596: step 27975, loss 0.0870713, acc 0.953125
2017-03-02T18:08:41.412042: step 27976, loss 0.266289, acc 0.921875
2017-03-02T18:08:41.486327: step 27977, loss 0.0906219, acc 0.96875
2017-03-02T18:08:41.560281: step 27978, loss 0.0777817, acc 0.953125
2017-03-02T18:08:41.628993: step 27979, loss 0.107301, acc 0.9375
2017-03-02T18:08:41.704240: step 27980, loss 0.125491, acc 0.9375
2017-03-02T18:08:41.780958: step 27981, loss 0.12481, acc 0.921875
2017-03-02T18:08:41.847501: step 27982, loss 0.100517, acc 0.96875
2017-03-02T18:08:41.917152: step 27983, loss 0.308657, acc 0.859375
2017-03-02T18:08:41.990767: step 27984, loss 0.112286, acc 0.96875
2017-03-02T18:08:42.068385: step 27985, loss 0.0669961, acc 0.984375
2017-03-02T18:08:42.138723: step 27986, loss 0.0644007, acc 0.984375
2017-03-02T18:08:42.210822: step 27987, loss 0.0758487, acc 0.96875
2017-03-02T18:08:42.279148: step 27988, loss 0.114839, acc 0.9375
2017-03-02T18:08:42.350681: step 27989, loss 0.124229, acc 0.921875
2017-03-02T18:08:42.422656: step 27990, loss 0.15606, acc 0.96875
2017-03-02T18:08:42.494655: step 27991, loss 0.0637573, acc 0.984375
2017-03-02T18:08:42.553436: step 27992, loss 0.131994, acc 0.9375
2017-03-02T18:08:42.629135: step 27993, loss 0.165628, acc 0.921875
2017-03-02T18:08:42.708709: step 27994, loss 0.117373, acc 0.921875
2017-03-02T18:08:42.778605: step 27995, loss 0.111029, acc 0.953125
2017-03-02T18:08:42.855124: step 27996, loss 0.147727, acc 0.9375
2017-03-02T18:08:42.931078: step 27997, loss 0.161154, acc 0.921875
2017-03-02T18:08:43.001697: step 27998, loss 0.140001, acc 0.953125
2017-03-02T18:08:43.073162: step 27999, loss 0.0644133, acc 0.984375
2017-03-02T18:08:43.147643: step 28000, loss 0.0371162, acc 1

Evaluation:
2017-03-02T18:08:43.174091: step 28000, loss 3.65839, acc 0.651045

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28000

2017-03-02T18:08:43.632044: step 28001, loss 0.0969992, acc 0.9375
2017-03-02T18:08:43.706087: step 28002, loss 0.149184, acc 0.9375
2017-03-02T18:08:43.778311: step 28003, loss 0.18722, acc 0.921875
2017-03-02T18:08:43.854430: step 28004, loss 0.0925082, acc 0.953125
2017-03-02T18:08:43.955076: step 28005, loss 0.21078, acc 0.890625
2017-03-02T18:08:44.023364: step 28006, loss 0.39935, acc 0.828125
2017-03-02T18:08:44.091846: step 28007, loss 0.144679, acc 0.90625
2017-03-02T18:08:44.155405: step 28008, loss 0.158602, acc 0.9375
2017-03-02T18:08:44.217981: step 28009, loss 0.0735868, acc 0.984375
2017-03-02T18:08:44.298191: step 28010, loss 0.140571, acc 0.96875
2017-03-02T18:08:44.364427: step 28011, loss 0.069472, acc 0.953125
2017-03-02T18:08:44.432167: step 28012, loss 0.112333, acc 0.9375
2017-03-02T18:08:44.506373: step 28013, loss 0.298161, acc 0.875
2017-03-02T18:08:44.572696: step 28014, loss 0.207533, acc 0.890625
2017-03-02T18:08:44.647167: step 28015, loss 0.142237, acc 0.96875
2017-03-02T18:08:44.716041: step 28016, loss 0.194463, acc 0.9375
2017-03-02T18:08:44.778501: step 28017, loss 0.14007, acc 0.921875
2017-03-02T18:08:44.844006: step 28018, loss 0.186116, acc 0.9375
2017-03-02T18:08:44.911405: step 28019, loss 0.123807, acc 0.96875
2017-03-02T18:08:44.982667: step 28020, loss 0.107522, acc 0.953125
2017-03-02T18:08:45.052058: step 28021, loss 0.289313, acc 0.90625
2017-03-02T18:08:45.122016: step 28022, loss 0.173686, acc 0.90625
2017-03-02T18:08:45.197403: step 28023, loss 0.253752, acc 0.90625
2017-03-02T18:08:45.270082: step 28024, loss 0.140724, acc 0.90625
2017-03-02T18:08:45.360882: step 28025, loss 0.17357, acc 0.921875
2017-03-02T18:08:45.437216: step 28026, loss 0.0470965, acc 0.984375
2017-03-02T18:08:45.509607: step 28027, loss 0.0992372, acc 0.9375
2017-03-02T18:08:45.584530: step 28028, loss 0.197454, acc 1
2017-03-02T18:08:45.662807: step 28029, loss 0.167761, acc 0.921875
2017-03-02T18:08:45.741621: step 28030, loss 0.146857, acc 0.9375
2017-03-02T18:08:45.821015: step 28031, loss 0.0923834, acc 0.953125
2017-03-02T18:08:45.892969: step 28032, loss 0.201057, acc 0.921875
2017-03-02T18:08:45.961763: step 28033, loss 0.102938, acc 0.96875
2017-03-02T18:08:46.034007: step 28034, loss 0.0833315, acc 0.953125
2017-03-02T18:08:46.103908: step 28035, loss 0.0916232, acc 0.96875
2017-03-02T18:08:46.170943: step 28036, loss 0.104002, acc 0.9375
2017-03-02T18:08:46.241540: step 28037, loss 0.0844904, acc 0.96875
2017-03-02T18:08:46.305008: step 28038, loss 0.193069, acc 0.921875
2017-03-02T18:08:46.381048: step 28039, loss 0.251986, acc 0.890625
2017-03-02T18:08:46.453130: step 28040, loss 0.151965, acc 0.9375
2017-03-02T18:08:46.523220: step 28041, loss 0.303683, acc 0.859375
2017-03-02T18:08:46.607186: step 28042, loss 0.151022, acc 0.9375
2017-03-02T18:08:46.675379: step 28043, loss 0.210657, acc 0.90625
2017-03-02T18:08:46.749037: step 28044, loss 0.153939, acc 0.921875
2017-03-02T18:08:46.819451: step 28045, loss 0.187554, acc 0.90625
2017-03-02T18:08:46.896560: step 28046, loss 0.133239, acc 0.921875
2017-03-02T18:08:46.974675: step 28047, loss 0.169986, acc 0.9375
2017-03-02T18:08:47.042455: step 28048, loss 0.0957882, acc 0.953125
2017-03-02T18:08:47.111417: step 28049, loss 0.255244, acc 0.90625
2017-03-02T18:08:47.187366: step 28050, loss 0.102098, acc 0.96875
2017-03-02T18:08:47.266624: step 28051, loss 0.0834429, acc 0.9375
2017-03-02T18:08:47.341693: step 28052, loss 0.109536, acc 0.953125
2017-03-02T18:08:47.417350: step 28053, loss 0.136413, acc 0.9375
2017-03-02T18:08:47.489551: step 28054, loss 0.100456, acc 0.9375
2017-03-02T18:08:47.555269: step 28055, loss 0.174519, acc 0.90625
2017-03-02T18:08:47.630438: step 28056, loss 0.110985, acc 0.96875
2017-03-02T18:08:47.703418: step 28057, loss 0.106812, acc 0.953125
2017-03-02T18:08:47.774763: step 28058, loss 0.11784, acc 0.953125
2017-03-02T18:08:47.848484: step 28059, loss 0.154575, acc 0.9375
2017-03-02T18:08:47.919258: step 28060, loss 0.147505, acc 0.953125
2017-03-02T18:08:47.994411: step 28061, loss 0.135118, acc 0.953125
2017-03-02T18:08:48.060657: step 28062, loss 0.0843272, acc 0.953125
2017-03-02T18:08:48.129241: step 28063, loss 0.131904, acc 0.953125
2017-03-02T18:08:48.209269: step 28064, loss 0.0848146, acc 0.953125
2017-03-02T18:08:48.280958: step 28065, loss 0.218958, acc 0.90625
2017-03-02T18:08:48.363088: step 28066, loss 0.230342, acc 0.90625
2017-03-02T18:08:48.436229: step 28067, loss 0.072141, acc 0.96875
2017-03-02T18:08:48.503151: step 28068, loss 0.0916356, acc 0.9375
2017-03-02T18:08:48.591055: step 28069, loss 0.207704, acc 0.9375
2017-03-02T18:08:48.666875: step 28070, loss 0.119208, acc 0.9375
2017-03-02T18:08:48.735053: step 28071, loss 0.167776, acc 0.921875
2017-03-02T18:08:48.807859: step 28072, loss 0.172706, acc 0.90625
2017-03-02T18:08:48.898550: step 28073, loss 0.225223, acc 0.859375
2017-03-02T18:08:48.965926: step 28074, loss 0.181834, acc 0.921875
2017-03-02T18:08:49.036540: step 28075, loss 0.14114, acc 0.953125
2017-03-02T18:08:49.106435: step 28076, loss 0.15621, acc 0.90625
2017-03-02T18:08:49.168782: step 28077, loss 0.172148, acc 0.921875
2017-03-02T18:08:49.234335: step 28078, loss 0.0659255, acc 1
2017-03-02T18:08:49.307076: step 28079, loss 0.120918, acc 0.9375
2017-03-02T18:08:49.377078: step 28080, loss 0.176368, acc 0.953125
2017-03-02T18:08:49.452757: step 28081, loss 0.143747, acc 0.953125
2017-03-02T18:08:49.517647: step 28082, loss 0.0980555, acc 0.953125
2017-03-02T18:08:49.587069: step 28083, loss 0.0998621, acc 0.96875
2017-03-02T18:08:49.659464: step 28084, loss 0.190101, acc 0.9375
2017-03-02T18:08:49.739392: step 28085, loss 0.121344, acc 0.953125
2017-03-02T18:08:49.815885: step 28086, loss 0.141632, acc 0.921875
2017-03-02T18:08:49.884914: step 28087, loss 0.113853, acc 0.921875
2017-03-02T18:08:49.955348: step 28088, loss 0.109632, acc 0.953125
2017-03-02T18:08:50.030628: step 28089, loss 0.124848, acc 0.984375
2017-03-02T18:08:50.105557: step 28090, loss 0.185555, acc 0.921875
2017-03-02T18:08:50.181898: step 28091, loss 0.174508, acc 0.921875
2017-03-02T18:08:50.254872: step 28092, loss 0.2602, acc 0.859375
2017-03-02T18:08:50.327852: step 28093, loss 0.131218, acc 0.921875
2017-03-02T18:08:50.402594: step 28094, loss 0.135601, acc 0.9375
2017-03-02T18:08:50.476371: step 28095, loss 0.0839244, acc 0.953125
2017-03-02T18:08:50.552522: step 28096, loss 0.118401, acc 0.953125
2017-03-02T18:08:50.627761: step 28097, loss 0.111994, acc 0.9375
2017-03-02T18:08:50.683394: step 28098, loss 0.214529, acc 0.9375
2017-03-02T18:08:50.760728: step 28099, loss 0.0793557, acc 0.953125
2017-03-02T18:08:50.826511: step 28100, loss 0.136553, acc 0.96875

Evaluation:
2017-03-02T18:08:50.858769: step 28100, loss 3.68369, acc 0.648882

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28100

2017-03-02T18:08:51.292660: step 28101, loss 0.175563, acc 0.921875
2017-03-02T18:08:51.365566: step 28102, loss 0.132271, acc 0.9375
2017-03-02T18:08:51.440687: step 28103, loss 0.0761999, acc 0.984375
2017-03-02T18:08:51.507157: step 28104, loss 0.106009, acc 0.953125
2017-03-02T18:08:51.580152: step 28105, loss 0.0686919, acc 0.96875
2017-03-02T18:08:51.645242: step 28106, loss 0.154923, acc 0.9375
2017-03-02T18:08:51.721380: step 28107, loss 0.156915, acc 0.9375
2017-03-02T18:08:51.794235: step 28108, loss 0.247331, acc 0.921875
2017-03-02T18:08:51.871526: step 28109, loss 0.0428643, acc 0.984375
2017-03-02T18:08:51.940797: step 28110, loss 0.258777, acc 0.890625
2017-03-02T18:08:52.010947: step 28111, loss 0.0566397, acc 0.984375
2017-03-02T18:08:52.089494: step 28112, loss 0.0919972, acc 0.96875
2017-03-02T18:08:52.165111: step 28113, loss 0.113223, acc 0.96875
2017-03-02T18:08:52.239584: step 28114, loss 0.145384, acc 0.9375
2017-03-02T18:08:52.315783: step 28115, loss 0.233723, acc 0.890625
2017-03-02T18:08:52.385542: step 28116, loss 0.220618, acc 0.875
2017-03-02T18:08:52.457888: step 28117, loss 0.0702773, acc 0.96875
2017-03-02T18:08:52.533865: step 28118, loss 0.0983206, acc 0.984375
2017-03-02T18:08:52.598115: step 28119, loss 0.0838651, acc 0.96875
2017-03-02T18:08:52.671877: step 28120, loss 0.166814, acc 0.90625
2017-03-02T18:08:52.745011: step 28121, loss 0.117973, acc 0.96875
2017-03-02T18:08:52.815907: step 28122, loss 0.227978, acc 0.875
2017-03-02T18:08:52.881927: step 28123, loss 0.114206, acc 0.953125
2017-03-02T18:08:52.961998: step 28124, loss 0.137252, acc 0.921875
2017-03-02T18:08:53.033240: step 28125, loss 0.184097, acc 0.921875
2017-03-02T18:08:53.109151: step 28126, loss 0.128021, acc 0.921875
2017-03-02T18:08:53.184473: step 28127, loss 0.0346149, acc 1
2017-03-02T18:08:53.259982: step 28128, loss 0.125459, acc 0.953125
2017-03-02T18:08:53.328434: step 28129, loss 0.13038, acc 0.9375
2017-03-02T18:08:53.403115: step 28130, loss 0.192246, acc 0.9375
2017-03-02T18:08:53.472148: step 28131, loss 0.167941, acc 0.890625
2017-03-02T18:08:53.539029: step 28132, loss 0.10502, acc 0.984375
2017-03-02T18:08:53.609523: step 28133, loss 0.303972, acc 0.890625
2017-03-02T18:08:53.686862: step 28134, loss 0.10973, acc 0.96875
2017-03-02T18:08:53.767359: step 28135, loss 0.105641, acc 0.953125
2017-03-02T18:08:53.842327: step 28136, loss 0.133589, acc 0.96875
2017-03-02T18:08:53.911772: step 28137, loss 0.0995733, acc 0.953125
2017-03-02T18:08:53.990948: step 28138, loss 0.0791874, acc 0.984375
2017-03-02T18:08:54.068738: step 28139, loss 0.252318, acc 0.921875
2017-03-02T18:08:54.149928: step 28140, loss 0.23575, acc 0.90625
2017-03-02T18:08:54.224657: step 28141, loss 0.110562, acc 0.953125
2017-03-02T18:08:54.290290: step 28142, loss 0.0516583, acc 0.984375
2017-03-02T18:08:54.370900: step 28143, loss 0.11962, acc 0.9375
2017-03-02T18:08:54.434561: step 28144, loss 0.117047, acc 0.953125
2017-03-02T18:08:54.513144: step 28145, loss 0.126011, acc 0.953125
2017-03-02T18:08:54.592975: step 28146, loss 0.141381, acc 0.921875
2017-03-02T18:08:54.664839: step 28147, loss 0.0889886, acc 0.953125
2017-03-02T18:08:54.735534: step 28148, loss 0.204949, acc 0.90625
2017-03-02T18:08:54.815361: step 28149, loss 0.175091, acc 0.921875
2017-03-02T18:08:54.886479: step 28150, loss 0.101855, acc 0.9375
2017-03-02T18:08:54.946506: step 28151, loss 0.165628, acc 0.9375
2017-03-02T18:08:55.019928: step 28152, loss 0.161829, acc 0.953125
2017-03-02T18:08:55.092712: step 28153, loss 0.0764259, acc 0.96875
2017-03-02T18:08:55.159136: step 28154, loss 0.193182, acc 0.921875
2017-03-02T18:08:55.220007: step 28155, loss 0.165446, acc 0.90625
2017-03-02T18:08:55.288547: step 28156, loss 0.0660013, acc 0.96875
2017-03-02T18:08:55.364133: step 28157, loss 0.107646, acc 0.9375
2017-03-02T18:08:55.433165: step 28158, loss 0.157481, acc 0.9375
2017-03-02T18:08:55.504458: step 28159, loss 0.135621, acc 0.9375
2017-03-02T18:08:55.576483: step 28160, loss 0.0770504, acc 0.953125
2017-03-02T18:08:55.642744: step 28161, loss 0.0885381, acc 0.953125
2017-03-02T18:08:55.706876: step 28162, loss 0.115431, acc 0.953125
2017-03-02T18:08:55.781912: step 28163, loss 0.238994, acc 0.90625
2017-03-02T18:08:55.863883: step 28164, loss 0.0936925, acc 0.953125
2017-03-02T18:08:55.932035: step 28165, loss 0.0927373, acc 0.953125
2017-03-02T18:08:55.999413: step 28166, loss 0.215143, acc 0.90625
2017-03-02T18:08:56.068896: step 28167, loss 0.134403, acc 0.90625
2017-03-02T18:08:56.146909: step 28168, loss 0.186287, acc 0.90625
2017-03-02T18:08:56.222413: step 28169, loss 0.145987, acc 0.953125
2017-03-02T18:08:56.295265: step 28170, loss 0.127296, acc 0.9375
2017-03-02T18:08:56.380193: step 28171, loss 0.159944, acc 0.921875
2017-03-02T18:08:56.453771: step 28172, loss 0.132455, acc 0.9375
2017-03-02T18:08:56.525842: step 28173, loss 0.13776, acc 0.953125
2017-03-02T18:08:56.601887: step 28174, loss 0.206781, acc 0.921875
2017-03-02T18:08:56.669236: step 28175, loss 0.281184, acc 0.90625
2017-03-02T18:08:56.742341: step 28176, loss 0.17292, acc 0.890625
2017-03-02T18:08:56.816347: step 28177, loss 0.0968227, acc 0.953125
2017-03-02T18:08:56.889320: step 28178, loss 0.185537, acc 0.90625
2017-03-02T18:08:56.958677: step 28179, loss 0.175575, acc 0.921875
2017-03-02T18:08:57.029995: step 28180, loss 0.302621, acc 0.921875
2017-03-02T18:08:57.100636: step 28181, loss 0.142296, acc 0.9375
2017-03-02T18:08:57.172850: step 28182, loss 0.15884, acc 0.921875
2017-03-02T18:08:57.253052: step 28183, loss 0.115323, acc 0.9375
2017-03-02T18:08:57.319998: step 28184, loss 0.185441, acc 0.875
2017-03-02T18:08:57.390714: step 28185, loss 0.157763, acc 0.921875
2017-03-02T18:08:57.463138: step 28186, loss 0.0850237, acc 0.953125
2017-03-02T18:08:57.536798: step 28187, loss 0.163245, acc 0.9375
2017-03-02T18:08:57.635291: step 28188, loss 0.201724, acc 0.90625
2017-03-02T18:08:57.710100: step 28189, loss 0.212572, acc 0.90625
2017-03-02T18:08:57.780848: step 28190, loss 0.208705, acc 0.90625
2017-03-02T18:08:57.852219: step 28191, loss 0.112753, acc 0.9375
2017-03-02T18:08:57.925628: step 28192, loss 0.0768418, acc 0.96875
2017-03-02T18:08:57.998804: step 28193, loss 0.111413, acc 0.9375
2017-03-02T18:08:58.079991: step 28194, loss 0.142265, acc 0.921875
2017-03-02T18:08:58.153299: step 28195, loss 0.156599, acc 0.921875
2017-03-02T18:08:58.233938: step 28196, loss 0.0825843, acc 0.96875
2017-03-02T18:08:58.305737: step 28197, loss 0.231323, acc 0.90625
2017-03-02T18:08:58.375888: step 28198, loss 0.0859345, acc 0.984375
2017-03-02T18:08:58.448247: step 28199, loss 0.146244, acc 0.90625
2017-03-02T18:08:58.515354: step 28200, loss 0.196547, acc 0.9375

Evaluation:
2017-03-02T18:08:58.549716: step 28200, loss 3.66932, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28200

2017-03-02T18:08:59.007039: step 28201, loss 0.200288, acc 0.953125
2017-03-02T18:08:59.070351: step 28202, loss 0.300999, acc 0.859375
2017-03-02T18:08:59.143349: step 28203, loss 0.111637, acc 0.96875
2017-03-02T18:08:59.211643: step 28204, loss 0.0814827, acc 0.96875
2017-03-02T18:08:59.286972: step 28205, loss 0.139957, acc 0.90625
2017-03-02T18:08:59.367255: step 28206, loss 0.203621, acc 0.90625
2017-03-02T18:08:59.440713: step 28207, loss 0.275689, acc 0.875
2017-03-02T18:08:59.514491: step 28208, loss 0.112369, acc 0.9375
2017-03-02T18:08:59.583280: step 28209, loss 0.104741, acc 0.96875
2017-03-02T18:08:59.657161: step 28210, loss 0.130359, acc 0.9375
2017-03-02T18:08:59.730661: step 28211, loss 0.126589, acc 0.90625
2017-03-02T18:08:59.809800: step 28212, loss 0.144845, acc 0.953125
2017-03-02T18:08:59.881469: step 28213, loss 0.0464735, acc 0.984375
2017-03-02T18:08:59.957449: step 28214, loss 0.087721, acc 0.96875
2017-03-02T18:09:00.028801: step 28215, loss 0.165039, acc 0.921875
2017-03-02T18:09:00.100141: step 28216, loss 0.238385, acc 0.921875
2017-03-02T18:09:00.173346: step 28217, loss 0.101451, acc 0.96875
2017-03-02T18:09:00.252418: step 28218, loss 0.266478, acc 0.859375
2017-03-02T18:09:00.320009: step 28219, loss 0.0841286, acc 0.96875
2017-03-02T18:09:00.389717: step 28220, loss 0.326056, acc 0.875
2017-03-02T18:09:00.465074: step 28221, loss 0.113256, acc 0.953125
2017-03-02T18:09:00.537612: step 28222, loss 0.102373, acc 0.953125
2017-03-02T18:09:00.613242: step 28223, loss 0.175271, acc 0.9375
2017-03-02T18:09:00.682253: step 28224, loss 0.00111198, acc 1
2017-03-02T18:09:00.761377: step 28225, loss 0.145062, acc 0.9375
2017-03-02T18:09:00.834920: step 28226, loss 0.135226, acc 0.9375
2017-03-02T18:09:00.911636: step 28227, loss 0.106307, acc 0.984375
2017-03-02T18:09:00.989571: step 28228, loss 0.145731, acc 0.921875
2017-03-02T18:09:01.067309: step 28229, loss 0.1768, acc 0.890625
2017-03-02T18:09:01.142865: step 28230, loss 0.102031, acc 0.984375
2017-03-02T18:09:01.214751: step 28231, loss 0.177687, acc 0.90625
2017-03-02T18:09:01.284754: step 28232, loss 0.12741, acc 0.9375
2017-03-02T18:09:01.359706: step 28233, loss 0.184336, acc 0.921875
2017-03-02T18:09:01.442385: step 28234, loss 0.239679, acc 0.875
2017-03-02T18:09:01.509815: step 28235, loss 0.126963, acc 0.96875
2017-03-02T18:09:01.588634: step 28236, loss 0.191602, acc 0.90625
2017-03-02T18:09:01.665146: step 28237, loss 0.0792595, acc 1
2017-03-02T18:09:01.748209: step 28238, loss 0.0904686, acc 0.96875
2017-03-02T18:09:01.820726: step 28239, loss 0.112309, acc 0.953125
2017-03-02T18:09:01.892788: step 28240, loss 0.11998, acc 0.96875
2017-03-02T18:09:01.959856: step 28241, loss 0.111143, acc 0.953125
2017-03-02T18:09:02.031774: step 28242, loss 0.168091, acc 0.90625
2017-03-02T18:09:02.102873: step 28243, loss 0.0923448, acc 0.96875
2017-03-02T18:09:02.174774: step 28244, loss 0.0834414, acc 0.96875
2017-03-02T18:09:02.247575: step 28245, loss 0.16118, acc 0.921875
2017-03-02T18:09:02.325946: step 28246, loss 0.0581431, acc 0.984375
2017-03-02T18:09:02.401620: step 28247, loss 0.168383, acc 0.921875
2017-03-02T18:09:02.474462: step 28248, loss 0.102151, acc 0.921875
2017-03-02T18:09:02.547798: step 28249, loss 0.181487, acc 0.90625
2017-03-02T18:09:02.622376: step 28250, loss 0.0456894, acc 1
2017-03-02T18:09:02.693021: step 28251, loss 0.138257, acc 0.953125
2017-03-02T18:09:02.771158: step 28252, loss 0.155167, acc 0.953125
2017-03-02T18:09:02.852529: step 28253, loss 0.189472, acc 0.90625
2017-03-02T18:09:02.927487: step 28254, loss 0.144515, acc 0.90625
2017-03-02T18:09:02.998226: step 28255, loss 0.104518, acc 0.953125
2017-03-02T18:09:03.076912: step 28256, loss 0.125894, acc 0.96875
2017-03-02T18:09:03.147652: step 28257, loss 0.121916, acc 0.9375
2017-03-02T18:09:03.219487: step 28258, loss 0.459254, acc 0.875
2017-03-02T18:09:03.297855: step 28259, loss 0.0757278, acc 0.953125
2017-03-02T18:09:03.377590: step 28260, loss 0.145431, acc 0.953125
2017-03-02T18:09:03.458671: step 28261, loss 0.0704548, acc 0.984375
2017-03-02T18:09:03.531907: step 28262, loss 0.0620338, acc 1
2017-03-02T18:09:03.604083: step 28263, loss 0.155661, acc 0.9375
2017-03-02T18:09:03.678772: step 28264, loss 0.108563, acc 0.953125
2017-03-02T18:09:03.754947: step 28265, loss 0.149648, acc 0.90625
2017-03-02T18:09:03.830383: step 28266, loss 0.106485, acc 0.96875
2017-03-02T18:09:03.906557: step 28267, loss 0.214183, acc 0.90625
2017-03-02T18:09:03.977684: step 28268, loss 0.205739, acc 0.859375
2017-03-02T18:09:04.049137: step 28269, loss 0.134672, acc 0.9375
2017-03-02T18:09:04.132671: step 28270, loss 0.175832, acc 0.890625
2017-03-02T18:09:04.204450: step 28271, loss 0.158274, acc 0.9375
2017-03-02T18:09:04.287050: step 28272, loss 0.0886691, acc 0.953125
2017-03-02T18:09:04.364669: step 28273, loss 0.106203, acc 0.921875
2017-03-02T18:09:04.423906: step 28274, loss 0.17319, acc 0.890625
2017-03-02T18:09:04.489671: step 28275, loss 0.13187, acc 0.921875
2017-03-02T18:09:04.556317: step 28276, loss 0.0461369, acc 0.96875
2017-03-02T18:09:04.625603: step 28277, loss 0.165558, acc 0.90625
2017-03-02T18:09:04.703710: step 28278, loss 0.166137, acc 0.921875
2017-03-02T18:09:04.770984: step 28279, loss 0.174993, acc 0.921875
2017-03-02T18:09:04.844734: step 28280, loss 0.119778, acc 0.953125
2017-03-02T18:09:04.918386: step 28281, loss 0.221658, acc 0.859375
2017-03-02T18:09:04.985509: step 28282, loss 0.138127, acc 0.953125
2017-03-02T18:09:05.061347: step 28283, loss 0.110958, acc 0.96875
2017-03-02T18:09:05.141806: step 28284, loss 0.127983, acc 0.921875
2017-03-02T18:09:05.205936: step 28285, loss 0.116982, acc 0.921875
2017-03-02T18:09:05.288569: step 28286, loss 0.206004, acc 0.890625
2017-03-02T18:09:05.362155: step 28287, loss 0.127835, acc 0.9375
2017-03-02T18:09:05.432924: step 28288, loss 0.148244, acc 0.9375
2017-03-02T18:09:05.505608: step 28289, loss 0.176129, acc 0.90625
2017-03-02T18:09:05.580195: step 28290, loss 0.153111, acc 0.9375
2017-03-02T18:09:05.654048: step 28291, loss 0.0908775, acc 0.96875
2017-03-02T18:09:05.724645: step 28292, loss 0.245764, acc 0.890625
2017-03-02T18:09:05.794806: step 28293, loss 0.154489, acc 0.953125
2017-03-02T18:09:05.874259: step 28294, loss 0.173606, acc 0.9375
2017-03-02T18:09:05.945923: step 28295, loss 0.142701, acc 0.90625
2017-03-02T18:09:06.023764: step 28296, loss 0.0750081, acc 0.953125
2017-03-02T18:09:06.096346: step 28297, loss 0.180976, acc 0.921875
2017-03-02T18:09:06.164606: step 28298, loss 0.100292, acc 0.9375
2017-03-02T18:09:06.237916: step 28299, loss 0.177835, acc 0.90625
2017-03-02T18:09:06.317336: step 28300, loss 0.137224, acc 0.9375

Evaluation:
2017-03-02T18:09:06.348221: step 28300, loss 3.65325, acc 0.635184

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28300

2017-03-02T18:09:06.811888: step 28301, loss 0.118401, acc 0.953125
2017-03-02T18:09:06.893259: step 28302, loss 0.0983292, acc 0.96875
2017-03-02T18:09:06.964890: step 28303, loss 0.250177, acc 0.921875
2017-03-02T18:09:07.046495: step 28304, loss 0.118151, acc 0.921875
2017-03-02T18:09:07.125693: step 28305, loss 0.168951, acc 0.953125
2017-03-02T18:09:07.194906: step 28306, loss 0.0445669, acc 0.984375
2017-03-02T18:09:07.261257: step 28307, loss 0.157408, acc 0.90625
2017-03-02T18:09:07.332444: step 28308, loss 0.0570264, acc 0.984375
2017-03-02T18:09:07.407426: step 28309, loss 0.13697, acc 0.90625
2017-03-02T18:09:07.478928: step 28310, loss 0.152009, acc 0.9375
2017-03-02T18:09:07.554929: step 28311, loss 0.149939, acc 0.921875
2017-03-02T18:09:07.625377: step 28312, loss 0.149765, acc 0.90625
2017-03-02T18:09:07.694772: step 28313, loss 0.164112, acc 0.90625
2017-03-02T18:09:07.764921: step 28314, loss 0.107801, acc 0.9375
2017-03-02T18:09:07.839717: step 28315, loss 0.104055, acc 0.953125
2017-03-02T18:09:07.914308: step 28316, loss 0.194068, acc 0.90625
2017-03-02T18:09:07.979924: step 28317, loss 0.172771, acc 0.90625
2017-03-02T18:09:08.049821: step 28318, loss 0.0946148, acc 0.921875
2017-03-02T18:09:08.119754: step 28319, loss 0.255644, acc 0.859375
2017-03-02T18:09:08.184716: step 28320, loss 0.149202, acc 0.9375
2017-03-02T18:09:08.252605: step 28321, loss 0.101981, acc 0.9375
2017-03-02T18:09:08.324804: step 28322, loss 0.115692, acc 1
2017-03-02T18:09:08.390399: step 28323, loss 0.0757217, acc 0.953125
2017-03-02T18:09:08.461836: step 28324, loss 0.174735, acc 0.921875
2017-03-02T18:09:08.534236: step 28325, loss 0.143199, acc 0.9375
2017-03-02T18:09:08.606305: step 28326, loss 0.122306, acc 0.921875
2017-03-02T18:09:08.671864: step 28327, loss 0.247147, acc 0.890625
2017-03-02T18:09:08.744852: step 28328, loss 0.0651559, acc 0.96875
2017-03-02T18:09:08.824214: step 28329, loss 0.202618, acc 0.9375
2017-03-02T18:09:08.890102: step 28330, loss 0.139956, acc 0.953125
2017-03-02T18:09:08.961100: step 28331, loss 0.176974, acc 0.921875
2017-03-02T18:09:09.041287: step 28332, loss 0.0953541, acc 0.9375
2017-03-02T18:09:09.106443: step 28333, loss 0.139255, acc 0.890625
2017-03-02T18:09:09.178189: step 28334, loss 0.365794, acc 0.859375
2017-03-02T18:09:09.241943: step 28335, loss 0.213124, acc 0.921875
2017-03-02T18:09:09.314110: step 28336, loss 0.198793, acc 0.890625
2017-03-02T18:09:09.413364: step 28337, loss 0.1757, acc 0.953125
2017-03-02T18:09:09.485003: step 28338, loss 0.145583, acc 0.921875
2017-03-02T18:09:09.555354: step 28339, loss 0.0971, acc 0.9375
2017-03-02T18:09:09.620395: step 28340, loss 0.0876569, acc 0.9375
2017-03-02T18:09:09.697399: step 28341, loss 0.186236, acc 0.90625
2017-03-02T18:09:09.770671: step 28342, loss 0.218405, acc 0.90625
2017-03-02T18:09:09.844847: step 28343, loss 0.0678446, acc 0.953125
2017-03-02T18:09:09.917229: step 28344, loss 0.044182, acc 0.984375
2017-03-02T18:09:09.999304: step 28345, loss 0.221455, acc 0.875
2017-03-02T18:09:10.076295: step 28346, loss 0.174831, acc 0.90625
2017-03-02T18:09:10.146445: step 28347, loss 0.156594, acc 0.90625
2017-03-02T18:09:10.219906: step 28348, loss 0.160994, acc 0.90625
2017-03-02T18:09:10.289673: step 28349, loss 0.276919, acc 0.84375
2017-03-02T18:09:10.366247: step 28350, loss 0.085295, acc 0.953125
2017-03-02T18:09:10.440942: step 28351, loss 0.186914, acc 0.9375
2017-03-02T18:09:10.510829: step 28352, loss 0.0477206, acc 0.984375
2017-03-02T18:09:10.584998: step 28353, loss 0.163219, acc 0.90625
2017-03-02T18:09:10.658259: step 28354, loss 0.198864, acc 0.9375
2017-03-02T18:09:10.737761: step 28355, loss 0.0928835, acc 0.953125
2017-03-02T18:09:10.814750: step 28356, loss 0.131639, acc 0.96875
2017-03-02T18:09:10.886840: step 28357, loss 0.189083, acc 0.90625
2017-03-02T18:09:10.952881: step 28358, loss 0.140372, acc 0.90625
2017-03-02T18:09:11.021539: step 28359, loss 0.125201, acc 0.96875
2017-03-02T18:09:11.091917: step 28360, loss 0.25271, acc 0.859375
2017-03-02T18:09:11.165582: step 28361, loss 0.351394, acc 0.84375
2017-03-02T18:09:11.229343: step 28362, loss 0.241081, acc 0.875
2017-03-02T18:09:11.296749: step 28363, loss 0.164306, acc 0.90625
2017-03-02T18:09:11.378103: step 28364, loss 0.0659217, acc 0.96875
2017-03-02T18:09:11.448502: step 28365, loss 0.122824, acc 0.9375
2017-03-02T18:09:11.523112: step 28366, loss 0.140674, acc 0.96875
2017-03-02T18:09:11.591793: step 28367, loss 0.122782, acc 0.96875
2017-03-02T18:09:11.662803: step 28368, loss 0.133348, acc 0.9375
2017-03-02T18:09:11.746636: step 28369, loss 0.104075, acc 0.953125
2017-03-02T18:09:11.821136: step 28370, loss 0.112839, acc 0.953125
2017-03-02T18:09:11.892251: step 28371, loss 0.1133, acc 0.953125
2017-03-02T18:09:11.961978: step 28372, loss 0.121433, acc 0.953125
2017-03-02T18:09:12.036919: step 28373, loss 0.100934, acc 0.953125
2017-03-02T18:09:12.113838: step 28374, loss 0.1533, acc 0.953125
2017-03-02T18:09:12.182235: step 28375, loss 0.130261, acc 0.9375
2017-03-02T18:09:12.262369: step 28376, loss 0.172337, acc 0.90625
2017-03-02T18:09:12.336176: step 28377, loss 0.11737, acc 0.9375
2017-03-02T18:09:12.411881: step 28378, loss 0.343903, acc 0.84375
2017-03-02T18:09:12.480800: step 28379, loss 0.15245, acc 0.9375
2017-03-02T18:09:12.560712: step 28380, loss 0.178003, acc 0.9375
2017-03-02T18:09:12.627877: step 28381, loss 0.136101, acc 0.9375
2017-03-02T18:09:12.710812: step 28382, loss 0.121228, acc 0.96875
2017-03-02T18:09:12.787706: step 28383, loss 0.0808535, acc 0.953125
2017-03-02T18:09:12.862742: step 28384, loss 0.0837704, acc 0.953125
2017-03-02T18:09:12.930881: step 28385, loss 0.202381, acc 0.90625
2017-03-02T18:09:13.007094: step 28386, loss 0.115493, acc 0.9375
2017-03-02T18:09:13.075036: step 28387, loss 0.145186, acc 0.921875
2017-03-02T18:09:13.141206: step 28388, loss 0.133072, acc 0.9375
2017-03-02T18:09:13.215284: step 28389, loss 0.183367, acc 0.921875
2017-03-02T18:09:13.297855: step 28390, loss 0.20774, acc 0.9375
2017-03-02T18:09:13.368287: step 28391, loss 0.115968, acc 0.96875
2017-03-02T18:09:13.442675: step 28392, loss 0.265535, acc 0.875
2017-03-02T18:09:13.515037: step 28393, loss 0.0811117, acc 0.96875
2017-03-02T18:09:13.583458: step 28394, loss 0.268592, acc 0.875
2017-03-02T18:09:13.659066: step 28395, loss 0.174283, acc 0.90625
2017-03-02T18:09:13.732703: step 28396, loss 0.193916, acc 0.921875
2017-03-02T18:09:13.800436: step 28397, loss 0.0897105, acc 0.953125
2017-03-02T18:09:13.882673: step 28398, loss 0.183197, acc 0.90625
2017-03-02T18:09:13.959474: step 28399, loss 0.190747, acc 0.921875
2017-03-02T18:09:14.032874: step 28400, loss 0.125105, acc 0.90625

Evaluation:
2017-03-02T18:09:14.069340: step 28400, loss 3.63615, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28400

2017-03-02T18:09:14.524561: step 28401, loss 0.11764, acc 0.9375
2017-03-02T18:09:14.599787: step 28402, loss 0.114306, acc 0.96875
2017-03-02T18:09:14.669283: step 28403, loss 0.117165, acc 0.921875
2017-03-02T18:09:14.740217: step 28404, loss 0.134797, acc 0.9375
2017-03-02T18:09:14.813745: step 28405, loss 0.222839, acc 0.921875
2017-03-02T18:09:14.884157: step 28406, loss 0.0669021, acc 0.984375
2017-03-02T18:09:14.952842: step 28407, loss 0.110726, acc 0.953125
2017-03-02T18:09:15.027455: step 28408, loss 0.178171, acc 0.921875
2017-03-02T18:09:15.101971: step 28409, loss 0.132342, acc 0.9375
2017-03-02T18:09:15.170794: step 28410, loss 0.320715, acc 0.890625
2017-03-02T18:09:15.242122: step 28411, loss 0.0684532, acc 0.984375
2017-03-02T18:09:15.318156: step 28412, loss 0.0881061, acc 0.96875
2017-03-02T18:09:15.395029: step 28413, loss 0.0773077, acc 0.96875
2017-03-02T18:09:15.462938: step 28414, loss 0.239172, acc 0.890625
2017-03-02T18:09:15.535047: step 28415, loss 0.11233, acc 0.953125
2017-03-02T18:09:15.602653: step 28416, loss 0.102773, acc 0.953125
2017-03-02T18:09:15.671709: step 28417, loss 0.151082, acc 0.90625
2017-03-02T18:09:15.734469: step 28418, loss 0.142575, acc 0.9375
2017-03-02T18:09:15.816503: step 28419, loss 0.153419, acc 0.9375
2017-03-02T18:09:15.882113: step 28420, loss 0.00858719, acc 1
2017-03-02T18:09:15.959795: step 28421, loss 0.060499, acc 0.984375
2017-03-02T18:09:16.035948: step 28422, loss 0.0630489, acc 0.96875
2017-03-02T18:09:16.111028: step 28423, loss 0.155761, acc 0.921875
2017-03-02T18:09:16.182537: step 28424, loss 0.095569, acc 0.96875
2017-03-02T18:09:16.264133: step 28425, loss 0.18108, acc 0.90625
2017-03-02T18:09:16.339230: step 28426, loss 0.133766, acc 0.9375
2017-03-02T18:09:16.410576: step 28427, loss 0.157693, acc 0.890625
2017-03-02T18:09:16.488724: step 28428, loss 0.165714, acc 0.90625
2017-03-02T18:09:16.558655: step 28429, loss 0.0638441, acc 0.984375
2017-03-02T18:09:16.626699: step 28430, loss 0.0695017, acc 0.984375
2017-03-02T18:09:16.694941: step 28431, loss 0.129646, acc 0.953125
2017-03-02T18:09:16.774725: step 28432, loss 0.0532307, acc 0.984375
2017-03-02T18:09:16.854223: step 28433, loss 0.179636, acc 0.875
2017-03-02T18:09:16.926531: step 28434, loss 0.167848, acc 0.9375
2017-03-02T18:09:16.999214: step 28435, loss 0.0676981, acc 0.953125
2017-03-02T18:09:17.075106: step 28436, loss 0.088066, acc 0.984375
2017-03-02T18:09:17.155528: step 28437, loss 0.204614, acc 0.90625
2017-03-02T18:09:17.225086: step 28438, loss 0.226975, acc 0.90625
2017-03-02T18:09:17.288569: step 28439, loss 0.228929, acc 0.875
2017-03-02T18:09:17.368423: step 28440, loss 0.140044, acc 0.953125
2017-03-02T18:09:17.450497: step 28441, loss 0.108622, acc 0.9375
2017-03-02T18:09:17.514039: step 28442, loss 0.0899771, acc 0.96875
2017-03-02T18:09:17.586945: step 28443, loss 0.0950701, acc 0.96875
2017-03-02T18:09:17.662417: step 28444, loss 0.172993, acc 0.890625
2017-03-02T18:09:17.730059: step 28445, loss 0.0530427, acc 0.984375
2017-03-02T18:09:17.807889: step 28446, loss 0.142016, acc 0.9375
2017-03-02T18:09:17.883482: step 28447, loss 0.205865, acc 0.90625
2017-03-02T18:09:17.966373: step 28448, loss 0.106633, acc 0.9375
2017-03-02T18:09:18.039924: step 28449, loss 0.235405, acc 0.859375
2017-03-02T18:09:18.109273: step 28450, loss 0.140308, acc 0.9375
2017-03-02T18:09:18.173901: step 28451, loss 0.17954, acc 0.921875
2017-03-02T18:09:18.243235: step 28452, loss 0.156134, acc 0.9375
2017-03-02T18:09:18.315272: step 28453, loss 0.122675, acc 0.921875
2017-03-02T18:09:18.390086: step 28454, loss 0.150083, acc 0.921875
2017-03-02T18:09:18.453089: step 28455, loss 0.0694969, acc 0.96875
2017-03-02T18:09:18.528885: step 28456, loss 0.195292, acc 0.9375
2017-03-02T18:09:18.602367: step 28457, loss 0.150104, acc 0.953125
2017-03-02T18:09:18.674436: step 28458, loss 0.0551105, acc 0.96875
2017-03-02T18:09:18.747334: step 28459, loss 0.172975, acc 0.90625
2017-03-02T18:09:18.823645: step 28460, loss 0.126412, acc 0.9375
2017-03-02T18:09:18.891265: step 28461, loss 0.14183, acc 0.921875
2017-03-02T18:09:18.965173: step 28462, loss 0.161352, acc 0.921875
2017-03-02T18:09:19.037800: step 28463, loss 0.200476, acc 0.9375
2017-03-02T18:09:19.110899: step 28464, loss 0.0937827, acc 0.953125
2017-03-02T18:09:19.213717: step 28465, loss 0.0878106, acc 0.953125
2017-03-02T18:09:19.288010: step 28466, loss 0.114271, acc 0.9375
2017-03-02T18:09:19.359548: step 28467, loss 0.165969, acc 0.90625
2017-03-02T18:09:19.432622: step 28468, loss 0.228035, acc 0.90625
2017-03-02T18:09:19.510374: step 28469, loss 0.126277, acc 0.921875
2017-03-02T18:09:19.577455: step 28470, loss 0.153522, acc 0.921875
2017-03-02T18:09:19.649077: step 28471, loss 0.0695657, acc 0.984375
2017-03-02T18:09:19.720219: step 28472, loss 0.170106, acc 0.90625
2017-03-02T18:09:19.793825: step 28473, loss 0.197027, acc 0.890625
2017-03-02T18:09:19.858920: step 28474, loss 0.106568, acc 0.96875
2017-03-02T18:09:19.927278: step 28475, loss 0.19753, acc 0.90625
2017-03-02T18:09:20.005134: step 28476, loss 0.0917012, acc 0.96875
2017-03-02T18:09:20.075182: step 28477, loss 0.139033, acc 0.921875
2017-03-02T18:09:20.148713: step 28478, loss 0.169591, acc 0.921875
2017-03-02T18:09:20.221217: step 28479, loss 0.084832, acc 0.96875
2017-03-02T18:09:20.289677: step 28480, loss 0.24387, acc 0.859375
2017-03-02T18:09:20.363293: step 28481, loss 0.224815, acc 0.890625
2017-03-02T18:09:20.437464: step 28482, loss 0.0722073, acc 0.96875
2017-03-02T18:09:20.511961: step 28483, loss 0.189102, acc 0.953125
2017-03-02T18:09:20.586974: step 28484, loss 0.232297, acc 0.859375
2017-03-02T18:09:20.660047: step 28485, loss 0.135807, acc 0.96875
2017-03-02T18:09:20.730268: step 28486, loss 0.19537, acc 0.90625
2017-03-02T18:09:20.798143: step 28487, loss 0.181208, acc 0.9375
2017-03-02T18:09:20.883295: step 28488, loss 0.226768, acc 0.890625
2017-03-02T18:09:20.954806: step 28489, loss 0.128595, acc 0.953125
2017-03-02T18:09:21.021880: step 28490, loss 0.17145, acc 0.90625
2017-03-02T18:09:21.094650: step 28491, loss 0.147185, acc 0.921875
2017-03-02T18:09:21.165848: step 28492, loss 0.0220757, acc 1
2017-03-02T18:09:21.231459: step 28493, loss 0.177902, acc 0.9375
2017-03-02T18:09:21.304621: step 28494, loss 0.179708, acc 0.921875
2017-03-02T18:09:21.383012: step 28495, loss 0.147665, acc 0.9375
2017-03-02T18:09:21.456101: step 28496, loss 0.140778, acc 0.921875
2017-03-02T18:09:21.533439: step 28497, loss 0.240293, acc 0.9375
2017-03-02T18:09:21.600579: step 28498, loss 0.188131, acc 0.890625
2017-03-02T18:09:21.669751: step 28499, loss 0.0915369, acc 0.984375
2017-03-02T18:09:21.743270: step 28500, loss 0.154032, acc 0.9375

Evaluation:
2017-03-02T18:09:21.777957: step 28500, loss 3.64142, acc 0.6323

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28500

2017-03-02T18:09:22.212703: step 28501, loss 0.157205, acc 0.9375
2017-03-02T18:09:22.293258: step 28502, loss 0.24863, acc 0.890625
2017-03-02T18:09:22.369730: step 28503, loss 0.0751986, acc 0.96875
2017-03-02T18:09:22.438077: step 28504, loss 0.179499, acc 0.90625
2017-03-02T18:09:22.513644: step 28505, loss 0.219618, acc 0.875
2017-03-02T18:09:22.599624: step 28506, loss 0.225425, acc 0.90625
2017-03-02T18:09:22.675025: step 28507, loss 0.0129108, acc 1
2017-03-02T18:09:22.747357: step 28508, loss 0.213419, acc 0.890625
2017-03-02T18:09:22.818770: step 28509, loss 0.0924769, acc 0.9375
2017-03-02T18:09:22.886550: step 28510, loss 0.10943, acc 0.921875
2017-03-02T18:09:22.964216: step 28511, loss 0.167092, acc 0.90625
2017-03-02T18:09:23.038783: step 28512, loss 0.0861522, acc 0.96875
2017-03-02T18:09:23.112248: step 28513, loss 0.0521174, acc 0.984375
2017-03-02T18:09:23.196907: step 28514, loss 0.103374, acc 0.953125
2017-03-02T18:09:23.272627: step 28515, loss 0.220707, acc 0.96875
2017-03-02T18:09:23.339853: step 28516, loss 0.168585, acc 0.953125
2017-03-02T18:09:23.417057: step 28517, loss 0.139824, acc 0.921875
2017-03-02T18:09:23.495986: step 28518, loss 0.117065, acc 0.953125
2017-03-02T18:09:23.560192: step 28519, loss 0.156161, acc 0.90625
2017-03-02T18:09:23.633679: step 28520, loss 0.191192, acc 0.953125
2017-03-02T18:09:23.711626: step 28521, loss 0.0896752, acc 0.953125
2017-03-02T18:09:23.779638: step 28522, loss 0.134157, acc 0.921875
2017-03-02T18:09:23.853858: step 28523, loss 0.185734, acc 0.921875
2017-03-02T18:09:23.928645: step 28524, loss 0.275056, acc 0.859375
2017-03-02T18:09:23.992448: step 28525, loss 0.112021, acc 0.953125
2017-03-02T18:09:24.068391: step 28526, loss 0.11038, acc 0.9375
2017-03-02T18:09:24.145285: step 28527, loss 0.0555233, acc 0.984375
2017-03-02T18:09:24.210739: step 28528, loss 0.0827906, acc 0.953125
2017-03-02T18:09:24.278830: step 28529, loss 0.214089, acc 0.875
2017-03-02T18:09:24.353914: step 28530, loss 0.13574, acc 0.953125
2017-03-02T18:09:24.424770: step 28531, loss 0.114482, acc 0.96875
2017-03-02T18:09:24.493469: step 28532, loss 0.0883466, acc 0.9375
2017-03-02T18:09:24.568879: step 28533, loss 0.08858, acc 0.953125
2017-03-02T18:09:24.659954: step 28534, loss 0.164019, acc 0.90625
2017-03-02T18:09:24.728518: step 28535, loss 0.101311, acc 0.953125
2017-03-02T18:09:24.798389: step 28536, loss 0.0908434, acc 0.953125
2017-03-02T18:09:24.869099: step 28537, loss 0.133414, acc 0.953125
2017-03-02T18:09:24.931307: step 28538, loss 0.138922, acc 0.984375
2017-03-02T18:09:25.005242: step 28539, loss 0.214887, acc 0.921875
2017-03-02T18:09:25.090026: step 28540, loss 0.167293, acc 0.9375
2017-03-02T18:09:25.157362: step 28541, loss 0.100299, acc 0.96875
2017-03-02T18:09:25.231148: step 28542, loss 0.15231, acc 0.953125
2017-03-02T18:09:25.297406: step 28543, loss 0.123501, acc 0.9375
2017-03-02T18:09:25.377053: step 28544, loss 0.267581, acc 0.890625
2017-03-02T18:09:25.459988: step 28545, loss 0.164879, acc 0.9375
2017-03-02T18:09:25.537359: step 28546, loss 0.149872, acc 0.921875
2017-03-02T18:09:25.605138: step 28547, loss 0.0104827, acc 1
2017-03-02T18:09:25.676943: step 28548, loss 0.11649, acc 0.96875
2017-03-02T18:09:25.745963: step 28549, loss 0.215351, acc 0.84375
2017-03-02T18:09:25.817874: step 28550, loss 0.156231, acc 0.921875
2017-03-02T18:09:25.890609: step 28551, loss 0.108374, acc 0.921875
2017-03-02T18:09:25.961164: step 28552, loss 0.110673, acc 0.96875
2017-03-02T18:09:26.037481: step 28553, loss 0.150508, acc 0.953125
2017-03-02T18:09:26.104294: step 28554, loss 0.164657, acc 0.9375
2017-03-02T18:09:26.179253: step 28555, loss 0.21836, acc 0.875
2017-03-02T18:09:26.278825: step 28556, loss 0.160665, acc 0.953125
2017-03-02T18:09:26.351313: step 28557, loss 0.0786075, acc 0.953125
2017-03-02T18:09:26.428964: step 28558, loss 0.0941828, acc 0.9375
2017-03-02T18:09:26.503902: step 28559, loss 0.194264, acc 0.921875
2017-03-02T18:09:26.571645: step 28560, loss 0.0758827, acc 0.984375
2017-03-02T18:09:26.644644: step 28561, loss 0.186246, acc 0.890625
2017-03-02T18:09:26.719272: step 28562, loss 0.157968, acc 0.90625
2017-03-02T18:09:26.791984: step 28563, loss 0.144606, acc 0.9375
2017-03-02T18:09:26.871927: step 28564, loss 0.0765638, acc 0.953125
2017-03-02T18:09:26.946222: step 28565, loss 0.269462, acc 0.84375
2017-03-02T18:09:27.013796: step 28566, loss 0.14542, acc 0.9375
2017-03-02T18:09:27.080691: step 28567, loss 0.124698, acc 0.96875
2017-03-02T18:09:27.150714: step 28568, loss 0.124442, acc 0.9375
2017-03-02T18:09:27.221397: step 28569, loss 0.104643, acc 0.953125
2017-03-02T18:09:27.291022: step 28570, loss 0.0186158, acc 1
2017-03-02T18:09:27.360230: step 28571, loss 0.0925739, acc 0.9375
2017-03-02T18:09:27.432905: step 28572, loss 0.156211, acc 0.921875
2017-03-02T18:09:27.501183: step 28573, loss 0.16329, acc 0.921875
2017-03-02T18:09:27.568305: step 28574, loss 0.25782, acc 0.875
2017-03-02T18:09:27.641975: step 28575, loss 0.225474, acc 0.90625
2017-03-02T18:09:27.723521: step 28576, loss 0.164821, acc 0.890625
2017-03-02T18:09:27.791209: step 28577, loss 0.158765, acc 0.9375
2017-03-02T18:09:27.864960: step 28578, loss 0.19218, acc 0.9375
2017-03-02T18:09:27.938057: step 28579, loss 0.150881, acc 0.90625
2017-03-02T18:09:28.006026: step 28580, loss 0.10402, acc 0.96875
2017-03-02T18:09:28.079631: step 28581, loss 0.163792, acc 0.953125
2017-03-02T18:09:28.147797: step 28582, loss 0.0829768, acc 0.96875
2017-03-02T18:09:28.218856: step 28583, loss 0.163764, acc 0.9375
2017-03-02T18:09:28.290605: step 28584, loss 0.0787375, acc 0.96875
2017-03-02T18:09:28.364920: step 28585, loss 0.202668, acc 0.921875
2017-03-02T18:09:28.433327: step 28586, loss 0.234493, acc 0.890625
2017-03-02T18:09:28.499935: step 28587, loss 0.181071, acc 0.921875
2017-03-02T18:09:28.570163: step 28588, loss 0.158039, acc 0.921875
2017-03-02T18:09:28.652628: step 28589, loss 0.0653502, acc 0.984375
2017-03-02T18:09:28.726178: step 28590, loss 0.283424, acc 0.875
2017-03-02T18:09:28.794311: step 28591, loss 0.0853876, acc 0.953125
2017-03-02T18:09:28.867793: step 28592, loss 0.141747, acc 0.953125
2017-03-02T18:09:28.936614: step 28593, loss 0.13529, acc 0.9375
2017-03-02T18:09:29.009795: step 28594, loss 0.108374, acc 0.953125
2017-03-02T18:09:29.081024: step 28595, loss 0.164713, acc 0.9375
2017-03-02T18:09:29.152326: step 28596, loss 0.202798, acc 0.90625
2017-03-02T18:09:29.225897: step 28597, loss 0.155057, acc 0.90625
2017-03-02T18:09:29.298000: step 28598, loss 0.0779019, acc 0.984375
2017-03-02T18:09:29.366827: step 28599, loss 0.142208, acc 0.953125
2017-03-02T18:09:29.438136: step 28600, loss 0.187854, acc 0.921875

Evaluation:
2017-03-02T18:09:29.472248: step 28600, loss 3.63414, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28600

2017-03-02T18:09:29.928488: step 28601, loss 0.146803, acc 0.921875
2017-03-02T18:09:30.007285: step 28602, loss 0.0880715, acc 0.9375
2017-03-02T18:09:30.085629: step 28603, loss 0.281012, acc 0.875
2017-03-02T18:09:30.159769: step 28604, loss 0.115979, acc 0.953125
2017-03-02T18:09:30.237639: step 28605, loss 0.158966, acc 0.9375
2017-03-02T18:09:30.304963: step 28606, loss 0.298202, acc 0.90625
2017-03-02T18:09:30.375540: step 28607, loss 0.164803, acc 0.90625
2017-03-02T18:09:30.448093: step 28608, loss 0.221199, acc 0.921875
2017-03-02T18:09:30.521768: step 28609, loss 0.172478, acc 0.90625
2017-03-02T18:09:30.588599: step 28610, loss 0.0587395, acc 0.96875
2017-03-02T18:09:30.655406: step 28611, loss 0.111455, acc 0.9375
2017-03-02T18:09:30.735880: step 28612, loss 0.156155, acc 0.9375
2017-03-02T18:09:30.799706: step 28613, loss 0.186864, acc 0.9375
2017-03-02T18:09:30.873173: step 28614, loss 0.213707, acc 0.90625
2017-03-02T18:09:30.949044: step 28615, loss 0.160293, acc 0.921875
2017-03-02T18:09:31.012877: step 28616, loss 0.52196, acc 0.75
2017-03-02T18:09:31.095247: step 28617, loss 0.175319, acc 0.921875
2017-03-02T18:09:31.169439: step 28618, loss 0.12845, acc 0.953125
2017-03-02T18:09:31.238090: step 28619, loss 0.144953, acc 0.9375
2017-03-02T18:09:31.306128: step 28620, loss 0.0601484, acc 0.984375
2017-03-02T18:09:31.378544: step 28621, loss 0.150252, acc 0.9375
2017-03-02T18:09:31.658469: step 28622, loss 0.253619, acc 0.9375
2017-03-02T18:09:31.722894: step 28623, loss 0.185751, acc 0.921875
2017-03-02T18:09:31.794094: step 28624, loss 0.216643, acc 0.921875
2017-03-02T18:09:31.863989: step 28625, loss 0.0598338, acc 0.984375
2017-03-02T18:09:31.937160: step 28626, loss 0.148441, acc 0.890625
2017-03-02T18:09:32.010074: step 28627, loss 0.167576, acc 0.953125
2017-03-02T18:09:32.086784: step 28628, loss 0.186892, acc 0.921875
2017-03-02T18:09:32.162251: step 28629, loss 0.114845, acc 0.9375
2017-03-02T18:09:32.243743: step 28630, loss 0.240228, acc 0.875
2017-03-02T18:09:32.324940: step 28631, loss 0.193588, acc 0.921875
2017-03-02T18:09:32.391228: step 28632, loss 0.106051, acc 0.953125
2017-03-02T18:09:32.466322: step 28633, loss 0.0949829, acc 0.953125
2017-03-02T18:09:32.537170: step 28634, loss 0.0918788, acc 0.9375
2017-03-02T18:09:32.606104: step 28635, loss 0.128069, acc 0.890625
2017-03-02T18:09:32.676571: step 28636, loss 0.197198, acc 0.875
2017-03-02T18:09:32.750086: step 28637, loss 0.125548, acc 0.953125
2017-03-02T18:09:32.826235: step 28638, loss 0.210442, acc 0.921875
2017-03-02T18:09:32.898098: step 28639, loss 0.150501, acc 0.9375
2017-03-02T18:09:32.976971: step 28640, loss 0.205754, acc 0.90625
2017-03-02T18:09:33.051257: step 28641, loss 0.202771, acc 0.890625
2017-03-02T18:09:33.122960: step 28642, loss 0.195073, acc 0.921875
2017-03-02T18:09:33.198258: step 28643, loss 0.118158, acc 0.921875
2017-03-02T18:09:33.271482: step 28644, loss 0.0588811, acc 0.96875
2017-03-02T18:09:33.339039: step 28645, loss 0.0727917, acc 0.96875
2017-03-02T18:09:33.412235: step 28646, loss 0.168346, acc 0.9375
2017-03-02T18:09:33.492976: step 28647, loss 0.132205, acc 0.953125
2017-03-02T18:09:33.558659: step 28648, loss 0.060576, acc 0.984375
2017-03-02T18:09:33.628605: step 28649, loss 0.110678, acc 0.9375
2017-03-02T18:09:33.697990: step 28650, loss 0.0814184, acc 0.953125
2017-03-02T18:09:33.772250: step 28651, loss 0.132631, acc 0.953125
2017-03-02T18:09:33.848320: step 28652, loss 0.129084, acc 0.96875
2017-03-02T18:09:33.921204: step 28653, loss 0.0946861, acc 0.96875
2017-03-02T18:09:34.004358: step 28654, loss 0.0787614, acc 0.96875
2017-03-02T18:09:34.073362: step 28655, loss 0.0572, acc 0.96875
2017-03-02T18:09:34.147553: step 28656, loss 0.2552, acc 0.875
2017-03-02T18:09:34.223569: step 28657, loss 0.0698306, acc 0.984375
2017-03-02T18:09:34.294856: step 28658, loss 0.10938, acc 0.953125
2017-03-02T18:09:34.372040: step 28659, loss 0.0903587, acc 0.9375
2017-03-02T18:09:34.450057: step 28660, loss 0.163305, acc 0.90625
2017-03-02T18:09:34.529760: step 28661, loss 0.0892151, acc 0.96875
2017-03-02T18:09:34.602081: step 28662, loss 0.112241, acc 0.9375
2017-03-02T18:09:34.671414: step 28663, loss 0.171716, acc 0.90625
2017-03-02T18:09:34.744041: step 28664, loss 0.203575, acc 0.875
2017-03-02T18:09:34.815116: step 28665, loss 0.069232, acc 0.9375
2017-03-02T18:09:34.906859: step 28666, loss 0.15243, acc 0.9375
2017-03-02T18:09:34.977323: step 28667, loss 0.176037, acc 0.921875
2017-03-02T18:09:35.048883: step 28668, loss 0.20487, acc 0.953125
2017-03-02T18:09:35.119158: step 28669, loss 0.0681436, acc 0.96875
2017-03-02T18:09:35.188393: step 28670, loss 0.135347, acc 0.921875
2017-03-02T18:09:35.261600: step 28671, loss 0.0674319, acc 0.96875
2017-03-02T18:09:35.332637: step 28672, loss 0.14338, acc 0.9375
2017-03-02T18:09:35.404570: step 28673, loss 0.181319, acc 0.90625
2017-03-02T18:09:35.490166: step 28674, loss 0.0939684, acc 0.953125
2017-03-02T18:09:35.571702: step 28675, loss 0.10844, acc 0.9375
2017-03-02T18:09:35.643439: step 28676, loss 0.212489, acc 0.953125
2017-03-02T18:09:35.732831: step 28677, loss 0.104592, acc 0.96875
2017-03-02T18:09:35.813368: step 28678, loss 0.256157, acc 0.859375
2017-03-02T18:09:35.886462: step 28679, loss 0.191341, acc 0.859375
2017-03-02T18:09:35.968234: step 28680, loss 0.184429, acc 0.921875
2017-03-02T18:09:36.035592: step 28681, loss 0.0602017, acc 0.984375
2017-03-02T18:09:36.108368: step 28682, loss 0.173349, acc 0.9375
2017-03-02T18:09:36.170857: step 28683, loss 0.185951, acc 0.9375
2017-03-02T18:09:36.239789: step 28684, loss 0.0833473, acc 0.984375
2017-03-02T18:09:36.310432: step 28685, loss 0.123343, acc 0.953125
2017-03-02T18:09:36.384910: step 28686, loss 0.0746776, acc 0.96875
2017-03-02T18:09:36.460951: step 28687, loss 0.0631258, acc 0.96875
2017-03-02T18:09:36.532790: step 28688, loss 0.20963, acc 0.90625
2017-03-02T18:09:36.612812: step 28689, loss 0.092094, acc 0.953125
2017-03-02T18:09:36.685866: step 28690, loss 0.0579158, acc 0.96875
2017-03-02T18:09:36.762349: step 28691, loss 0.201853, acc 0.859375
2017-03-02T18:09:36.839611: step 28692, loss 0.182818, acc 0.890625
2017-03-02T18:09:36.910559: step 28693, loss 0.21613, acc 0.921875
2017-03-02T18:09:36.986671: step 28694, loss 0.190872, acc 0.921875
2017-03-02T18:09:37.053338: step 28695, loss 0.205072, acc 0.890625
2017-03-02T18:09:37.120821: step 28696, loss 0.237102, acc 0.890625
2017-03-02T18:09:37.197876: step 28697, loss 0.131257, acc 0.9375
2017-03-02T18:09:37.278716: step 28698, loss 0.163591, acc 0.9375
2017-03-02T18:09:37.345970: step 28699, loss 0.167415, acc 0.90625
2017-03-02T18:09:37.416409: step 28700, loss 0.0940115, acc 0.96875

Evaluation:
2017-03-02T18:09:37.452878: step 28700, loss 3.65545, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28700

2017-03-02T18:09:37.902363: step 28701, loss 0.129306, acc 0.9375
2017-03-02T18:09:37.977608: step 28702, loss 0.156326, acc 0.9375
2017-03-02T18:09:38.055306: step 28703, loss 0.114284, acc 0.984375
2017-03-02T18:09:38.127245: step 28704, loss 0.216472, acc 0.890625
2017-03-02T18:09:38.206171: step 28705, loss 0.105479, acc 0.984375
2017-03-02T18:09:38.278232: step 28706, loss 0.174417, acc 0.890625
2017-03-02T18:09:38.348128: step 28707, loss 0.186828, acc 0.9375
2017-03-02T18:09:38.424324: step 28708, loss 0.0677318, acc 0.984375
2017-03-02T18:09:38.511525: step 28709, loss 0.102621, acc 0.9375
2017-03-02T18:09:38.581971: step 28710, loss 0.123003, acc 0.953125
2017-03-02T18:09:38.664392: step 28711, loss 0.131366, acc 0.96875
2017-03-02T18:09:38.736858: step 28712, loss 0.181563, acc 0.921875
2017-03-02T18:09:38.810882: step 28713, loss 0.0790213, acc 0.96875
2017-03-02T18:09:38.892410: step 28714, loss 0.29037, acc 0.859375
2017-03-02T18:09:38.964928: step 28715, loss 0.130415, acc 0.9375
2017-03-02T18:09:39.039330: step 28716, loss 0.161675, acc 0.921875
2017-03-02T18:09:39.119638: step 28717, loss 0.228785, acc 0.9375
2017-03-02T18:09:39.186942: step 28718, loss 0.294345, acc 0.859375
2017-03-02T18:09:39.264915: step 28719, loss 0.12157, acc 0.953125
2017-03-02T18:09:39.340876: step 28720, loss 0.137005, acc 0.96875
2017-03-02T18:09:39.419127: step 28721, loss 0.126281, acc 0.953125
2017-03-02T18:09:39.487673: step 28722, loss 0.155577, acc 0.953125
2017-03-02T18:09:39.560048: step 28723, loss 0.227763, acc 0.84375
2017-03-02T18:09:39.633354: step 28724, loss 0.172808, acc 0.9375
2017-03-02T18:09:39.703941: step 28725, loss 0.105632, acc 0.953125
2017-03-02T18:09:39.777813: step 28726, loss 0.0991611, acc 0.953125
2017-03-02T18:09:39.855849: step 28727, loss 0.21949, acc 0.9375
2017-03-02T18:09:39.928528: step 28728, loss 0.113504, acc 0.921875
2017-03-02T18:09:39.998839: step 28729, loss 0.154304, acc 0.921875
2017-03-02T18:09:40.069489: step 28730, loss 0.159138, acc 0.921875
2017-03-02T18:09:40.145913: step 28731, loss 0.140646, acc 0.9375
2017-03-02T18:09:40.216104: step 28732, loss 0.171643, acc 0.921875
2017-03-02T18:09:40.287869: step 28733, loss 0.153585, acc 0.9375
2017-03-02T18:09:40.363162: step 28734, loss 0.123884, acc 0.953125
2017-03-02T18:09:40.439066: step 28735, loss 0.224954, acc 0.890625
2017-03-02T18:09:40.515547: step 28736, loss 0.221618, acc 0.90625
2017-03-02T18:09:40.594983: step 28737, loss 0.241702, acc 0.90625
2017-03-02T18:09:40.671320: step 28738, loss 0.0923982, acc 0.96875
2017-03-02T18:09:40.751447: step 28739, loss 0.0932611, acc 0.953125
2017-03-02T18:09:40.821203: step 28740, loss 0.0840102, acc 0.953125
2017-03-02T18:09:40.898084: step 28741, loss 0.0645063, acc 0.953125
2017-03-02T18:09:40.975530: step 28742, loss 0.0642547, acc 1
2017-03-02T18:09:41.042146: step 28743, loss 0.238909, acc 0.875
2017-03-02T18:09:41.115964: step 28744, loss 0.151072, acc 0.96875
2017-03-02T18:09:41.193923: step 28745, loss 0.144914, acc 0.96875
2017-03-02T18:09:41.264398: step 28746, loss 0.12748, acc 0.953125
2017-03-02T18:09:41.337613: step 28747, loss 0.157861, acc 0.890625
2017-03-02T18:09:41.405875: step 28748, loss 0.261496, acc 0.859375
2017-03-02T18:09:41.480335: step 28749, loss 0.10981, acc 0.9375
2017-03-02T18:09:41.548930: step 28750, loss 0.232171, acc 0.859375
2017-03-02T18:09:41.622044: step 28751, loss 0.155252, acc 0.9375
2017-03-02T18:09:41.695161: step 28752, loss 0.0946483, acc 0.984375
2017-03-02T18:09:41.761621: step 28753, loss 0.137769, acc 0.921875
2017-03-02T18:09:41.835569: step 28754, loss 0.152922, acc 0.9375
2017-03-02T18:09:41.914811: step 28755, loss 0.0893082, acc 0.96875
2017-03-02T18:09:41.981575: step 28756, loss 0.121109, acc 0.953125
2017-03-02T18:09:42.053499: step 28757, loss 0.259439, acc 0.875
2017-03-02T18:09:42.129401: step 28758, loss 0.211923, acc 0.90625
2017-03-02T18:09:42.196461: step 28759, loss 0.232315, acc 0.9375
2017-03-02T18:09:42.268715: step 28760, loss 0.152483, acc 0.9375
2017-03-02T18:09:42.340665: step 28761, loss 0.0533199, acc 0.96875
2017-03-02T18:09:42.406970: step 28762, loss 0.155804, acc 0.9375
2017-03-02T18:09:42.481341: step 28763, loss 0.13442, acc 0.921875
2017-03-02T18:09:42.555936: step 28764, loss 0.1087, acc 0.953125
2017-03-02T18:09:42.628008: step 28765, loss 0.0596255, acc 0.984375
2017-03-02T18:09:42.699094: step 28766, loss 0.0775108, acc 0.96875
2017-03-02T18:09:42.772284: step 28767, loss 0.179444, acc 0.90625
2017-03-02T18:09:42.847123: step 28768, loss 0.114558, acc 0.953125
2017-03-02T18:09:42.915485: step 28769, loss 0.102816, acc 0.953125
2017-03-02T18:09:42.989491: step 28770, loss 0.0736419, acc 0.96875
2017-03-02T18:09:43.061715: step 28771, loss 0.181864, acc 0.90625
2017-03-02T18:09:43.132110: step 28772, loss 0.1173, acc 0.921875
2017-03-02T18:09:43.205606: step 28773, loss 0.222641, acc 0.90625
2017-03-02T18:09:43.277131: step 28774, loss 0.0622822, acc 0.96875
2017-03-02T18:09:43.348128: step 28775, loss 0.128608, acc 0.96875
2017-03-02T18:09:43.420367: step 28776, loss 0.151732, acc 0.890625
2017-03-02T18:09:43.499357: step 28777, loss 0.0809053, acc 0.953125
2017-03-02T18:09:43.581055: step 28778, loss 0.0529619, acc 0.984375
2017-03-02T18:09:43.652102: step 28779, loss 0.152361, acc 0.9375
2017-03-02T18:09:43.724880: step 28780, loss 0.174622, acc 0.921875
2017-03-02T18:09:43.805666: step 28781, loss 0.175841, acc 0.921875
2017-03-02T18:09:43.880639: step 28782, loss 0.0603627, acc 1
2017-03-02T18:09:43.950584: step 28783, loss 0.154065, acc 0.921875
2017-03-02T18:09:44.019641: step 28784, loss 0.255995, acc 0.890625
2017-03-02T18:09:44.089406: step 28785, loss 0.108877, acc 0.921875
2017-03-02T18:09:44.163430: step 28786, loss 0.119042, acc 0.953125
2017-03-02T18:09:44.236281: step 28787, loss 0.126371, acc 0.9375
2017-03-02T18:09:44.305486: step 28788, loss 0.123096, acc 0.921875
2017-03-02T18:09:44.379309: step 28789, loss 0.153743, acc 0.9375
2017-03-02T18:09:44.461712: step 28790, loss 0.16145, acc 0.90625
2017-03-02T18:09:44.530168: step 28791, loss 0.158677, acc 0.9375
2017-03-02T18:09:44.606793: step 28792, loss 0.301485, acc 0.890625
2017-03-02T18:09:44.678459: step 28793, loss 0.105776, acc 0.921875
2017-03-02T18:09:44.755107: step 28794, loss 0.087519, acc 0.9375
2017-03-02T18:09:44.818309: step 28795, loss 0.116126, acc 0.9375
2017-03-02T18:09:44.891668: step 28796, loss 0.0647024, acc 0.953125
2017-03-02T18:09:44.969249: step 28797, loss 0.256553, acc 0.90625
2017-03-02T18:09:45.037123: step 28798, loss 0.15485, acc 0.953125
2017-03-02T18:09:45.118062: step 28799, loss 0.0817344, acc 0.96875
2017-03-02T18:09:45.187592: step 28800, loss 0.168555, acc 0.921875

Evaluation:
2017-03-02T18:09:45.220934: step 28800, loss 3.65983, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28800

2017-03-02T18:09:45.758076: step 28801, loss 0.229669, acc 0.921875
2017-03-02T18:09:45.835533: step 28802, loss 0.150807, acc 0.921875
2017-03-02T18:09:45.909245: step 28803, loss 0.264352, acc 0.90625
2017-03-02T18:09:45.984375: step 28804, loss 0.165293, acc 0.921875
2017-03-02T18:09:46.058900: step 28805, loss 0.0642713, acc 0.984375
2017-03-02T18:09:46.129704: step 28806, loss 0.131299, acc 0.96875
2017-03-02T18:09:46.205786: step 28807, loss 0.129616, acc 0.9375
2017-03-02T18:09:46.278777: step 28808, loss 0.159008, acc 0.9375
2017-03-02T18:09:46.363716: step 28809, loss 0.161994, acc 0.921875
2017-03-02T18:09:46.435396: step 28810, loss 0.19644, acc 0.890625
2017-03-02T18:09:46.507985: step 28811, loss 0.11692, acc 0.9375
2017-03-02T18:09:46.573337: step 28812, loss 0.232479, acc 1
2017-03-02T18:09:46.647959: step 28813, loss 0.14877, acc 0.921875
2017-03-02T18:09:46.725724: step 28814, loss 0.104741, acc 0.953125
2017-03-02T18:09:46.789620: step 28815, loss 0.235463, acc 0.875
2017-03-02T18:09:46.865931: step 28816, loss 0.118986, acc 0.96875
2017-03-02T18:09:46.937855: step 28817, loss 0.0991326, acc 0.9375
2017-03-02T18:09:47.016050: step 28818, loss 0.124842, acc 0.953125
2017-03-02T18:09:47.083223: step 28819, loss 0.0626647, acc 0.984375
2017-03-02T18:09:47.158979: step 28820, loss 0.0939948, acc 0.9375
2017-03-02T18:09:47.230752: step 28821, loss 0.150566, acc 0.953125
2017-03-02T18:09:47.303666: step 28822, loss 0.0688575, acc 0.953125
2017-03-02T18:09:47.376238: step 28823, loss 0.0861651, acc 0.953125
2017-03-02T18:09:47.454645: step 28824, loss 0.0528331, acc 0.984375
2017-03-02T18:09:47.526851: step 28825, loss 0.164253, acc 0.9375
2017-03-02T18:09:47.599107: step 28826, loss 0.184568, acc 0.921875
2017-03-02T18:09:47.671693: step 28827, loss 0.0394102, acc 0.984375
2017-03-02T18:09:47.742981: step 28828, loss 0.172654, acc 0.921875
2017-03-02T18:09:47.812981: step 28829, loss 0.135306, acc 0.953125
2017-03-02T18:09:47.894663: step 28830, loss 0.0820421, acc 0.984375
2017-03-02T18:09:47.961750: step 28831, loss 0.0651429, acc 0.96875
2017-03-02T18:09:48.031537: step 28832, loss 0.176709, acc 0.9375
2017-03-02T18:09:48.105436: step 28833, loss 0.129516, acc 0.921875
2017-03-02T18:09:48.173206: step 28834, loss 0.0938053, acc 0.953125
2017-03-02T18:09:48.276374: step 28835, loss 0.144944, acc 0.90625
2017-03-02T18:09:48.351059: step 28836, loss 0.151215, acc 0.9375
2017-03-02T18:09:48.418720: step 28837, loss 0.0936396, acc 0.96875
2017-03-02T18:09:48.523537: step 28838, loss 0.0761062, acc 0.984375
2017-03-02T18:09:48.597171: step 28839, loss 0.123617, acc 0.9375
2017-03-02T18:09:48.667160: step 28840, loss 0.0707219, acc 0.96875
2017-03-02T18:09:48.736896: step 28841, loss 0.223637, acc 0.921875
2017-03-02T18:09:48.812114: step 28842, loss 0.245885, acc 0.890625
2017-03-02T18:09:48.880540: step 28843, loss 0.188085, acc 0.890625
2017-03-02T18:09:48.954871: step 28844, loss 0.0485732, acc 1
2017-03-02T18:09:49.024430: step 28845, loss 0.102458, acc 0.953125
2017-03-02T18:09:49.105006: step 28846, loss 0.0907781, acc 0.953125
2017-03-02T18:09:49.186463: step 28847, loss 0.113825, acc 0.984375
2017-03-02T18:09:49.264480: step 28848, loss 0.147876, acc 0.953125
2017-03-02T18:09:49.335243: step 28849, loss 0.0957581, acc 0.953125
2017-03-02T18:09:49.409358: step 28850, loss 0.103182, acc 0.953125
2017-03-02T18:09:49.481384: step 28851, loss 0.117207, acc 0.96875
2017-03-02T18:09:49.553275: step 28852, loss 0.126907, acc 0.9375
2017-03-02T18:09:49.626743: step 28853, loss 0.0518752, acc 0.96875
2017-03-02T18:09:49.696320: step 28854, loss 0.251228, acc 0.921875
2017-03-02T18:09:49.779089: step 28855, loss 0.271031, acc 0.90625
2017-03-02T18:09:49.851457: step 28856, loss 0.118479, acc 0.953125
2017-03-02T18:09:49.942286: step 28857, loss 0.290469, acc 0.84375
2017-03-02T18:09:50.017062: step 28858, loss 0.063027, acc 0.96875
2017-03-02T18:09:50.080654: step 28859, loss 0.110669, acc 0.953125
2017-03-02T18:09:50.149415: step 28860, loss 0.109258, acc 0.984375
2017-03-02T18:09:50.226955: step 28861, loss 0.200824, acc 0.90625
2017-03-02T18:09:50.295392: step 28862, loss 0.195834, acc 0.90625
2017-03-02T18:09:50.368417: step 28863, loss 0.21054, acc 0.921875
2017-03-02T18:09:50.439874: step 28864, loss 0.282035, acc 0.828125
2017-03-02T18:09:50.505268: step 28865, loss 0.230829, acc 0.890625
2017-03-02T18:09:50.581286: step 28866, loss 0.0920724, acc 0.96875
2017-03-02T18:09:50.657080: step 28867, loss 0.248337, acc 0.921875
2017-03-02T18:09:50.722529: step 28868, loss 0.18206, acc 0.890625
2017-03-02T18:09:50.788557: step 28869, loss 0.0769673, acc 0.953125
2017-03-02T18:09:50.860781: step 28870, loss 0.13992, acc 0.96875
2017-03-02T18:09:50.940597: step 28871, loss 0.112608, acc 0.96875
2017-03-02T18:09:51.015861: step 28872, loss 0.138015, acc 0.9375
2017-03-02T18:09:51.091481: step 28873, loss 0.121817, acc 0.953125
2017-03-02T18:09:51.165639: step 28874, loss 0.0980738, acc 0.96875
2017-03-02T18:09:51.232969: step 28875, loss 0.19623, acc 0.921875
2017-03-02T18:09:51.312786: step 28876, loss 0.0887601, acc 0.96875
2017-03-02T18:09:51.387237: step 28877, loss 0.143195, acc 0.90625
2017-03-02T18:09:51.462254: step 28878, loss 0.150567, acc 0.9375
2017-03-02T18:09:51.535984: step 28879, loss 0.163531, acc 0.921875
2017-03-02T18:09:51.614080: step 28880, loss 0.172684, acc 0.875
2017-03-02T18:09:51.685272: step 28881, loss 0.115407, acc 0.96875
2017-03-02T18:09:51.760719: step 28882, loss 0.145266, acc 0.9375
2017-03-02T18:09:51.837100: step 28883, loss 0.0614976, acc 0.984375
2017-03-02T18:09:51.905210: step 28884, loss 0.0491794, acc 0.984375
2017-03-02T18:09:51.973600: step 28885, loss 0.191577, acc 0.90625
2017-03-02T18:09:52.045558: step 28886, loss 0.14392, acc 0.90625
2017-03-02T18:09:52.122281: step 28887, loss 0.124521, acc 0.953125
2017-03-02T18:09:52.191794: step 28888, loss 0.172381, acc 0.921875
2017-03-02T18:09:52.256827: step 28889, loss 0.110764, acc 0.96875
2017-03-02T18:09:52.334051: step 28890, loss 0.127224, acc 0.921875
2017-03-02T18:09:52.404850: step 28891, loss 0.139095, acc 0.9375
2017-03-02T18:09:52.472430: step 28892, loss 0.129814, acc 0.921875
2017-03-02T18:09:52.541562: step 28893, loss 0.164533, acc 0.921875
2017-03-02T18:09:52.615016: step 28894, loss 0.0606796, acc 0.96875
2017-03-02T18:09:52.676879: step 28895, loss 0.24358, acc 0.90625
2017-03-02T18:09:52.750608: step 28896, loss 0.198184, acc 0.921875
2017-03-02T18:09:52.822364: step 28897, loss 0.181793, acc 0.9375
2017-03-02T18:09:52.889068: step 28898, loss 0.148179, acc 0.9375
2017-03-02T18:09:52.984045: step 28899, loss 0.143908, acc 0.90625
2017-03-02T18:09:53.062754: step 28900, loss 0.108184, acc 0.953125

Evaluation:
2017-03-02T18:09:53.094799: step 28900, loss 3.76931, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-28900

2017-03-02T18:09:53.558195: step 28901, loss 0.141005, acc 0.9375
2017-03-02T18:09:53.631549: step 28902, loss 0.105638, acc 0.9375
2017-03-02T18:09:53.705309: step 28903, loss 0.137964, acc 0.9375
2017-03-02T18:09:53.770769: step 28904, loss 0.152974, acc 0.90625
2017-03-02T18:09:53.843794: step 28905, loss 0.148611, acc 0.9375
2017-03-02T18:09:53.914150: step 28906, loss 0.133993, acc 0.921875
2017-03-02T18:09:53.983348: step 28907, loss 0.163427, acc 0.921875
2017-03-02T18:09:54.060692: step 28908, loss 0.165437, acc 0.890625
2017-03-02T18:09:54.135571: step 28909, loss 0.0389273, acc 0.984375
2017-03-02T18:09:54.209083: step 28910, loss 0.185139, acc 0.9375
2017-03-02T18:09:54.285410: step 28911, loss 0.0874311, acc 0.953125
2017-03-02T18:09:54.375700: step 28912, loss 0.179514, acc 0.890625
2017-03-02T18:09:54.439385: step 28913, loss 0.167361, acc 0.890625
2017-03-02T18:09:54.516961: step 28914, loss 0.0216175, acc 0.984375
2017-03-02T18:09:54.606673: step 28915, loss 0.203705, acc 0.890625
2017-03-02T18:09:54.671106: step 28916, loss 0.126414, acc 0.921875
2017-03-02T18:09:54.761102: step 28917, loss 0.126513, acc 0.96875
2017-03-02T18:09:54.840794: step 28918, loss 0.223818, acc 0.921875
2017-03-02T18:09:54.907853: step 28919, loss 0.103941, acc 0.953125
2017-03-02T18:09:54.991648: step 28920, loss 0.192915, acc 0.953125
2017-03-02T18:09:55.061747: step 28921, loss 0.150796, acc 0.9375
2017-03-02T18:09:55.134404: step 28922, loss 0.100032, acc 0.953125
2017-03-02T18:09:55.215131: step 28923, loss 0.127091, acc 0.9375
2017-03-02T18:09:55.290045: step 28924, loss 0.088344, acc 0.96875
2017-03-02T18:09:55.359072: step 28925, loss 0.111953, acc 0.953125
2017-03-02T18:09:55.431351: step 28926, loss 0.0569797, acc 0.984375
2017-03-02T18:09:55.504114: step 28927, loss 0.200464, acc 0.90625
2017-03-02T18:09:55.576575: step 28928, loss 0.183793, acc 0.96875
2017-03-02T18:09:55.646580: step 28929, loss 0.152621, acc 0.90625
2017-03-02T18:09:55.718227: step 28930, loss 0.286581, acc 0.890625
2017-03-02T18:09:55.788682: step 28931, loss 0.226601, acc 0.875
2017-03-02T18:09:55.871309: step 28932, loss 0.172816, acc 0.953125
2017-03-02T18:09:55.948163: step 28933, loss 0.125922, acc 0.921875
2017-03-02T18:09:56.025224: step 28934, loss 0.134818, acc 0.9375
2017-03-02T18:09:56.093142: step 28935, loss 0.132908, acc 0.9375
2017-03-02T18:09:56.167493: step 28936, loss 0.0891662, acc 0.953125
2017-03-02T18:09:56.238512: step 28937, loss 0.152964, acc 0.9375
2017-03-02T18:09:56.303673: step 28938, loss 0.233261, acc 0.890625
2017-03-02T18:09:56.378615: step 28939, loss 0.109733, acc 0.953125
2017-03-02T18:09:56.452861: step 28940, loss 0.144068, acc 0.96875
2017-03-02T18:09:56.522504: step 28941, loss 0.216744, acc 0.875
2017-03-02T18:09:56.595616: step 28942, loss 0.129739, acc 0.9375
2017-03-02T18:09:56.667501: step 28943, loss 0.0647614, acc 0.96875
2017-03-02T18:09:56.742527: step 28944, loss 0.0879529, acc 0.953125
2017-03-02T18:09:56.808099: step 28945, loss 0.079107, acc 0.96875
2017-03-02T18:09:56.888763: step 28946, loss 0.246297, acc 0.828125
2017-03-02T18:09:56.972730: step 28947, loss 0.128448, acc 0.9375
2017-03-02T18:09:57.039936: step 28948, loss 0.150758, acc 0.96875
2017-03-02T18:09:57.114557: step 28949, loss 0.251416, acc 0.890625
2017-03-02T18:09:57.190056: step 28950, loss 0.092373, acc 0.953125
2017-03-02T18:09:57.258959: step 28951, loss 0.319226, acc 0.828125
2017-03-02T18:09:57.328714: step 28952, loss 0.138769, acc 0.953125
2017-03-02T18:09:57.402926: step 28953, loss 0.0835834, acc 0.984375
2017-03-02T18:09:57.479164: step 28954, loss 0.132679, acc 0.96875
2017-03-02T18:09:57.557116: step 28955, loss 0.149693, acc 0.953125
2017-03-02T18:09:57.622519: step 28956, loss 0.20821, acc 0.890625
2017-03-02T18:09:57.688224: step 28957, loss 0.0808249, acc 0.96875
2017-03-02T18:09:57.763348: step 28958, loss 0.229521, acc 0.859375
2017-03-02T18:09:57.831406: step 28959, loss 0.156085, acc 0.921875
2017-03-02T18:09:57.900219: step 28960, loss 0.0941458, acc 0.9375
2017-03-02T18:09:57.965599: step 28961, loss 0.0655854, acc 0.984375
2017-03-02T18:09:58.036977: step 28962, loss 0.152589, acc 0.875
2017-03-02T18:09:58.108724: step 28963, loss 0.160817, acc 0.96875
2017-03-02T18:09:58.182969: step 28964, loss 0.133773, acc 0.921875
2017-03-02T18:09:58.254969: step 28965, loss 0.141447, acc 0.921875
2017-03-02T18:09:58.325291: step 28966, loss 0.167523, acc 0.90625
2017-03-02T18:09:58.400218: step 28967, loss 0.211275, acc 0.90625
2017-03-02T18:09:58.468131: step 28968, loss 0.193411, acc 0.890625
2017-03-02T18:09:58.542321: step 28969, loss 0.0758509, acc 0.953125
2017-03-02T18:09:58.610248: step 28970, loss 0.167797, acc 0.9375
2017-03-02T18:09:58.680526: step 28971, loss 0.219943, acc 0.90625
2017-03-02T18:09:58.753981: step 28972, loss 0.169025, acc 0.921875
2017-03-02T18:09:58.831094: step 28973, loss 0.127221, acc 0.953125
2017-03-02T18:09:58.899560: step 28974, loss 0.228998, acc 0.90625
2017-03-02T18:09:58.975493: step 28975, loss 0.186579, acc 0.890625
2017-03-02T18:09:59.047559: step 28976, loss 0.218941, acc 0.875
2017-03-02T18:09:59.112377: step 28977, loss 0.240927, acc 0.921875
2017-03-02T18:09:59.189594: step 28978, loss 0.111314, acc 0.9375
2017-03-02T18:09:59.261770: step 28979, loss 0.146837, acc 0.9375
2017-03-02T18:09:59.337946: step 28980, loss 0.130418, acc 0.9375
2017-03-02T18:09:59.408988: step 28981, loss 0.219257, acc 0.90625
2017-03-02T18:09:59.478828: step 28982, loss 0.0776487, acc 0.953125
2017-03-02T18:09:59.548555: step 28983, loss 0.0739597, acc 0.984375
2017-03-02T18:09:59.624734: step 28984, loss 0.0530926, acc 1
2017-03-02T18:09:59.708693: step 28985, loss 0.128338, acc 0.9375
2017-03-02T18:09:59.776213: step 28986, loss 0.0863826, acc 0.921875
2017-03-02T18:09:59.851157: step 28987, loss 0.115076, acc 0.953125
2017-03-02T18:09:59.921196: step 28988, loss 0.108466, acc 0.9375
2017-03-02T18:09:59.989785: step 28989, loss 0.237452, acc 0.921875
2017-03-02T18:10:00.068342: step 28990, loss 0.213594, acc 0.921875
2017-03-02T18:10:00.146250: step 28991, loss 0.167386, acc 0.921875
2017-03-02T18:10:00.228319: step 28992, loss 0.132098, acc 0.953125
2017-03-02T18:10:00.293924: step 28993, loss 0.0658863, acc 0.984375
2017-03-02T18:10:00.363579: step 28994, loss 0.155459, acc 0.921875
2017-03-02T18:10:00.444352: step 28995, loss 0.156823, acc 0.921875
2017-03-02T18:10:00.509255: step 28996, loss 0.139393, acc 0.96875
2017-03-02T18:10:00.584502: step 28997, loss 0.242937, acc 0.875
2017-03-02T18:10:00.659592: step 28998, loss 0.169057, acc 0.90625
2017-03-02T18:10:00.726102: step 28999, loss 0.217876, acc 0.890625
2017-03-02T18:10:00.799385: step 29000, loss 0.166283, acc 0.96875

Evaluation:
2017-03-02T18:10:00.833586: step 29000, loss 3.65459, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29000

2017-03-02T18:10:01.270619: step 29001, loss 0.145719, acc 0.9375
2017-03-02T18:10:01.341363: step 29002, loss 0.174117, acc 0.921875
2017-03-02T18:10:01.409298: step 29003, loss 0.177647, acc 0.921875
2017-03-02T18:10:01.480082: step 29004, loss 0.155105, acc 0.90625
2017-03-02T18:10:01.553618: step 29005, loss 0.241045, acc 0.875
2017-03-02T18:10:01.629481: step 29006, loss 0.174683, acc 0.921875
2017-03-02T18:10:01.706511: step 29007, loss 0.223777, acc 0.875
2017-03-02T18:10:01.779520: step 29008, loss 0, acc 1
2017-03-02T18:10:01.856876: step 29009, loss 0.14222, acc 0.953125
2017-03-02T18:10:01.927512: step 29010, loss 0.0354071, acc 0.984375
2017-03-02T18:10:01.997151: step 29011, loss 0.20948, acc 0.890625
2017-03-02T18:10:02.070853: step 29012, loss 0.102298, acc 0.9375
2017-03-02T18:10:02.143473: step 29013, loss 0.132673, acc 0.9375
2017-03-02T18:10:02.215026: step 29014, loss 0.237765, acc 0.890625
2017-03-02T18:10:02.297189: step 29015, loss 0.179719, acc 0.953125
2017-03-02T18:10:02.367017: step 29016, loss 0.192061, acc 0.890625
2017-03-02T18:10:02.439974: step 29017, loss 0.145468, acc 0.9375
2017-03-02T18:10:02.514291: step 29018, loss 0.181444, acc 0.953125
2017-03-02T18:10:02.579411: step 29019, loss 0.164956, acc 0.890625
2017-03-02T18:10:02.656999: step 29020, loss 0.0685525, acc 0.984375
2017-03-02T18:10:02.731709: step 29021, loss 0.155998, acc 0.953125
2017-03-02T18:10:02.800959: step 29022, loss 0.0368361, acc 0.984375
2017-03-02T18:10:02.868462: step 29023, loss 0.226435, acc 0.890625
2017-03-02T18:10:02.945736: step 29024, loss 0.203627, acc 0.90625
2017-03-02T18:10:03.027103: step 29025, loss 0.20903, acc 0.859375
2017-03-02T18:10:03.099155: step 29026, loss 0.178013, acc 0.9375
2017-03-02T18:10:03.171773: step 29027, loss 0.0809556, acc 0.984375
2017-03-02T18:10:03.241651: step 29028, loss 0.142378, acc 0.9375
2017-03-02T18:10:03.313395: step 29029, loss 0.11903, acc 0.9375
2017-03-02T18:10:03.391337: step 29030, loss 0.0742565, acc 0.953125
2017-03-02T18:10:03.470118: step 29031, loss 0.118955, acc 0.953125
2017-03-02T18:10:03.545423: step 29032, loss 0.160015, acc 0.9375
2017-03-02T18:10:03.617282: step 29033, loss 0.173204, acc 0.90625
2017-03-02T18:10:03.686947: step 29034, loss 0.196978, acc 0.90625
2017-03-02T18:10:03.753060: step 29035, loss 0.107044, acc 0.96875
2017-03-02T18:10:03.831607: step 29036, loss 0.238667, acc 0.90625
2017-03-02T18:10:03.903797: step 29037, loss 0.0839239, acc 0.96875
2017-03-02T18:10:03.975803: step 29038, loss 0.134298, acc 0.953125
2017-03-02T18:10:04.046775: step 29039, loss 0.12088, acc 0.953125
2017-03-02T18:10:04.119497: step 29040, loss 0.196615, acc 0.921875
2017-03-02T18:10:04.186157: step 29041, loss 0.150732, acc 0.953125
2017-03-02T18:10:04.253106: step 29042, loss 0.168874, acc 0.921875
2017-03-02T18:10:04.325290: step 29043, loss 0.154752, acc 0.953125
2017-03-02T18:10:04.396879: step 29044, loss 0.0355314, acc 0.984375
2017-03-02T18:10:04.468734: step 29045, loss 0.0902429, acc 0.96875
2017-03-02T18:10:04.552226: step 29046, loss 0.343559, acc 0.84375
2017-03-02T18:10:04.622786: step 29047, loss 0.182327, acc 0.921875
2017-03-02T18:10:04.696808: step 29048, loss 0.101108, acc 0.9375
2017-03-02T18:10:04.769800: step 29049, loss 0.124442, acc 0.96875
2017-03-02T18:10:04.843107: step 29050, loss 0.181617, acc 0.921875
2017-03-02T18:10:04.918024: step 29051, loss 0.220411, acc 0.890625
2017-03-02T18:10:04.990031: step 29052, loss 0.244274, acc 0.859375
2017-03-02T18:10:05.064908: step 29053, loss 0.181434, acc 0.921875
2017-03-02T18:10:05.136458: step 29054, loss 0.0790565, acc 0.953125
2017-03-02T18:10:05.214813: step 29055, loss 0.137217, acc 0.9375
2017-03-02T18:10:05.288547: step 29056, loss 0.078817, acc 0.96875
2017-03-02T18:10:05.354489: step 29057, loss 0.135814, acc 0.9375
2017-03-02T18:10:05.434263: step 29058, loss 0.0744674, acc 0.96875
2017-03-02T18:10:05.514810: step 29059, loss 0.111364, acc 0.9375
2017-03-02T18:10:05.592512: step 29060, loss 0.115166, acc 0.9375
2017-03-02T18:10:05.661318: step 29061, loss 0.0800428, acc 0.96875
2017-03-02T18:10:05.735132: step 29062, loss 0.218868, acc 0.890625
2017-03-02T18:10:05.802058: step 29063, loss 0.122601, acc 0.9375
2017-03-02T18:10:05.874749: step 29064, loss 0.196634, acc 0.921875
2017-03-02T18:10:05.947077: step 29065, loss 0.171728, acc 0.90625
2017-03-02T18:10:06.018997: step 29066, loss 0.190917, acc 0.921875
2017-03-02T18:10:06.101138: step 29067, loss 0.0967042, acc 0.953125
2017-03-02T18:10:06.175340: step 29068, loss 0.152223, acc 0.90625
2017-03-02T18:10:06.248801: step 29069, loss 0.174896, acc 0.921875
2017-03-02T18:10:06.315736: step 29070, loss 0.126958, acc 0.953125
2017-03-02T18:10:06.388011: step 29071, loss 0.141605, acc 0.953125
2017-03-02T18:10:06.455757: step 29072, loss 0.125342, acc 0.953125
2017-03-02T18:10:06.517875: step 29073, loss 0.146928, acc 0.9375
2017-03-02T18:10:06.593402: step 29074, loss 0.0814117, acc 0.984375
2017-03-02T18:10:06.667173: step 29075, loss 0.0754517, acc 0.96875
2017-03-02T18:10:06.734599: step 29076, loss 0.104728, acc 0.953125
2017-03-02T18:10:06.816292: step 29077, loss 0.141251, acc 0.9375
2017-03-02T18:10:06.889968: step 29078, loss 0.112302, acc 0.96875
2017-03-02T18:10:06.967300: step 29079, loss 0.14359, acc 0.9375
2017-03-02T18:10:07.040886: step 29080, loss 0.0993563, acc 0.9375
2017-03-02T18:10:07.109523: step 29081, loss 0.188729, acc 0.921875
2017-03-02T18:10:07.177852: step 29082, loss 0.0788177, acc 0.984375
2017-03-02T18:10:07.260500: step 29083, loss 0.0851768, acc 0.953125
2017-03-02T18:10:07.344254: step 29084, loss 0.134432, acc 0.9375
2017-03-02T18:10:07.408777: step 29085, loss 0.0748821, acc 0.984375
2017-03-02T18:10:07.505920: step 29086, loss 0.225021, acc 0.921875
2017-03-02T18:10:07.584285: step 29087, loss 0.106928, acc 0.9375
2017-03-02T18:10:07.651090: step 29088, loss 0.142776, acc 0.921875
2017-03-02T18:10:07.729869: step 29089, loss 0.0250135, acc 1
2017-03-02T18:10:07.802080: step 29090, loss 0.226816, acc 0.890625
2017-03-02T18:10:07.873577: step 29091, loss 0.217641, acc 0.90625
2017-03-02T18:10:07.955262: step 29092, loss 0.201102, acc 0.875
2017-03-02T18:10:08.036616: step 29093, loss 0.189772, acc 0.921875
2017-03-02T18:10:08.103275: step 29094, loss 0.135958, acc 0.9375
2017-03-02T18:10:08.173747: step 29095, loss 0.0968258, acc 0.9375
2017-03-02T18:10:08.246013: step 29096, loss 0.0990029, acc 0.9375
2017-03-02T18:10:08.324883: step 29097, loss 0.123399, acc 0.953125
2017-03-02T18:10:08.395418: step 29098, loss 0.230817, acc 0.875
2017-03-02T18:10:08.469758: step 29099, loss 0.0377073, acc 0.984375
2017-03-02T18:10:08.545891: step 29100, loss 0.182949, acc 0.921875

Evaluation:
2017-03-02T18:10:08.585195: step 29100, loss 3.67092, acc 0.638789

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29100

2017-03-02T18:10:09.040724: step 29101, loss 0.116883, acc 0.9375
2017-03-02T18:10:09.115008: step 29102, loss 0.0965801, acc 0.96875
2017-03-02T18:10:09.184696: step 29103, loss 0.184746, acc 0.890625
2017-03-02T18:10:09.254138: step 29104, loss 0.298293, acc 0.875
2017-03-02T18:10:09.321607: step 29105, loss 0.131424, acc 0.9375
2017-03-02T18:10:09.397330: step 29106, loss 0.112059, acc 0.953125
2017-03-02T18:10:09.486943: step 29107, loss 0.152892, acc 0.90625
2017-03-02T18:10:09.558890: step 29108, loss 0.112038, acc 0.953125
2017-03-02T18:10:09.629781: step 29109, loss 0.10794, acc 0.9375
2017-03-02T18:10:09.699877: step 29110, loss 0.0587569, acc 0.96875
2017-03-02T18:10:09.770583: step 29111, loss 0.219161, acc 0.90625
2017-03-02T18:10:09.852522: step 29112, loss 0.220846, acc 0.9375
2017-03-02T18:10:09.925880: step 29113, loss 0.119687, acc 0.984375
2017-03-02T18:10:09.992394: step 29114, loss 0.26377, acc 0.84375
2017-03-02T18:10:10.063568: step 29115, loss 0.128666, acc 0.953125
2017-03-02T18:10:10.142273: step 29116, loss 0.1319, acc 0.9375
2017-03-02T18:10:10.220294: step 29117, loss 0.14701, acc 0.921875
2017-03-02T18:10:10.296123: step 29118, loss 0.0663992, acc 0.96875
2017-03-02T18:10:10.371408: step 29119, loss 0.125098, acc 0.9375
2017-03-02T18:10:10.444728: step 29120, loss 0.168683, acc 0.9375
2017-03-02T18:10:10.516916: step 29121, loss 0.120229, acc 0.9375
2017-03-02T18:10:10.594230: step 29122, loss 0.226572, acc 0.890625
2017-03-02T18:10:10.659559: step 29123, loss 0.146346, acc 0.921875
2017-03-02T18:10:10.733717: step 29124, loss 0.0518718, acc 0.984375
2017-03-02T18:10:10.811124: step 29125, loss 0.184988, acc 0.921875
2017-03-02T18:10:10.870969: step 29126, loss 0.245671, acc 0.90625
2017-03-02T18:10:10.933532: step 29127, loss 0.0660288, acc 0.984375
2017-03-02T18:10:11.021310: step 29128, loss 0.189788, acc 0.890625
2017-03-02T18:10:11.092935: step 29129, loss 0.297856, acc 0.859375
2017-03-02T18:10:11.168319: step 29130, loss 0.157565, acc 0.90625
2017-03-02T18:10:11.241571: step 29131, loss 0.168413, acc 0.921875
2017-03-02T18:10:11.314449: step 29132, loss 0.226978, acc 0.90625
2017-03-02T18:10:11.380698: step 29133, loss 0.129252, acc 0.9375
2017-03-02T18:10:11.456171: step 29134, loss 0.066159, acc 0.96875
2017-03-02T18:10:11.531957: step 29135, loss 0.107474, acc 0.984375
2017-03-02T18:10:11.600608: step 29136, loss 0.11634, acc 0.953125
2017-03-02T18:10:11.671711: step 29137, loss 0.197571, acc 0.890625
2017-03-02T18:10:11.738623: step 29138, loss 0.138049, acc 0.9375
2017-03-02T18:10:11.810255: step 29139, loss 0.112035, acc 0.9375
2017-03-02T18:10:11.876428: step 29140, loss 0.172767, acc 0.921875
2017-03-02T18:10:11.950874: step 29141, loss 0.116939, acc 0.9375
2017-03-02T18:10:12.026298: step 29142, loss 0.130445, acc 0.921875
2017-03-02T18:10:12.100079: step 29143, loss 0.24036, acc 0.890625
2017-03-02T18:10:12.173666: step 29144, loss 0.0759799, acc 0.96875
2017-03-02T18:10:12.244654: step 29145, loss 0.124444, acc 0.96875
2017-03-02T18:10:12.315261: step 29146, loss 0.152219, acc 0.953125
2017-03-02T18:10:12.391509: step 29147, loss 0.150865, acc 0.9375
2017-03-02T18:10:12.460687: step 29148, loss 0.167786, acc 0.9375
2017-03-02T18:10:12.552939: step 29149, loss 0.103613, acc 0.953125
2017-03-02T18:10:12.628900: step 29150, loss 0.0658875, acc 1
2017-03-02T18:10:12.703787: step 29151, loss 0.110095, acc 0.953125
2017-03-02T18:10:12.780748: step 29152, loss 0.130145, acc 0.953125
2017-03-02T18:10:12.855142: step 29153, loss 0.0760182, acc 0.96875
2017-03-02T18:10:12.926648: step 29154, loss 0.259888, acc 0.859375
2017-03-02T18:10:12.990790: step 29155, loss 0.149346, acc 0.9375
2017-03-02T18:10:13.058660: step 29156, loss 0.263419, acc 0.875
2017-03-02T18:10:13.131195: step 29157, loss 0.151114, acc 0.90625
2017-03-02T18:10:13.211282: step 29158, loss 0.0842465, acc 0.953125
2017-03-02T18:10:13.281398: step 29159, loss 0.146321, acc 0.9375
2017-03-02T18:10:13.353024: step 29160, loss 0.117256, acc 0.9375
2017-03-02T18:10:13.422330: step 29161, loss 0.10712, acc 0.953125
2017-03-02T18:10:13.488362: step 29162, loss 0.17046, acc 0.921875
2017-03-02T18:10:13.563648: step 29163, loss 0.167045, acc 0.90625
2017-03-02T18:10:13.637048: step 29164, loss 0.128591, acc 0.953125
2017-03-02T18:10:13.706920: step 29165, loss 0.118379, acc 0.953125
2017-03-02T18:10:13.778639: step 29166, loss 0.0796482, acc 0.984375
2017-03-02T18:10:13.851759: step 29167, loss 0.177914, acc 0.921875
2017-03-02T18:10:13.924019: step 29168, loss 0.0453831, acc 0.984375
2017-03-02T18:10:13.999575: step 29169, loss 0.142083, acc 0.9375
2017-03-02T18:10:14.065759: step 29170, loss 0.105801, acc 0.953125
2017-03-02T18:10:14.137810: step 29171, loss 0.138805, acc 0.9375
2017-03-02T18:10:14.204365: step 29172, loss 0.059978, acc 0.984375
2017-03-02T18:10:14.270442: step 29173, loss 0.153977, acc 0.9375
2017-03-02T18:10:14.337045: step 29174, loss 0.167175, acc 0.921875
2017-03-02T18:10:14.410460: step 29175, loss 0.290225, acc 0.875
2017-03-02T18:10:14.485192: step 29176, loss 0.129297, acc 0.9375
2017-03-02T18:10:14.556347: step 29177, loss 0.190546, acc 0.890625
2017-03-02T18:10:14.618512: step 29178, loss 0.103419, acc 0.9375
2017-03-02T18:10:14.687726: step 29179, loss 0.149263, acc 0.90625
2017-03-02T18:10:14.763817: step 29180, loss 0.106812, acc 0.953125
2017-03-02T18:10:14.831141: step 29181, loss 0.0875152, acc 0.953125
2017-03-02T18:10:14.899626: step 29182, loss 0.0906034, acc 0.953125
2017-03-02T18:10:14.968415: step 29183, loss 0.0828421, acc 0.96875
2017-03-02T18:10:15.040093: step 29184, loss 0.164508, acc 0.921875
2017-03-02T18:10:15.104821: step 29185, loss 0.209707, acc 0.890625
2017-03-02T18:10:15.165898: step 29186, loss 0.0651732, acc 0.96875
2017-03-02T18:10:15.240850: step 29187, loss 0.201906, acc 0.890625
2017-03-02T18:10:15.313166: step 29188, loss 0.166747, acc 0.9375
2017-03-02T18:10:15.388677: step 29189, loss 0.137749, acc 0.9375
2017-03-02T18:10:15.464607: step 29190, loss 0.10734, acc 0.96875
2017-03-02T18:10:15.541839: step 29191, loss 0.122482, acc 0.953125
2017-03-02T18:10:15.630231: step 29192, loss 0.119502, acc 0.9375
2017-03-02T18:10:15.703147: step 29193, loss 0.211587, acc 0.953125
2017-03-02T18:10:15.766402: step 29194, loss 0.1601, acc 0.9375
2017-03-02T18:10:15.838835: step 29195, loss 0.124736, acc 0.953125
2017-03-02T18:10:15.906956: step 29196, loss 0.187133, acc 0.921875
2017-03-02T18:10:15.981796: step 29197, loss 0.179203, acc 0.890625
2017-03-02T18:10:16.048702: step 29198, loss 0.170478, acc 0.921875
2017-03-02T18:10:16.124252: step 29199, loss 0.140525, acc 0.9375
2017-03-02T18:10:16.194875: step 29200, loss 0.14984, acc 0.9375

Evaluation:
2017-03-02T18:10:16.221538: step 29200, loss 3.66624, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29200

2017-03-02T18:10:16.683365: step 29201, loss 0.227302, acc 0.875
2017-03-02T18:10:16.755374: step 29202, loss 0.108663, acc 0.953125
2017-03-02T18:10:16.821357: step 29203, loss 0.076348, acc 0.953125
2017-03-02T18:10:16.893902: step 29204, loss 0.287448, acc 0.75
2017-03-02T18:10:16.964459: step 29205, loss 0.17255, acc 0.890625
2017-03-02T18:10:17.047044: step 29206, loss 0.11201, acc 0.953125
2017-03-02T18:10:17.118941: step 29207, loss 0.0795503, acc 0.953125
2017-03-02T18:10:17.189708: step 29208, loss 0.138229, acc 0.9375
2017-03-02T18:10:17.262988: step 29209, loss 0.0677442, acc 0.953125
2017-03-02T18:10:17.338316: step 29210, loss 0.134154, acc 0.90625
2017-03-02T18:10:17.404294: step 29211, loss 0.301916, acc 0.859375
2017-03-02T18:10:17.494203: step 29212, loss 0.0438509, acc 1
2017-03-02T18:10:17.568211: step 29213, loss 0.193846, acc 0.953125
2017-03-02T18:10:17.633358: step 29214, loss 0.164675, acc 0.9375
2017-03-02T18:10:17.698959: step 29215, loss 0.116709, acc 0.953125
2017-03-02T18:10:17.769552: step 29216, loss 0.130286, acc 0.953125
2017-03-02T18:10:17.836949: step 29217, loss 0.0955339, acc 0.96875
2017-03-02T18:10:17.903930: step 29218, loss 0.0754561, acc 0.953125
2017-03-02T18:10:17.974399: step 29219, loss 0.176283, acc 0.921875
2017-03-02T18:10:18.052127: step 29220, loss 0.139624, acc 0.921875
2017-03-02T18:10:18.130593: step 29221, loss 0.122389, acc 0.953125
2017-03-02T18:10:18.223515: step 29222, loss 0.119278, acc 0.953125
2017-03-02T18:10:18.301281: step 29223, loss 0.160094, acc 0.921875
2017-03-02T18:10:18.379744: step 29224, loss 0.101184, acc 0.9375
2017-03-02T18:10:18.458123: step 29225, loss 0.193537, acc 0.921875
2017-03-02T18:10:18.534154: step 29226, loss 0.262639, acc 0.890625
2017-03-02T18:10:18.607018: step 29227, loss 0.0672884, acc 0.96875
2017-03-02T18:10:18.681070: step 29228, loss 0.125179, acc 0.9375
2017-03-02T18:10:18.758102: step 29229, loss 0.138998, acc 0.9375
2017-03-02T18:10:18.827892: step 29230, loss 0.175041, acc 0.921875
2017-03-02T18:10:18.901390: step 29231, loss 0.164796, acc 0.9375
2017-03-02T18:10:18.984934: step 29232, loss 0.238827, acc 0.90625
2017-03-02T18:10:19.056240: step 29233, loss 0.158342, acc 0.90625
2017-03-02T18:10:19.143109: step 29234, loss 0.126801, acc 0.953125
2017-03-02T18:10:19.208595: step 29235, loss 0.12472, acc 0.921875
2017-03-02T18:10:19.276945: step 29236, loss 0.185038, acc 0.90625
2017-03-02T18:10:19.347082: step 29237, loss 0.166633, acc 0.921875
2017-03-02T18:10:19.421894: step 29238, loss 0.18509, acc 0.890625
2017-03-02T18:10:19.496971: step 29239, loss 0.105505, acc 0.9375
2017-03-02T18:10:19.569281: step 29240, loss 0.149932, acc 0.953125
2017-03-02T18:10:19.642072: step 29241, loss 0.121905, acc 0.96875
2017-03-02T18:10:19.721758: step 29242, loss 0.113541, acc 0.953125
2017-03-02T18:10:19.790865: step 29243, loss 0.0766819, acc 0.953125
2017-03-02T18:10:19.855530: step 29244, loss 0.139665, acc 0.96875
2017-03-02T18:10:19.929173: step 29245, loss 0.249923, acc 0.890625
2017-03-02T18:10:19.998858: step 29246, loss 0.151433, acc 0.9375
2017-03-02T18:10:20.082794: step 29247, loss 0.0564071, acc 0.96875
2017-03-02T18:10:20.146991: step 29248, loss 0.118745, acc 0.953125
2017-03-02T18:10:20.215623: step 29249, loss 0.102429, acc 0.96875
2017-03-02T18:10:20.283460: step 29250, loss 0.205087, acc 0.875
2017-03-02T18:10:20.354380: step 29251, loss 0.150142, acc 0.921875
2017-03-02T18:10:20.422297: step 29252, loss 0.229465, acc 0.90625
2017-03-02T18:10:20.505857: step 29253, loss 0.0568347, acc 0.984375
2017-03-02T18:10:20.574684: step 29254, loss 0.163174, acc 0.9375
2017-03-02T18:10:20.648845: step 29255, loss 0.12842, acc 0.9375
2017-03-02T18:10:20.721287: step 29256, loss 0.0999221, acc 0.953125
2017-03-02T18:10:20.797018: step 29257, loss 0.211406, acc 0.90625
2017-03-02T18:10:20.872894: step 29258, loss 0.22073, acc 0.890625
2017-03-02T18:10:20.940341: step 29259, loss 0.288233, acc 0.84375
2017-03-02T18:10:21.019391: step 29260, loss 0.138736, acc 0.9375
2017-03-02T18:10:21.086750: step 29261, loss 0.120684, acc 0.9375
2017-03-02T18:10:21.152418: step 29262, loss 0.102012, acc 0.9375
2017-03-02T18:10:21.227887: step 29263, loss 0.0954889, acc 0.953125
2017-03-02T18:10:21.295804: step 29264, loss 0.181997, acc 0.921875
2017-03-02T18:10:21.375731: step 29265, loss 0.15097, acc 0.9375
2017-03-02T18:10:21.447829: step 29266, loss 0.15751, acc 0.921875
2017-03-02T18:10:21.522785: step 29267, loss 0.150055, acc 0.921875
2017-03-02T18:10:21.593570: step 29268, loss 0.200817, acc 0.890625
2017-03-02T18:10:21.662616: step 29269, loss 0.187219, acc 0.921875
2017-03-02T18:10:21.741028: step 29270, loss 0.168513, acc 0.921875
2017-03-02T18:10:21.811753: step 29271, loss 0.0682695, acc 0.953125
2017-03-02T18:10:21.876602: step 29272, loss 0.19673, acc 0.921875
2017-03-02T18:10:21.951093: step 29273, loss 0.10466, acc 0.953125
2017-03-02T18:10:22.028028: step 29274, loss 0.20721, acc 0.90625
2017-03-02T18:10:22.095810: step 29275, loss 0.207542, acc 0.890625
2017-03-02T18:10:22.169974: step 29276, loss 0.160574, acc 0.921875
2017-03-02T18:10:22.241901: step 29277, loss 0.317556, acc 0.859375
2017-03-02T18:10:22.308137: step 29278, loss 0.179746, acc 0.890625
2017-03-02T18:10:22.379130: step 29279, loss 0.150952, acc 0.9375
2017-03-02T18:10:22.453691: step 29280, loss 0.130535, acc 0.9375
2017-03-02T18:10:22.525903: step 29281, loss 0.160843, acc 0.9375
2017-03-02T18:10:22.593092: step 29282, loss 0.0970278, acc 0.984375
2017-03-02T18:10:22.663459: step 29283, loss 0.179179, acc 0.9375
2017-03-02T18:10:22.733803: step 29284, loss 0.158952, acc 0.921875
2017-03-02T18:10:22.810617: step 29285, loss 0.192866, acc 0.9375
2017-03-02T18:10:22.882221: step 29286, loss 0.115359, acc 0.953125
2017-03-02T18:10:22.953043: step 29287, loss 0.173196, acc 0.921875
2017-03-02T18:10:23.022971: step 29288, loss 0.145416, acc 0.921875
2017-03-02T18:10:23.083910: step 29289, loss 0.154715, acc 0.953125
2017-03-02T18:10:23.160407: step 29290, loss 0.229238, acc 0.90625
2017-03-02T18:10:23.229441: step 29291, loss 0.0806041, acc 0.96875
2017-03-02T18:10:23.291069: step 29292, loss 0.112669, acc 0.9375
2017-03-02T18:10:23.365645: step 29293, loss 0.0841696, acc 0.984375
2017-03-02T18:10:23.447518: step 29294, loss 0.0883657, acc 0.953125
2017-03-02T18:10:23.515179: step 29295, loss 0.0534393, acc 0.984375
2017-03-02T18:10:23.588090: step 29296, loss 0.0644385, acc 0.953125
2017-03-02T18:10:23.660520: step 29297, loss 0.209145, acc 0.890625
2017-03-02T18:10:23.727668: step 29298, loss 0.0826828, acc 0.953125
2017-03-02T18:10:23.801062: step 29299, loss 0.235022, acc 0.859375
2017-03-02T18:10:23.871284: step 29300, loss 0.174276, acc 0.90625

Evaluation:
2017-03-02T18:10:23.902000: step 29300, loss 3.69503, acc 0.630137

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29300

2017-03-02T18:10:24.376899: step 29301, loss 0.151799, acc 0.9375
2017-03-02T18:10:24.448734: step 29302, loss 0.327027, acc 0.875
2017-03-02T18:10:24.520721: step 29303, loss 0.109457, acc 0.96875
2017-03-02T18:10:24.594822: step 29304, loss 0.196862, acc 0.90625
2017-03-02T18:10:24.657882: step 29305, loss 0.0831518, acc 0.984375
2017-03-02T18:10:24.726423: step 29306, loss 0.128155, acc 0.921875
2017-03-02T18:10:24.799561: step 29307, loss 0.0963739, acc 0.96875
2017-03-02T18:10:24.866999: step 29308, loss 0.135625, acc 0.921875
2017-03-02T18:10:24.941275: step 29309, loss 0.128964, acc 0.96875
2017-03-02T18:10:25.019919: step 29310, loss 0.161491, acc 0.9375
2017-03-02T18:10:25.092126: step 29311, loss 0.14153, acc 0.921875
2017-03-02T18:10:25.170574: step 29312, loss 0.0991474, acc 0.96875
2017-03-02T18:10:25.246999: step 29313, loss 0.188075, acc 0.921875
2017-03-02T18:10:25.336773: step 29314, loss 0.0500784, acc 0.96875
2017-03-02T18:10:25.417260: step 29315, loss 0.145169, acc 0.921875
2017-03-02T18:10:25.491396: step 29316, loss 0.147213, acc 0.921875
2017-03-02T18:10:25.562392: step 29317, loss 0.0823721, acc 0.984375
2017-03-02T18:10:25.647434: step 29318, loss 0.171881, acc 0.953125
2017-03-02T18:10:25.720835: step 29319, loss 0.108006, acc 0.953125
2017-03-02T18:10:25.799405: step 29320, loss 0.151908, acc 0.921875
2017-03-02T18:10:25.875559: step 29321, loss 0.279413, acc 0.90625
2017-03-02T18:10:25.953787: step 29322, loss 0.145123, acc 0.9375
2017-03-02T18:10:26.023513: step 29323, loss 0.146708, acc 0.953125
2017-03-02T18:10:26.100381: step 29324, loss 0.183394, acc 0.90625
2017-03-02T18:10:26.176131: step 29325, loss 0.121012, acc 0.9375
2017-03-02T18:10:26.248569: step 29326, loss 0.0684792, acc 0.96875
2017-03-02T18:10:26.319271: step 29327, loss 0.10816, acc 0.9375
2017-03-02T18:10:26.383553: step 29328, loss 0.211352, acc 0.84375
2017-03-02T18:10:26.439167: step 29329, loss 0.224045, acc 0.9375
2017-03-02T18:10:26.527653: step 29330, loss 0.0697119, acc 0.984375
2017-03-02T18:10:26.600642: step 29331, loss 0.170932, acc 0.9375
2017-03-02T18:10:26.670203: step 29332, loss 0.14131, acc 0.9375
2017-03-02T18:10:26.731439: step 29333, loss 0.131432, acc 0.9375
2017-03-02T18:10:26.799524: step 29334, loss 0.145079, acc 0.921875
2017-03-02T18:10:26.878986: step 29335, loss 0.0521105, acc 0.96875
2017-03-02T18:10:26.950181: step 29336, loss 0.126467, acc 0.9375
2017-03-02T18:10:27.017289: step 29337, loss 0.143704, acc 0.9375
2017-03-02T18:10:27.088773: step 29338, loss 0.107353, acc 0.953125
2017-03-02T18:10:27.160528: step 29339, loss 0.211785, acc 0.90625
2017-03-02T18:10:27.230835: step 29340, loss 0.169775, acc 0.90625
2017-03-02T18:10:27.299880: step 29341, loss 0.217018, acc 0.921875
2017-03-02T18:10:27.369846: step 29342, loss 0.16976, acc 0.921875
2017-03-02T18:10:27.432812: step 29343, loss 0.322975, acc 0.890625
2017-03-02T18:10:27.513995: step 29344, loss 0.0764346, acc 0.984375
2017-03-02T18:10:27.592691: step 29345, loss 0.127755, acc 0.953125
2017-03-02T18:10:27.665908: step 29346, loss 0.199475, acc 0.9375
2017-03-02T18:10:27.741881: step 29347, loss 0.0747417, acc 0.953125
2017-03-02T18:10:27.821103: step 29348, loss 0.131798, acc 0.953125
2017-03-02T18:10:27.885256: step 29349, loss 0.0755608, acc 0.96875
2017-03-02T18:10:27.958695: step 29350, loss 0.162299, acc 0.9375
2017-03-02T18:10:28.029908: step 29351, loss 0.107075, acc 0.9375
2017-03-02T18:10:28.098796: step 29352, loss 0.16101, acc 0.90625
2017-03-02T18:10:28.181911: step 29353, loss 0.218903, acc 0.90625
2017-03-02T18:10:28.261550: step 29354, loss 0.179603, acc 0.90625
2017-03-02T18:10:28.328473: step 29355, loss 0.197524, acc 0.890625
2017-03-02T18:10:28.403010: step 29356, loss 0.0589097, acc 1
2017-03-02T18:10:28.477807: step 29357, loss 0.199642, acc 0.921875
2017-03-02T18:10:28.551018: step 29358, loss 0.168916, acc 0.90625
2017-03-02T18:10:28.619535: step 29359, loss 0.204669, acc 0.90625
2017-03-02T18:10:28.702287: step 29360, loss 0.104858, acc 0.96875
2017-03-02T18:10:28.781663: step 29361, loss 0.128892, acc 0.90625
2017-03-02T18:10:28.851168: step 29362, loss 0.183664, acc 0.921875
2017-03-02T18:10:28.924480: step 29363, loss 0.0957384, acc 0.953125
2017-03-02T18:10:28.990661: step 29364, loss 0.079819, acc 0.96875
2017-03-02T18:10:29.057419: step 29365, loss 0.112466, acc 0.9375
2017-03-02T18:10:29.128578: step 29366, loss 0.136954, acc 0.9375
2017-03-02T18:10:29.191679: step 29367, loss 0.0341766, acc 1
2017-03-02T18:10:29.253742: step 29368, loss 0.202128, acc 0.90625
2017-03-02T18:10:29.316609: step 29369, loss 0.0545731, acc 0.984375
2017-03-02T18:10:29.391488: step 29370, loss 0.158672, acc 0.890625
2017-03-02T18:10:29.458764: step 29371, loss 0.167388, acc 0.9375
2017-03-02T18:10:29.524303: step 29372, loss 0.101523, acc 0.953125
2017-03-02T18:10:29.598214: step 29373, loss 0.0483095, acc 0.984375
2017-03-02T18:10:29.670598: step 29374, loss 0.242423, acc 0.921875
2017-03-02T18:10:29.739701: step 29375, loss 0.177622, acc 0.921875
2017-03-02T18:10:29.809718: step 29376, loss 0.0929781, acc 0.984375
2017-03-02T18:10:29.886679: step 29377, loss 0.11595, acc 0.9375
2017-03-02T18:10:29.966556: step 29378, loss 0.227466, acc 0.890625
2017-03-02T18:10:30.036281: step 29379, loss 0.0566681, acc 0.96875
2017-03-02T18:10:30.110445: step 29380, loss 0.207839, acc 0.890625
2017-03-02T18:10:30.184581: step 29381, loss 0.173166, acc 0.90625
2017-03-02T18:10:30.249703: step 29382, loss 0.138021, acc 0.921875
2017-03-02T18:10:30.329953: step 29383, loss 0.205938, acc 0.9375
2017-03-02T18:10:30.409031: step 29384, loss 0.119344, acc 0.96875
2017-03-02T18:10:30.476698: step 29385, loss 0.197778, acc 0.921875
2017-03-02T18:10:30.556769: step 29386, loss 0.217003, acc 0.90625
2017-03-02T18:10:30.635328: step 29387, loss 0.141644, acc 0.921875
2017-03-02T18:10:30.710260: step 29388, loss 0.0899103, acc 0.953125
2017-03-02T18:10:30.789874: step 29389, loss 0.172137, acc 0.890625
2017-03-02T18:10:30.866115: step 29390, loss 0.217683, acc 0.90625
2017-03-02T18:10:30.937408: step 29391, loss 0.247967, acc 0.90625
2017-03-02T18:10:31.012684: step 29392, loss 0.1314, acc 0.921875
2017-03-02T18:10:31.087261: step 29393, loss 0.240329, acc 0.859375
2017-03-02T18:10:31.154975: step 29394, loss 0.153307, acc 0.875
2017-03-02T18:10:31.226030: step 29395, loss 0.187485, acc 0.90625
2017-03-02T18:10:31.300765: step 29396, loss 0.0891989, acc 0.953125
2017-03-02T18:10:31.374836: step 29397, loss 0.18501, acc 0.9375
2017-03-02T18:10:31.447431: step 29398, loss 0.180404, acc 0.9375
2017-03-02T18:10:31.519673: step 29399, loss 0.0574157, acc 0.984375
2017-03-02T18:10:31.586738: step 29400, loss 0.321846, acc 0.75

Evaluation:
2017-03-02T18:10:31.625898: step 29400, loss 3.67438, acc 0.638068

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29400

2017-03-02T18:10:32.180168: step 29401, loss 0.153484, acc 0.9375
2017-03-02T18:10:32.260776: step 29402, loss 0.0804609, acc 0.96875
2017-03-02T18:10:32.336767: step 29403, loss 0.110051, acc 0.953125
2017-03-02T18:10:32.412744: step 29404, loss 0.151815, acc 0.953125
2017-03-02T18:10:32.491521: step 29405, loss 0.082611, acc 0.953125
2017-03-02T18:10:32.566170: step 29406, loss 0.194578, acc 0.921875
2017-03-02T18:10:32.639561: step 29407, loss 0.12777, acc 0.921875
2017-03-02T18:10:32.721126: step 29408, loss 0.0396701, acc 1
2017-03-02T18:10:32.789828: step 29409, loss 0.0584938, acc 0.984375
2017-03-02T18:10:32.874224: step 29410, loss 0.124626, acc 0.953125
2017-03-02T18:10:32.947185: step 29411, loss 0.176798, acc 0.9375
2017-03-02T18:10:33.019804: step 29412, loss 0.116356, acc 0.96875
2017-03-02T18:10:33.101844: step 29413, loss 0.146829, acc 0.921875
2017-03-02T18:10:33.176994: step 29414, loss 0.100858, acc 0.9375
2017-03-02T18:10:33.247893: step 29415, loss 0.127796, acc 0.953125
2017-03-02T18:10:33.309682: step 29416, loss 0.174117, acc 0.9375
2017-03-02T18:10:33.389848: step 29417, loss 0.175558, acc 0.921875
2017-03-02T18:10:33.459817: step 29418, loss 0.141018, acc 0.9375
2017-03-02T18:10:33.531787: step 29419, loss 0.04424, acc 0.984375
2017-03-02T18:10:33.604050: step 29420, loss 0.118954, acc 0.953125
2017-03-02T18:10:33.668811: step 29421, loss 0.190628, acc 0.9375
2017-03-02T18:10:33.736146: step 29422, loss 0.126601, acc 0.953125
2017-03-02T18:10:33.813708: step 29423, loss 0.118927, acc 0.9375
2017-03-02T18:10:33.889773: step 29424, loss 0.221992, acc 0.90625
2017-03-02T18:10:33.966198: step 29425, loss 0.174867, acc 0.9375
2017-03-02T18:10:34.048923: step 29426, loss 0.173191, acc 0.96875
2017-03-02T18:10:34.123566: step 29427, loss 0.195587, acc 0.875
2017-03-02T18:10:34.197997: step 29428, loss 0.150959, acc 0.953125
2017-03-02T18:10:34.273506: step 29429, loss 0.204957, acc 0.890625
2017-03-02T18:10:34.347394: step 29430, loss 0.19318, acc 0.921875
2017-03-02T18:10:34.418723: step 29431, loss 0.19484, acc 0.890625
2017-03-02T18:10:34.494166: step 29432, loss 0.133879, acc 0.921875
2017-03-02T18:10:34.570507: step 29433, loss 0.105486, acc 0.9375
2017-03-02T18:10:34.639292: step 29434, loss 0.225029, acc 0.890625
2017-03-02T18:10:34.708711: step 29435, loss 0.164509, acc 0.921875
2017-03-02T18:10:34.774689: step 29436, loss 0.143011, acc 0.9375
2017-03-02T18:10:34.843277: step 29437, loss 0.133662, acc 0.953125
2017-03-02T18:10:34.910048: step 29438, loss 0.0759422, acc 0.953125
2017-03-02T18:10:34.985565: step 29439, loss 0.062095, acc 0.96875
2017-03-02T18:10:35.057658: step 29440, loss 0.133863, acc 0.9375
2017-03-02T18:10:35.126995: step 29441, loss 0.175923, acc 0.9375
2017-03-02T18:10:35.199144: step 29442, loss 0.147663, acc 0.9375
2017-03-02T18:10:35.272613: step 29443, loss 0.168718, acc 0.90625
2017-03-02T18:10:35.337864: step 29444, loss 0.210286, acc 0.90625
2017-03-02T18:10:35.406433: step 29445, loss 0.091487, acc 0.96875
2017-03-02T18:10:35.479096: step 29446, loss 0.098709, acc 0.96875
2017-03-02T18:10:35.559886: step 29447, loss 0.216012, acc 0.890625
2017-03-02T18:10:35.624626: step 29448, loss 0.2487, acc 0.859375
2017-03-02T18:10:35.711878: step 29449, loss 0.0651498, acc 0.96875
2017-03-02T18:10:35.782298: step 29450, loss 0.146055, acc 0.9375
2017-03-02T18:10:35.852626: step 29451, loss 0.121977, acc 0.953125
2017-03-02T18:10:35.923816: step 29452, loss 0.239965, acc 0.890625
2017-03-02T18:10:36.006144: step 29453, loss 0.212774, acc 0.9375
2017-03-02T18:10:36.085206: step 29454, loss 0.0939266, acc 0.96875
2017-03-02T18:10:36.161008: step 29455, loss 0.260537, acc 0.90625
2017-03-02T18:10:36.234814: step 29456, loss 0.209839, acc 0.921875
2017-03-02T18:10:36.299604: step 29457, loss 0.120561, acc 0.9375
2017-03-02T18:10:36.372804: step 29458, loss 0.226515, acc 0.890625
2017-03-02T18:10:36.448674: step 29459, loss 0.101807, acc 0.953125
2017-03-02T18:10:36.519811: step 29460, loss 0.0878119, acc 0.953125
2017-03-02T18:10:36.594277: step 29461, loss 0.148539, acc 0.9375
2017-03-02T18:10:36.656861: step 29462, loss 0.096998, acc 0.953125
2017-03-02T18:10:36.732992: step 29463, loss 0.223604, acc 0.875
2017-03-02T18:10:36.805057: step 29464, loss 0.196399, acc 0.9375
2017-03-02T18:10:36.869902: step 29465, loss 0.0894584, acc 0.96875
2017-03-02T18:10:36.946886: step 29466, loss 0.1002, acc 0.953125
2017-03-02T18:10:37.009057: step 29467, loss 0.101702, acc 0.9375
2017-03-02T18:10:37.083362: step 29468, loss 0.0631432, acc 0.96875
2017-03-02T18:10:37.158107: step 29469, loss 0.0853653, acc 0.953125
2017-03-02T18:10:37.224814: step 29470, loss 0.0722746, acc 0.953125
2017-03-02T18:10:37.298739: step 29471, loss 0.170735, acc 0.921875
2017-03-02T18:10:37.376314: step 29472, loss 0.208083, acc 0.890625
2017-03-02T18:10:37.441871: step 29473, loss 0.0820423, acc 0.953125
2017-03-02T18:10:37.508985: step 29474, loss 0.117774, acc 0.953125
2017-03-02T18:10:37.579156: step 29475, loss 0.108677, acc 0.9375
2017-03-02T18:10:37.659749: step 29476, loss 0.0669024, acc 0.984375
2017-03-02T18:10:37.727819: step 29477, loss 0.112834, acc 0.953125
2017-03-02T18:10:37.824842: step 29478, loss 0.199992, acc 0.90625
2017-03-02T18:10:37.898122: step 29479, loss 0.175114, acc 0.921875
2017-03-02T18:10:37.985189: step 29480, loss 0.0988812, acc 0.953125
2017-03-02T18:10:38.056860: step 29481, loss 0.265391, acc 0.859375
2017-03-02T18:10:38.134043: step 29482, loss 0.182608, acc 0.90625
2017-03-02T18:10:38.204815: step 29483, loss 0.133747, acc 0.953125
2017-03-02T18:10:38.282321: step 29484, loss 0.17461, acc 0.921875
2017-03-02T18:10:38.361454: step 29485, loss 0.193738, acc 0.890625
2017-03-02T18:10:38.432101: step 29486, loss 0.132225, acc 0.953125
2017-03-02T18:10:38.503489: step 29487, loss 0.0951236, acc 0.9375
2017-03-02T18:10:38.579772: step 29488, loss 0.0687155, acc 0.96875
2017-03-02T18:10:38.652778: step 29489, loss 0.0878581, acc 0.96875
2017-03-02T18:10:38.725065: step 29490, loss 0.155934, acc 0.921875
2017-03-02T18:10:38.797218: step 29491, loss 0.112295, acc 0.953125
2017-03-02T18:10:38.870615: step 29492, loss 0.190041, acc 0.921875
2017-03-02T18:10:38.943925: step 29493, loss 0.089833, acc 0.953125
2017-03-02T18:10:39.014530: step 29494, loss 0.104321, acc 0.984375
2017-03-02T18:10:39.087665: step 29495, loss 0.156468, acc 0.890625
2017-03-02T18:10:39.169922: step 29496, loss 0.17069, acc 0.90625
2017-03-02T18:10:39.243848: step 29497, loss 0.189334, acc 0.921875
2017-03-02T18:10:39.314258: step 29498, loss 0.0965011, acc 0.9375
2017-03-02T18:10:39.388028: step 29499, loss 0.131185, acc 0.9375
2017-03-02T18:10:39.465023: step 29500, loss 0.119409, acc 0.953125

Evaluation:
2017-03-02T18:10:39.492542: step 29500, loss 3.68269, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29500

2017-03-02T18:10:39.945623: step 29501, loss 0.0967591, acc 0.953125
2017-03-02T18:10:40.010036: step 29502, loss 0.164743, acc 0.921875
2017-03-02T18:10:40.090010: step 29503, loss 0.185124, acc 0.953125
2017-03-02T18:10:40.161697: step 29504, loss 0.143757, acc 0.953125
2017-03-02T18:10:40.228520: step 29505, loss 0.116974, acc 0.953125
2017-03-02T18:10:40.297897: step 29506, loss 0.389317, acc 0.84375
2017-03-02T18:10:40.370183: step 29507, loss 0.100643, acc 0.953125
2017-03-02T18:10:40.443894: step 29508, loss 0.186137, acc 0.9375
2017-03-02T18:10:40.509819: step 29509, loss 0.180299, acc 0.890625
2017-03-02T18:10:40.580220: step 29510, loss 0.077894, acc 0.96875
2017-03-02T18:10:40.655591: step 29511, loss 0.125679, acc 0.9375
2017-03-02T18:10:40.735770: step 29512, loss 0.293756, acc 0.859375
2017-03-02T18:10:40.815065: step 29513, loss 0.161428, acc 0.921875
2017-03-02T18:10:40.891427: step 29514, loss 0.0759222, acc 0.984375
2017-03-02T18:10:40.958117: step 29515, loss 0.247697, acc 0.875
2017-03-02T18:10:41.030319: step 29516, loss 0.105131, acc 0.96875
2017-03-02T18:10:41.107881: step 29517, loss 0.17302, acc 0.9375
2017-03-02T18:10:41.175312: step 29518, loss 0.042681, acc 1
2017-03-02T18:10:41.248221: step 29519, loss 0.17001, acc 0.96875
2017-03-02T18:10:41.320088: step 29520, loss 0.14463, acc 0.96875
2017-03-02T18:10:41.400325: step 29521, loss 0.21504, acc 0.828125
2017-03-02T18:10:41.473033: step 29522, loss 0.174849, acc 0.96875
2017-03-02T18:10:41.547186: step 29523, loss 0.135685, acc 0.953125
2017-03-02T18:10:41.620374: step 29524, loss 0.155509, acc 0.890625
2017-03-02T18:10:41.692506: step 29525, loss 0.136733, acc 0.9375
2017-03-02T18:10:41.766407: step 29526, loss 0.135043, acc 0.921875
2017-03-02T18:10:41.847267: step 29527, loss 0.0998957, acc 0.96875
2017-03-02T18:10:41.919936: step 29528, loss 0.224988, acc 0.890625
2017-03-02T18:10:41.999272: step 29529, loss 0.226168, acc 0.921875
2017-03-02T18:10:42.070303: step 29530, loss 0.120181, acc 0.96875
2017-03-02T18:10:42.138052: step 29531, loss 0.123801, acc 0.9375
2017-03-02T18:10:42.208740: step 29532, loss 0.164211, acc 0.921875
2017-03-02T18:10:42.273214: step 29533, loss 0.0728238, acc 0.96875
2017-03-02T18:10:42.338742: step 29534, loss 0.171679, acc 0.9375
2017-03-02T18:10:42.413386: step 29535, loss 0.184032, acc 0.9375
2017-03-02T18:10:42.489330: step 29536, loss 0.13122, acc 0.9375
2017-03-02T18:10:42.556234: step 29537, loss 0.0968762, acc 0.984375
2017-03-02T18:10:42.624998: step 29538, loss 0.0959781, acc 0.953125
2017-03-02T18:10:42.691406: step 29539, loss 0.308181, acc 0.859375
2017-03-02T18:10:42.759438: step 29540, loss 0.0896238, acc 0.9375
2017-03-02T18:10:42.831864: step 29541, loss 0.219842, acc 0.90625
2017-03-02T18:10:42.903970: step 29542, loss 0.18918, acc 0.90625
2017-03-02T18:10:42.976357: step 29543, loss 0.0627091, acc 0.984375
2017-03-02T18:10:43.044194: step 29544, loss 0.150579, acc 0.9375
2017-03-02T18:10:43.114867: step 29545, loss 0.118967, acc 0.921875
2017-03-02T18:10:43.185621: step 29546, loss 0.126543, acc 0.953125
2017-03-02T18:10:43.249332: step 29547, loss 0.207099, acc 0.9375
2017-03-02T18:10:43.320707: step 29548, loss 0.0909634, acc 0.96875
2017-03-02T18:10:43.394815: step 29549, loss 0.0797711, acc 0.953125
2017-03-02T18:10:43.460397: step 29550, loss 0.16068, acc 0.921875
2017-03-02T18:10:43.533116: step 29551, loss 0.111237, acc 0.96875
2017-03-02T18:10:43.615990: step 29552, loss 0.128336, acc 0.921875
2017-03-02T18:10:43.685599: step 29553, loss 0.175553, acc 0.890625
2017-03-02T18:10:43.757981: step 29554, loss 0.0516692, acc 0.984375
2017-03-02T18:10:43.829197: step 29555, loss 0.105878, acc 0.9375
2017-03-02T18:10:43.903151: step 29556, loss 0.143726, acc 0.9375
2017-03-02T18:10:43.972890: step 29557, loss 0.167677, acc 0.921875
2017-03-02T18:10:44.044484: step 29558, loss 0.147256, acc 0.921875
2017-03-02T18:10:44.121986: step 29559, loss 0.212727, acc 0.90625
2017-03-02T18:10:44.199631: step 29560, loss 0.0519616, acc 1
2017-03-02T18:10:44.271864: step 29561, loss 0.148118, acc 0.9375
2017-03-02T18:10:44.350104: step 29562, loss 0.16931, acc 0.890625
2017-03-02T18:10:44.422230: step 29563, loss 0.147084, acc 0.953125
2017-03-02T18:10:44.487653: step 29564, loss 0.109944, acc 0.96875
2017-03-02T18:10:44.560999: step 29565, loss 0.345655, acc 0.875
2017-03-02T18:10:44.629977: step 29566, loss 0.252301, acc 0.890625
2017-03-02T18:10:44.701122: step 29567, loss 0.162096, acc 0.90625
2017-03-02T18:10:44.779590: step 29568, loss 0.111503, acc 0.9375
2017-03-02T18:10:44.846610: step 29569, loss 0.14001, acc 0.921875
2017-03-02T18:10:44.914837: step 29570, loss 0.143856, acc 0.9375
2017-03-02T18:10:44.989728: step 29571, loss 0.249666, acc 0.859375
2017-03-02T18:10:45.064835: step 29572, loss 0.154776, acc 0.90625
2017-03-02T18:10:45.139266: step 29573, loss 0.112412, acc 0.921875
2017-03-02T18:10:45.211294: step 29574, loss 0.205777, acc 0.921875
2017-03-02T18:10:45.282416: step 29575, loss 0.148907, acc 0.9375
2017-03-02T18:10:45.352300: step 29576, loss 0.213912, acc 0.890625
2017-03-02T18:10:45.427256: step 29577, loss 0.165131, acc 0.90625
2017-03-02T18:10:45.507540: step 29578, loss 0.169238, acc 0.890625
2017-03-02T18:10:45.598951: step 29579, loss 0.196295, acc 0.921875
2017-03-02T18:10:45.674028: step 29580, loss 0.149335, acc 0.921875
2017-03-02T18:10:45.756919: step 29581, loss 0.147556, acc 0.921875
2017-03-02T18:10:45.827058: step 29582, loss 0.129375, acc 0.9375
2017-03-02T18:10:45.900345: step 29583, loss 0.155697, acc 0.953125
2017-03-02T18:10:45.978044: step 29584, loss 0.168678, acc 0.9375
2017-03-02T18:10:46.043652: step 29585, loss 0.195275, acc 0.890625
2017-03-02T18:10:46.110478: step 29586, loss 0.111932, acc 0.9375
2017-03-02T18:10:46.182927: step 29587, loss 0.137054, acc 0.921875
2017-03-02T18:10:46.253899: step 29588, loss 0.172284, acc 0.90625
2017-03-02T18:10:46.327578: step 29589, loss 0.215843, acc 0.890625
2017-03-02T18:10:46.399472: step 29590, loss 0.128301, acc 0.96875
2017-03-02T18:10:46.485903: step 29591, loss 0.178072, acc 0.96875
2017-03-02T18:10:46.554793: step 29592, loss 0.0890059, acc 0.984375
2017-03-02T18:10:46.624717: step 29593, loss 0.121716, acc 0.953125
2017-03-02T18:10:46.698416: step 29594, loss 0.0811082, acc 0.953125
2017-03-02T18:10:46.774561: step 29595, loss 0.204653, acc 0.921875
2017-03-02T18:10:46.848806: step 29596, loss 0.597929, acc 0.75
2017-03-02T18:10:46.925713: step 29597, loss 0.0961529, acc 0.96875
2017-03-02T18:10:46.996350: step 29598, loss 0.0875387, acc 0.9375
2017-03-02T18:10:47.067508: step 29599, loss 0.223205, acc 0.84375
2017-03-02T18:10:47.140079: step 29600, loss 0.232423, acc 0.875

Evaluation:
2017-03-02T18:10:47.175322: step 29600, loss 3.66585, acc 0.648162

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29600

2017-03-02T18:10:47.641955: step 29601, loss 0.15432, acc 0.9375
2017-03-02T18:10:47.713262: step 29602, loss 0.048805, acc 1
2017-03-02T18:10:47.789691: step 29603, loss 0.0938766, acc 0.953125
2017-03-02T18:10:47.863501: step 29604, loss 0.16697, acc 0.9375
2017-03-02T18:10:47.941382: step 29605, loss 0.182775, acc 0.90625
2017-03-02T18:10:48.008690: step 29606, loss 0.109176, acc 0.953125
2017-03-02T18:10:48.078424: step 29607, loss 0.26512, acc 0.890625
2017-03-02T18:10:48.145693: step 29608, loss 0.130378, acc 0.9375
2017-03-02T18:10:48.220338: step 29609, loss 0.210017, acc 0.921875
2017-03-02T18:10:48.292179: step 29610, loss 0.249358, acc 0.890625
2017-03-02T18:10:48.380949: step 29611, loss 0.0727959, acc 0.96875
2017-03-02T18:10:48.457231: step 29612, loss 0.141393, acc 0.921875
2017-03-02T18:10:48.530943: step 29613, loss 0.0982293, acc 0.953125
2017-03-02T18:10:48.613342: step 29614, loss 0.147649, acc 0.9375
2017-03-02T18:10:48.691113: step 29615, loss 0.200601, acc 0.921875
2017-03-02T18:10:48.768759: step 29616, loss 0.101536, acc 0.984375
2017-03-02T18:10:48.834323: step 29617, loss 0.117273, acc 0.9375
2017-03-02T18:10:48.910702: step 29618, loss 0.209179, acc 0.90625
2017-03-02T18:10:48.994987: step 29619, loss 0.221957, acc 0.875
2017-03-02T18:10:49.069439: step 29620, loss 0.164983, acc 0.90625
2017-03-02T18:10:49.138139: step 29621, loss 0.219099, acc 0.90625
2017-03-02T18:10:49.217404: step 29622, loss 0.222647, acc 0.890625
2017-03-02T18:10:49.290101: step 29623, loss 0.141749, acc 0.90625
2017-03-02T18:10:49.367896: step 29624, loss 0.166172, acc 0.90625
2017-03-02T18:10:49.440346: step 29625, loss 0.1168, acc 0.953125
2017-03-02T18:10:49.527790: step 29626, loss 0.1428, acc 0.9375
2017-03-02T18:10:49.612930: step 29627, loss 0.105003, acc 0.96875
2017-03-02T18:10:49.681293: step 29628, loss 0.101346, acc 0.9375
2017-03-02T18:10:49.753531: step 29629, loss 0.201555, acc 0.921875
2017-03-02T18:10:49.827138: step 29630, loss 0.120773, acc 0.9375
2017-03-02T18:10:49.899111: step 29631, loss 0.126762, acc 0.984375
2017-03-02T18:10:49.972117: step 29632, loss 0.190037, acc 0.90625
2017-03-02T18:10:50.050844: step 29633, loss 0.12268, acc 0.9375
2017-03-02T18:10:50.132718: step 29634, loss 0.179243, acc 0.921875
2017-03-02T18:10:50.202938: step 29635, loss 0.167354, acc 0.921875
2017-03-02T18:10:50.270606: step 29636, loss 0.194642, acc 0.90625
2017-03-02T18:10:50.345201: step 29637, loss 0.158512, acc 0.921875
2017-03-02T18:10:50.417495: step 29638, loss 0.179679, acc 0.9375
2017-03-02T18:10:50.489053: step 29639, loss 0.120012, acc 0.953125
2017-03-02T18:10:50.561915: step 29640, loss 0.0826352, acc 0.984375
2017-03-02T18:10:50.635844: step 29641, loss 0.0989287, acc 0.96875
2017-03-02T18:10:50.711618: step 29642, loss 0.120582, acc 0.953125
2017-03-02T18:10:50.785033: step 29643, loss 0.13707, acc 0.921875
2017-03-02T18:10:50.860908: step 29644, loss 0.0873519, acc 0.96875
2017-03-02T18:10:50.928768: step 29645, loss 0.0911654, acc 0.953125
2017-03-02T18:10:51.006282: step 29646, loss 0.254658, acc 0.859375
2017-03-02T18:10:51.076245: step 29647, loss 0.115761, acc 0.921875
2017-03-02T18:10:51.148927: step 29648, loss 0.217321, acc 0.90625
2017-03-02T18:10:51.221933: step 29649, loss 0.0575481, acc 0.984375
2017-03-02T18:10:51.285508: step 29650, loss 0.145014, acc 0.953125
2017-03-02T18:10:51.359250: step 29651, loss 0.0915913, acc 0.96875
2017-03-02T18:10:51.427051: step 29652, loss 0.0819704, acc 0.953125
2017-03-02T18:10:51.497381: step 29653, loss 0.256715, acc 0.890625
2017-03-02T18:10:51.572711: step 29654, loss 0.0861389, acc 0.96875
2017-03-02T18:10:51.644546: step 29655, loss 0.115454, acc 0.9375
2017-03-02T18:10:51.712937: step 29656, loss 0.217113, acc 0.921875
2017-03-02T18:10:51.784199: step 29657, loss 0.0331152, acc 0.984375
2017-03-02T18:10:51.861585: step 29658, loss 0.142681, acc 0.90625
2017-03-02T18:10:51.952254: step 29659, loss 0.104507, acc 0.9375
2017-03-02T18:10:52.026350: step 29660, loss 0.0802687, acc 0.96875
2017-03-02T18:10:52.095518: step 29661, loss 0.147695, acc 0.9375
2017-03-02T18:10:52.168041: step 29662, loss 0.137808, acc 0.9375
2017-03-02T18:10:52.240004: step 29663, loss 0.237596, acc 0.90625
2017-03-02T18:10:52.312348: step 29664, loss 0.104551, acc 0.9375
2017-03-02T18:10:52.382204: step 29665, loss 0.0923265, acc 0.984375
2017-03-02T18:10:52.453963: step 29666, loss 0.108149, acc 0.96875
2017-03-02T18:10:52.527191: step 29667, loss 0.335262, acc 0.875
2017-03-02T18:10:52.604331: step 29668, loss 0.21657, acc 0.90625
2017-03-02T18:10:52.698400: step 29669, loss 0.139011, acc 0.921875
2017-03-02T18:10:52.769931: step 29670, loss 0.131198, acc 0.953125
2017-03-02T18:10:52.841963: step 29671, loss 0.126212, acc 0.953125
2017-03-02T18:10:52.916579: step 29672, loss 0.152544, acc 0.953125
2017-03-02T18:10:52.987084: step 29673, loss 0.0899129, acc 0.953125
2017-03-02T18:10:53.057719: step 29674, loss 0.0800381, acc 0.96875
2017-03-02T18:10:53.127468: step 29675, loss 0.21586, acc 0.875
2017-03-02T18:10:53.196794: step 29676, loss 0.136182, acc 0.96875
2017-03-02T18:10:53.271250: step 29677, loss 0.227354, acc 0.875
2017-03-02T18:10:53.344303: step 29678, loss 0.133875, acc 0.921875
2017-03-02T18:10:53.421034: step 29679, loss 0.121076, acc 0.921875
2017-03-02T18:10:53.493330: step 29680, loss 0.190167, acc 0.9375
2017-03-02T18:10:53.557658: step 29681, loss 0.297214, acc 0.859375
2017-03-02T18:10:53.627062: step 29682, loss 0.0598016, acc 1
2017-03-02T18:10:53.697106: step 29683, loss 0.165742, acc 0.890625
2017-03-02T18:10:53.765837: step 29684, loss 0.216806, acc 0.921875
2017-03-02T18:10:53.828282: step 29685, loss 0.151088, acc 0.921875
2017-03-02T18:10:53.908120: step 29686, loss 0.184416, acc 0.9375
2017-03-02T18:10:53.981867: step 29687, loss 0.202827, acc 0.9375
2017-03-02T18:10:54.056965: step 29688, loss 0.174729, acc 0.9375
2017-03-02T18:10:54.126287: step 29689, loss 0.119663, acc 0.9375
2017-03-02T18:10:54.206117: step 29690, loss 0.130566, acc 0.90625
2017-03-02T18:10:54.276752: step 29691, loss 0.0718782, acc 0.96875
2017-03-02T18:10:54.351481: step 29692, loss 0.22008, acc 0.9375
2017-03-02T18:10:54.416488: step 29693, loss 0.104572, acc 0.953125
2017-03-02T18:10:54.489507: step 29694, loss 0.226489, acc 0.890625
2017-03-02T18:10:54.561296: step 29695, loss 0.192794, acc 0.90625
2017-03-02T18:10:54.634379: step 29696, loss 0.127611, acc 0.953125
2017-03-02T18:10:54.705457: step 29697, loss 0.0978114, acc 0.953125
2017-03-02T18:10:54.773847: step 29698, loss 0.250427, acc 0.859375
2017-03-02T18:10:54.859159: step 29699, loss 0.180369, acc 0.90625
2017-03-02T18:10:54.930941: step 29700, loss 0.178096, acc 0.921875

Evaluation:
2017-03-02T18:10:54.968871: step 29700, loss 3.67435, acc 0.647441

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29700

2017-03-02T18:10:55.446734: step 29701, loss 0.111477, acc 0.953125
2017-03-02T18:10:55.520634: step 29702, loss 0.115626, acc 0.9375
2017-03-02T18:10:55.592608: step 29703, loss 0.228836, acc 0.90625
2017-03-02T18:10:55.665302: step 29704, loss 0.211519, acc 0.9375
2017-03-02T18:10:55.738282: step 29705, loss 0.252844, acc 0.859375
2017-03-02T18:10:55.811700: step 29706, loss 0.206189, acc 0.875
2017-03-02T18:10:55.884639: step 29707, loss 0.123631, acc 0.953125
2017-03-02T18:10:55.956779: step 29708, loss 0.0631263, acc 0.96875
2017-03-02T18:10:56.032656: step 29709, loss 0.276651, acc 0.890625
2017-03-02T18:10:56.108737: step 29710, loss 0.132686, acc 0.921875
2017-03-02T18:10:56.180978: step 29711, loss 0.120727, acc 0.9375
2017-03-02T18:10:56.251358: step 29712, loss 0.112527, acc 0.96875
2017-03-02T18:10:56.323523: step 29713, loss 0.115299, acc 0.96875
2017-03-02T18:10:56.404995: step 29714, loss 0.056586, acc 0.984375
2017-03-02T18:10:56.482096: step 29715, loss 0.112411, acc 0.9375
2017-03-02T18:10:56.556501: step 29716, loss 0.210126, acc 0.875
2017-03-02T18:10:56.625916: step 29717, loss 0.139663, acc 0.921875
2017-03-02T18:10:56.700220: step 29718, loss 0.119973, acc 0.96875
2017-03-02T18:10:56.774097: step 29719, loss 0.181464, acc 0.921875
2017-03-02T18:10:56.844365: step 29720, loss 0.0795151, acc 0.96875
2017-03-02T18:10:56.925073: step 29721, loss 0.204533, acc 0.90625
2017-03-02T18:10:56.998696: step 29722, loss 0.140469, acc 0.9375
2017-03-02T18:10:57.077645: step 29723, loss 0.163964, acc 0.875
2017-03-02T18:10:57.149049: step 29724, loss 0.0840307, acc 0.953125
2017-03-02T18:10:57.216239: step 29725, loss 0.156862, acc 0.9375
2017-03-02T18:10:57.289433: step 29726, loss 0.156188, acc 0.9375
2017-03-02T18:10:57.361233: step 29727, loss 0.116229, acc 0.953125
2017-03-02T18:10:57.441005: step 29728, loss 0.116604, acc 0.9375
2017-03-02T18:10:57.518569: step 29729, loss 0.0649771, acc 0.96875
2017-03-02T18:10:57.590126: step 29730, loss 0.168185, acc 0.921875
2017-03-02T18:10:57.665688: step 29731, loss 0.132158, acc 0.921875
2017-03-02T18:10:57.738782: step 29732, loss 0.218602, acc 0.890625
2017-03-02T18:10:57.809942: step 29733, loss 0.27866, acc 0.890625
2017-03-02T18:10:57.880254: step 29734, loss 0.120585, acc 0.9375
2017-03-02T18:10:57.951262: step 29735, loss 0.115257, acc 0.921875
2017-03-02T18:10:58.042613: step 29736, loss 0.100221, acc 0.9375
2017-03-02T18:10:58.121961: step 29737, loss 0.169252, acc 0.921875
2017-03-02T18:10:58.195050: step 29738, loss 0.0874601, acc 0.96875
2017-03-02T18:10:58.266301: step 29739, loss 0.19649, acc 0.921875
2017-03-02T18:10:58.336179: step 29740, loss 0.0900513, acc 0.96875
2017-03-02T18:10:58.407048: step 29741, loss 0.211638, acc 0.921875
2017-03-02T18:10:58.483948: step 29742, loss 0.124432, acc 0.9375
2017-03-02T18:10:58.551186: step 29743, loss 0.235904, acc 0.859375
2017-03-02T18:10:58.619715: step 29744, loss 0.207716, acc 0.90625
2017-03-02T18:10:58.688097: step 29745, loss 0.206342, acc 0.921875
2017-03-02T18:10:58.767543: step 29746, loss 0.151172, acc 0.953125
2017-03-02T18:10:58.841386: step 29747, loss 0.112884, acc 0.9375
2017-03-02T18:10:58.916493: step 29748, loss 0.109809, acc 0.9375
2017-03-02T18:10:58.986743: step 29749, loss 0.190195, acc 0.90625
2017-03-02T18:10:59.057895: step 29750, loss 0.145051, acc 0.921875
2017-03-02T18:10:59.135374: step 29751, loss 0.160573, acc 0.9375
2017-03-02T18:10:59.202384: step 29752, loss 0.0336107, acc 0.984375
2017-03-02T18:10:59.271685: step 29753, loss 0.0922125, acc 0.984375
2017-03-02T18:10:59.341738: step 29754, loss 0.157519, acc 0.921875
2017-03-02T18:10:59.416871: step 29755, loss 0.0837257, acc 0.96875
2017-03-02T18:10:59.489316: step 29756, loss 0.236695, acc 0.921875
2017-03-02T18:10:59.579386: step 29757, loss 0.138707, acc 0.921875
2017-03-02T18:10:59.659574: step 29758, loss 0.0862465, acc 0.953125
2017-03-02T18:10:59.738776: step 29759, loss 0.0434818, acc 1
2017-03-02T18:10:59.803602: step 29760, loss 0.149344, acc 0.9375
2017-03-02T18:10:59.875008: step 29761, loss 0.210748, acc 0.890625
2017-03-02T18:10:59.950841: step 29762, loss 0.103192, acc 0.953125
2017-03-02T18:11:00.018317: step 29763, loss 0.128541, acc 0.953125
2017-03-02T18:11:00.090685: step 29764, loss 0.197767, acc 0.890625
2017-03-02T18:11:00.162378: step 29765, loss 0.0826788, acc 0.984375
2017-03-02T18:11:00.231095: step 29766, loss 0.042409, acc 1
2017-03-02T18:11:00.303140: step 29767, loss 0.121267, acc 0.953125
2017-03-02T18:11:00.376579: step 29768, loss 0.206133, acc 0.875
2017-03-02T18:11:00.450037: step 29769, loss 0.234466, acc 0.9375
2017-03-02T18:11:00.525072: step 29770, loss 0.147021, acc 0.9375
2017-03-02T18:11:00.598107: step 29771, loss 0.189937, acc 0.90625
2017-03-02T18:11:00.666250: step 29772, loss 0.0440313, acc 0.984375
2017-03-02T18:11:00.735157: step 29773, loss 0.2065, acc 0.890625
2017-03-02T18:11:00.811113: step 29774, loss 0.157009, acc 0.9375
2017-03-02T18:11:00.886729: step 29775, loss 0.178675, acc 0.9375
2017-03-02T18:11:00.958814: step 29776, loss 0.153254, acc 0.90625
2017-03-02T18:11:01.040099: step 29777, loss 0.121535, acc 0.96875
2017-03-02T18:11:01.111922: step 29778, loss 0.0902404, acc 0.96875
2017-03-02T18:11:01.189227: step 29779, loss 0.0510853, acc 0.96875
2017-03-02T18:11:01.271197: step 29780, loss 0.194633, acc 0.921875
2017-03-02T18:11:01.340406: step 29781, loss 0.130902, acc 0.953125
2017-03-02T18:11:01.403167: step 29782, loss 0.0585359, acc 0.96875
2017-03-02T18:11:01.480083: step 29783, loss 0.0926595, acc 0.96875
2017-03-02T18:11:01.555760: step 29784, loss 0.149412, acc 0.953125
2017-03-02T18:11:01.636716: step 29785, loss 0.0816298, acc 0.9375
2017-03-02T18:11:01.710027: step 29786, loss 0.215463, acc 0.875
2017-03-02T18:11:01.818475: step 29787, loss 0.0530737, acc 0.984375
2017-03-02T18:11:01.892148: step 29788, loss 0.16657, acc 0.90625
2017-03-02T18:11:01.983092: step 29789, loss 0.126021, acc 0.9375
2017-03-02T18:11:02.064027: step 29790, loss 0.13785, acc 0.921875
2017-03-02T18:11:02.135740: step 29791, loss 0.118148, acc 0.953125
2017-03-02T18:11:02.227752: step 29792, loss 0.123636, acc 1
2017-03-02T18:11:02.311204: step 29793, loss 0.119851, acc 0.96875
2017-03-02T18:11:02.389870: step 29794, loss 0.17672, acc 0.921875
2017-03-02T18:11:02.463047: step 29795, loss 0.147339, acc 0.90625
2017-03-02T18:11:02.529330: step 29796, loss 0.115692, acc 0.953125
2017-03-02T18:11:02.608436: step 29797, loss 0.101317, acc 0.953125
2017-03-02T18:11:02.680620: step 29798, loss 0.128317, acc 0.9375
2017-03-02T18:11:02.752363: step 29799, loss 0.100966, acc 0.953125
2017-03-02T18:11:02.823080: step 29800, loss 0.128232, acc 0.96875

Evaluation:
2017-03-02T18:11:02.861487: step 29800, loss 3.74647, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29800

2017-03-02T18:11:03.301689: step 29801, loss 0.108163, acc 0.9375
2017-03-02T18:11:03.378755: step 29802, loss 0.0762495, acc 0.953125
2017-03-02T18:11:03.446517: step 29803, loss 0.162584, acc 0.921875
2017-03-02T18:11:03.518893: step 29804, loss 0.235524, acc 0.921875
2017-03-02T18:11:03.590593: step 29805, loss 0.135052, acc 0.9375
2017-03-02T18:11:03.667567: step 29806, loss 0.149918, acc 0.921875
2017-03-02T18:11:03.737889: step 29807, loss 0.161952, acc 0.953125
2017-03-02T18:11:03.809490: step 29808, loss 0.267306, acc 0.875
2017-03-02T18:11:03.883312: step 29809, loss 0.112619, acc 0.9375
2017-03-02T18:11:03.954269: step 29810, loss 0.0644863, acc 0.984375
2017-03-02T18:11:04.027301: step 29811, loss 0.0897006, acc 0.96875
2017-03-02T18:11:04.098635: step 29812, loss 0.0710691, acc 0.984375
2017-03-02T18:11:04.168089: step 29813, loss 0.0840405, acc 0.96875
2017-03-02T18:11:04.238505: step 29814, loss 0.141537, acc 0.9375
2017-03-02T18:11:04.312091: step 29815, loss 0.0740218, acc 0.96875
2017-03-02T18:11:04.385244: step 29816, loss 0.155691, acc 0.90625
2017-03-02T18:11:04.474531: step 29817, loss 0.0893412, acc 0.9375
2017-03-02T18:11:04.550737: step 29818, loss 0.103759, acc 0.9375
2017-03-02T18:11:04.621797: step 29819, loss 0.11528, acc 0.953125
2017-03-02T18:11:04.695217: step 29820, loss 0.134778, acc 0.921875
2017-03-02T18:11:04.770680: step 29821, loss 0.0993989, acc 0.984375
2017-03-02T18:11:04.840938: step 29822, loss 0.103954, acc 0.953125
2017-03-02T18:11:04.915603: step 29823, loss 0.104122, acc 0.984375
2017-03-02T18:11:04.987087: step 29824, loss 0.25815, acc 0.890625
2017-03-02T18:11:05.059416: step 29825, loss 0.281696, acc 0.921875
2017-03-02T18:11:05.138440: step 29826, loss 0.11067, acc 0.953125
2017-03-02T18:11:05.212585: step 29827, loss 0.125519, acc 0.953125
2017-03-02T18:11:05.285249: step 29828, loss 0.152521, acc 0.9375
2017-03-02T18:11:05.359416: step 29829, loss 0.101801, acc 0.953125
2017-03-02T18:11:05.434896: step 29830, loss 0.0931775, acc 0.96875
2017-03-02T18:11:05.501402: step 29831, loss 0.104567, acc 0.9375
2017-03-02T18:11:05.567434: step 29832, loss 0.200445, acc 0.921875
2017-03-02T18:11:05.640121: step 29833, loss 0.101445, acc 0.953125
2017-03-02T18:11:05.712540: step 29834, loss 0.258039, acc 0.890625
2017-03-02T18:11:05.786162: step 29835, loss 0.087943, acc 0.984375
2017-03-02T18:11:05.857768: step 29836, loss 0.0872835, acc 0.96875
2017-03-02T18:11:05.929269: step 29837, loss 0.160697, acc 0.90625
2017-03-02T18:11:06.006269: step 29838, loss 0.162126, acc 0.90625
2017-03-02T18:11:06.076792: step 29839, loss 0.169728, acc 0.90625
2017-03-02T18:11:06.148034: step 29840, loss 0.129304, acc 0.9375
2017-03-02T18:11:06.215820: step 29841, loss 0.100013, acc 0.96875
2017-03-02T18:11:06.292579: step 29842, loss 0.106669, acc 0.953125
2017-03-02T18:11:06.362092: step 29843, loss 0.127284, acc 0.921875
2017-03-02T18:11:06.444175: step 29844, loss 0.0873233, acc 0.953125
2017-03-02T18:11:06.517811: step 29845, loss 0.175266, acc 0.90625
2017-03-02T18:11:06.588350: step 29846, loss 0.137175, acc 0.9375
2017-03-02T18:11:06.659017: step 29847, loss 0.165704, acc 0.90625
2017-03-02T18:11:06.729364: step 29848, loss 0.283816, acc 0.90625
2017-03-02T18:11:06.805223: step 29849, loss 0.172533, acc 0.984375
2017-03-02T18:11:06.872222: step 29850, loss 0.107565, acc 0.9375
2017-03-02T18:11:06.935332: step 29851, loss 0.0885396, acc 0.953125
2017-03-02T18:11:07.006865: step 29852, loss 0.0873223, acc 0.953125
2017-03-02T18:11:07.074945: step 29853, loss 0.15271, acc 0.9375
2017-03-02T18:11:07.145943: step 29854, loss 0.114654, acc 0.9375
2017-03-02T18:11:07.214591: step 29855, loss 0.0948345, acc 0.96875
2017-03-02T18:11:07.294944: step 29856, loss 0.0636443, acc 0.96875
2017-03-02T18:11:07.370286: step 29857, loss 0.0916677, acc 0.953125
2017-03-02T18:11:07.443202: step 29858, loss 0.241736, acc 0.875
2017-03-02T18:11:07.525918: step 29859, loss 0.201658, acc 0.953125
2017-03-02T18:11:07.595738: step 29860, loss 0.0719723, acc 0.953125
2017-03-02T18:11:07.662938: step 29861, loss 0.0850166, acc 0.953125
2017-03-02T18:11:07.753417: step 29862, loss 0.120119, acc 0.953125
2017-03-02T18:11:07.825521: step 29863, loss 0.172515, acc 0.953125
2017-03-02T18:11:07.900587: step 29864, loss 0.100181, acc 0.9375
2017-03-02T18:11:07.972319: step 29865, loss 0.128551, acc 0.9375
2017-03-02T18:11:08.044348: step 29866, loss 0.207553, acc 0.921875
2017-03-02T18:11:08.121009: step 29867, loss 0.143913, acc 0.953125
2017-03-02T18:11:08.198783: step 29868, loss 0.218936, acc 0.859375
2017-03-02T18:11:08.271204: step 29869, loss 0.120998, acc 0.921875
2017-03-02T18:11:08.362934: step 29870, loss 0.0544775, acc 0.984375
2017-03-02T18:11:08.447294: step 29871, loss 0.218247, acc 0.890625
2017-03-02T18:11:08.521077: step 29872, loss 0.0562842, acc 0.96875
2017-03-02T18:11:08.595050: step 29873, loss 0.159588, acc 0.953125
2017-03-02T18:11:08.672154: step 29874, loss 0.192828, acc 0.890625
2017-03-02T18:11:08.742710: step 29875, loss 0.090293, acc 0.984375
2017-03-02T18:11:08.812427: step 29876, loss 0.0904932, acc 0.953125
2017-03-02T18:11:08.884634: step 29877, loss 0.223332, acc 0.921875
2017-03-02T18:11:08.953267: step 29878, loss 0.0990675, acc 0.96875
2017-03-02T18:11:09.017015: step 29879, loss 0.157164, acc 0.9375
2017-03-02T18:11:09.089697: step 29880, loss 0.242872, acc 0.90625
2017-03-02T18:11:09.168827: step 29881, loss 0.157296, acc 0.90625
2017-03-02T18:11:09.235752: step 29882, loss 0.110908, acc 0.96875
2017-03-02T18:11:09.301452: step 29883, loss 0.11869, acc 0.9375
2017-03-02T18:11:09.360461: step 29884, loss 0.152046, acc 0.90625
2017-03-02T18:11:09.433772: step 29885, loss 0.23219, acc 0.921875
2017-03-02T18:11:09.523155: step 29886, loss 0.264519, acc 0.90625
2017-03-02T18:11:09.594784: step 29887, loss 0.139651, acc 0.921875
2017-03-02T18:11:09.667946: step 29888, loss 0.125125, acc 0.96875
2017-03-02T18:11:09.738763: step 29889, loss 0.142193, acc 0.9375
2017-03-02T18:11:09.813872: step 29890, loss 0.13648, acc 0.9375
2017-03-02T18:11:09.886386: step 29891, loss 0.117295, acc 0.953125
2017-03-02T18:11:09.970776: step 29892, loss 0.146153, acc 0.9375
2017-03-02T18:11:10.047472: step 29893, loss 0.140997, acc 0.953125
2017-03-02T18:11:10.125690: step 29894, loss 0.206787, acc 0.921875
2017-03-02T18:11:10.197550: step 29895, loss 0.0727957, acc 0.984375
2017-03-02T18:11:10.268405: step 29896, loss 0.111368, acc 0.9375
2017-03-02T18:11:10.336406: step 29897, loss 0.180461, acc 0.90625
2017-03-02T18:11:10.407089: step 29898, loss 0.100447, acc 0.921875
2017-03-02T18:11:10.489957: step 29899, loss 0.187017, acc 0.921875
2017-03-02T18:11:10.569984: step 29900, loss 0.229587, acc 0.875

Evaluation:
2017-03-02T18:11:10.605329: step 29900, loss 3.8133, acc 0.63951

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-29900

2017-03-02T18:11:11.058989: step 29901, loss 0.142277, acc 0.953125
2017-03-02T18:11:11.143292: step 29902, loss 0.194805, acc 0.90625
2017-03-02T18:11:11.220958: step 29903, loss 0.0957226, acc 0.953125
2017-03-02T18:11:11.296460: step 29904, loss 0.25193, acc 0.90625
2017-03-02T18:11:11.369739: step 29905, loss 0.0893204, acc 0.953125
2017-03-02T18:11:11.446720: step 29906, loss 0.157541, acc 0.9375
2017-03-02T18:11:11.527886: step 29907, loss 0.168729, acc 0.921875
2017-03-02T18:11:11.608441: step 29908, loss 0.116554, acc 0.90625
2017-03-02T18:11:11.681878: step 29909, loss 0.0687917, acc 0.984375
2017-03-02T18:11:11.747958: step 29910, loss 0.1086, acc 0.953125
2017-03-02T18:11:11.829313: step 29911, loss 0.131953, acc 0.953125
2017-03-02T18:11:11.902219: step 29912, loss 0.140315, acc 0.921875
2017-03-02T18:11:11.984632: step 29913, loss 0.186055, acc 0.921875
2017-03-02T18:11:12.056351: step 29914, loss 0.0961685, acc 0.96875
2017-03-02T18:11:12.129173: step 29915, loss 0.191632, acc 0.921875
2017-03-02T18:11:12.195676: step 29916, loss 0.0819241, acc 0.984375
2017-03-02T18:11:12.266383: step 29917, loss 0.0521013, acc 0.984375
2017-03-02T18:11:12.334401: step 29918, loss 0.24429, acc 0.875
2017-03-02T18:11:12.406427: step 29919, loss 0.0860694, acc 0.953125
2017-03-02T18:11:12.478325: step 29920, loss 0.181014, acc 0.875
2017-03-02T18:11:12.547162: step 29921, loss 0.138942, acc 0.953125
2017-03-02T18:11:12.617508: step 29922, loss 0.165431, acc 0.921875
2017-03-02T18:11:12.688095: step 29923, loss 0.12398, acc 0.921875
2017-03-02T18:11:12.762294: step 29924, loss 0.0929989, acc 0.984375
2017-03-02T18:11:12.830689: step 29925, loss 0.240917, acc 0.890625
2017-03-02T18:11:12.906620: step 29926, loss 0.120793, acc 0.921875
2017-03-02T18:11:12.983016: step 29927, loss 0.209341, acc 0.890625
2017-03-02T18:11:13.046620: step 29928, loss 0.116679, acc 0.9375
2017-03-02T18:11:13.115470: step 29929, loss 0.458404, acc 0.8125
2017-03-02T18:11:13.181686: step 29930, loss 0.0644147, acc 0.984375
2017-03-02T18:11:13.253017: step 29931, loss 0.212267, acc 0.90625
2017-03-02T18:11:13.326506: step 29932, loss 0.07265, acc 0.953125
2017-03-02T18:11:13.398762: step 29933, loss 0.136498, acc 0.921875
2017-03-02T18:11:13.472887: step 29934, loss 0.0841384, acc 0.96875
2017-03-02T18:11:13.543984: step 29935, loss 0.0696705, acc 0.96875
2017-03-02T18:11:13.608222: step 29936, loss 0.0890672, acc 0.953125
2017-03-02T18:11:13.678998: step 29937, loss 0.164938, acc 0.921875
2017-03-02T18:11:13.745529: step 29938, loss 0.143536, acc 0.9375
2017-03-02T18:11:13.809526: step 29939, loss 0.103592, acc 0.9375
2017-03-02T18:11:13.867133: step 29940, loss 0.197037, acc 0.9375
2017-03-02T18:11:13.938364: step 29941, loss 0.194295, acc 0.921875
2017-03-02T18:11:13.999106: step 29942, loss 0.229293, acc 0.890625
2017-03-02T18:11:14.088147: step 29943, loss 0.318238, acc 0.859375
2017-03-02T18:11:14.179179: step 29944, loss 0.134979, acc 0.9375
2017-03-02T18:11:14.255879: step 29945, loss 0.143535, acc 0.90625
2017-03-02T18:11:14.332974: step 29946, loss 0.124438, acc 0.90625
2017-03-02T18:11:14.408625: step 29947, loss 0.128911, acc 0.921875
2017-03-02T18:11:14.486999: step 29948, loss 0.0964547, acc 0.953125
2017-03-02T18:11:14.555900: step 29949, loss 0.124295, acc 0.953125
2017-03-02T18:11:14.640236: step 29950, loss 0.229343, acc 0.90625
2017-03-02T18:11:14.715009: step 29951, loss 0.100176, acc 0.9375
2017-03-02T18:11:14.789395: step 29952, loss 0.203684, acc 0.90625
2017-03-02T18:11:14.861052: step 29953, loss 0.124117, acc 0.953125
2017-03-02T18:11:14.935414: step 29954, loss 0.0730723, acc 0.96875
2017-03-02T18:11:15.013447: step 29955, loss 0.0898661, acc 0.96875
2017-03-02T18:11:15.090405: step 29956, loss 0.156753, acc 0.890625
2017-03-02T18:11:15.164014: step 29957, loss 0.194145, acc 0.90625
2017-03-02T18:11:15.236887: step 29958, loss 0.165487, acc 0.9375
2017-03-02T18:11:15.313273: step 29959, loss 0.174758, acc 0.9375
2017-03-02T18:11:15.384859: step 29960, loss 0.156223, acc 0.921875
2017-03-02T18:11:15.457123: step 29961, loss 0.0811544, acc 0.96875
2017-03-02T18:11:15.529714: step 29962, loss 0.112675, acc 0.96875
2017-03-02T18:11:15.605547: step 29963, loss 0.087597, acc 0.96875
2017-03-02T18:11:15.677595: step 29964, loss 0.113427, acc 0.921875
2017-03-02T18:11:15.753902: step 29965, loss 0.105198, acc 0.9375
2017-03-02T18:11:15.825768: step 29966, loss 0.12801, acc 0.9375
2017-03-02T18:11:15.892224: step 29967, loss 0.198573, acc 0.890625
2017-03-02T18:11:15.964086: step 29968, loss 0.131416, acc 0.953125
2017-03-02T18:11:16.046349: step 29969, loss 0.0727154, acc 0.953125
2017-03-02T18:11:16.129949: step 29970, loss 0.136405, acc 0.953125
2017-03-02T18:11:16.193304: step 29971, loss 0.3748, acc 0.859375
2017-03-02T18:11:16.268708: step 29972, loss 0.123817, acc 0.9375
2017-03-02T18:11:16.341443: step 29973, loss 0.184492, acc 0.9375
2017-03-02T18:11:16.415134: step 29974, loss 0.135351, acc 0.96875
2017-03-02T18:11:16.482505: step 29975, loss 0.188334, acc 0.90625
2017-03-02T18:11:16.554055: step 29976, loss 0.116264, acc 0.96875
2017-03-02T18:11:16.628444: step 29977, loss 0.29068, acc 0.9375
2017-03-02T18:11:16.704775: step 29978, loss 0.140328, acc 0.953125
2017-03-02T18:11:16.779759: step 29979, loss 0.169443, acc 0.953125
2017-03-02T18:11:16.853551: step 29980, loss 0.175071, acc 0.90625
2017-03-02T18:11:16.920240: step 29981, loss 0.0763072, acc 0.953125
2017-03-02T18:11:16.989015: step 29982, loss 0.159197, acc 0.921875
2017-03-02T18:11:17.059689: step 29983, loss 0.220025, acc 0.890625
2017-03-02T18:11:17.137019: step 29984, loss 0.269961, acc 0.875
2017-03-02T18:11:17.203599: step 29985, loss 0.125152, acc 0.9375
2017-03-02T18:11:17.272010: step 29986, loss 0.0549694, acc 0.984375
2017-03-02T18:11:17.362438: step 29987, loss 0.189376, acc 0.9375
2017-03-02T18:11:17.446981: step 29988, loss 2.01456e-05, acc 1
2017-03-02T18:11:17.533248: step 29989, loss 0.116193, acc 0.96875
2017-03-02T18:11:17.600667: step 29990, loss 0.141016, acc 0.9375
2017-03-02T18:11:17.667762: step 29991, loss 0.128461, acc 0.9375
2017-03-02T18:11:17.749595: step 29992, loss 0.0531678, acc 0.96875
2017-03-02T18:11:17.824513: step 29993, loss 0.126557, acc 0.953125
2017-03-02T18:11:17.898138: step 29994, loss 0.191846, acc 0.859375
2017-03-02T18:11:17.980967: step 29995, loss 0.151933, acc 0.953125
2017-03-02T18:11:18.056571: step 29996, loss 0.231177, acc 0.859375
2017-03-02T18:11:18.132053: step 29997, loss 0.176286, acc 0.921875
2017-03-02T18:11:18.205853: step 29998, loss 0.142242, acc 0.921875
2017-03-02T18:11:18.278813: step 29999, loss 0.117853, acc 0.96875
2017-03-02T18:11:18.356474: step 30000, loss 0.123379, acc 0.9375

Evaluation:
2017-03-02T18:11:18.388083: step 30000, loss 3.90941, acc 0.648883

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30000

2017-03-02T18:11:18.901561: step 30001, loss 0.166797, acc 0.90625
2017-03-02T18:11:18.977204: step 30002, loss 0.229403, acc 0.90625
2017-03-02T18:11:19.051302: step 30003, loss 0.206832, acc 0.90625
2017-03-02T18:11:19.122455: step 30004, loss 0.139372, acc 0.921875
2017-03-02T18:11:19.194851: step 30005, loss 0.253993, acc 0.921875
2017-03-02T18:11:19.266372: step 30006, loss 0.173043, acc 0.921875
2017-03-02T18:11:19.327297: step 30007, loss 0.123565, acc 0.953125
2017-03-02T18:11:19.400034: step 30008, loss 0.207946, acc 0.90625
2017-03-02T18:11:19.473191: step 30009, loss 0.201743, acc 0.875
2017-03-02T18:11:19.546244: step 30010, loss 0.22238, acc 0.90625
2017-03-02T18:11:19.620814: step 30011, loss 0.0609047, acc 0.984375
2017-03-02T18:11:19.687877: step 30012, loss 0.121413, acc 0.96875
2017-03-02T18:11:19.759828: step 30013, loss 0.0836832, acc 0.953125
2017-03-02T18:11:19.837191: step 30014, loss 0.166042, acc 0.953125
2017-03-02T18:11:19.906600: step 30015, loss 0.204391, acc 0.90625
2017-03-02T18:11:19.977139: step 30016, loss 0.114905, acc 0.921875
2017-03-02T18:11:20.058487: step 30017, loss 0.115504, acc 0.9375
2017-03-02T18:11:20.127519: step 30018, loss 0.0597496, acc 0.96875
2017-03-02T18:11:20.201849: step 30019, loss 0.146476, acc 0.9375
2017-03-02T18:11:20.274084: step 30020, loss 0.0910618, acc 0.953125
2017-03-02T18:11:20.346102: step 30021, loss 0.121216, acc 0.9375
2017-03-02T18:11:20.416364: step 30022, loss 0.184933, acc 0.859375
2017-03-02T18:11:20.490009: step 30023, loss 0.146298, acc 0.953125
2017-03-02T18:11:20.564791: step 30024, loss 0.281089, acc 0.84375
2017-03-02T18:11:20.633432: step 30025, loss 0.163251, acc 0.921875
2017-03-02T18:11:20.704794: step 30026, loss 0.193841, acc 0.921875
2017-03-02T18:11:20.775814: step 30027, loss 0.103557, acc 0.953125
2017-03-02T18:11:20.848704: step 30028, loss 0.116943, acc 0.96875
2017-03-02T18:11:20.933723: step 30029, loss 0.117015, acc 0.9375
2017-03-02T18:11:21.005584: step 30030, loss 0.133056, acc 0.9375
2017-03-02T18:11:21.077341: step 30031, loss 0.154873, acc 0.921875
2017-03-02T18:11:21.151128: step 30032, loss 0.223063, acc 0.890625
2017-03-02T18:11:21.221306: step 30033, loss 0.143525, acc 0.9375
2017-03-02T18:11:21.287260: step 30034, loss 0.0698498, acc 0.96875
2017-03-02T18:11:21.358071: step 30035, loss 0.114041, acc 0.953125
2017-03-02T18:11:21.426687: step 30036, loss 0.137517, acc 0.9375
2017-03-02T18:11:21.495318: step 30037, loss 0.108484, acc 0.9375
2017-03-02T18:11:21.567173: step 30038, loss 0.156008, acc 0.953125
2017-03-02T18:11:21.641992: step 30039, loss 0.0807753, acc 0.96875
2017-03-02T18:11:21.713669: step 30040, loss 0.142262, acc 0.921875
2017-03-02T18:11:21.785523: step 30041, loss 0.120271, acc 0.921875
2017-03-02T18:11:21.860118: step 30042, loss 0.100637, acc 0.953125
2017-03-02T18:11:21.942499: step 30043, loss 0.152056, acc 0.953125
2017-03-02T18:11:22.015149: step 30044, loss 0.0869684, acc 0.953125
2017-03-02T18:11:22.083125: step 30045, loss 0.157708, acc 0.9375
2017-03-02T18:11:22.153922: step 30046, loss 0.257284, acc 0.875
2017-03-02T18:11:22.247265: step 30047, loss 0.0858735, acc 0.984375
2017-03-02T18:11:22.329958: step 30048, loss 0.27875, acc 0.859375
2017-03-02T18:11:22.399057: step 30049, loss 0.1075, acc 0.96875
2017-03-02T18:11:22.477707: step 30050, loss 0.0756425, acc 0.984375
2017-03-02T18:11:22.548011: step 30051, loss 0.0777175, acc 0.953125
2017-03-02T18:11:22.617265: step 30052, loss 0.143028, acc 0.953125
2017-03-02T18:11:22.687934: step 30053, loss 0.0949392, acc 0.96875
2017-03-02T18:11:22.754431: step 30054, loss 0.215339, acc 0.890625
2017-03-02T18:11:22.825845: step 30055, loss 0.209226, acc 0.90625
2017-03-02T18:11:22.900072: step 30056, loss 0.288135, acc 0.859375
2017-03-02T18:11:22.970346: step 30057, loss 0.16395, acc 0.90625
2017-03-02T18:11:23.046302: step 30058, loss 0.194703, acc 0.90625
2017-03-02T18:11:23.116462: step 30059, loss 0.0706827, acc 0.96875
2017-03-02T18:11:23.193397: step 30060, loss 0.141015, acc 0.9375
2017-03-02T18:11:23.270220: step 30061, loss 0.108867, acc 0.9375
2017-03-02T18:11:23.346086: step 30062, loss 0.103367, acc 0.96875
2017-03-02T18:11:23.431033: step 30063, loss 0.139471, acc 0.9375
2017-03-02T18:11:23.503156: step 30064, loss 0.0739218, acc 0.953125
2017-03-02T18:11:23.576900: step 30065, loss 0.104114, acc 0.953125
2017-03-02T18:11:23.652891: step 30066, loss 0.179121, acc 0.921875
2017-03-02T18:11:23.726727: step 30067, loss 0.074121, acc 0.96875
2017-03-02T18:11:23.800454: step 30068, loss 0.0798578, acc 0.953125
2017-03-02T18:11:23.872243: step 30069, loss 0.11857, acc 0.9375
2017-03-02T18:11:23.950426: step 30070, loss 0.229656, acc 0.90625
2017-03-02T18:11:24.033519: step 30071, loss 0.179656, acc 0.9375
2017-03-02T18:11:24.100516: step 30072, loss 0.0241885, acc 1
2017-03-02T18:11:24.171080: step 30073, loss 0.231858, acc 0.890625
2017-03-02T18:11:24.237137: step 30074, loss 0.068087, acc 0.96875
2017-03-02T18:11:24.333088: step 30075, loss 0.143393, acc 0.9375
2017-03-02T18:11:24.402733: step 30076, loss 0.121222, acc 0.9375
2017-03-02T18:11:24.475154: step 30077, loss 0.126857, acc 0.96875
2017-03-02T18:11:24.548221: step 30078, loss 0.112797, acc 0.9375
2017-03-02T18:11:24.619473: step 30079, loss 0.198544, acc 0.875
2017-03-02T18:11:24.698269: step 30080, loss 0.0523216, acc 0.96875
2017-03-02T18:11:24.770607: step 30081, loss 0.0775838, acc 0.96875
2017-03-02T18:11:24.841660: step 30082, loss 0.175538, acc 0.90625
2017-03-02T18:11:24.911927: step 30083, loss 0.192662, acc 0.859375
2017-03-02T18:11:24.985131: step 30084, loss 0.164566, acc 0.921875
2017-03-02T18:11:25.059680: step 30085, loss 0.227572, acc 0.875
2017-03-02T18:11:25.132020: step 30086, loss 0.109402, acc 0.953125
2017-03-02T18:11:25.202166: step 30087, loss 0.232016, acc 0.90625
2017-03-02T18:11:25.286971: step 30088, loss 0.10268, acc 0.953125
2017-03-02T18:11:25.358402: step 30089, loss 0.0747812, acc 0.953125
2017-03-02T18:11:25.451044: step 30090, loss 0.0903505, acc 0.9375
2017-03-02T18:11:25.516541: step 30091, loss 0.160376, acc 0.921875
2017-03-02T18:11:25.584499: step 30092, loss 0.0619791, acc 0.984375
2017-03-02T18:11:25.657056: step 30093, loss 0.130773, acc 0.953125
2017-03-02T18:11:25.741461: step 30094, loss 0.109029, acc 0.953125
2017-03-02T18:11:25.815548: step 30095, loss 0.0771637, acc 0.984375
2017-03-02T18:11:25.883164: step 30096, loss 0.0678558, acc 0.96875
2017-03-02T18:11:25.956070: step 30097, loss 0.192554, acc 0.890625
2017-03-02T18:11:26.041230: step 30098, loss 0.164049, acc 0.90625
2017-03-02T18:11:26.109392: step 30099, loss 0.114024, acc 0.9375
2017-03-02T18:11:26.179019: step 30100, loss 0.204456, acc 0.9375

Evaluation:
2017-03-02T18:11:26.211608: step 30100, loss 3.95696, acc 0.645998

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30100

2017-03-02T18:11:26.672910: step 30101, loss 0.0441557, acc 0.96875
2017-03-02T18:11:26.745023: step 30102, loss 0.154524, acc 0.90625
2017-03-02T18:11:26.816996: step 30103, loss 0.079674, acc 0.96875
2017-03-02T18:11:26.888879: step 30104, loss 0.178506, acc 0.921875
2017-03-02T18:11:26.960559: step 30105, loss 0.139663, acc 0.921875
2017-03-02T18:11:27.039374: step 30106, loss 0.191179, acc 0.921875
2017-03-02T18:11:27.119129: step 30107, loss 0.148925, acc 0.9375
2017-03-02T18:11:27.189335: step 30108, loss 0.0744382, acc 0.984375
2017-03-02T18:11:27.263739: step 30109, loss 0.250837, acc 0.953125
2017-03-02T18:11:27.343946: step 30110, loss 0.109458, acc 0.953125
2017-03-02T18:11:27.418004: step 30111, loss 0.0871619, acc 0.984375
2017-03-02T18:11:27.488500: step 30112, loss 0.18567, acc 0.90625
2017-03-02T18:11:27.560289: step 30113, loss 0.157404, acc 0.9375
2017-03-02T18:11:27.626985: step 30114, loss 0.10563, acc 0.953125
2017-03-02T18:11:27.702012: step 30115, loss 0.059912, acc 0.96875
2017-03-02T18:11:27.768984: step 30116, loss 0.161971, acc 0.921875
2017-03-02T18:11:27.844298: step 30117, loss 0.14714, acc 0.953125
2017-03-02T18:11:27.916508: step 30118, loss 0.212649, acc 0.921875
2017-03-02T18:11:27.998770: step 30119, loss 0.0891733, acc 0.96875
2017-03-02T18:11:28.076813: step 30120, loss 0.120323, acc 0.9375
2017-03-02T18:11:28.151441: step 30121, loss 0.106946, acc 0.9375
2017-03-02T18:11:28.224069: step 30122, loss 0.227884, acc 0.90625
2017-03-02T18:11:28.291699: step 30123, loss 0.190169, acc 0.921875
2017-03-02T18:11:28.363944: step 30124, loss 0.110212, acc 0.96875
2017-03-02T18:11:28.434728: step 30125, loss 0.235631, acc 0.875
2017-03-02T18:11:28.508466: step 30126, loss 0.0766735, acc 0.96875
2017-03-02T18:11:28.580250: step 30127, loss 0.140694, acc 0.9375
2017-03-02T18:11:28.666454: step 30128, loss 0.153486, acc 0.921875
2017-03-02T18:11:28.742346: step 30129, loss 0.0676936, acc 0.984375
2017-03-02T18:11:28.820957: step 30130, loss 0.0525071, acc 0.96875
2017-03-02T18:11:28.890848: step 30131, loss 0.177862, acc 0.9375
2017-03-02T18:11:28.956663: step 30132, loss 0.108478, acc 0.9375
2017-03-02T18:11:29.022985: step 30133, loss 0.245752, acc 0.875
2017-03-02T18:11:29.091694: step 30134, loss 0.11817, acc 0.921875
2017-03-02T18:11:29.178976: step 30135, loss 0.15247, acc 0.90625
2017-03-02T18:11:29.250787: step 30136, loss 0.139679, acc 0.9375
2017-03-02T18:11:29.322341: step 30137, loss 0.275604, acc 0.875
2017-03-02T18:11:29.391727: step 30138, loss 0.0783662, acc 0.984375
2017-03-02T18:11:29.465995: step 30139, loss 0.188375, acc 0.9375
2017-03-02T18:11:29.536306: step 30140, loss 0.073473, acc 1
2017-03-02T18:11:29.607272: step 30141, loss 0.192268, acc 0.90625
2017-03-02T18:11:29.680174: step 30142, loss 0.119723, acc 0.953125
2017-03-02T18:11:29.751412: step 30143, loss 0.151702, acc 0.9375
2017-03-02T18:11:29.859935: step 30144, loss 0.118259, acc 0.9375
2017-03-02T18:11:29.931001: step 30145, loss 0.175913, acc 0.953125
2017-03-02T18:11:30.003375: step 30146, loss 0.218195, acc 0.921875
2017-03-02T18:11:30.076190: step 30147, loss 0.200158, acc 0.875
2017-03-02T18:11:30.151582: step 30148, loss 0.081218, acc 0.96875
2017-03-02T18:11:30.221750: step 30149, loss 0.0545723, acc 0.984375
2017-03-02T18:11:30.297214: step 30150, loss 0.157754, acc 0.9375
2017-03-02T18:11:30.365756: step 30151, loss 0.064537, acc 0.984375
2017-03-02T18:11:30.433789: step 30152, loss 0.09852, acc 0.953125
2017-03-02T18:11:30.511258: step 30153, loss 0.237132, acc 0.890625
2017-03-02T18:11:30.584968: step 30154, loss 0.11422, acc 0.953125
2017-03-02T18:11:30.662028: step 30155, loss 0.0556296, acc 0.984375
2017-03-02T18:11:30.737354: step 30156, loss 0.100913, acc 0.953125
2017-03-02T18:11:30.808842: step 30157, loss 0.127527, acc 0.953125
2017-03-02T18:11:30.885807: step 30158, loss 0.141426, acc 0.90625
2017-03-02T18:11:30.958997: step 30159, loss 0.273321, acc 0.84375
2017-03-02T18:11:31.027672: step 30160, loss 0.149398, acc 0.953125
2017-03-02T18:11:31.099506: step 30161, loss 0.1309, acc 0.921875
2017-03-02T18:11:31.178682: step 30162, loss 0.268114, acc 0.90625
2017-03-02T18:11:31.253810: step 30163, loss 0.205707, acc 0.890625
2017-03-02T18:11:31.329681: step 30164, loss 0.110695, acc 0.953125
2017-03-02T18:11:31.401098: step 30165, loss 0.254225, acc 0.875
2017-03-02T18:11:31.482130: step 30166, loss 0.189759, acc 0.890625
2017-03-02T18:11:31.554905: step 30167, loss 0.174943, acc 0.90625
2017-03-02T18:11:31.628364: step 30168, loss 0.16725, acc 0.9375
2017-03-02T18:11:31.700312: step 30169, loss 0.157389, acc 0.9375
2017-03-02T18:11:31.773231: step 30170, loss 0.117937, acc 0.9375
2017-03-02T18:11:31.842229: step 30171, loss 0.264275, acc 0.890625
2017-03-02T18:11:31.908526: step 30172, loss 0.162441, acc 0.9375
2017-03-02T18:11:31.975091: step 30173, loss 0.258178, acc 0.859375
2017-03-02T18:11:32.051811: step 30174, loss 0.103583, acc 0.953125
2017-03-02T18:11:32.127356: step 30175, loss 0.241731, acc 0.875
2017-03-02T18:11:32.202337: step 30176, loss 0.149622, acc 0.90625
2017-03-02T18:11:32.280548: step 30177, loss 0.29041, acc 0.859375
2017-03-02T18:11:32.358635: step 30178, loss 0.14484, acc 0.921875
2017-03-02T18:11:32.423263: step 30179, loss 0.221636, acc 0.875
2017-03-02T18:11:32.490781: step 30180, loss 0.210984, acc 0.90625
2017-03-02T18:11:32.560710: step 30181, loss 0.115729, acc 0.96875
2017-03-02T18:11:32.633730: step 30182, loss 0.143601, acc 0.9375
2017-03-02T18:11:32.706451: step 30183, loss 0.279418, acc 0.890625
2017-03-02T18:11:32.776846: step 30184, loss 1.04306e-05, acc 1
2017-03-02T18:11:32.850902: step 30185, loss 0.157191, acc 0.953125
2017-03-02T18:11:32.921713: step 30186, loss 0.126454, acc 0.9375
2017-03-02T18:11:32.996221: step 30187, loss 0.0630065, acc 0.984375
2017-03-02T18:11:33.066958: step 30188, loss 0.0596376, acc 0.96875
2017-03-02T18:11:33.145929: step 30189, loss 0.101286, acc 0.96875
2017-03-02T18:11:33.221825: step 30190, loss 0.261363, acc 0.890625
2017-03-02T18:11:33.292862: step 30191, loss 0.223066, acc 0.890625
2017-03-02T18:11:33.366422: step 30192, loss 0.156608, acc 0.953125
2017-03-02T18:11:33.432186: step 30193, loss 0.132363, acc 0.9375
2017-03-02T18:11:33.513636: step 30194, loss 0.179071, acc 0.921875
2017-03-02T18:11:33.587010: step 30195, loss 0.0655194, acc 0.96875
2017-03-02T18:11:33.653157: step 30196, loss 0.138766, acc 0.96875
2017-03-02T18:11:33.731823: step 30197, loss 0.123553, acc 0.953125
2017-03-02T18:11:33.800363: step 30198, loss 0.12068, acc 0.953125
2017-03-02T18:11:33.865384: step 30199, loss 0.15451, acc 0.921875
2017-03-02T18:11:33.943101: step 30200, loss 0.250892, acc 0.890625

Evaluation:
2017-03-02T18:11:33.979721: step 30200, loss 3.87655, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30200

2017-03-02T18:11:34.435875: step 30201, loss 0.0645573, acc 0.96875
2017-03-02T18:11:34.502863: step 30202, loss 0.097026, acc 0.953125
2017-03-02T18:11:34.577792: step 30203, loss 0.127593, acc 0.953125
2017-03-02T18:11:34.663310: step 30204, loss 0.225223, acc 0.875
2017-03-02T18:11:34.733879: step 30205, loss 0.0469986, acc 1
2017-03-02T18:11:34.812860: step 30206, loss 0.113402, acc 0.9375
2017-03-02T18:11:34.886448: step 30207, loss 0.18244, acc 0.890625
2017-03-02T18:11:34.953786: step 30208, loss 0.129964, acc 0.953125
2017-03-02T18:11:35.027174: step 30209, loss 0.151838, acc 0.9375
2017-03-02T18:11:35.102446: step 30210, loss 0.114916, acc 0.9375
2017-03-02T18:11:35.167546: step 30211, loss 0.145469, acc 0.953125
2017-03-02T18:11:35.235501: step 30212, loss 0.136883, acc 0.9375
2017-03-02T18:11:35.300150: step 30213, loss 0.173388, acc 0.953125
2017-03-02T18:11:35.370031: step 30214, loss 0.101741, acc 0.96875
2017-03-02T18:11:35.439606: step 30215, loss 0.154011, acc 0.90625
2017-03-02T18:11:35.504341: step 30216, loss 0.171639, acc 0.921875
2017-03-02T18:11:35.579856: step 30217, loss 0.077288, acc 0.96875
2017-03-02T18:11:35.645629: step 30218, loss 0.189668, acc 0.90625
2017-03-02T18:11:35.722699: step 30219, loss 0.14628, acc 0.953125
2017-03-02T18:11:35.794588: step 30220, loss 0.142833, acc 0.953125
2017-03-02T18:11:35.861360: step 30221, loss 0.213108, acc 0.890625
2017-03-02T18:11:35.931910: step 30222, loss 0.0842756, acc 0.96875
2017-03-02T18:11:36.008181: step 30223, loss 0.118502, acc 0.953125
2017-03-02T18:11:36.079283: step 30224, loss 0.121024, acc 0.953125
2017-03-02T18:11:36.153015: step 30225, loss 0.0979788, acc 0.96875
2017-03-02T18:11:36.226281: step 30226, loss 0.131998, acc 0.90625
2017-03-02T18:11:36.300894: step 30227, loss 0.155216, acc 0.921875
2017-03-02T18:11:36.374834: step 30228, loss 0.175641, acc 0.953125
2017-03-02T18:11:36.453993: step 30229, loss 0.0894382, acc 0.96875
2017-03-02T18:11:36.524980: step 30230, loss 0.0816389, acc 0.96875
2017-03-02T18:11:36.589545: step 30231, loss 0.138222, acc 0.921875
2017-03-02T18:11:36.665216: step 30232, loss 0.274824, acc 0.828125
2017-03-02T18:11:36.730942: step 30233, loss 0.0617666, acc 0.984375
2017-03-02T18:11:36.807040: step 30234, loss 0.139267, acc 0.9375
2017-03-02T18:11:36.876636: step 30235, loss 0.101294, acc 0.953125
2017-03-02T18:11:36.954526: step 30236, loss 0.170488, acc 0.9375
2017-03-02T18:11:37.022377: step 30237, loss 0.035471, acc 0.984375
2017-03-02T18:11:37.095940: step 30238, loss 0.107421, acc 0.953125
2017-03-02T18:11:37.173462: step 30239, loss 0.163308, acc 0.921875
2017-03-02T18:11:37.242501: step 30240, loss 0.104738, acc 0.953125
2017-03-02T18:11:37.319262: step 30241, loss 0.11945, acc 0.921875
2017-03-02T18:11:37.394833: step 30242, loss 0.137681, acc 0.9375
2017-03-02T18:11:37.472585: step 30243, loss 0.0569872, acc 0.984375
2017-03-02T18:11:37.546468: step 30244, loss 0.0876103, acc 0.984375
2017-03-02T18:11:37.616365: step 30245, loss 0.112349, acc 0.921875
2017-03-02T18:11:37.693868: step 30246, loss 0.0823949, acc 0.96875
2017-03-02T18:11:37.770314: step 30247, loss 0.125741, acc 0.953125
2017-03-02T18:11:37.842456: step 30248, loss 0.113109, acc 0.9375
2017-03-02T18:11:37.905303: step 30249, loss 0.118611, acc 0.984375
2017-03-02T18:11:37.972472: step 30250, loss 0.14449, acc 0.9375
2017-03-02T18:11:38.042336: step 30251, loss 0.146665, acc 0.953125
2017-03-02T18:11:38.114872: step 30252, loss 0.110099, acc 0.96875
2017-03-02T18:11:38.193726: step 30253, loss 0.241113, acc 0.90625
2017-03-02T18:11:38.272087: step 30254, loss 0.0847025, acc 0.953125
2017-03-02T18:11:38.345911: step 30255, loss 0.170093, acc 0.921875
2017-03-02T18:11:38.417278: step 30256, loss 0.271945, acc 0.875
2017-03-02T18:11:38.487853: step 30257, loss 0.0879456, acc 0.96875
2017-03-02T18:11:38.562137: step 30258, loss 0.117172, acc 0.96875
2017-03-02T18:11:38.631380: step 30259, loss 0.213734, acc 0.890625
2017-03-02T18:11:38.695546: step 30260, loss 0.105423, acc 0.9375
2017-03-02T18:11:38.786854: step 30261, loss 0.244388, acc 0.875
2017-03-02T18:11:38.856369: step 30262, loss 0.109885, acc 0.9375
2017-03-02T18:11:38.931252: step 30263, loss 0.0722758, acc 0.96875
2017-03-02T18:11:39.010969: step 30264, loss 0.241048, acc 0.890625
2017-03-02T18:11:39.086467: step 30265, loss 0.100784, acc 0.953125
2017-03-02T18:11:39.163718: step 30266, loss 0.11328, acc 0.9375
2017-03-02T18:11:39.239917: step 30267, loss 0.217524, acc 0.890625
2017-03-02T18:11:39.308078: step 30268, loss 0.144578, acc 0.90625
2017-03-02T18:11:39.375278: step 30269, loss 0.12432, acc 0.9375
2017-03-02T18:11:39.447390: step 30270, loss 0.0984148, acc 0.96875
2017-03-02T18:11:39.518292: step 30271, loss 0.0707389, acc 0.96875
2017-03-02T18:11:39.599074: step 30272, loss 0.143518, acc 0.96875
2017-03-02T18:11:39.667937: step 30273, loss 0.0664168, acc 0.984375
2017-03-02T18:11:39.743884: step 30274, loss 0.121079, acc 0.921875
2017-03-02T18:11:39.815082: step 30275, loss 0.0906229, acc 0.953125
2017-03-02T18:11:39.889710: step 30276, loss 0.215664, acc 0.890625
2017-03-02T18:11:39.957524: step 30277, loss 0.172385, acc 0.90625
2017-03-02T18:11:40.024182: step 30278, loss 0.0975973, acc 0.9375
2017-03-02T18:11:40.091652: step 30279, loss 0.173325, acc 0.921875
2017-03-02T18:11:40.174855: step 30280, loss 0.197136, acc 0.890625
2017-03-02T18:11:40.251814: step 30281, loss 0.262636, acc 0.890625
2017-03-02T18:11:40.324984: step 30282, loss 0.244106, acc 0.890625
2017-03-02T18:11:40.411407: step 30283, loss 0.111445, acc 0.9375
2017-03-02T18:11:40.481055: step 30284, loss 0.0505919, acc 0.96875
2017-03-02T18:11:40.556164: step 30285, loss 0.0839841, acc 0.953125
2017-03-02T18:11:40.635457: step 30286, loss 0.142522, acc 0.9375
2017-03-02T18:11:40.706197: step 30287, loss 0.143467, acc 0.921875
2017-03-02T18:11:40.779618: step 30288, loss 0.0902029, acc 0.953125
2017-03-02T18:11:40.884536: step 30289, loss 0.125569, acc 0.953125
2017-03-02T18:11:40.967092: step 30290, loss 0.0897176, acc 0.96875
2017-03-02T18:11:41.059379: step 30291, loss 0.146719, acc 0.921875
2017-03-02T18:11:41.135505: step 30292, loss 0.181342, acc 0.921875
2017-03-02T18:11:41.210306: step 30293, loss 0.120913, acc 0.9375
2017-03-02T18:11:41.286807: step 30294, loss 0.174037, acc 0.921875
2017-03-02T18:11:41.357844: step 30295, loss 0.148197, acc 0.90625
2017-03-02T18:11:41.427326: step 30296, loss 0.161827, acc 0.96875
2017-03-02T18:11:41.503863: step 30297, loss 0.209191, acc 0.921875
2017-03-02T18:11:41.576698: step 30298, loss 0.0690618, acc 0.984375
2017-03-02T18:11:41.651858: step 30299, loss 0.161149, acc 0.9375
2017-03-02T18:11:41.720870: step 30300, loss 0.164835, acc 0.921875

Evaluation:
2017-03-02T18:11:41.756324: step 30300, loss 4.00118, acc 0.64672

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30300

2017-03-02T18:11:42.209638: step 30301, loss 0.194716, acc 0.90625
2017-03-02T18:11:42.281409: step 30302, loss 0.129978, acc 0.953125
2017-03-02T18:11:42.352530: step 30303, loss 0.134296, acc 0.984375
2017-03-02T18:11:42.422738: step 30304, loss 0.106353, acc 0.984375
2017-03-02T18:11:42.486886: step 30305, loss 0.170834, acc 0.9375
2017-03-02T18:11:42.560009: step 30306, loss 0.257334, acc 0.875
2017-03-02T18:11:42.635341: step 30307, loss 0.244212, acc 0.921875
2017-03-02T18:11:42.703969: step 30308, loss 0.150461, acc 0.921875
2017-03-02T18:11:42.772440: step 30309, loss 0.140127, acc 0.9375
2017-03-02T18:11:42.850564: step 30310, loss 0.147091, acc 0.921875
2017-03-02T18:11:42.913293: step 30311, loss 0.100177, acc 0.953125
2017-03-02T18:11:42.992626: step 30312, loss 0.153552, acc 0.9375
2017-03-02T18:11:43.063066: step 30313, loss 0.099951, acc 0.96875
2017-03-02T18:11:43.134433: step 30314, loss 0.0730582, acc 0.9375
2017-03-02T18:11:43.206666: step 30315, loss 0.153234, acc 0.9375
2017-03-02T18:11:43.277482: step 30316, loss 0.163612, acc 0.921875
2017-03-02T18:11:43.355989: step 30317, loss 0.23914, acc 0.875
2017-03-02T18:11:43.449248: step 30318, loss 0.0751581, acc 0.953125
2017-03-02T18:11:43.516390: step 30319, loss 0.162783, acc 0.921875
2017-03-02T18:11:43.593695: step 30320, loss 0.106588, acc 0.953125
2017-03-02T18:11:43.673621: step 30321, loss 0.175911, acc 0.90625
2017-03-02T18:11:43.757749: step 30322, loss 0.274872, acc 0.875
2017-03-02T18:11:43.828447: step 30323, loss 0.12056, acc 0.96875
2017-03-02T18:11:43.909240: step 30324, loss 0.0722527, acc 0.9375
2017-03-02T18:11:43.984030: step 30325, loss 0.176913, acc 0.921875
2017-03-02T18:11:44.057434: step 30326, loss 0.150095, acc 0.953125
2017-03-02T18:11:44.147339: step 30327, loss 0.0798158, acc 0.96875
2017-03-02T18:11:44.219626: step 30328, loss 0.0755993, acc 0.96875
2017-03-02T18:11:44.299299: step 30329, loss 0.127614, acc 0.96875
2017-03-02T18:11:44.378078: step 30330, loss 0.0924262, acc 0.96875
2017-03-02T18:11:44.454124: step 30331, loss 0.117719, acc 0.96875
2017-03-02T18:11:44.526778: step 30332, loss 0.252202, acc 0.90625
2017-03-02T18:11:44.598792: step 30333, loss 0.219648, acc 0.890625
2017-03-02T18:11:44.672289: step 30334, loss 0.115027, acc 0.953125
2017-03-02T18:11:44.740825: step 30335, loss 0.174411, acc 0.890625
2017-03-02T18:11:44.810488: step 30336, loss 0.156962, acc 0.953125
2017-03-02T18:11:44.882370: step 30337, loss 0.213546, acc 0.921875
2017-03-02T18:11:44.953206: step 30338, loss 0.0860989, acc 0.953125
2017-03-02T18:11:45.029291: step 30339, loss 0.0843486, acc 0.96875
2017-03-02T18:11:45.101017: step 30340, loss 0.123839, acc 0.9375
2017-03-02T18:11:45.171992: step 30341, loss 0.11685, acc 0.953125
2017-03-02T18:11:45.244997: step 30342, loss 0.23874, acc 0.890625
2017-03-02T18:11:45.316142: step 30343, loss 0.177005, acc 0.921875
2017-03-02T18:11:45.387012: step 30344, loss 0.180881, acc 0.890625
2017-03-02T18:11:45.457948: step 30345, loss 0.212463, acc 0.90625
2017-03-02T18:11:45.529689: step 30346, loss 0.105705, acc 0.96875
2017-03-02T18:11:45.605479: step 30347, loss 0.0550059, acc 0.984375
2017-03-02T18:11:45.677460: step 30348, loss 0.15003, acc 0.953125
2017-03-02T18:11:45.752482: step 30349, loss 0.167369, acc 0.9375
2017-03-02T18:11:45.831179: step 30350, loss 0.0874947, acc 0.9375
2017-03-02T18:11:45.902663: step 30351, loss 0.158642, acc 0.921875
2017-03-02T18:11:45.975566: step 30352, loss 0.0799677, acc 0.96875
2017-03-02T18:11:46.046723: step 30353, loss 0.0874285, acc 0.953125
2017-03-02T18:11:46.123111: step 30354, loss 0.104033, acc 0.953125
2017-03-02T18:11:46.191371: step 30355, loss 0.181959, acc 0.90625
2017-03-02T18:11:46.265952: step 30356, loss 0.126192, acc 0.9375
2017-03-02T18:11:46.336753: step 30357, loss 0.168723, acc 0.890625
2017-03-02T18:11:46.406667: step 30358, loss 0.0839994, acc 0.96875
2017-03-02T18:11:46.482023: step 30359, loss 0.161614, acc 0.890625
2017-03-02T18:11:46.550955: step 30360, loss 0.179465, acc 0.875
2017-03-02T18:11:46.620548: step 30361, loss 0.123155, acc 0.9375
2017-03-02T18:11:46.694167: step 30362, loss 0.0645945, acc 0.96875
2017-03-02T18:11:46.767153: step 30363, loss 0.0850693, acc 0.953125
2017-03-02T18:11:46.835303: step 30364, loss 0.175777, acc 0.90625
2017-03-02T18:11:46.911187: step 30365, loss 0.168506, acc 0.921875
2017-03-02T18:11:46.984661: step 30366, loss 0.200964, acc 0.921875
2017-03-02T18:11:47.056457: step 30367, loss 0.12869, acc 0.96875
2017-03-02T18:11:47.147554: step 30368, loss 0.103712, acc 0.953125
2017-03-02T18:11:47.230860: step 30369, loss 0.364851, acc 0.828125
2017-03-02T18:11:47.308130: step 30370, loss 0.121193, acc 0.921875
2017-03-02T18:11:47.381228: step 30371, loss 0.159704, acc 0.9375
2017-03-02T18:11:47.454509: step 30372, loss 0.195994, acc 0.90625
2017-03-02T18:11:47.521222: step 30373, loss 0.179458, acc 0.9375
2017-03-02T18:11:47.589620: step 30374, loss 0.324953, acc 0.859375
2017-03-02T18:11:47.663688: step 30375, loss 0.13399, acc 0.96875
2017-03-02T18:11:47.733608: step 30376, loss 0.194369, acc 0.875
2017-03-02T18:11:47.826565: step 30377, loss 0.094345, acc 0.96875
2017-03-02T18:11:47.897971: step 30378, loss 0.307002, acc 0.921875
2017-03-02T18:11:47.977954: step 30379, loss 0.145737, acc 0.921875
2017-03-02T18:11:48.049607: step 30380, loss 0.0316071, acc 1
2017-03-02T18:11:48.135338: step 30381, loss 0.121754, acc 0.96875
2017-03-02T18:11:48.204957: step 30382, loss 0.117811, acc 0.96875
2017-03-02T18:11:48.271997: step 30383, loss 0.16519, acc 0.921875
2017-03-02T18:11:48.346950: step 30384, loss 0.119896, acc 0.9375
2017-03-02T18:11:48.427482: step 30385, loss 0.132266, acc 0.9375
2017-03-02T18:11:48.513644: step 30386, loss 0.0857804, acc 0.984375
2017-03-02T18:11:48.588553: step 30387, loss 0.134994, acc 0.9375
2017-03-02T18:11:48.663713: step 30388, loss 0.154063, acc 0.9375
2017-03-02T18:11:48.735266: step 30389, loss 0.082586, acc 0.984375
2017-03-02T18:11:48.810208: step 30390, loss 0.156246, acc 0.953125
2017-03-02T18:11:48.879613: step 30391, loss 0.264147, acc 0.875
2017-03-02T18:11:48.957449: step 30392, loss 0.0790326, acc 0.96875
2017-03-02T18:11:49.026447: step 30393, loss 0.161259, acc 0.953125
2017-03-02T18:11:49.099184: step 30394, loss 0.126736, acc 0.921875
2017-03-02T18:11:49.187207: step 30395, loss 0.0731491, acc 0.953125
2017-03-02T18:11:49.258784: step 30396, loss 0.166942, acc 0.90625
2017-03-02T18:11:49.325929: step 30397, loss 0.136663, acc 0.90625
2017-03-02T18:11:49.397890: step 30398, loss 0.152762, acc 0.90625
2017-03-02T18:11:49.468629: step 30399, loss 0.0806273, acc 0.953125
2017-03-02T18:11:49.541039: step 30400, loss 0.095808, acc 0.984375

Evaluation:
2017-03-02T18:11:49.572848: step 30400, loss 3.91854, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30400

2017-03-02T18:11:50.046229: step 30401, loss 0.208503, acc 0.90625
2017-03-02T18:11:50.120997: step 30402, loss 0.132856, acc 0.921875
2017-03-02T18:11:50.193769: step 30403, loss 0.0703279, acc 0.984375
2017-03-02T18:11:50.261176: step 30404, loss 0.233539, acc 0.90625
2017-03-02T18:11:50.353567: step 30405, loss 0.0918396, acc 0.984375
2017-03-02T18:11:50.427687: step 30406, loss 0.181055, acc 0.90625
2017-03-02T18:11:50.501463: step 30407, loss 0.0674885, acc 0.96875
2017-03-02T18:11:50.574299: step 30408, loss 0.11793, acc 0.953125
2017-03-02T18:11:50.650054: step 30409, loss 0.131445, acc 0.953125
2017-03-02T18:11:50.719154: step 30410, loss 0.222512, acc 0.90625
2017-03-02T18:11:50.790021: step 30411, loss 0.093381, acc 0.953125
2017-03-02T18:11:50.869284: step 30412, loss 0.101298, acc 0.96875
2017-03-02T18:11:50.937288: step 30413, loss 0.170668, acc 0.9375
2017-03-02T18:11:51.005793: step 30414, loss 0.0672403, acc 0.96875
2017-03-02T18:11:51.091251: step 30415, loss 0.0685489, acc 0.984375
2017-03-02T18:11:51.164711: step 30416, loss 0.123942, acc 0.9375
2017-03-02T18:11:51.245004: step 30417, loss 0.0915741, acc 0.96875
2017-03-02T18:11:51.325617: step 30418, loss 0.104694, acc 0.9375
2017-03-02T18:11:51.392603: step 30419, loss 0.122502, acc 0.9375
2017-03-02T18:11:51.467911: step 30420, loss 0.106298, acc 0.9375
2017-03-02T18:11:51.549894: step 30421, loss 0.112001, acc 0.953125
2017-03-02T18:11:51.610834: step 30422, loss 0.0883397, acc 0.984375
2017-03-02T18:11:51.679892: step 30423, loss 0.112213, acc 0.953125
2017-03-02T18:11:51.746643: step 30424, loss 0.15006, acc 0.921875
2017-03-02T18:11:51.812020: step 30425, loss 0.173927, acc 0.953125
2017-03-02T18:11:51.888695: step 30426, loss 0.0729503, acc 0.96875
2017-03-02T18:11:51.975345: step 30427, loss 0.220035, acc 0.921875
2017-03-02T18:11:52.050765: step 30428, loss 0.170932, acc 0.953125
2017-03-02T18:11:52.122062: step 30429, loss 0.173925, acc 0.890625
2017-03-02T18:11:52.202422: step 30430, loss 0.200827, acc 0.890625
2017-03-02T18:11:52.285829: step 30431, loss 0.250014, acc 0.90625
2017-03-02T18:11:52.352408: step 30432, loss 0.185259, acc 0.90625
2017-03-02T18:11:52.416939: step 30433, loss 0.207812, acc 0.890625
2017-03-02T18:11:52.497923: step 30434, loss 0.0698479, acc 0.984375
2017-03-02T18:11:52.559812: step 30435, loss 0.250746, acc 0.90625
2017-03-02T18:11:52.641852: step 30436, loss 0.204926, acc 0.90625
2017-03-02T18:11:52.720702: step 30437, loss 0.131993, acc 0.9375
2017-03-02T18:11:52.791890: step 30438, loss 0.07773, acc 0.984375
2017-03-02T18:11:52.870108: step 30439, loss 0.164262, acc 0.90625
2017-03-02T18:11:52.953963: step 30440, loss 0.229979, acc 0.90625
2017-03-02T18:11:53.027793: step 30441, loss 0.0890942, acc 0.96875
2017-03-02T18:11:53.102038: step 30442, loss 0.243247, acc 0.90625
2017-03-02T18:11:53.178247: step 30443, loss 0.125957, acc 0.9375
2017-03-02T18:11:53.252220: step 30444, loss 0.226529, acc 0.921875
2017-03-02T18:11:53.330219: step 30445, loss 0.223449, acc 0.90625
2017-03-02T18:11:53.412068: step 30446, loss 0.119906, acc 0.9375
2017-03-02T18:11:53.487094: step 30447, loss 0.0972135, acc 0.96875
2017-03-02T18:11:53.567490: step 30448, loss 0.148958, acc 0.921875
2017-03-02T18:11:53.644199: step 30449, loss 0.161155, acc 0.921875
2017-03-02T18:11:53.714936: step 30450, loss 0.0381337, acc 1
2017-03-02T18:11:53.786118: step 30451, loss 0.103355, acc 0.953125
2017-03-02T18:11:53.863513: step 30452, loss 0.0930856, acc 0.96875
2017-03-02T18:11:53.933918: step 30453, loss 0.130378, acc 0.953125
2017-03-02T18:11:54.008232: step 30454, loss 0.13993, acc 0.921875
2017-03-02T18:11:54.087937: step 30455, loss 0.065057, acc 0.96875
2017-03-02T18:11:54.160683: step 30456, loss 0.143747, acc 0.9375
2017-03-02T18:11:54.233993: step 30457, loss 0.163063, acc 0.921875
2017-03-02T18:11:54.308493: step 30458, loss 0.0671305, acc 0.96875
2017-03-02T18:11:54.380137: step 30459, loss 0.148982, acc 0.9375
2017-03-02T18:11:54.448844: step 30460, loss 0.143229, acc 0.90625
2017-03-02T18:11:54.521451: step 30461, loss 0.104239, acc 0.96875
2017-03-02T18:11:54.597599: step 30462, loss 0.0992698, acc 0.984375
2017-03-02T18:11:54.666326: step 30463, loss 0.148144, acc 0.9375
2017-03-02T18:11:54.743325: step 30464, loss 0.0867058, acc 0.96875
2017-03-02T18:11:54.835587: step 30465, loss 0.138316, acc 0.9375
2017-03-02T18:11:54.916898: step 30466, loss 0.128704, acc 0.96875
2017-03-02T18:11:54.992370: step 30467, loss 0.200918, acc 0.90625
2017-03-02T18:11:55.062800: step 30468, loss 0.0952075, acc 0.96875
2017-03-02T18:11:55.131699: step 30469, loss 0.120498, acc 0.9375
2017-03-02T18:11:55.207778: step 30470, loss 0.178978, acc 0.890625
2017-03-02T18:11:55.316072: step 30471, loss 0.224128, acc 0.90625
2017-03-02T18:11:55.412707: step 30472, loss 0.286883, acc 0.875
2017-03-02T18:11:55.487733: step 30473, loss 0.132864, acc 0.953125
2017-03-02T18:11:55.555955: step 30474, loss 0.1988, acc 0.890625
2017-03-02T18:11:55.633066: step 30475, loss 0.105491, acc 0.9375
2017-03-02T18:11:55.712415: step 30476, loss 0.120579, acc 0.953125
2017-03-02T18:11:55.778432: step 30477, loss 0.0354824, acc 0.984375
2017-03-02T18:11:55.847686: step 30478, loss 0.0315199, acc 1
2017-03-02T18:11:55.941115: step 30479, loss 0.183028, acc 0.9375
2017-03-02T18:11:56.016590: step 30480, loss 0.193188, acc 0.921875
2017-03-02T18:11:56.083967: step 30481, loss 0.122085, acc 0.953125
2017-03-02T18:11:56.143517: step 30482, loss 0.0703332, acc 0.96875
2017-03-02T18:11:56.226953: step 30483, loss 0.196479, acc 0.921875
2017-03-02T18:11:56.299771: step 30484, loss 0.33879, acc 0.90625
2017-03-02T18:11:56.376332: step 30485, loss 0.212874, acc 0.875
2017-03-02T18:11:56.443136: step 30486, loss 0.187991, acc 0.890625
2017-03-02T18:11:56.513433: step 30487, loss 0.271022, acc 0.84375
2017-03-02T18:11:56.586184: step 30488, loss 0.288514, acc 0.875
2017-03-02T18:11:56.658527: step 30489, loss 0.0550256, acc 0.984375
2017-03-02T18:11:56.732380: step 30490, loss 0.0592873, acc 0.984375
2017-03-02T18:11:56.809802: step 30491, loss 0.167326, acc 0.921875
2017-03-02T18:11:56.883778: step 30492, loss 0.0882025, acc 0.96875
2017-03-02T18:11:56.956927: step 30493, loss 0.169997, acc 0.90625
2017-03-02T18:11:57.031289: step 30494, loss 0.258535, acc 0.9375
2017-03-02T18:11:57.108811: step 30495, loss 0.0792069, acc 0.984375
2017-03-02T18:11:57.176686: step 30496, loss 0.0392529, acc 1
2017-03-02T18:11:57.239140: step 30497, loss 0.172498, acc 0.953125
2017-03-02T18:11:57.318661: step 30498, loss 0.125292, acc 0.921875
2017-03-02T18:11:57.397221: step 30499, loss 0.108222, acc 0.953125
2017-03-02T18:11:57.478453: step 30500, loss 0.0889813, acc 0.96875

Evaluation:
2017-03-02T18:11:57.517439: step 30500, loss 4.00708, acc 0.651766

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30500

2017-03-02T18:11:57.977358: step 30501, loss 0.176638, acc 0.890625
2017-03-02T18:11:58.051695: step 30502, loss 0.0800651, acc 0.96875
2017-03-02T18:11:58.122909: step 30503, loss 0.145589, acc 0.9375
2017-03-02T18:11:58.187708: step 30504, loss 0.282672, acc 0.859375
2017-03-02T18:11:58.258494: step 30505, loss 0.150853, acc 0.921875
2017-03-02T18:11:58.331857: step 30506, loss 0.121645, acc 0.9375
2017-03-02T18:11:58.405032: step 30507, loss 0.195595, acc 0.890625
2017-03-02T18:11:58.482189: step 30508, loss 0.176913, acc 0.9375
2017-03-02T18:11:58.551707: step 30509, loss 0.0898707, acc 0.953125
2017-03-02T18:11:58.625380: step 30510, loss 0.0680156, acc 0.96875
2017-03-02T18:11:58.701105: step 30511, loss 0.104828, acc 0.96875
2017-03-02T18:11:58.771135: step 30512, loss 0.0551032, acc 0.96875
2017-03-02T18:11:58.841110: step 30513, loss 0.162705, acc 0.953125
2017-03-02T18:11:58.911537: step 30514, loss 0.159289, acc 0.953125
2017-03-02T18:11:58.982530: step 30515, loss 0.1469, acc 0.921875
2017-03-02T18:11:59.056382: step 30516, loss 0.197722, acc 0.9375
2017-03-02T18:11:59.130587: step 30517, loss 0.0956514, acc 0.90625
2017-03-02T18:11:59.203949: step 30518, loss 0.265173, acc 0.875
2017-03-02T18:11:59.272576: step 30519, loss 0.144095, acc 0.921875
2017-03-02T18:11:59.343381: step 30520, loss 0.207052, acc 0.875
2017-03-02T18:11:59.421158: step 30521, loss 0.219902, acc 0.90625
2017-03-02T18:11:59.500030: step 30522, loss 0.0788688, acc 0.953125
2017-03-02T18:11:59.578746: step 30523, loss 0.0445305, acc 0.96875
2017-03-02T18:11:59.648378: step 30524, loss 0.157124, acc 0.9375
2017-03-02T18:11:59.723702: step 30525, loss 0.326911, acc 0.8125
2017-03-02T18:11:59.796386: step 30526, loss 0.0907742, acc 0.953125
2017-03-02T18:11:59.862472: step 30527, loss 0.200899, acc 0.890625
2017-03-02T18:11:59.932087: step 30528, loss 0.148842, acc 0.9375
2017-03-02T18:11:59.999755: step 30529, loss 0.156386, acc 0.90625
2017-03-02T18:12:00.068697: step 30530, loss 0.106547, acc 0.953125
2017-03-02T18:12:00.147479: step 30531, loss 0.0289205, acc 1
2017-03-02T18:12:00.221185: step 30532, loss 0.139374, acc 0.921875
2017-03-02T18:12:00.292870: step 30533, loss 0.224685, acc 0.90625
2017-03-02T18:12:00.368049: step 30534, loss 0.176963, acc 0.921875
2017-03-02T18:12:00.439948: step 30535, loss 0.0874627, acc 0.953125
2017-03-02T18:12:00.505398: step 30536, loss 0.133025, acc 0.9375
2017-03-02T18:12:00.579604: step 30537, loss 0.163603, acc 0.953125
2017-03-02T18:12:00.649044: step 30538, loss 0.0842887, acc 0.96875
2017-03-02T18:12:00.724717: step 30539, loss 0.172544, acc 0.890625
2017-03-02T18:12:00.794556: step 30540, loss 0.0865241, acc 0.9375
2017-03-02T18:12:00.875402: step 30541, loss 0.111421, acc 0.90625
2017-03-02T18:12:00.953864: step 30542, loss 0.108495, acc 0.953125
2017-03-02T18:12:01.027623: step 30543, loss 0.0951989, acc 1
2017-03-02T18:12:01.099018: step 30544, loss 0.193208, acc 0.9375
2017-03-02T18:12:01.169887: step 30545, loss 0.133811, acc 0.9375
2017-03-02T18:12:01.232522: step 30546, loss 0.162707, acc 0.90625
2017-03-02T18:12:01.294618: step 30547, loss 0.151131, acc 0.875
2017-03-02T18:12:01.362254: step 30548, loss 0.225848, acc 0.890625
2017-03-02T18:12:01.433488: step 30549, loss 0.144541, acc 0.9375
2017-03-02T18:12:01.498191: step 30550, loss 0.0968896, acc 0.953125
2017-03-02T18:12:01.576982: step 30551, loss 0.145253, acc 0.921875
2017-03-02T18:12:01.642615: step 30552, loss 0.256002, acc 0.90625
2017-03-02T18:12:01.723770: step 30553, loss 0.105071, acc 0.9375
2017-03-02T18:12:01.788138: step 30554, loss 0.177581, acc 0.921875
2017-03-02T18:12:01.853600: step 30555, loss 0.12596, acc 0.9375
2017-03-02T18:12:01.922593: step 30556, loss 0.124077, acc 0.953125
2017-03-02T18:12:01.987112: step 30557, loss 0.128499, acc 0.953125
2017-03-02T18:12:02.068698: step 30558, loss 0.143064, acc 0.921875
2017-03-02T18:12:02.145500: step 30559, loss 0.151482, acc 0.9375
2017-03-02T18:12:02.212978: step 30560, loss 0.0427961, acc 0.984375
2017-03-02T18:12:02.282935: step 30561, loss 0.214888, acc 0.90625
2017-03-02T18:12:02.354776: step 30562, loss 0.195173, acc 0.921875
2017-03-02T18:12:02.429867: step 30563, loss 0.273364, acc 0.90625
2017-03-02T18:12:02.507960: step 30564, loss 0.153646, acc 0.953125
2017-03-02T18:12:02.583500: step 30565, loss 0.117747, acc 0.953125
2017-03-02T18:12:02.658024: step 30566, loss 0.0901932, acc 0.96875
2017-03-02T18:12:02.731835: step 30567, loss 0.152851, acc 0.953125
2017-03-02T18:12:02.801182: step 30568, loss 0.181697, acc 0.953125
2017-03-02T18:12:02.875266: step 30569, loss 0.205223, acc 0.921875
2017-03-02T18:12:02.946638: step 30570, loss 0.15459, acc 0.953125
2017-03-02T18:12:03.019394: step 30571, loss 0.165085, acc 0.90625
2017-03-02T18:12:03.088693: step 30572, loss 0.101271, acc 0.9375
2017-03-02T18:12:03.160702: step 30573, loss 0.251556, acc 0.921875
2017-03-02T18:12:03.231671: step 30574, loss 0.153078, acc 0.90625
2017-03-02T18:12:03.303409: step 30575, loss 0.286296, acc 0.859375
2017-03-02T18:12:03.371087: step 30576, loss 0.000766977, acc 1
2017-03-02T18:12:03.441943: step 30577, loss 0.107216, acc 0.953125
2017-03-02T18:12:03.516605: step 30578, loss 0.108899, acc 0.9375
2017-03-02T18:12:03.591062: step 30579, loss 0.0858662, acc 0.9375
2017-03-02T18:12:03.676626: step 30580, loss 0.250021, acc 0.890625
2017-03-02T18:12:03.753551: step 30581, loss 0.0957457, acc 0.9375
2017-03-02T18:12:03.823028: step 30582, loss 0.12511, acc 0.953125
2017-03-02T18:12:03.908624: step 30583, loss 0.194318, acc 0.90625
2017-03-02T18:12:03.976445: step 30584, loss 0.101114, acc 0.953125
2017-03-02T18:12:04.046339: step 30585, loss 0.0732689, acc 0.984375
2017-03-02T18:12:04.120064: step 30586, loss 0.0634376, acc 0.96875
2017-03-02T18:12:04.191924: step 30587, loss 0.111612, acc 0.953125
2017-03-02T18:12:04.265850: step 30588, loss 0.0901713, acc 0.953125
2017-03-02T18:12:04.337228: step 30589, loss 0.177682, acc 0.9375
2017-03-02T18:12:04.409939: step 30590, loss 0.205772, acc 0.9375
2017-03-02T18:12:04.494053: step 30591, loss 0.166234, acc 0.90625
2017-03-02T18:12:04.567403: step 30592, loss 0.0989313, acc 0.953125
2017-03-02T18:12:04.638779: step 30593, loss 0.115816, acc 0.9375
2017-03-02T18:12:04.710124: step 30594, loss 0.198733, acc 0.890625
2017-03-02T18:12:04.790023: step 30595, loss 0.120759, acc 0.9375
2017-03-02T18:12:04.867992: step 30596, loss 0.162676, acc 0.953125
2017-03-02T18:12:04.949585: step 30597, loss 0.217477, acc 0.90625
2017-03-02T18:12:05.023003: step 30598, loss 0.137106, acc 0.921875
2017-03-02T18:12:05.094688: step 30599, loss 0.182523, acc 0.9375
2017-03-02T18:12:05.167977: step 30600, loss 0.111992, acc 0.921875

Evaluation:
2017-03-02T18:12:05.200938: step 30600, loss 3.9818, acc 0.651045

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30600

2017-03-02T18:12:05.667990: step 30601, loss 0.158102, acc 0.9375
2017-03-02T18:12:05.742188: step 30602, loss 0.0796189, acc 0.96875
2017-03-02T18:12:05.816715: step 30603, loss 0.177397, acc 0.953125
2017-03-02T18:12:05.887860: step 30604, loss 0.156209, acc 0.9375
2017-03-02T18:12:05.963308: step 30605, loss 0.12949, acc 0.953125
2017-03-02T18:12:06.029424: step 30606, loss 0.0793672, acc 0.96875
2017-03-02T18:12:06.097136: step 30607, loss 0.0351491, acc 0.984375
2017-03-02T18:12:06.168798: step 30608, loss 0.149326, acc 0.953125
2017-03-02T18:12:06.236223: step 30609, loss 0.271489, acc 0.875
2017-03-02T18:12:06.310031: step 30610, loss 0.110254, acc 0.953125
2017-03-02T18:12:06.387300: step 30611, loss 0.186468, acc 0.890625
2017-03-02T18:12:06.458277: step 30612, loss 0.0662338, acc 0.984375
2017-03-02T18:12:06.534315: step 30613, loss 0.341178, acc 0.859375
2017-03-02T18:12:06.602869: step 30614, loss 0.139163, acc 0.9375
2017-03-02T18:12:06.676902: step 30615, loss 0.071821, acc 0.96875
2017-03-02T18:12:06.746042: step 30616, loss 0.157411, acc 0.953125
2017-03-02T18:12:06.812580: step 30617, loss 0.285194, acc 0.875
2017-03-02T18:12:06.884418: step 30618, loss 0.0966364, acc 0.953125
2017-03-02T18:12:06.964927: step 30619, loss 0.15956, acc 0.921875
2017-03-02T18:12:07.039399: step 30620, loss 0.111902, acc 0.953125
2017-03-02T18:12:07.119011: step 30621, loss 0.117176, acc 0.953125
2017-03-02T18:12:07.191931: step 30622, loss 0.203144, acc 0.921875
2017-03-02T18:12:07.280118: step 30623, loss 0.179689, acc 0.90625
2017-03-02T18:12:07.354542: step 30624, loss 0.0598977, acc 0.96875
2017-03-02T18:12:07.431429: step 30625, loss 0.0985651, acc 0.96875
2017-03-02T18:12:07.503511: step 30626, loss 0.154815, acc 0.9375
2017-03-02T18:12:07.578144: step 30627, loss 0.158029, acc 0.953125
2017-03-02T18:12:07.650038: step 30628, loss 0.280068, acc 0.84375
2017-03-02T18:12:07.719533: step 30629, loss 0.201662, acc 0.921875
2017-03-02T18:12:07.792116: step 30630, loss 0.112932, acc 0.96875
2017-03-02T18:12:07.866545: step 30631, loss 0.133927, acc 0.9375
2017-03-02T18:12:07.939305: step 30632, loss 0.198641, acc 0.921875
2017-03-02T18:12:08.009956: step 30633, loss 0.240183, acc 0.890625
2017-03-02T18:12:08.078842: step 30634, loss 0.0970883, acc 0.96875
2017-03-02T18:12:08.142192: step 30635, loss 0.202008, acc 0.90625
2017-03-02T18:12:08.206386: step 30636, loss 0.111475, acc 0.9375
2017-03-02T18:12:08.280672: step 30637, loss 0.133833, acc 0.9375
2017-03-02T18:12:08.357306: step 30638, loss 0.113783, acc 0.96875
2017-03-02T18:12:08.441093: step 30639, loss 0.14034, acc 0.890625
2017-03-02T18:12:08.512771: step 30640, loss 0.125085, acc 0.9375
2017-03-02T18:12:08.590449: step 30641, loss 0.12366, acc 0.953125
2017-03-02T18:12:08.663013: step 30642, loss 0.213692, acc 0.890625
2017-03-02T18:12:08.739287: step 30643, loss 0.195397, acc 0.90625
2017-03-02T18:12:08.802485: step 30644, loss 0.228464, acc 0.921875
2017-03-02T18:12:08.867321: step 30645, loss 0.156483, acc 0.921875
2017-03-02T18:12:08.935440: step 30646, loss 0.195603, acc 0.890625
2017-03-02T18:12:09.004667: step 30647, loss 0.128255, acc 0.9375
2017-03-02T18:12:09.078346: step 30648, loss 0.148894, acc 0.9375
2017-03-02T18:12:09.154860: step 30649, loss 0.114918, acc 0.9375
2017-03-02T18:12:09.236641: step 30650, loss 0.129892, acc 0.953125
2017-03-02T18:12:09.311228: step 30651, loss 0.146044, acc 0.953125
2017-03-02T18:12:09.391463: step 30652, loss 0.106704, acc 0.96875
2017-03-02T18:12:09.461031: step 30653, loss 0.114637, acc 0.953125
2017-03-02T18:12:09.528710: step 30654, loss 0.1294, acc 0.9375
2017-03-02T18:12:09.601807: step 30655, loss 0.187609, acc 0.90625
2017-03-02T18:12:09.673323: step 30656, loss 0.114683, acc 0.953125
2017-03-02T18:12:09.749066: step 30657, loss 0.285325, acc 0.859375
2017-03-02T18:12:09.821567: step 30658, loss 0.214532, acc 0.9375
2017-03-02T18:12:09.890694: step 30659, loss 0.161239, acc 0.9375
2017-03-02T18:12:09.964095: step 30660, loss 0.16134, acc 0.9375
2017-03-02T18:12:10.050402: step 30661, loss 0.0932381, acc 0.96875
2017-03-02T18:12:10.130168: step 30662, loss 0.229875, acc 0.890625
2017-03-02T18:12:10.203260: step 30663, loss 0.12974, acc 0.921875
2017-03-02T18:12:10.272560: step 30664, loss 0.158455, acc 0.90625
2017-03-02T18:12:10.344888: step 30665, loss 0.0598794, acc 1
2017-03-02T18:12:10.418343: step 30666, loss 0.0970087, acc 0.9375
2017-03-02T18:12:10.496548: step 30667, loss 0.098004, acc 0.953125
2017-03-02T18:12:10.567646: step 30668, loss 0.0588234, acc 0.984375
2017-03-02T18:12:10.662570: step 30669, loss 0.142559, acc 0.90625
2017-03-02T18:12:10.734281: step 30670, loss 0.132725, acc 0.90625
2017-03-02T18:12:10.812977: step 30671, loss 0.224484, acc 0.90625
2017-03-02T18:12:10.884757: step 30672, loss 0.167456, acc 0.921875
2017-03-02T18:12:10.961100: step 30673, loss 0.130866, acc 0.9375
2017-03-02T18:12:11.032025: step 30674, loss 0.10642, acc 0.9375
2017-03-02T18:12:11.123389: step 30675, loss 0.164043, acc 0.9375
2017-03-02T18:12:11.197148: step 30676, loss 0.0745706, acc 0.953125
2017-03-02T18:12:11.268576: step 30677, loss 0.121377, acc 0.953125
2017-03-02T18:12:11.344589: step 30678, loss 0.103099, acc 0.96875
2017-03-02T18:12:11.416666: step 30679, loss 0.0863545, acc 0.953125
2017-03-02T18:12:11.488890: step 30680, loss 0.130485, acc 0.953125
2017-03-02T18:12:11.556941: step 30681, loss 0.269163, acc 0.859375
2017-03-02T18:12:11.624078: step 30682, loss 0.0605894, acc 0.96875
2017-03-02T18:12:11.693218: step 30683, loss 0.104018, acc 0.953125
2017-03-02T18:12:11.765224: step 30684, loss 0.167006, acc 0.90625
2017-03-02T18:12:11.836440: step 30685, loss 0.201118, acc 0.921875
2017-03-02T18:12:11.908189: step 30686, loss 0.147197, acc 0.921875
2017-03-02T18:12:11.983559: step 30687, loss 0.134426, acc 0.9375
2017-03-02T18:12:12.056003: step 30688, loss 0.140394, acc 0.921875
2017-03-02T18:12:12.138149: step 30689, loss 0.249904, acc 0.921875
2017-03-02T18:12:12.212770: step 30690, loss 0.122738, acc 0.953125
2017-03-02T18:12:12.287521: step 30691, loss 0.187313, acc 0.890625
2017-03-02T18:12:12.347690: step 30692, loss 0.260435, acc 0.9375
2017-03-02T18:12:12.414264: step 30693, loss 0.110599, acc 0.96875
2017-03-02T18:12:12.486889: step 30694, loss 0.183096, acc 0.90625
2017-03-02T18:12:12.560427: step 30695, loss 0.170496, acc 0.90625
2017-03-02T18:12:12.629620: step 30696, loss 0.131, acc 0.9375
2017-03-02T18:12:12.711928: step 30697, loss 0.0879967, acc 0.953125
2017-03-02T18:12:12.784525: step 30698, loss 0.075143, acc 0.96875
2017-03-02T18:12:12.858852: step 30699, loss 0.125509, acc 0.9375
2017-03-02T18:12:12.934171: step 30700, loss 0.204974, acc 0.921875

Evaluation:
2017-03-02T18:12:12.961339: step 30700, loss 3.94734, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30700

2017-03-02T18:12:13.446652: step 30701, loss 0.113327, acc 0.953125
2017-03-02T18:12:13.517497: step 30702, loss 0.0831035, acc 0.96875
2017-03-02T18:12:13.589960: step 30703, loss 0.0780176, acc 0.96875
2017-03-02T18:12:13.660325: step 30704, loss 0.19309, acc 0.921875
2017-03-02T18:12:13.730974: step 30705, loss 0.17345, acc 0.9375
2017-03-02T18:12:13.805112: step 30706, loss 0.239393, acc 0.875
2017-03-02T18:12:13.874244: step 30707, loss 0.161939, acc 0.9375
2017-03-02T18:12:13.949502: step 30708, loss 0.14429, acc 0.9375
2017-03-02T18:12:14.021838: step 30709, loss 0.0544866, acc 0.984375
2017-03-02T18:12:14.102004: step 30710, loss 0.211711, acc 0.875
2017-03-02T18:12:14.175813: step 30711, loss 0.248546, acc 0.90625
2017-03-02T18:12:14.250728: step 30712, loss 0.191179, acc 0.953125
2017-03-02T18:12:14.325010: step 30713, loss 0.149909, acc 0.953125
2017-03-02T18:12:14.396213: step 30714, loss 0.147223, acc 0.9375
2017-03-02T18:12:14.465661: step 30715, loss 0.171838, acc 0.953125
2017-03-02T18:12:14.537335: step 30716, loss 0.141679, acc 0.953125
2017-03-02T18:12:14.612985: step 30717, loss 0.0992595, acc 0.953125
2017-03-02T18:12:14.687209: step 30718, loss 0.113909, acc 0.90625
2017-03-02T18:12:14.764963: step 30719, loss 0.137915, acc 0.921875
2017-03-02T18:12:14.838828: step 30720, loss 0.147291, acc 0.9375
2017-03-02T18:12:14.915025: step 30721, loss 0.172252, acc 0.953125
2017-03-02T18:12:14.989763: step 30722, loss 0.0438458, acc 0.96875
2017-03-02T18:12:15.063633: step 30723, loss 0.102743, acc 0.96875
2017-03-02T18:12:15.132167: step 30724, loss 0.201164, acc 0.9375
2017-03-02T18:12:15.207596: step 30725, loss 0.136936, acc 0.9375
2017-03-02T18:12:15.282107: step 30726, loss 0.209642, acc 0.90625
2017-03-02T18:12:15.352409: step 30727, loss 0.113803, acc 0.921875
2017-03-02T18:12:15.424132: step 30728, loss 0.139788, acc 0.953125
2017-03-02T18:12:15.497592: step 30729, loss 0.140665, acc 0.921875
2017-03-02T18:12:15.568389: step 30730, loss 0.0957598, acc 0.953125
2017-03-02T18:12:15.648730: step 30731, loss 0.116962, acc 0.953125
2017-03-02T18:12:15.713233: step 30732, loss 0.114121, acc 0.953125
2017-03-02T18:12:15.789327: step 30733, loss 0.194706, acc 0.921875
2017-03-02T18:12:15.857869: step 30734, loss 0.108243, acc 0.953125
2017-03-02T18:12:15.928154: step 30735, loss 0.0699779, acc 1
2017-03-02T18:12:16.001789: step 30736, loss 0.168907, acc 0.90625
2017-03-02T18:12:16.071568: step 30737, loss 0.141197, acc 0.9375
2017-03-02T18:12:16.149965: step 30738, loss 0.172043, acc 0.90625
2017-03-02T18:12:16.225475: step 30739, loss 0.146347, acc 0.90625
2017-03-02T18:12:16.303312: step 30740, loss 0.150777, acc 0.921875
2017-03-02T18:12:16.375275: step 30741, loss 0.100961, acc 0.953125
2017-03-02T18:12:16.443312: step 30742, loss 0.195304, acc 0.921875
2017-03-02T18:12:16.515552: step 30743, loss 0.20445, acc 0.9375
2017-03-02T18:12:16.586430: step 30744, loss 0.0962803, acc 0.96875
2017-03-02T18:12:16.660338: step 30745, loss 0.0838333, acc 0.96875
2017-03-02T18:12:16.734364: step 30746, loss 0.135845, acc 0.9375
2017-03-02T18:12:16.813683: step 30747, loss 0.13134, acc 0.9375
2017-03-02T18:12:16.888204: step 30748, loss 0.102093, acc 0.9375
2017-03-02T18:12:16.960995: step 30749, loss 0.0916938, acc 0.953125
2017-03-02T18:12:17.036524: step 30750, loss 0.225405, acc 0.921875
2017-03-02T18:12:17.105643: step 30751, loss 0.194743, acc 0.90625
2017-03-02T18:12:17.172118: step 30752, loss 0.170695, acc 0.890625
2017-03-02T18:12:17.242527: step 30753, loss 0.081212, acc 0.96875
2017-03-02T18:12:17.327950: step 30754, loss 0.167426, acc 0.90625
2017-03-02T18:12:17.404376: step 30755, loss 0.161408, acc 0.9375
2017-03-02T18:12:17.471383: step 30756, loss 0.309643, acc 0.859375
2017-03-02T18:12:17.546649: step 30757, loss 0.176134, acc 0.9375
2017-03-02T18:12:17.622562: step 30758, loss 0.122015, acc 0.921875
2017-03-02T18:12:17.702304: step 30759, loss 0.155743, acc 0.90625
2017-03-02T18:12:17.772974: step 30760, loss 0.190831, acc 0.90625
2017-03-02T18:12:17.844867: step 30761, loss 0.101984, acc 0.9375
2017-03-02T18:12:17.917964: step 30762, loss 0.142033, acc 0.96875
2017-03-02T18:12:17.994814: step 30763, loss 0.0361245, acc 0.984375
2017-03-02T18:12:18.068144: step 30764, loss 0.123673, acc 0.953125
2017-03-02T18:12:18.144091: step 30765, loss 0.133768, acc 0.921875
2017-03-02T18:12:18.214971: step 30766, loss 0.140951, acc 0.90625
2017-03-02T18:12:18.284671: step 30767, loss 0.075715, acc 0.953125
2017-03-02T18:12:18.357790: step 30768, loss 0.0884263, acc 0.953125
2017-03-02T18:12:18.430971: step 30769, loss 0.084669, acc 0.96875
2017-03-02T18:12:18.499212: step 30770, loss 0.0816813, acc 0.953125
2017-03-02T18:12:18.570762: step 30771, loss 0.17498, acc 0.921875
2017-03-02T18:12:18.632032: step 30772, loss 0.0147127, acc 1
2017-03-02T18:12:18.703401: step 30773, loss 0.0914948, acc 0.953125
2017-03-02T18:12:18.783766: step 30774, loss 0.0855961, acc 0.9375
2017-03-02T18:12:18.857291: step 30775, loss 0.126465, acc 0.9375
2017-03-02T18:12:18.941325: step 30776, loss 0.0690329, acc 0.96875
2017-03-02T18:12:19.015895: step 30777, loss 0.228894, acc 0.875
2017-03-02T18:12:19.086616: step 30778, loss 0.127409, acc 0.921875
2017-03-02T18:12:19.146070: step 30779, loss 0.13398, acc 0.921875
2017-03-02T18:12:19.209076: step 30780, loss 0.133461, acc 0.953125
2017-03-02T18:12:19.306015: step 30781, loss 0.101519, acc 0.9375
2017-03-02T18:12:19.383335: step 30782, loss 0.135046, acc 0.953125
2017-03-02T18:12:19.456536: step 30783, loss 0.171986, acc 0.9375
2017-03-02T18:12:19.527320: step 30784, loss 0.176758, acc 0.921875
2017-03-02T18:12:19.605542: step 30785, loss 0.217329, acc 0.90625
2017-03-02T18:12:19.686454: step 30786, loss 0.100541, acc 0.9375
2017-03-02T18:12:19.757552: step 30787, loss 0.0942797, acc 0.9375
2017-03-02T18:12:19.832739: step 30788, loss 0.184129, acc 0.921875
2017-03-02T18:12:19.905462: step 30789, loss 0.189313, acc 0.890625
2017-03-02T18:12:19.983022: step 30790, loss 0.146442, acc 0.90625
2017-03-02T18:12:20.063314: step 30791, loss 0.140839, acc 0.921875
2017-03-02T18:12:20.137056: step 30792, loss 0.129283, acc 0.953125
2017-03-02T18:12:20.214625: step 30793, loss 0.170295, acc 0.921875
2017-03-02T18:12:20.288616: step 30794, loss 0.263039, acc 0.921875
2017-03-02T18:12:20.359546: step 30795, loss 0.141586, acc 0.921875
2017-03-02T18:12:20.432560: step 30796, loss 0.109965, acc 0.953125
2017-03-02T18:12:20.499898: step 30797, loss 0.0180881, acc 1
2017-03-02T18:12:20.566190: step 30798, loss 0.178582, acc 0.90625
2017-03-02T18:12:20.633345: step 30799, loss 0.209068, acc 0.921875
2017-03-02T18:12:20.703019: step 30800, loss 0.0960371, acc 0.96875

Evaluation:
2017-03-02T18:12:20.738842: step 30800, loss 4.11654, acc 0.649603

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30800

2017-03-02T18:12:21.200733: step 30801, loss 0.141895, acc 0.9375
2017-03-02T18:12:21.269739: step 30802, loss 0.0889089, acc 0.96875
2017-03-02T18:12:21.343514: step 30803, loss 0.172611, acc 0.90625
2017-03-02T18:12:21.415371: step 30804, loss 0.1366, acc 0.921875
2017-03-02T18:12:21.485704: step 30805, loss 0.321691, acc 0.859375
2017-03-02T18:12:21.556245: step 30806, loss 0.12393, acc 0.921875
2017-03-02T18:12:21.627856: step 30807, loss 0.221117, acc 0.875
2017-03-02T18:12:21.708871: step 30808, loss 0.158877, acc 0.9375
2017-03-02T18:12:21.779160: step 30809, loss 0.128184, acc 0.921875
2017-03-02T18:12:21.851821: step 30810, loss 0.12681, acc 0.953125
2017-03-02T18:12:21.918551: step 30811, loss 0.0576315, acc 0.96875
2017-03-02T18:12:21.987896: step 30812, loss 0.269937, acc 0.859375
2017-03-02T18:12:22.068017: step 30813, loss 0.159899, acc 0.9375
2017-03-02T18:12:22.140333: step 30814, loss 0.161751, acc 0.90625
2017-03-02T18:12:22.224228: step 30815, loss 0.174197, acc 0.921875
2017-03-02T18:12:22.300179: step 30816, loss 0.285063, acc 0.90625
2017-03-02T18:12:22.379797: step 30817, loss 0.140027, acc 0.9375
2017-03-02T18:12:22.457400: step 30818, loss 0.10618, acc 0.953125
2017-03-02T18:12:22.541972: step 30819, loss 0.0741191, acc 0.984375
2017-03-02T18:12:22.612935: step 30820, loss 0.146221, acc 0.9375
2017-03-02T18:12:22.682236: step 30821, loss 0.164902, acc 0.921875
2017-03-02T18:12:22.755086: step 30822, loss 0.130557, acc 0.9375
2017-03-02T18:12:22.825986: step 30823, loss 0.0762487, acc 0.96875
2017-03-02T18:12:22.903268: step 30824, loss 0.253949, acc 0.921875
2017-03-02T18:12:22.980831: step 30825, loss 0.158051, acc 0.90625
2017-03-02T18:12:23.057519: step 30826, loss 0.188572, acc 0.90625
2017-03-02T18:12:23.129283: step 30827, loss 0.129151, acc 0.90625
2017-03-02T18:12:23.210817: step 30828, loss 0.156719, acc 0.9375
2017-03-02T18:12:23.282728: step 30829, loss 0.127986, acc 0.96875
2017-03-02T18:12:23.359855: step 30830, loss 0.175789, acc 0.921875
2017-03-02T18:12:23.434629: step 30831, loss 0.152181, acc 0.921875
2017-03-02T18:12:23.503202: step 30832, loss 0.13171, acc 0.90625
2017-03-02T18:12:23.576042: step 30833, loss 0.078425, acc 0.96875
2017-03-02T18:12:23.651507: step 30834, loss 0.144667, acc 0.953125
2017-03-02T18:12:23.727526: step 30835, loss 0.153995, acc 0.890625
2017-03-02T18:12:23.799601: step 30836, loss 0.13993, acc 0.953125
2017-03-02T18:12:23.872062: step 30837, loss 0.115099, acc 0.953125
2017-03-02T18:12:23.944564: step 30838, loss 0.11518, acc 0.953125
2017-03-02T18:12:24.011823: step 30839, loss 0.0903656, acc 0.921875
2017-03-02T18:12:24.079133: step 30840, loss 0.109266, acc 0.9375
2017-03-02T18:12:24.153558: step 30841, loss 0.230656, acc 0.875
2017-03-02T18:12:24.229114: step 30842, loss 0.1834, acc 0.9375
2017-03-02T18:12:24.295335: step 30843, loss 0.148694, acc 0.921875
2017-03-02T18:12:24.364786: step 30844, loss 0.0835261, acc 0.984375
2017-03-02T18:12:24.438568: step 30845, loss 0.142492, acc 0.953125
2017-03-02T18:12:24.503833: step 30846, loss 0.294551, acc 0.890625
2017-03-02T18:12:24.571371: step 30847, loss 0.142274, acc 0.953125
2017-03-02T18:12:24.642756: step 30848, loss 0.0977299, acc 0.96875
2017-03-02T18:12:24.712194: step 30849, loss 0.217198, acc 0.890625
2017-03-02T18:12:24.784239: step 30850, loss 0.228275, acc 0.90625
2017-03-02T18:12:24.853274: step 30851, loss 0.151687, acc 0.921875
2017-03-02T18:12:24.924773: step 30852, loss 0.0890777, acc 0.953125
2017-03-02T18:12:24.990314: step 30853, loss 0.0645565, acc 0.96875
2017-03-02T18:12:25.062404: step 30854, loss 0.117448, acc 0.9375
2017-03-02T18:12:25.136156: step 30855, loss 0.14076, acc 0.90625
2017-03-02T18:12:25.213920: step 30856, loss 0.114953, acc 0.9375
2017-03-02T18:12:25.289650: step 30857, loss 0.0940104, acc 0.953125
2017-03-02T18:12:25.353774: step 30858, loss 0.109719, acc 0.953125
2017-03-02T18:12:25.422193: step 30859, loss 0.142516, acc 0.921875
2017-03-02T18:12:25.492009: step 30860, loss 0.123107, acc 0.9375
2017-03-02T18:12:25.569508: step 30861, loss 0.108328, acc 0.953125
2017-03-02T18:12:25.637001: step 30862, loss 0.104377, acc 0.96875
2017-03-02T18:12:25.710184: step 30863, loss 0.255674, acc 0.84375
2017-03-02T18:12:25.778751: step 30864, loss 0.170765, acc 0.90625
2017-03-02T18:12:25.857148: step 30865, loss 0.126125, acc 0.921875
2017-03-02T18:12:25.926780: step 30866, loss 0.163549, acc 0.90625
2017-03-02T18:12:26.003469: step 30867, loss 0.13365, acc 0.9375
2017-03-02T18:12:26.073427: step 30868, loss 0.143793, acc 0.890625
2017-03-02T18:12:26.159354: step 30869, loss 0.183276, acc 0.921875
2017-03-02T18:12:26.226997: step 30870, loss 0.189002, acc 0.90625
2017-03-02T18:12:26.294162: step 30871, loss 0.204976, acc 0.921875
2017-03-02T18:12:26.372581: step 30872, loss 0.0819496, acc 0.984375
2017-03-02T18:12:26.445129: step 30873, loss 0.145932, acc 0.921875
2017-03-02T18:12:26.513500: step 30874, loss 0.155593, acc 0.9375
2017-03-02T18:12:26.584034: step 30875, loss 0.155885, acc 0.921875
2017-03-02T18:12:26.659991: step 30876, loss 0.191773, acc 0.90625
2017-03-02T18:12:26.731114: step 30877, loss 0.0931316, acc 0.9375
2017-03-02T18:12:26.814117: step 30878, loss 0.233464, acc 0.90625
2017-03-02T18:12:26.884916: step 30879, loss 0.0631412, acc 0.96875
2017-03-02T18:12:26.958630: step 30880, loss 0.0987588, acc 0.953125
2017-03-02T18:12:27.031982: step 30881, loss 0.113718, acc 0.96875
2017-03-02T18:12:27.122375: step 30882, loss 0.161446, acc 0.953125
2017-03-02T18:12:27.197429: step 30883, loss 0.149908, acc 0.953125
2017-03-02T18:12:27.269308: step 30884, loss 0.25615, acc 0.875
2017-03-02T18:12:27.345980: step 30885, loss 0.113737, acc 0.9375
2017-03-02T18:12:27.411026: step 30886, loss 0.137516, acc 0.9375
2017-03-02T18:12:27.484507: step 30887, loss 0.176648, acc 0.9375
2017-03-02T18:12:27.557865: step 30888, loss 0.201545, acc 0.921875
2017-03-02T18:12:27.631253: step 30889, loss 0.179188, acc 0.9375
2017-03-02T18:12:27.694761: step 30890, loss 0.0844139, acc 0.953125
2017-03-02T18:12:27.770909: step 30891, loss 0.21879, acc 0.90625
2017-03-02T18:12:27.846956: step 30892, loss 0.144807, acc 0.96875
2017-03-02T18:12:27.916115: step 30893, loss 0.128637, acc 0.9375
2017-03-02T18:12:27.989556: step 30894, loss 0.115514, acc 0.921875
2017-03-02T18:12:28.070578: step 30895, loss 0.166331, acc 0.9375
2017-03-02T18:12:28.145081: step 30896, loss 0.1191, acc 0.96875
2017-03-02T18:12:28.217284: step 30897, loss 0.245508, acc 0.921875
2017-03-02T18:12:28.277851: step 30898, loss 0.0491412, acc 1
2017-03-02T18:12:28.351745: step 30899, loss 0.0765451, acc 0.953125
2017-03-02T18:12:28.421797: step 30900, loss 0.192967, acc 0.921875

Evaluation:
2017-03-02T18:12:28.453228: step 30900, loss 4.13572, acc 0.651045

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-30900

2017-03-02T18:12:28.895481: step 30901, loss 0.170339, acc 0.9375
2017-03-02T18:12:28.972423: step 30902, loss 0.205558, acc 0.921875
2017-03-02T18:12:29.047272: step 30903, loss 0.172318, acc 0.921875
2017-03-02T18:12:29.150252: step 30904, loss 0.104833, acc 0.96875
2017-03-02T18:12:29.221402: step 30905, loss 0.169794, acc 0.90625
2017-03-02T18:12:29.290918: step 30906, loss 0.105764, acc 0.96875
2017-03-02T18:12:29.365601: step 30907, loss 0.147179, acc 0.921875
2017-03-02T18:12:29.438770: step 30908, loss 0.102977, acc 0.953125
2017-03-02T18:12:29.502456: step 30909, loss 0.187612, acc 0.90625
2017-03-02T18:12:29.572017: step 30910, loss 0.302403, acc 0.875
2017-03-02T18:12:29.644901: step 30911, loss 0.0868396, acc 0.953125
2017-03-02T18:12:29.724247: step 30912, loss 0.146037, acc 0.9375
2017-03-02T18:12:29.802697: step 30913, loss 0.244942, acc 0.890625
2017-03-02T18:12:29.875565: step 30914, loss 0.11336, acc 0.953125
2017-03-02T18:12:29.951611: step 30915, loss 0.162717, acc 0.921875
2017-03-02T18:12:30.025769: step 30916, loss 0.200708, acc 0.875
2017-03-02T18:12:30.099195: step 30917, loss 0.198586, acc 0.90625
2017-03-02T18:12:30.169109: step 30918, loss 0.175756, acc 0.90625
2017-03-02T18:12:30.238802: step 30919, loss 0.18531, acc 0.90625
2017-03-02T18:12:30.308429: step 30920, loss 0.319369, acc 0.859375
2017-03-02T18:12:30.386372: step 30921, loss 0.239383, acc 0.90625
2017-03-02T18:12:30.459426: step 30922, loss 0.130598, acc 0.9375
2017-03-02T18:12:30.533374: step 30923, loss 0.0633115, acc 0.96875
2017-03-02T18:12:30.606308: step 30924, loss 0.0449406, acc 0.96875
2017-03-02T18:12:30.679973: step 30925, loss 0.132818, acc 0.921875
2017-03-02T18:12:30.751311: step 30926, loss 0.182515, acc 0.90625
2017-03-02T18:12:30.831579: step 30927, loss 0.181135, acc 0.953125
2017-03-02T18:12:30.905580: step 30928, loss 0.0926094, acc 0.96875
2017-03-02T18:12:30.972613: step 30929, loss 0.154637, acc 0.921875
2017-03-02T18:12:31.045937: step 30930, loss 0.14683, acc 0.921875
2017-03-02T18:12:31.119004: step 30931, loss 0.1036, acc 0.9375
2017-03-02T18:12:31.190459: step 30932, loss 0.0934164, acc 0.9375
2017-03-02T18:12:31.262664: step 30933, loss 0.152961, acc 0.921875
2017-03-02T18:12:31.335778: step 30934, loss 0.151765, acc 0.953125
2017-03-02T18:12:31.418753: step 30935, loss 0.222797, acc 0.921875
2017-03-02T18:12:31.496567: step 30936, loss 0.0991137, acc 0.953125
2017-03-02T18:12:31.574452: step 30937, loss 0.145305, acc 0.921875
2017-03-02T18:12:31.643580: step 30938, loss 0.177705, acc 0.953125
2017-03-02T18:12:31.710806: step 30939, loss 0.0492153, acc 1
2017-03-02T18:12:31.783301: step 30940, loss 0.0860012, acc 0.953125
2017-03-02T18:12:31.863528: step 30941, loss 0.153514, acc 0.921875
2017-03-02T18:12:31.935130: step 30942, loss 0.138314, acc 0.953125
2017-03-02T18:12:32.008831: step 30943, loss 0.134792, acc 0.953125
2017-03-02T18:12:32.076463: step 30944, loss 0.218138, acc 0.890625
2017-03-02T18:12:32.155408: step 30945, loss 0.0952143, acc 0.984375
2017-03-02T18:12:32.225455: step 30946, loss 0.17259, acc 0.9375
2017-03-02T18:12:32.297020: step 30947, loss 0.21379, acc 0.921875
2017-03-02T18:12:32.369558: step 30948, loss 0.0933523, acc 0.953125
2017-03-02T18:12:32.439836: step 30949, loss 0.130381, acc 0.953125
2017-03-02T18:12:32.510142: step 30950, loss 0.107726, acc 0.953125
2017-03-02T18:12:32.585262: step 30951, loss 0.169708, acc 0.9375
2017-03-02T18:12:32.661160: step 30952, loss 0.0916711, acc 0.984375
2017-03-02T18:12:32.731465: step 30953, loss 0.134098, acc 0.90625
2017-03-02T18:12:32.803128: step 30954, loss 0.286947, acc 0.90625
2017-03-02T18:12:32.880428: step 30955, loss 0.14827, acc 0.90625
2017-03-02T18:12:32.947377: step 30956, loss 0.144624, acc 0.9375
2017-03-02T18:12:33.019841: step 30957, loss 0.219202, acc 0.875
2017-03-02T18:12:33.091857: step 30958, loss 0.143246, acc 0.9375
2017-03-02T18:12:33.165040: step 30959, loss 0.0619431, acc 0.96875
2017-03-02T18:12:33.236024: step 30960, loss 0.149031, acc 0.953125
2017-03-02T18:12:33.310975: step 30961, loss 0.146804, acc 0.90625
2017-03-02T18:12:33.389244: step 30962, loss 0.0563344, acc 0.96875
2017-03-02T18:12:33.468372: step 30963, loss 0.06999, acc 0.96875
2017-03-02T18:12:33.543194: step 30964, loss 0.152229, acc 0.90625
2017-03-02T18:12:33.612696: step 30965, loss 0.0398909, acc 0.984375
2017-03-02T18:12:33.684353: step 30966, loss 0.106898, acc 0.96875
2017-03-02T18:12:33.760088: step 30967, loss 0.0551284, acc 0.984375
2017-03-02T18:12:33.836674: step 30968, loss 2.68221e-07, acc 1
2017-03-02T18:12:33.914933: step 30969, loss 0.0700248, acc 0.96875
2017-03-02T18:12:33.987190: step 30970, loss 0.163289, acc 0.90625
2017-03-02T18:12:34.053678: step 30971, loss 0.107197, acc 0.96875
2017-03-02T18:12:34.122243: step 30972, loss 0.181908, acc 0.9375
2017-03-02T18:12:34.192511: step 30973, loss 0.110883, acc 0.9375
2017-03-02T18:12:34.261342: step 30974, loss 0.216812, acc 0.890625
2017-03-02T18:12:34.335799: step 30975, loss 0.225573, acc 0.921875
2017-03-02T18:12:34.405989: step 30976, loss 0.193935, acc 0.90625
2017-03-02T18:12:34.479036: step 30977, loss 0.0742781, acc 0.953125
2017-03-02T18:12:34.551276: step 30978, loss 0.219712, acc 0.921875
2017-03-02T18:12:34.637971: step 30979, loss 0.0966994, acc 0.953125
2017-03-02T18:12:34.700668: step 30980, loss 0.160404, acc 0.921875
2017-03-02T18:12:34.764391: step 30981, loss 0.0841623, acc 0.96875
2017-03-02T18:12:34.830734: step 30982, loss 0.0972131, acc 0.953125
2017-03-02T18:12:34.902732: step 30983, loss 0.103814, acc 0.9375
2017-03-02T18:12:34.969175: step 30984, loss 0.157102, acc 0.90625
2017-03-02T18:12:35.039039: step 30985, loss 0.0951368, acc 0.96875
2017-03-02T18:12:35.105560: step 30986, loss 0.140813, acc 0.953125
2017-03-02T18:12:35.195816: step 30987, loss 0.185401, acc 0.90625
2017-03-02T18:12:35.280841: step 30988, loss 0.118169, acc 0.921875
2017-03-02T18:12:35.366083: step 30989, loss 0.111629, acc 0.9375
2017-03-02T18:12:35.438901: step 30990, loss 0.178947, acc 0.921875
2017-03-02T18:12:35.505258: step 30991, loss 0.0711318, acc 0.96875
2017-03-02T18:12:35.584301: step 30992, loss 0.152401, acc 0.9375
2017-03-02T18:12:35.645293: step 30993, loss 0.136459, acc 0.953125
2017-03-02T18:12:35.712446: step 30994, loss 0.122763, acc 0.921875
2017-03-02T18:12:35.785059: step 30995, loss 0.08789, acc 0.96875
2017-03-02T18:12:35.863307: step 30996, loss 0.101241, acc 0.96875
2017-03-02T18:12:35.935127: step 30997, loss 0.104975, acc 0.96875
2017-03-02T18:12:36.006188: step 30998, loss 0.0948665, acc 0.96875
2017-03-02T18:12:36.075029: step 30999, loss 0.0565723, acc 0.984375
2017-03-02T18:12:36.153813: step 31000, loss 0.137397, acc 0.921875

Evaluation:
2017-03-02T18:12:36.188054: step 31000, loss 4.09966, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31000

2017-03-02T18:12:36.628012: step 31001, loss 0.0538408, acc 0.984375
2017-03-02T18:12:36.703489: step 31002, loss 0.277724, acc 0.875
2017-03-02T18:12:36.778591: step 31003, loss 0.123895, acc 0.953125
2017-03-02T18:12:36.848817: step 31004, loss 0.117801, acc 0.9375
2017-03-02T18:12:36.922852: step 31005, loss 0.0526523, acc 0.96875
2017-03-02T18:12:36.992492: step 31006, loss 0.157518, acc 0.9375
2017-03-02T18:12:37.055982: step 31007, loss 0.0929355, acc 0.96875
2017-03-02T18:12:37.121714: step 31008, loss 0.0721388, acc 1
2017-03-02T18:12:37.192759: step 31009, loss 0.0537796, acc 1
2017-03-02T18:12:37.265071: step 31010, loss 0.115733, acc 0.9375
2017-03-02T18:12:37.337103: step 31011, loss 0.16774, acc 0.921875
2017-03-02T18:12:37.409146: step 31012, loss 0.117027, acc 0.9375
2017-03-02T18:12:37.489723: step 31013, loss 0.189683, acc 0.890625
2017-03-02T18:12:37.562524: step 31014, loss 0.151119, acc 0.9375
2017-03-02T18:12:37.634805: step 31015, loss 0.143011, acc 0.953125
2017-03-02T18:12:37.703730: step 31016, loss 0.112949, acc 0.921875
2017-03-02T18:12:37.769668: step 31017, loss 0.141598, acc 0.9375
2017-03-02T18:12:37.846702: step 31018, loss 0.12993, acc 0.953125
2017-03-02T18:12:37.922947: step 31019, loss 0.192621, acc 0.90625
2017-03-02T18:12:37.997091: step 31020, loss 0.0550716, acc 0.984375
2017-03-02T18:12:38.076089: step 31021, loss 0.0601773, acc 0.96875
2017-03-02T18:12:38.138260: step 31022, loss 0.235, acc 0.9375
2017-03-02T18:12:38.200753: step 31023, loss 0.117553, acc 0.953125
2017-03-02T18:12:38.271602: step 31024, loss 0.144789, acc 0.921875
2017-03-02T18:12:38.354274: step 31025, loss 0.218876, acc 0.9375
2017-03-02T18:12:38.421915: step 31026, loss 0.0712749, acc 0.953125
2017-03-02T18:12:38.487814: step 31027, loss 0.207923, acc 0.875
2017-03-02T18:12:38.559529: step 31028, loss 0.150267, acc 0.9375
2017-03-02T18:12:38.634990: step 31029, loss 0.216208, acc 0.90625
2017-03-02T18:12:38.715336: step 31030, loss 0.213367, acc 0.90625
2017-03-02T18:12:38.789086: step 31031, loss 0.276671, acc 0.859375
2017-03-02T18:12:38.864583: step 31032, loss 0.0834227, acc 0.96875
2017-03-02T18:12:38.945008: step 31033, loss 0.132851, acc 0.953125
2017-03-02T18:12:39.015051: step 31034, loss 0.173652, acc 0.921875
2017-03-02T18:12:39.081446: step 31035, loss 0.115107, acc 0.953125
2017-03-02T18:12:39.151319: step 31036, loss 0.140543, acc 0.921875
2017-03-02T18:12:39.222802: step 31037, loss 0.103314, acc 0.96875
2017-03-02T18:12:39.291980: step 31038, loss 0.137804, acc 0.921875
2017-03-02T18:12:39.365587: step 31039, loss 0.134548, acc 0.953125
2017-03-02T18:12:39.443287: step 31040, loss 0.166284, acc 0.96875
2017-03-02T18:12:39.523296: step 31041, loss 0.15854, acc 0.921875
2017-03-02T18:12:39.596798: step 31042, loss 0.0436622, acc 0.984375
2017-03-02T18:12:39.676563: step 31043, loss 0.116759, acc 0.953125
2017-03-02T18:12:39.752766: step 31044, loss 0.181802, acc 0.890625
2017-03-02T18:12:39.823587: step 31045, loss 0.167218, acc 0.921875
2017-03-02T18:12:39.895001: step 31046, loss 0.165508, acc 0.90625
2017-03-02T18:12:39.966162: step 31047, loss 0.223509, acc 0.953125
2017-03-02T18:12:40.039210: step 31048, loss 0.0850905, acc 0.953125
2017-03-02T18:12:40.116241: step 31049, loss 0.208711, acc 0.921875
2017-03-02T18:12:40.190251: step 31050, loss 0.156708, acc 0.90625
2017-03-02T18:12:40.262620: step 31051, loss 0.167025, acc 0.90625
2017-03-02T18:12:40.334454: step 31052, loss 0.0507129, acc 0.984375
2017-03-02T18:12:40.410609: step 31053, loss 0.196987, acc 0.921875
2017-03-02T18:12:40.475653: step 31054, loss 0.107343, acc 0.953125
2017-03-02T18:12:40.541935: step 31055, loss 0.228827, acc 0.890625
2017-03-02T18:12:40.612889: step 31056, loss 0.0953693, acc 0.96875
2017-03-02T18:12:40.682763: step 31057, loss 0.11978, acc 0.953125
2017-03-02T18:12:40.756288: step 31058, loss 0.0924064, acc 0.96875
2017-03-02T18:12:40.827579: step 31059, loss 0.245026, acc 0.953125
2017-03-02T18:12:40.901384: step 31060, loss 0.211013, acc 0.890625
2017-03-02T18:12:40.977322: step 31061, loss 0.0979385, acc 0.953125
2017-03-02T18:12:41.054289: step 31062, loss 0.163921, acc 0.90625
2017-03-02T18:12:41.135277: step 31063, loss 0.0844066, acc 0.953125
2017-03-02T18:12:41.235992: step 31064, loss 0.0547949, acc 0.984375
2017-03-02T18:12:41.307992: step 31065, loss 0.191422, acc 0.90625
2017-03-02T18:12:41.377838: step 31066, loss 0.158006, acc 0.9375
2017-03-02T18:12:41.459155: step 31067, loss 0.188638, acc 0.9375
2017-03-02T18:12:41.535826: step 31068, loss 0.0902073, acc 0.96875
2017-03-02T18:12:41.610203: step 31069, loss 0.103779, acc 0.9375
2017-03-02T18:12:41.682242: step 31070, loss 0.0524708, acc 0.984375
2017-03-02T18:12:41.752211: step 31071, loss 0.114731, acc 0.953125
2017-03-02T18:12:41.824534: step 31072, loss 0.0882192, acc 0.96875
2017-03-02T18:12:41.893683: step 31073, loss 0.0685844, acc 0.984375
2017-03-02T18:12:41.961437: step 31074, loss 0.147178, acc 0.90625
2017-03-02T18:12:42.033732: step 31075, loss 0.188934, acc 0.9375
2017-03-02T18:12:42.101106: step 31076, loss 0.0958447, acc 0.96875
2017-03-02T18:12:42.169297: step 31077, loss 0.136383, acc 0.9375
2017-03-02T18:12:42.240922: step 31078, loss 0.0597369, acc 0.96875
2017-03-02T18:12:42.314555: step 31079, loss 0.11348, acc 0.9375
2017-03-02T18:12:42.386572: step 31080, loss 0.151983, acc 0.890625
2017-03-02T18:12:42.454729: step 31081, loss 0.0451559, acc 0.984375
2017-03-02T18:12:42.534207: step 31082, loss 0.205858, acc 0.890625
2017-03-02T18:12:42.606869: step 31083, loss 0.0764447, acc 0.96875
2017-03-02T18:12:42.678572: step 31084, loss 0.119732, acc 0.9375
2017-03-02T18:12:42.749541: step 31085, loss 0.150398, acc 0.9375
2017-03-02T18:12:42.822254: step 31086, loss 0.186413, acc 0.9375
2017-03-02T18:12:42.910796: step 31087, loss 0.0466399, acc 0.984375
2017-03-02T18:12:42.985165: step 31088, loss 0.11826, acc 0.953125
2017-03-02T18:12:43.057700: step 31089, loss 0.157157, acc 0.953125
2017-03-02T18:12:43.130587: step 31090, loss 0.172458, acc 0.921875
2017-03-02T18:12:43.211389: step 31091, loss 0.157395, acc 0.9375
2017-03-02T18:12:43.282071: step 31092, loss 0.199406, acc 0.875
2017-03-02T18:12:43.371854: step 31093, loss 0.112107, acc 0.953125
2017-03-02T18:12:43.449103: step 31094, loss 0.14196, acc 0.90625
2017-03-02T18:12:43.527916: step 31095, loss 0.0905013, acc 0.96875
2017-03-02T18:12:43.599465: step 31096, loss 0.144599, acc 0.921875
2017-03-02T18:12:43.665709: step 31097, loss 0.253123, acc 0.890625
2017-03-02T18:12:43.749728: step 31098, loss 0.137918, acc 0.953125
2017-03-02T18:12:43.820971: step 31099, loss 0.192297, acc 0.921875
2017-03-02T18:12:43.891867: step 31100, loss 0.17083, acc 0.921875

Evaluation:
2017-03-02T18:12:43.923987: step 31100, loss 4.10509, acc 0.651045

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31100

2017-03-02T18:12:44.442863: step 31101, loss 0.152459, acc 0.9375
2017-03-02T18:12:44.518640: step 31102, loss 0.315165, acc 0.890625
2017-03-02T18:12:44.588956: step 31103, loss 0.0656502, acc 0.953125
2017-03-02T18:12:44.656290: step 31104, loss 0.150911, acc 0.9375
2017-03-02T18:12:44.726760: step 31105, loss 0.150835, acc 0.921875
2017-03-02T18:12:44.798439: step 31106, loss 0.110048, acc 0.953125
2017-03-02T18:12:44.871740: step 31107, loss 0.0780411, acc 0.984375
2017-03-02T18:12:44.944138: step 31108, loss 0.179592, acc 0.90625
2017-03-02T18:12:45.024966: step 31109, loss 0.274743, acc 0.875
2017-03-02T18:12:45.097499: step 31110, loss 0.21311, acc 0.90625
2017-03-02T18:12:45.170542: step 31111, loss 0.110195, acc 0.953125
2017-03-02T18:12:45.244170: step 31112, loss 0.21752, acc 0.9375
2017-03-02T18:12:45.321522: step 31113, loss 0.199211, acc 0.90625
2017-03-02T18:12:45.387507: step 31114, loss 0.105352, acc 0.9375
2017-03-02T18:12:45.460604: step 31115, loss 0.0943769, acc 0.96875
2017-03-02T18:12:45.530243: step 31116, loss 0.113704, acc 0.921875
2017-03-02T18:12:45.604680: step 31117, loss 0.185521, acc 0.921875
2017-03-02T18:12:45.676061: step 31118, loss 0.128066, acc 0.953125
2017-03-02T18:12:45.746568: step 31119, loss 0.137616, acc 0.9375
2017-03-02T18:12:45.817871: step 31120, loss 0.125035, acc 0.9375
2017-03-02T18:12:45.890182: step 31121, loss 0.233077, acc 0.90625
2017-03-02T18:12:45.962679: step 31122, loss 0.101306, acc 0.984375
2017-03-02T18:12:46.036868: step 31123, loss 0.106128, acc 0.953125
2017-03-02T18:12:46.105349: step 31124, loss 0.191367, acc 0.921875
2017-03-02T18:12:46.177549: step 31125, loss 0.138651, acc 0.921875
2017-03-02T18:12:46.258169: step 31126, loss 0.109835, acc 0.953125
2017-03-02T18:12:46.329835: step 31127, loss 0.180659, acc 0.921875
2017-03-02T18:12:46.406195: step 31128, loss 0.213907, acc 0.890625
2017-03-02T18:12:46.488889: step 31129, loss 0.158775, acc 0.921875
2017-03-02T18:12:46.570264: step 31130, loss 0.225213, acc 0.875
2017-03-02T18:12:46.641956: step 31131, loss 0.152622, acc 0.9375
2017-03-02T18:12:46.708853: step 31132, loss 0.252185, acc 0.90625
2017-03-02T18:12:46.777714: step 31133, loss 0.138941, acc 0.90625
2017-03-02T18:12:46.848554: step 31134, loss 0.150611, acc 0.9375
2017-03-02T18:12:46.939934: step 31135, loss 0.108356, acc 0.96875
2017-03-02T18:12:47.009354: step 31136, loss 0.201833, acc 0.890625
2017-03-02T18:12:47.111243: step 31137, loss 0.2651, acc 0.90625
2017-03-02T18:12:47.205535: step 31138, loss 0.122428, acc 0.953125
2017-03-02T18:12:47.285545: step 31139, loss 0.138865, acc 0.953125
2017-03-02T18:12:47.357299: step 31140, loss 0.160349, acc 0.921875
2017-03-02T18:12:47.429346: step 31141, loss 0.129255, acc 0.9375
2017-03-02T18:12:47.498494: step 31142, loss 0.103773, acc 0.9375
2017-03-02T18:12:47.564746: step 31143, loss 0.157967, acc 0.90625
2017-03-02T18:12:47.634678: step 31144, loss 0.120676, acc 0.921875
2017-03-02T18:12:47.704593: step 31145, loss 0.0867482, acc 0.96875
2017-03-02T18:12:47.776903: step 31146, loss 0.24871, acc 0.859375
2017-03-02T18:12:47.857479: step 31147, loss 0.280227, acc 0.875
2017-03-02T18:12:47.930703: step 31148, loss 0.0755455, acc 0.984375
2017-03-02T18:12:48.004348: step 31149, loss 0.225834, acc 0.921875
2017-03-02T18:12:48.070476: step 31150, loss 0.203452, acc 0.9375
2017-03-02T18:12:48.136757: step 31151, loss 0.131878, acc 0.953125
2017-03-02T18:12:48.209727: step 31152, loss 0.172344, acc 0.921875
2017-03-02T18:12:48.299528: step 31153, loss 0.189633, acc 0.890625
2017-03-02T18:12:48.373450: step 31154, loss 0.154507, acc 0.90625
2017-03-02T18:12:48.447260: step 31155, loss 0.0696665, acc 0.953125
2017-03-02T18:12:48.533624: step 31156, loss 0.223191, acc 0.890625
2017-03-02T18:12:48.605408: step 31157, loss 0.150925, acc 0.921875
2017-03-02T18:12:48.677750: step 31158, loss 0.202559, acc 0.90625
2017-03-02T18:12:48.752062: step 31159, loss 0.253559, acc 0.921875
2017-03-02T18:12:48.816697: step 31160, loss 0.14283, acc 0.921875
2017-03-02T18:12:48.891458: step 31161, loss 0.0927181, acc 0.984375
2017-03-02T18:12:48.963885: step 31162, loss 0.111138, acc 0.953125
2017-03-02T18:12:49.037227: step 31163, loss 0.139481, acc 0.96875
2017-03-02T18:12:49.110652: step 31164, loss 0.0428602, acc 1
2017-03-02T18:12:49.189140: step 31165, loss 0.173276, acc 0.921875
2017-03-02T18:12:49.267650: step 31166, loss 0.365077, acc 0.84375
2017-03-02T18:12:49.340055: step 31167, loss 0.0999269, acc 0.9375
2017-03-02T18:12:49.414385: step 31168, loss 0.166476, acc 0.9375
2017-03-02T18:12:49.482562: step 31169, loss 0.136091, acc 0.9375
2017-03-02T18:12:49.551243: step 31170, loss 0.242486, acc 0.90625
2017-03-02T18:12:49.621867: step 31171, loss 0.0844612, acc 0.953125
2017-03-02T18:12:49.697596: step 31172, loss 0.102297, acc 0.96875
2017-03-02T18:12:49.769232: step 31173, loss 0.172377, acc 0.921875
2017-03-02T18:12:49.838500: step 31174, loss 0.0947515, acc 0.953125
2017-03-02T18:12:49.913070: step 31175, loss 0.103591, acc 0.953125
2017-03-02T18:12:49.986640: step 31176, loss 0.0987992, acc 0.96875
2017-03-02T18:12:50.065171: step 31177, loss 0.246354, acc 0.890625
2017-03-02T18:12:50.138334: step 31178, loss 0.153426, acc 0.9375
2017-03-02T18:12:50.203465: step 31179, loss 0.148665, acc 0.953125
2017-03-02T18:12:50.275362: step 31180, loss 0.0735264, acc 0.96875
2017-03-02T18:12:50.344370: step 31181, loss 0.213536, acc 0.890625
2017-03-02T18:12:50.415948: step 31182, loss 0.125036, acc 0.9375
2017-03-02T18:12:50.493407: step 31183, loss 0.114626, acc 0.953125
2017-03-02T18:12:50.556989: step 31184, loss 0.106207, acc 0.953125
2017-03-02T18:12:50.631510: step 31185, loss 0.0888644, acc 0.953125
2017-03-02T18:12:50.705233: step 31186, loss 0.22453, acc 0.875
2017-03-02T18:12:50.779102: step 31187, loss 0.0933037, acc 0.96875
2017-03-02T18:12:50.836902: step 31188, loss 0.16233, acc 0.890625
2017-03-02T18:12:50.905488: step 31189, loss 0.154461, acc 0.953125
2017-03-02T18:12:50.979571: step 31190, loss 0.269488, acc 0.875
2017-03-02T18:12:51.049106: step 31191, loss 0.103117, acc 0.9375
2017-03-02T18:12:51.123840: step 31192, loss 0.115719, acc 0.96875
2017-03-02T18:12:51.198786: step 31193, loss 0.10655, acc 0.96875
2017-03-02T18:12:51.268698: step 31194, loss 0.150957, acc 0.921875
2017-03-02T18:12:51.339829: step 31195, loss 0.0686031, acc 0.96875
2017-03-02T18:12:51.411567: step 31196, loss 0.213401, acc 0.90625
2017-03-02T18:12:51.484288: step 31197, loss 0.0964319, acc 0.96875
2017-03-02T18:12:51.561535: step 31198, loss 0.127112, acc 0.9375
2017-03-02T18:12:51.628992: step 31199, loss 0.178107, acc 0.9375
2017-03-02T18:12:51.709158: step 31200, loss 0.167965, acc 0.921875

Evaluation:
2017-03-02T18:12:51.749305: step 31200, loss 4.06022, acc 0.652487

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31200

2017-03-02T18:12:52.279547: step 31201, loss 0.189872, acc 0.890625
2017-03-02T18:12:52.353092: step 31202, loss 0.0588704, acc 0.96875
2017-03-02T18:12:52.422762: step 31203, loss 0.113717, acc 0.953125
2017-03-02T18:12:52.495381: step 31204, loss 0.0884709, acc 0.953125
2017-03-02T18:12:52.567417: step 31205, loss 0.0995569, acc 0.96875
2017-03-02T18:12:52.639410: step 31206, loss 0.117073, acc 0.9375
2017-03-02T18:12:52.711215: step 31207, loss 0.0822221, acc 0.9375
2017-03-02T18:12:52.785034: step 31208, loss 0.140445, acc 0.921875
2017-03-02T18:12:52.856007: step 31209, loss 0.144492, acc 0.9375
2017-03-02T18:12:52.925719: step 31210, loss 0.100821, acc 0.953125
2017-03-02T18:12:52.995670: step 31211, loss 0.0969361, acc 0.96875
2017-03-02T18:12:53.067935: step 31212, loss 0.125285, acc 0.921875
2017-03-02T18:12:53.143192: step 31213, loss 0.0926995, acc 0.96875
2017-03-02T18:12:53.214702: step 31214, loss 0.18685, acc 0.921875
2017-03-02T18:12:53.292541: step 31215, loss 0.147728, acc 0.90625
2017-03-02T18:12:53.369168: step 31216, loss 0.0931871, acc 0.953125
2017-03-02T18:12:53.448474: step 31217, loss 0.188641, acc 0.890625
2017-03-02T18:12:53.525780: step 31218, loss 0.15448, acc 0.9375
2017-03-02T18:12:53.594675: step 31219, loss 0.178791, acc 0.90625
2017-03-02T18:12:53.661890: step 31220, loss 0.0999013, acc 0.96875
2017-03-02T18:12:53.731510: step 31221, loss 0.0491622, acc 0.96875
2017-03-02T18:12:53.803277: step 31222, loss 0.124401, acc 0.921875
2017-03-02T18:12:53.873949: step 31223, loss 0.0785569, acc 0.96875
2017-03-02T18:12:53.945294: step 31224, loss 0.247522, acc 0.890625
2017-03-02T18:12:54.021495: step 31225, loss 0.153378, acc 0.890625
2017-03-02T18:12:54.094557: step 31226, loss 0.169338, acc 0.921875
2017-03-02T18:12:54.170690: step 31227, loss 0.078527, acc 0.984375
2017-03-02T18:12:54.247753: step 31228, loss 0.094428, acc 0.96875
2017-03-02T18:12:54.330711: step 31229, loss 0.17985, acc 0.9375
2017-03-02T18:12:54.398068: step 31230, loss 0.117174, acc 0.953125
2017-03-02T18:12:54.479724: step 31231, loss 0.161263, acc 0.921875
2017-03-02T18:12:54.547047: step 31232, loss 0.12021, acc 0.953125
2017-03-02T18:12:54.617898: step 31233, loss 0.0793616, acc 0.96875
2017-03-02T18:12:54.698978: step 31234, loss 0.188877, acc 0.90625
2017-03-02T18:12:54.773741: step 31235, loss 0.210948, acc 0.875
2017-03-02T18:12:54.848505: step 31236, loss 0.0638023, acc 0.96875
2017-03-02T18:12:54.922895: step 31237, loss 0.108139, acc 0.9375
2017-03-02T18:12:54.990207: step 31238, loss 0.27835, acc 0.84375
2017-03-02T18:12:55.061263: step 31239, loss 0.0797899, acc 0.953125
2017-03-02T18:12:55.133866: step 31240, loss 0.0505444, acc 0.96875
2017-03-02T18:12:55.205368: step 31241, loss 0.0796253, acc 0.96875
2017-03-02T18:12:55.272716: step 31242, loss 0.116904, acc 0.9375
2017-03-02T18:12:55.348458: step 31243, loss 0.124865, acc 0.921875
2017-03-02T18:12:55.419496: step 31244, loss 0.26637, acc 0.890625
2017-03-02T18:12:55.493861: step 31245, loss 0.110484, acc 0.96875
2017-03-02T18:12:55.566962: step 31246, loss 0.127661, acc 0.921875
2017-03-02T18:12:55.640446: step 31247, loss 0.0415309, acc 0.984375
2017-03-02T18:12:55.709046: step 31248, loss 0.200722, acc 0.890625
2017-03-02T18:12:55.784719: step 31249, loss 0.111766, acc 0.96875
2017-03-02T18:12:55.856766: step 31250, loss 0.117358, acc 0.9375
2017-03-02T18:12:55.926298: step 31251, loss 0.212163, acc 0.9375
2017-03-02T18:12:55.995021: step 31252, loss 0.20146, acc 0.921875
2017-03-02T18:12:56.069020: step 31253, loss 0.17487, acc 0.90625
2017-03-02T18:12:56.140699: step 31254, loss 0.0737407, acc 0.984375
2017-03-02T18:12:56.220932: step 31255, loss 0.100914, acc 0.953125
2017-03-02T18:12:56.295116: step 31256, loss 0.106776, acc 0.953125
2017-03-02T18:12:56.362712: step 31257, loss 0.0624345, acc 1
2017-03-02T18:12:56.428730: step 31258, loss 0.196561, acc 0.890625
2017-03-02T18:12:56.502787: step 31259, loss 0.174281, acc 0.921875
2017-03-02T18:12:56.577792: step 31260, loss 0.139411, acc 0.9375
2017-03-02T18:12:56.646017: step 31261, loss 0.2479, acc 0.890625
2017-03-02T18:12:56.720666: step 31262, loss 0.0710612, acc 0.96875
2017-03-02T18:12:56.793584: step 31263, loss 0.158816, acc 0.953125
2017-03-02T18:12:56.878025: step 31264, loss 0.271931, acc 0.84375
2017-03-02T18:12:56.958754: step 31265, loss 0.237389, acc 0.90625
2017-03-02T18:12:57.035774: step 31266, loss 0.179712, acc 0.90625
2017-03-02T18:12:57.103764: step 31267, loss 0.078887, acc 0.96875
2017-03-02T18:12:57.174091: step 31268, loss 0.151791, acc 0.921875
2017-03-02T18:12:57.248229: step 31269, loss 0.067949, acc 0.984375
2017-03-02T18:12:57.326600: step 31270, loss 0.249446, acc 0.859375
2017-03-02T18:12:57.400883: step 31271, loss 0.0793714, acc 0.96875
2017-03-02T18:12:57.465348: step 31272, loss 0.0916151, acc 0.9375
2017-03-02T18:12:57.534536: step 31273, loss 0.139264, acc 0.953125
2017-03-02T18:12:57.612926: step 31274, loss 0.239426, acc 0.921875
2017-03-02T18:12:57.686974: step 31275, loss 0.109912, acc 0.953125
2017-03-02T18:12:57.769393: step 31276, loss 0.109139, acc 0.96875
2017-03-02T18:12:57.840671: step 31277, loss 0.080983, acc 0.9375
2017-03-02T18:12:57.926123: step 31278, loss 0.27176, acc 0.921875
2017-03-02T18:12:57.998539: step 31279, loss 0.160263, acc 0.921875
2017-03-02T18:12:58.072469: step 31280, loss 0.0734156, acc 0.96875
2017-03-02T18:12:58.143323: step 31281, loss 0.216928, acc 0.921875
2017-03-02T18:12:58.223168: step 31282, loss 0.0527234, acc 1
2017-03-02T18:12:58.297589: step 31283, loss 0.113655, acc 0.96875
2017-03-02T18:12:58.371537: step 31284, loss 0.19549, acc 0.90625
2017-03-02T18:12:58.442395: step 31285, loss 0.108127, acc 0.9375
2017-03-02T18:12:58.514943: step 31286, loss 0.189266, acc 0.921875
2017-03-02T18:12:58.588542: step 31287, loss 0.140714, acc 0.9375
2017-03-02T18:12:58.662147: step 31288, loss 0.142472, acc 0.921875
2017-03-02T18:12:58.731796: step 31289, loss 0.180074, acc 0.90625
2017-03-02T18:12:58.804031: step 31290, loss 0.229316, acc 0.875
2017-03-02T18:12:58.878627: step 31291, loss 0.218636, acc 0.890625
2017-03-02T18:12:58.956071: step 31292, loss 0.146612, acc 0.953125
2017-03-02T18:12:59.028649: step 31293, loss 0.123148, acc 0.953125
2017-03-02T18:12:59.098351: step 31294, loss 0.202694, acc 0.890625
2017-03-02T18:12:59.171362: step 31295, loss 0.103242, acc 0.90625
2017-03-02T18:12:59.238903: step 31296, loss 0.147054, acc 0.90625
2017-03-02T18:12:59.315441: step 31297, loss 0.113045, acc 0.953125
2017-03-02T18:12:59.389311: step 31298, loss 0.0952847, acc 0.953125
2017-03-02T18:12:59.463325: step 31299, loss 0.0875993, acc 0.953125
2017-03-02T18:12:59.549010: step 31300, loss 0.125024, acc 0.953125

Evaluation:
2017-03-02T18:12:59.583177: step 31300, loss 4.07723, acc 0.640952

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31300

2017-03-02T18:13:00.066967: step 31301, loss 0.247537, acc 0.921875
2017-03-02T18:13:00.146935: step 31302, loss 0.152229, acc 0.921875
2017-03-02T18:13:00.222783: step 31303, loss 0.194243, acc 0.921875
2017-03-02T18:13:00.304869: step 31304, loss 0.136189, acc 0.90625
2017-03-02T18:13:00.379917: step 31305, loss 0.105507, acc 0.96875
2017-03-02T18:13:00.455137: step 31306, loss 0.15427, acc 0.9375
2017-03-02T18:13:00.522715: step 31307, loss 0.120991, acc 0.953125
2017-03-02T18:13:00.594055: step 31308, loss 0.105971, acc 0.9375
2017-03-02T18:13:00.666276: step 31309, loss 0.238503, acc 0.90625
2017-03-02T18:13:00.747795: step 31310, loss 0.0991117, acc 0.984375
2017-03-02T18:13:00.828577: step 31311, loss 0.217006, acc 0.953125
2017-03-02T18:13:00.909681: step 31312, loss 0.105578, acc 0.9375
2017-03-02T18:13:00.977181: step 31313, loss 0.0968184, acc 0.921875
2017-03-02T18:13:01.049729: step 31314, loss 0.090189, acc 0.96875
2017-03-02T18:13:01.120316: step 31315, loss 0.110981, acc 0.9375
2017-03-02T18:13:01.194553: step 31316, loss 0.129244, acc 0.96875
2017-03-02T18:13:01.274172: step 31317, loss 0.303811, acc 0.859375
2017-03-02T18:13:01.349129: step 31318, loss 0.0522147, acc 0.96875
2017-03-02T18:13:01.429133: step 31319, loss 0.0802947, acc 0.96875
2017-03-02T18:13:01.505960: step 31320, loss 0.212371, acc 0.890625
2017-03-02T18:13:01.576382: step 31321, loss 0.0725207, acc 0.984375
2017-03-02T18:13:01.647249: step 31322, loss 0.0907603, acc 0.96875
2017-03-02T18:13:01.719366: step 31323, loss 0.120657, acc 0.953125
2017-03-02T18:13:01.799416: step 31324, loss 0.0804977, acc 0.96875
2017-03-02T18:13:01.865661: step 31325, loss 0.216118, acc 0.875
2017-03-02T18:13:01.939018: step 31326, loss 0.247491, acc 0.890625
2017-03-02T18:13:02.013500: step 31327, loss 0.141488, acc 0.921875
2017-03-02T18:13:02.082603: step 31328, loss 0.17124, acc 0.90625
2017-03-02T18:13:02.152796: step 31329, loss 0.169989, acc 0.953125
2017-03-02T18:13:02.223154: step 31330, loss 0.184269, acc 0.90625
2017-03-02T18:13:02.296871: step 31331, loss 0.0726465, acc 0.96875
2017-03-02T18:13:02.372023: step 31332, loss 0.116302, acc 0.9375
2017-03-02T18:13:02.443444: step 31333, loss 0.0982073, acc 0.9375
2017-03-02T18:13:02.508378: step 31334, loss 0.130579, acc 0.9375
2017-03-02T18:13:02.579770: step 31335, loss 0.227364, acc 0.890625
2017-03-02T18:13:02.646074: step 31336, loss 0.303278, acc 0.890625
2017-03-02T18:13:02.718504: step 31337, loss 0.0772605, acc 0.953125
2017-03-02T18:13:02.787651: step 31338, loss 0.191629, acc 0.875
2017-03-02T18:13:02.862775: step 31339, loss 0.170241, acc 0.9375
2017-03-02T18:13:02.936599: step 31340, loss 0.273233, acc 0.90625
2017-03-02T18:13:03.014119: step 31341, loss 0.242839, acc 0.859375
2017-03-02T18:13:03.090672: step 31342, loss 0.0739938, acc 0.96875
2017-03-02T18:13:03.164611: step 31343, loss 0.133711, acc 0.921875
2017-03-02T18:13:03.231641: step 31344, loss 0.129729, acc 0.953125
2017-03-02T18:13:03.298017: step 31345, loss 0.165347, acc 0.921875
2017-03-02T18:13:03.373709: step 31346, loss 0.0875757, acc 0.96875
2017-03-02T18:13:03.449300: step 31347, loss 0.0955848, acc 0.984375
2017-03-02T18:13:03.521057: step 31348, loss 0.101686, acc 0.921875
2017-03-02T18:13:03.593236: step 31349, loss 0.16013, acc 0.953125
2017-03-02T18:13:03.668805: step 31350, loss 0.174602, acc 0.921875
2017-03-02T18:13:03.737992: step 31351, loss 0.178756, acc 0.875
2017-03-02T18:13:03.808638: step 31352, loss 0.0734066, acc 0.953125
2017-03-02T18:13:03.886824: step 31353, loss 0.0726861, acc 0.96875
2017-03-02T18:13:03.954354: step 31354, loss 0.202471, acc 0.921875
2017-03-02T18:13:04.033562: step 31355, loss 0.288348, acc 0.890625
2017-03-02T18:13:04.105406: step 31356, loss 0.153414, acc 0.921875
2017-03-02T18:13:04.179337: step 31357, loss 0.200903, acc 0.921875
2017-03-02T18:13:04.250402: step 31358, loss 0.0373972, acc 1
2017-03-02T18:13:04.322872: step 31359, loss 0.0758457, acc 0.9375
2017-03-02T18:13:04.394805: step 31360, loss 2.98023e-08, acc 1
2017-03-02T18:13:04.468620: step 31361, loss 0.137401, acc 0.9375
2017-03-02T18:13:04.543371: step 31362, loss 0.192273, acc 0.90625
2017-03-02T18:13:04.613350: step 31363, loss 0.113459, acc 0.9375
2017-03-02T18:13:04.681529: step 31364, loss 0.0374535, acc 0.984375
2017-03-02T18:13:04.751061: step 31365, loss 0.135817, acc 0.953125
2017-03-02T18:13:04.827764: step 31366, loss 0.193943, acc 0.890625
2017-03-02T18:13:04.900207: step 31367, loss 0.147676, acc 0.921875
2017-03-02T18:13:04.973216: step 31368, loss 0.0937811, acc 0.953125
2017-03-02T18:13:05.049873: step 31369, loss 0.214605, acc 0.90625
2017-03-02T18:13:05.126803: step 31370, loss 0.131707, acc 0.953125
2017-03-02T18:13:05.194821: step 31371, loss 0.107251, acc 0.953125
2017-03-02T18:13:05.266205: step 31372, loss 0.101339, acc 1
2017-03-02T18:13:05.334506: step 31373, loss 0.181245, acc 0.953125
2017-03-02T18:13:05.403192: step 31374, loss 0.183329, acc 0.921875
2017-03-02T18:13:05.477116: step 31375, loss 0.252533, acc 0.953125
2017-03-02T18:13:05.557129: step 31376, loss 0.174933, acc 0.921875
2017-03-02T18:13:05.643008: step 31377, loss 0.0782858, acc 0.96875
2017-03-02T18:13:05.716702: step 31378, loss 0.0954217, acc 0.96875
2017-03-02T18:13:05.798250: step 31379, loss 0.111372, acc 0.96875
2017-03-02T18:13:05.871604: step 31380, loss 0.0750386, acc 0.96875
2017-03-02T18:13:05.950216: step 31381, loss 0.0635016, acc 0.984375
2017-03-02T18:13:06.013446: step 31382, loss 0.0761465, acc 0.953125
2017-03-02T18:13:06.080483: step 31383, loss 0.212828, acc 0.890625
2017-03-02T18:13:06.160950: step 31384, loss 0.218769, acc 0.890625
2017-03-02T18:13:06.235310: step 31385, loss 0.113655, acc 0.9375
2017-03-02T18:13:06.307134: step 31386, loss 0.125593, acc 0.9375
2017-03-02T18:13:06.375685: step 31387, loss 0.123206, acc 0.9375
2017-03-02T18:13:06.450631: step 31388, loss 0.209773, acc 0.921875
2017-03-02T18:13:06.523626: step 31389, loss 0.170585, acc 0.90625
2017-03-02T18:13:06.594808: step 31390, loss 0.246321, acc 0.90625
2017-03-02T18:13:06.667553: step 31391, loss 0.168363, acc 0.953125
2017-03-02T18:13:06.736627: step 31392, loss 0.181703, acc 0.921875
2017-03-02T18:13:06.806636: step 31393, loss 0.164163, acc 0.9375
2017-03-02T18:13:06.879652: step 31394, loss 0.124733, acc 0.953125
2017-03-02T18:13:06.951833: step 31395, loss 0.140605, acc 0.921875
2017-03-02T18:13:07.026097: step 31396, loss 0.124594, acc 0.9375
2017-03-02T18:13:07.101269: step 31397, loss 0.301997, acc 0.84375
2017-03-02T18:13:07.170571: step 31398, loss 0.202058, acc 0.875
2017-03-02T18:13:07.241398: step 31399, loss 0.142276, acc 0.921875
2017-03-02T18:13:07.326404: step 31400, loss 0.109332, acc 0.953125

Evaluation:
2017-03-02T18:13:07.357028: step 31400, loss 4.06656, acc 0.630858

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31400

2017-03-02T18:13:07.844328: step 31401, loss 0.12842, acc 0.953125
2017-03-02T18:13:07.914173: step 31402, loss 0.209344, acc 0.875
2017-03-02T18:13:07.989117: step 31403, loss 0.211339, acc 0.875
2017-03-02T18:13:08.063852: step 31404, loss 0.119178, acc 0.953125
2017-03-02T18:13:08.131256: step 31405, loss 0.139688, acc 0.9375
2017-03-02T18:13:08.218609: step 31406, loss 0.205289, acc 0.921875
2017-03-02T18:13:08.299511: step 31407, loss 0.107081, acc 0.953125
2017-03-02T18:13:08.371149: step 31408, loss 0.181939, acc 0.921875
2017-03-02T18:13:08.441763: step 31409, loss 0.199003, acc 0.921875
2017-03-02T18:13:08.516145: step 31410, loss 0.0962021, acc 0.953125
2017-03-02T18:13:08.581796: step 31411, loss 0.228326, acc 0.921875
2017-03-02T18:13:08.661701: step 31412, loss 0.144704, acc 0.9375
2017-03-02T18:13:08.736929: step 31413, loss 0.253481, acc 0.875
2017-03-02T18:13:08.808372: step 31414, loss 0.0587489, acc 1
2017-03-02T18:13:08.887710: step 31415, loss 0.176756, acc 0.921875
2017-03-02T18:13:08.957945: step 31416, loss 0.149031, acc 0.90625
2017-03-02T18:13:09.030143: step 31417, loss 0.123417, acc 0.9375
2017-03-02T18:13:09.104228: step 31418, loss 0.0686828, acc 0.96875
2017-03-02T18:13:09.179800: step 31419, loss 0.186652, acc 0.921875
2017-03-02T18:13:09.257176: step 31420, loss 0.262562, acc 0.890625
2017-03-02T18:13:09.329971: step 31421, loss 0.162163, acc 0.90625
2017-03-02T18:13:09.401109: step 31422, loss 0.135911, acc 0.953125
2017-03-02T18:13:09.473428: step 31423, loss 0.158229, acc 0.984375
2017-03-02T18:13:09.543765: step 31424, loss 0.113942, acc 0.953125
2017-03-02T18:13:09.615233: step 31425, loss 0.135694, acc 0.953125
2017-03-02T18:13:09.690253: step 31426, loss 0.246224, acc 0.859375
2017-03-02T18:13:09.779674: step 31427, loss 0.105148, acc 0.9375
2017-03-02T18:13:09.859841: step 31428, loss 0.0758828, acc 0.96875
2017-03-02T18:13:09.931994: step 31429, loss 0.178399, acc 0.9375
2017-03-02T18:13:10.005796: step 31430, loss 0.13989, acc 0.921875
2017-03-02T18:13:10.082477: step 31431, loss 0.115791, acc 0.921875
2017-03-02T18:13:10.162151: step 31432, loss 0.132193, acc 0.921875
2017-03-02T18:13:10.229845: step 31433, loss 0.144247, acc 0.921875
2017-03-02T18:13:10.294939: step 31434, loss 0.139239, acc 0.9375
2017-03-02T18:13:10.386239: step 31435, loss 0.0887951, acc 0.96875
2017-03-02T18:13:10.458819: step 31436, loss 0.17762, acc 0.90625
2017-03-02T18:13:10.535083: step 31437, loss 0.063486, acc 0.96875
2017-03-02T18:13:10.608939: step 31438, loss 0.0648309, acc 0.96875
2017-03-02T18:13:10.676118: step 31439, loss 0.04427, acc 0.96875
2017-03-02T18:13:10.746176: step 31440, loss 0.160009, acc 0.890625
2017-03-02T18:13:10.818933: step 31441, loss 0.193535, acc 0.9375
2017-03-02T18:13:10.888354: step 31442, loss 0.155175, acc 0.890625
2017-03-02T18:13:10.961934: step 31443, loss 0.173712, acc 0.90625
2017-03-02T18:13:11.032096: step 31444, loss 0.107099, acc 0.921875
2017-03-02T18:13:11.108498: step 31445, loss 0.166768, acc 0.921875
2017-03-02T18:13:11.179512: step 31446, loss 0.118155, acc 0.9375
2017-03-02T18:13:11.258397: step 31447, loss 0.201267, acc 0.921875
2017-03-02T18:13:11.335082: step 31448, loss 0.142975, acc 0.96875
2017-03-02T18:13:11.406128: step 31449, loss 0.15712, acc 0.90625
2017-03-02T18:13:11.481481: step 31450, loss 0.0744988, acc 0.984375
2017-03-02T18:13:11.559122: step 31451, loss 0.152761, acc 0.953125
2017-03-02T18:13:11.633040: step 31452, loss 0.204596, acc 0.90625
2017-03-02T18:13:11.700740: step 31453, loss 0.190912, acc 0.890625
2017-03-02T18:13:11.786124: step 31454, loss 0.160515, acc 0.9375
2017-03-02T18:13:11.857316: step 31455, loss 0.144986, acc 0.9375
2017-03-02T18:13:11.930243: step 31456, loss 0.145714, acc 0.953125
2017-03-02T18:13:12.001715: step 31457, loss 0.113326, acc 0.9375
2017-03-02T18:13:12.076472: step 31458, loss 0.101568, acc 0.9375
2017-03-02T18:13:12.148174: step 31459, loss 0.244579, acc 0.890625
2017-03-02T18:13:12.222775: step 31460, loss 0.137882, acc 0.921875
2017-03-02T18:13:12.293188: step 31461, loss 0.217003, acc 0.890625
2017-03-02T18:13:12.362817: step 31462, loss 0.111582, acc 0.9375
2017-03-02T18:13:12.436111: step 31463, loss 0.203977, acc 0.890625
2017-03-02T18:13:12.509667: step 31464, loss 0.227529, acc 0.84375
2017-03-02T18:13:12.580719: step 31465, loss 0.205307, acc 0.921875
2017-03-02T18:13:12.660176: step 31466, loss 0.099237, acc 0.953125
2017-03-02T18:13:12.732377: step 31467, loss 0.206752, acc 0.90625
2017-03-02T18:13:12.814859: step 31468, loss 0.0620734, acc 1
2017-03-02T18:13:12.895482: step 31469, loss 0.129487, acc 0.921875
2017-03-02T18:13:12.964902: step 31470, loss 0.0893632, acc 0.953125
2017-03-02T18:13:13.041492: step 31471, loss 0.111858, acc 0.9375
2017-03-02T18:13:13.108865: step 31472, loss 0.170638, acc 0.890625
2017-03-02T18:13:13.183030: step 31473, loss 0.177797, acc 0.9375
2017-03-02T18:13:13.259362: step 31474, loss 0.122203, acc 0.921875
2017-03-02T18:13:13.328213: step 31475, loss 0.150293, acc 0.9375
2017-03-02T18:13:13.402595: step 31476, loss 0.153458, acc 0.953125
2017-03-02T18:13:13.486335: step 31477, loss 0.0724987, acc 0.96875
2017-03-02T18:13:13.564394: step 31478, loss 0.107145, acc 0.953125
2017-03-02T18:13:13.640635: step 31479, loss 0.132568, acc 0.9375
2017-03-02T18:13:13.706456: step 31480, loss 0.164471, acc 0.9375
2017-03-02T18:13:13.775639: step 31481, loss 0.0737925, acc 0.96875
2017-03-02T18:13:13.849912: step 31482, loss 0.103393, acc 0.953125
2017-03-02T18:13:13.924455: step 31483, loss 0.135236, acc 0.953125
2017-03-02T18:13:14.008520: step 31484, loss 0.193962, acc 0.90625
2017-03-02T18:13:14.080667: step 31485, loss 0.13877, acc 0.921875
2017-03-02T18:13:14.156663: step 31486, loss 0.118445, acc 0.96875
2017-03-02T18:13:14.227074: step 31487, loss 0.137843, acc 0.9375
2017-03-02T18:13:14.299981: step 31488, loss 0.109726, acc 0.953125
2017-03-02T18:13:14.365141: step 31489, loss 0.125993, acc 0.96875
2017-03-02T18:13:14.434167: step 31490, loss 0.0862434, acc 0.953125
2017-03-02T18:13:14.508229: step 31491, loss 0.213071, acc 0.921875
2017-03-02T18:13:14.581041: step 31492, loss 0.161878, acc 0.921875
2017-03-02T18:13:14.655412: step 31493, loss 0.173271, acc 0.921875
2017-03-02T18:13:14.737749: step 31494, loss 0.176413, acc 0.90625
2017-03-02T18:13:14.810774: step 31495, loss 0.0783127, acc 0.984375
2017-03-02T18:13:14.881575: step 31496, loss 0.150789, acc 0.90625
2017-03-02T18:13:14.956522: step 31497, loss 0.231766, acc 0.890625
2017-03-02T18:13:15.017970: step 31498, loss 0.130073, acc 0.9375
2017-03-02T18:13:15.095511: step 31499, loss 0.131847, acc 0.953125
2017-03-02T18:13:15.166699: step 31500, loss 0.100643, acc 0.953125

Evaluation:
2017-03-02T18:13:15.201722: step 31500, loss 4.15451, acc 0.652487

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31500

2017-03-02T18:13:15.670410: step 31501, loss 0.105172, acc 0.984375
2017-03-02T18:13:15.739877: step 31502, loss 0.109679, acc 0.953125
2017-03-02T18:13:15.802989: step 31503, loss 0.170279, acc 0.953125
2017-03-02T18:13:15.873101: step 31504, loss 0.127207, acc 0.921875
2017-03-02T18:13:15.955693: step 31505, loss 0.174223, acc 0.90625
2017-03-02T18:13:16.029323: step 31506, loss 0.148422, acc 0.921875
2017-03-02T18:13:16.104699: step 31507, loss 0.0480676, acc 0.984375
2017-03-02T18:13:16.179684: step 31508, loss 0.0988731, acc 0.96875
2017-03-02T18:13:16.252799: step 31509, loss 0.145093, acc 0.90625
2017-03-02T18:13:16.327952: step 31510, loss 0.19055, acc 0.890625
2017-03-02T18:13:16.400700: step 31511, loss 0.107101, acc 0.9375
2017-03-02T18:13:16.472735: step 31512, loss 0.104797, acc 0.9375
2017-03-02T18:13:16.541233: step 31513, loss 0.0974741, acc 0.9375
2017-03-02T18:13:16.613036: step 31514, loss 0.108619, acc 0.921875
2017-03-02T18:13:16.684153: step 31515, loss 0.18529, acc 0.921875
2017-03-02T18:13:16.760338: step 31516, loss 0.0965581, acc 0.96875
2017-03-02T18:13:16.839832: step 31517, loss 0.236467, acc 0.921875
2017-03-02T18:13:16.914832: step 31518, loss 0.0623351, acc 0.96875
2017-03-02T18:13:16.987032: step 31519, loss 0.108156, acc 0.9375
2017-03-02T18:13:17.049517: step 31520, loss 0.0912345, acc 0.953125
2017-03-02T18:13:17.118087: step 31521, loss 0.125938, acc 0.921875
2017-03-02T18:13:17.190090: step 31522, loss 0.118974, acc 0.9375
2017-03-02T18:13:17.260456: step 31523, loss 0.20723, acc 0.90625
2017-03-02T18:13:17.332330: step 31524, loss 0.168781, acc 0.890625
2017-03-02T18:13:17.394703: step 31525, loss 0.0873378, acc 0.96875
2017-03-02T18:13:17.463548: step 31526, loss 0.0543316, acc 0.984375
2017-03-02T18:13:17.534162: step 31527, loss 0.364801, acc 0.890625
2017-03-02T18:13:17.606869: step 31528, loss 0.132613, acc 0.96875
2017-03-02T18:13:17.683373: step 31529, loss 0.14268, acc 0.921875
2017-03-02T18:13:17.758113: step 31530, loss 0.0764023, acc 0.96875
2017-03-02T18:13:17.831153: step 31531, loss 0.230943, acc 0.90625
2017-03-02T18:13:17.900974: step 31532, loss 0.106707, acc 0.96875
2017-03-02T18:13:17.973489: step 31533, loss 0.132444, acc 0.90625
2017-03-02T18:13:18.049076: step 31534, loss 0.135382, acc 0.921875
2017-03-02T18:13:18.121889: step 31535, loss 0.193647, acc 0.890625
2017-03-02T18:13:18.199579: step 31536, loss 0.100874, acc 0.96875
2017-03-02T18:13:18.285974: step 31537, loss 0.159931, acc 0.90625
2017-03-02T18:13:18.356037: step 31538, loss 0.0773673, acc 0.96875
2017-03-02T18:13:18.418891: step 31539, loss 0.109789, acc 0.921875
2017-03-02T18:13:18.494048: step 31540, loss 0.209494, acc 0.921875
2017-03-02T18:13:18.566453: step 31541, loss 0.179766, acc 0.921875
2017-03-02T18:13:18.643064: step 31542, loss 0.0792958, acc 0.96875
2017-03-02T18:13:18.717096: step 31543, loss 0.173012, acc 0.921875
2017-03-02T18:13:18.793035: step 31544, loss 0.118144, acc 0.953125
2017-03-02T18:13:18.867084: step 31545, loss 0.0705341, acc 0.953125
2017-03-02T18:13:18.927925: step 31546, loss 0.0891381, acc 0.96875
2017-03-02T18:13:19.001169: step 31547, loss 0.259575, acc 0.90625
2017-03-02T18:13:19.071570: step 31548, loss 0.0645736, acc 0.984375
2017-03-02T18:13:19.143556: step 31549, loss 0.114706, acc 0.953125
2017-03-02T18:13:19.217073: step 31550, loss 0.0458828, acc 0.96875
2017-03-02T18:13:19.292736: step 31551, loss 0.178503, acc 0.921875
2017-03-02T18:13:19.356078: step 31552, loss 0.229824, acc 0.890625
2017-03-02T18:13:19.423686: step 31553, loss 0.152211, acc 0.96875
2017-03-02T18:13:19.497123: step 31554, loss 0.0906677, acc 0.953125
2017-03-02T18:13:19.567709: step 31555, loss 0.0647183, acc 0.984375
2017-03-02T18:13:19.638282: step 31556, loss 0, acc 1
2017-03-02T18:13:19.712839: step 31557, loss 0.144643, acc 0.9375
2017-03-02T18:13:19.779695: step 31558, loss 0.22498, acc 0.890625
2017-03-02T18:13:19.852007: step 31559, loss 0.0635572, acc 0.984375
2017-03-02T18:13:19.924802: step 31560, loss 0.109673, acc 0.96875
2017-03-02T18:13:19.992930: step 31561, loss 0.0735218, acc 1
2017-03-02T18:13:20.063592: step 31562, loss 0.18219, acc 0.9375
2017-03-02T18:13:20.156325: step 31563, loss 0.162302, acc 0.90625
2017-03-02T18:13:20.234003: step 31564, loss 0.13758, acc 0.921875
2017-03-02T18:13:20.310895: step 31565, loss 0.195035, acc 0.90625
2017-03-02T18:13:20.398046: step 31566, loss 0.0572689, acc 0.984375
2017-03-02T18:13:20.478116: step 31567, loss 0.13293, acc 0.953125
2017-03-02T18:13:20.548801: step 31568, loss 0.0627735, acc 0.96875
2017-03-02T18:13:20.620181: step 31569, loss 0.107205, acc 0.9375
2017-03-02T18:13:20.689399: step 31570, loss 0.222269, acc 0.875
2017-03-02T18:13:20.763975: step 31571, loss 0.091247, acc 0.984375
2017-03-02T18:13:20.830577: step 31572, loss 0.0895605, acc 0.9375
2017-03-02T18:13:20.909683: step 31573, loss 0.147021, acc 0.9375
2017-03-02T18:13:20.981563: step 31574, loss 0.221399, acc 0.90625
2017-03-02T18:13:21.054836: step 31575, loss 0.0730226, acc 0.96875
2017-03-02T18:13:21.122775: step 31576, loss 0.226338, acc 0.890625
2017-03-02T18:13:21.195733: step 31577, loss 0.18128, acc 0.921875
2017-03-02T18:13:21.266403: step 31578, loss 0.238619, acc 0.890625
2017-03-02T18:13:21.328036: step 31579, loss 0.0928177, acc 0.9375
2017-03-02T18:13:21.397761: step 31580, loss 0.133167, acc 0.90625
2017-03-02T18:13:21.472251: step 31581, loss 0.128797, acc 0.96875
2017-03-02T18:13:21.543768: step 31582, loss 0.0435008, acc 0.96875
2017-03-02T18:13:21.616464: step 31583, loss 0.107512, acc 0.96875
2017-03-02T18:13:21.691353: step 31584, loss 0.279224, acc 0.90625
2017-03-02T18:13:21.767881: step 31585, loss 0.164325, acc 0.9375
2017-03-02T18:13:21.840388: step 31586, loss 0.0615029, acc 0.984375
2017-03-02T18:13:21.914567: step 31587, loss 0.115489, acc 0.9375
2017-03-02T18:13:21.985112: step 31588, loss 0.147093, acc 0.9375
2017-03-02T18:13:22.065786: step 31589, loss 0.174828, acc 0.9375
2017-03-02T18:13:22.134409: step 31590, loss 0.151208, acc 0.921875
2017-03-02T18:13:22.199196: step 31591, loss 0.102943, acc 0.953125
2017-03-02T18:13:22.273782: step 31592, loss 0.17232, acc 0.921875
2017-03-02T18:13:22.347277: step 31593, loss 0.133239, acc 0.90625
2017-03-02T18:13:22.420007: step 31594, loss 0.0801156, acc 0.984375
2017-03-02T18:13:22.493381: step 31595, loss 0.180575, acc 0.921875
2017-03-02T18:13:22.566287: step 31596, loss 0.12723, acc 0.921875
2017-03-02T18:13:22.641818: step 31597, loss 0.0412392, acc 0.984375
2017-03-02T18:13:22.714345: step 31598, loss 0.0896242, acc 0.953125
2017-03-02T18:13:22.781849: step 31599, loss 0.111324, acc 0.9375
2017-03-02T18:13:22.854450: step 31600, loss 0.101831, acc 0.96875

Evaluation:
2017-03-02T18:13:22.891642: step 31600, loss 4.13977, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31600

2017-03-02T18:13:23.370217: step 31601, loss 0.0868258, acc 0.9375
2017-03-02T18:13:23.437342: step 31602, loss 0.24217, acc 0.875
2017-03-02T18:13:23.518224: step 31603, loss 0.110417, acc 0.9375
2017-03-02T18:13:23.593179: step 31604, loss 0.214633, acc 0.859375
2017-03-02T18:13:23.676986: step 31605, loss 0.0751692, acc 0.953125
2017-03-02T18:13:23.748805: step 31606, loss 0.282504, acc 0.875
2017-03-02T18:13:23.827753: step 31607, loss 0.154928, acc 0.90625
2017-03-02T18:13:23.896393: step 31608, loss 0.121013, acc 0.921875
2017-03-02T18:13:23.968875: step 31609, loss 0.123365, acc 0.9375
2017-03-02T18:13:24.037551: step 31610, loss 0.102779, acc 0.953125
2017-03-02T18:13:24.109080: step 31611, loss 0.163034, acc 0.921875
2017-03-02T18:13:24.174237: step 31612, loss 0.0891443, acc 0.953125
2017-03-02T18:13:24.242033: step 31613, loss 0.188967, acc 0.90625
2017-03-02T18:13:24.328127: step 31614, loss 0.107178, acc 0.96875
2017-03-02T18:13:24.391860: step 31615, loss 0.177214, acc 0.953125
2017-03-02T18:13:24.460913: step 31616, loss 0.115169, acc 0.9375
2017-03-02T18:13:24.524515: step 31617, loss 0.171027, acc 0.921875
2017-03-02T18:13:24.590383: step 31618, loss 0.139624, acc 0.921875
2017-03-02T18:13:24.662338: step 31619, loss 0.143657, acc 0.96875
2017-03-02T18:13:24.736363: step 31620, loss 0.178075, acc 0.90625
2017-03-02T18:13:24.807601: step 31621, loss 0.0422338, acc 0.984375
2017-03-02T18:13:24.879466: step 31622, loss 0.0790358, acc 0.953125
2017-03-02T18:13:24.955813: step 31623, loss 0.200356, acc 0.90625
2017-03-02T18:13:25.025577: step 31624, loss 0.159115, acc 0.921875
2017-03-02T18:13:25.098083: step 31625, loss 0.0706222, acc 0.96875
2017-03-02T18:13:25.173853: step 31626, loss 0.182351, acc 0.921875
2017-03-02T18:13:25.243298: step 31627, loss 0.161286, acc 0.953125
2017-03-02T18:13:25.312005: step 31628, loss 0.147981, acc 0.9375
2017-03-02T18:13:25.381813: step 31629, loss 0.136334, acc 0.96875
2017-03-02T18:13:25.465462: step 31630, loss 0.220873, acc 0.859375
2017-03-02T18:13:25.538663: step 31631, loss 0.234838, acc 0.859375
2017-03-02T18:13:25.600140: step 31632, loss 0.309879, acc 0.859375
2017-03-02T18:13:25.676695: step 31633, loss 0.0427059, acc 0.984375
2017-03-02T18:13:25.748801: step 31634, loss 0.175132, acc 0.953125
2017-03-02T18:13:25.822611: step 31635, loss 0.120782, acc 0.96875
2017-03-02T18:13:25.902696: step 31636, loss 0.216394, acc 0.90625
2017-03-02T18:13:25.977822: step 31637, loss 0.145772, acc 0.953125
2017-03-02T18:13:26.055099: step 31638, loss 0.115823, acc 0.953125
2017-03-02T18:13:26.125531: step 31639, loss 0.0589071, acc 0.96875
2017-03-02T18:13:26.197737: step 31640, loss 0.0792147, acc 0.953125
2017-03-02T18:13:26.269958: step 31641, loss 0.188737, acc 0.921875
2017-03-02T18:13:26.342956: step 31642, loss 0.174179, acc 0.9375
2017-03-02T18:13:26.418565: step 31643, loss 0.129835, acc 0.921875
2017-03-02T18:13:26.489683: step 31644, loss 0.194351, acc 0.90625
2017-03-02T18:13:26.573349: step 31645, loss 0.15725, acc 0.9375
2017-03-02T18:13:26.645424: step 31646, loss 0.0765032, acc 0.984375
2017-03-02T18:13:26.721221: step 31647, loss 0.164686, acc 0.921875
2017-03-02T18:13:26.798024: step 31648, loss 0.108358, acc 0.953125
2017-03-02T18:13:26.876640: step 31649, loss 0.111479, acc 0.953125
2017-03-02T18:13:26.946952: step 31650, loss 0.220485, acc 0.890625
2017-03-02T18:13:27.016471: step 31651, loss 0.0926442, acc 0.9375
2017-03-02T18:13:27.092612: step 31652, loss 0.16205, acc 0.90625
2017-03-02T18:13:27.167959: step 31653, loss 0.0784992, acc 0.96875
2017-03-02T18:13:27.243812: step 31654, loss 0.177028, acc 0.921875
2017-03-02T18:13:27.318203: step 31655, loss 0.0962315, acc 0.96875
2017-03-02T18:13:27.398015: step 31656, loss 0.064485, acc 0.96875
2017-03-02T18:13:27.471838: step 31657, loss 0.138808, acc 0.9375
2017-03-02T18:13:27.545386: step 31658, loss 0.12804, acc 0.953125
2017-03-02T18:13:27.614090: step 31659, loss 0.143404, acc 0.921875
2017-03-02T18:13:27.679961: step 31660, loss 0.0912519, acc 0.9375
2017-03-02T18:13:27.748936: step 31661, loss 0.0529332, acc 1
2017-03-02T18:13:27.820692: step 31662, loss 0.156996, acc 0.9375
2017-03-02T18:13:27.894514: step 31663, loss 0.0794006, acc 0.984375
2017-03-02T18:13:27.966016: step 31664, loss 0.0941688, acc 0.953125
2017-03-02T18:13:28.035965: step 31665, loss 0.14454, acc 0.90625
2017-03-02T18:13:28.111004: step 31666, loss 0.183957, acc 0.90625
2017-03-02T18:13:28.183366: step 31667, loss 0.145078, acc 0.921875
2017-03-02T18:13:28.266208: step 31668, loss 0.213011, acc 0.90625
2017-03-02T18:13:28.341092: step 31669, loss 0.218825, acc 0.921875
2017-03-02T18:13:28.431952: step 31670, loss 0.222836, acc 0.90625
2017-03-02T18:13:28.503876: step 31671, loss 0.12315, acc 0.953125
2017-03-02T18:13:28.581198: step 31672, loss 0.18387, acc 0.9375
2017-03-02T18:13:28.659254: step 31673, loss 0.117256, acc 0.9375
2017-03-02T18:13:28.736233: step 31674, loss 0.160681, acc 0.9375
2017-03-02T18:13:28.808082: step 31675, loss 0.205914, acc 0.921875
2017-03-02T18:13:28.878656: step 31676, loss 0.0861441, acc 0.96875
2017-03-02T18:13:28.952117: step 31677, loss 0.214468, acc 0.921875
2017-03-02T18:13:29.024036: step 31678, loss 0.156217, acc 0.9375
2017-03-02T18:13:29.095478: step 31679, loss 0.135731, acc 0.9375
2017-03-02T18:13:29.185671: step 31680, loss 0.206959, acc 0.875
2017-03-02T18:13:29.263082: step 31681, loss 0.178809, acc 0.90625
2017-03-02T18:13:29.342028: step 31682, loss 0.166187, acc 0.90625
2017-03-02T18:13:29.415045: step 31683, loss 0.105987, acc 0.96875
2017-03-02T18:13:29.489779: step 31684, loss 0.163784, acc 0.90625
2017-03-02T18:13:29.562307: step 31685, loss 0.117839, acc 0.953125
2017-03-02T18:13:29.635818: step 31686, loss 0.0618109, acc 0.984375
2017-03-02T18:13:29.705664: step 31687, loss 0.278354, acc 0.859375
2017-03-02T18:13:29.772069: step 31688, loss 0.124205, acc 0.96875
2017-03-02T18:13:29.842979: step 31689, loss 0.156001, acc 0.9375
2017-03-02T18:13:29.918341: step 31690, loss 0.178625, acc 0.953125
2017-03-02T18:13:29.992934: step 31691, loss 0.236241, acc 0.890625
2017-03-02T18:13:30.066478: step 31692, loss 0.11272, acc 0.953125
2017-03-02T18:13:30.146065: step 31693, loss 0.0989947, acc 0.953125
2017-03-02T18:13:30.220210: step 31694, loss 0.0980373, acc 0.984375
2017-03-02T18:13:30.295697: step 31695, loss 0.0978701, acc 0.953125
2017-03-02T18:13:30.364067: step 31696, loss 0.0301871, acc 0.984375
2017-03-02T18:13:30.428333: step 31697, loss 0.0961686, acc 0.953125
2017-03-02T18:13:30.494574: step 31698, loss 0.10946, acc 0.9375
2017-03-02T18:13:30.572580: step 31699, loss 0.116134, acc 0.96875
2017-03-02T18:13:30.650153: step 31700, loss 0.204209, acc 0.890625

Evaluation:
2017-03-02T18:13:30.695187: step 31700, loss 4.13553, acc 0.64744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31700

2017-03-02T18:13:31.133180: step 31701, loss 0.148962, acc 0.921875
2017-03-02T18:13:31.225113: step 31702, loss 0.167595, acc 0.90625
2017-03-02T18:13:31.304439: step 31703, loss 0.229617, acc 0.890625
2017-03-02T18:13:31.374770: step 31704, loss 0.103114, acc 0.96875
2017-03-02T18:13:31.448891: step 31705, loss 0.160925, acc 0.9375
2017-03-02T18:13:31.528199: step 31706, loss 0.108464, acc 0.921875
2017-03-02T18:13:31.602154: step 31707, loss 0.207694, acc 0.90625
2017-03-02T18:13:31.671642: step 31708, loss 0.204935, acc 0.921875
2017-03-02T18:13:31.745897: step 31709, loss 0.0755588, acc 0.96875
2017-03-02T18:13:31.817392: step 31710, loss 0.19679, acc 0.890625
2017-03-02T18:13:31.889194: step 31711, loss 0.236238, acc 0.90625
2017-03-02T18:13:31.979130: step 31712, loss 0.138201, acc 0.9375
2017-03-02T18:13:32.046446: step 31713, loss 0.153133, acc 0.90625
2017-03-02T18:13:32.111748: step 31714, loss 0.137814, acc 0.921875
2017-03-02T18:13:32.187720: step 31715, loss 0.179595, acc 0.890625
2017-03-02T18:13:32.260254: step 31716, loss 0.17344, acc 0.921875
2017-03-02T18:13:32.331973: step 31717, loss 0.0981958, acc 0.96875
2017-03-02T18:13:32.413008: step 31718, loss 0.200045, acc 0.890625
2017-03-02T18:13:32.486042: step 31719, loss 0.0785604, acc 0.984375
2017-03-02T18:13:32.559215: step 31720, loss 0.125429, acc 0.921875
2017-03-02T18:13:32.636246: step 31721, loss 0.172417, acc 0.890625
2017-03-02T18:13:32.716318: step 31722, loss 0.296703, acc 0.890625
2017-03-02T18:13:32.809240: step 31723, loss 0.178307, acc 0.90625
2017-03-02T18:13:32.883279: step 31724, loss 0.170638, acc 0.90625
2017-03-02T18:13:32.955718: step 31725, loss 0.139283, acc 0.9375
2017-03-02T18:13:33.029115: step 31726, loss 0.226982, acc 0.921875
2017-03-02T18:13:33.100735: step 31727, loss 0.0894695, acc 0.953125
2017-03-02T18:13:33.171437: step 31728, loss 0.21507, acc 0.875
2017-03-02T18:13:33.244425: step 31729, loss 0.191352, acc 0.921875
2017-03-02T18:13:33.330799: step 31730, loss 0.210907, acc 0.875
2017-03-02T18:13:33.401910: step 31731, loss 0.114688, acc 0.96875
2017-03-02T18:13:33.470217: step 31732, loss 0.14519, acc 0.9375
2017-03-02T18:13:33.543518: step 31733, loss 0.165717, acc 0.90625
2017-03-02T18:13:33.615470: step 31734, loss 0.0628285, acc 0.96875
2017-03-02T18:13:33.688188: step 31735, loss 0.060108, acc 0.96875
2017-03-02T18:13:33.761092: step 31736, loss 0.127005, acc 0.9375
2017-03-02T18:13:33.830217: step 31737, loss 0.222861, acc 0.90625
2017-03-02T18:13:33.894563: step 31738, loss 0.218354, acc 0.90625
2017-03-02T18:13:33.977054: step 31739, loss 0.0886583, acc 0.9375
2017-03-02T18:13:34.048662: step 31740, loss 0.188673, acc 0.890625
2017-03-02T18:13:34.120053: step 31741, loss 0.110727, acc 0.96875
2017-03-02T18:13:34.191061: step 31742, loss 0.118417, acc 0.921875
2017-03-02T18:13:34.261920: step 31743, loss 0.271627, acc 0.890625
2017-03-02T18:13:34.333410: step 31744, loss 0.148163, acc 0.90625
2017-03-02T18:13:34.399983: step 31745, loss 0.18037, acc 0.90625
2017-03-02T18:13:34.476994: step 31746, loss 0.115845, acc 0.953125
2017-03-02T18:13:34.547037: step 31747, loss 0.0881557, acc 0.9375
2017-03-02T18:13:34.619606: step 31748, loss 0.104206, acc 0.9375
2017-03-02T18:13:34.687593: step 31749, loss 0.21868, acc 0.890625
2017-03-02T18:13:34.759165: step 31750, loss 0.0766208, acc 0.96875
2017-03-02T18:13:34.839288: step 31751, loss 0.261191, acc 0.90625
2017-03-02T18:13:34.908776: step 31752, loss 0.428774, acc 0.75
2017-03-02T18:13:34.986289: step 31753, loss 0.169927, acc 0.890625
2017-03-02T18:13:35.059666: step 31754, loss 0.118442, acc 0.9375
2017-03-02T18:13:35.130911: step 31755, loss 0.221303, acc 0.875
2017-03-02T18:13:35.216580: step 31756, loss 0.0841183, acc 0.96875
2017-03-02T18:13:35.277845: step 31757, loss 0.14092, acc 0.921875
2017-03-02T18:13:35.353687: step 31758, loss 0.0797258, acc 0.953125
2017-03-02T18:13:35.436096: step 31759, loss 0.0776719, acc 0.953125
2017-03-02T18:13:35.522726: step 31760, loss 0.218913, acc 0.921875
2017-03-02T18:13:35.599771: step 31761, loss 0.0724183, acc 0.984375
2017-03-02T18:13:35.674564: step 31762, loss 0.110917, acc 0.953125
2017-03-02T18:13:35.750826: step 31763, loss 0.198082, acc 0.890625
2017-03-02T18:13:35.822945: step 31764, loss 0.110815, acc 0.96875
2017-03-02T18:13:35.893536: step 31765, loss 0.129461, acc 0.953125
2017-03-02T18:13:35.965222: step 31766, loss 0.123545, acc 0.984375
2017-03-02T18:13:36.052837: step 31767, loss 0.211344, acc 0.890625
2017-03-02T18:13:36.124487: step 31768, loss 0.134483, acc 0.953125
2017-03-02T18:13:36.198688: step 31769, loss 0.0960193, acc 0.96875
2017-03-02T18:13:36.271385: step 31770, loss 0.138885, acc 0.921875
2017-03-02T18:13:36.347571: step 31771, loss 0.11284, acc 0.9375
2017-03-02T18:13:36.414435: step 31772, loss 0.0766468, acc 0.96875
2017-03-02T18:13:36.484602: step 31773, loss 0.243113, acc 0.875
2017-03-02T18:13:36.561293: step 31774, loss 0.116424, acc 0.953125
2017-03-02T18:13:36.619082: step 31775, loss 0.139106, acc 0.921875
2017-03-02T18:13:36.697377: step 31776, loss 0.0984812, acc 0.953125
2017-03-02T18:13:36.769892: step 31777, loss 0.156659, acc 0.921875
2017-03-02T18:13:36.851275: step 31778, loss 0.362682, acc 0.84375
2017-03-02T18:13:36.918813: step 31779, loss 0.129458, acc 0.9375
2017-03-02T18:13:36.991084: step 31780, loss 0.146593, acc 0.9375
2017-03-02T18:13:37.075426: step 31781, loss 0.101437, acc 0.96875
2017-03-02T18:13:37.165236: step 31782, loss 0.104567, acc 0.96875
2017-03-02T18:13:37.239676: step 31783, loss 0.0938117, acc 0.96875
2017-03-02T18:13:37.314221: step 31784, loss 0.0654792, acc 0.96875
2017-03-02T18:13:37.382007: step 31785, loss 0.117002, acc 0.953125
2017-03-02T18:13:37.454518: step 31786, loss 0.0526277, acc 0.96875
2017-03-02T18:13:37.527689: step 31787, loss 0.185899, acc 0.9375
2017-03-02T18:13:37.601237: step 31788, loss 0.150107, acc 0.921875
2017-03-02T18:13:37.674428: step 31789, loss 0.2056, acc 0.9375
2017-03-02T18:13:37.753805: step 31790, loss 0.213112, acc 0.890625
2017-03-02T18:13:37.830931: step 31791, loss 0.207581, acc 0.90625
2017-03-02T18:13:37.899335: step 31792, loss 0.145364, acc 0.9375
2017-03-02T18:13:37.969400: step 31793, loss 0.138207, acc 0.90625
2017-03-02T18:13:38.036949: step 31794, loss 0.12098, acc 0.953125
2017-03-02T18:13:38.107077: step 31795, loss 0.129831, acc 0.953125
2017-03-02T18:13:38.175042: step 31796, loss 0.115587, acc 0.9375
2017-03-02T18:13:38.251655: step 31797, loss 0.141028, acc 0.90625
2017-03-02T18:13:38.320993: step 31798, loss 0.12009, acc 0.96875
2017-03-02T18:13:38.392359: step 31799, loss 0.0816479, acc 0.953125
2017-03-02T18:13:38.468781: step 31800, loss 0.135309, acc 0.9375

Evaluation:
2017-03-02T18:13:38.503700: step 31800, loss 4.19779, acc 0.64744

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31800

2017-03-02T18:13:38.966909: step 31801, loss 0.0796124, acc 0.96875
2017-03-02T18:13:39.043955: step 31802, loss 0.105545, acc 0.953125
2017-03-02T18:13:39.116856: step 31803, loss 0.0859857, acc 0.953125
2017-03-02T18:13:39.196806: step 31804, loss 0.0551314, acc 0.96875
2017-03-02T18:13:39.273646: step 31805, loss 0.104864, acc 0.953125
2017-03-02T18:13:39.345566: step 31806, loss 0.0766321, acc 0.96875
2017-03-02T18:13:39.412511: step 31807, loss 0.182451, acc 0.9375
2017-03-02T18:13:39.483735: step 31808, loss 0.200048, acc 0.90625
2017-03-02T18:13:39.556858: step 31809, loss 0.149181, acc 0.9375
2017-03-02T18:13:39.630488: step 31810, loss 0.0808801, acc 0.953125
2017-03-02T18:13:39.710690: step 31811, loss 0.0994556, acc 0.9375
2017-03-02T18:13:39.783584: step 31812, loss 0.188662, acc 0.9375
2017-03-02T18:13:39.896019: step 31813, loss 0.175984, acc 0.921875
2017-03-02T18:13:39.972774: step 31814, loss 0.0969366, acc 0.953125
2017-03-02T18:13:40.050339: step 31815, loss 0.211925, acc 0.90625
2017-03-02T18:13:40.133508: step 31816, loss 0.154655, acc 0.90625
2017-03-02T18:13:40.204773: step 31817, loss 0.222912, acc 0.921875
2017-03-02T18:13:40.274188: step 31818, loss 0.108893, acc 0.953125
2017-03-02T18:13:40.350032: step 31819, loss 0.126525, acc 0.921875
2017-03-02T18:13:40.421288: step 31820, loss 0.198912, acc 0.890625
2017-03-02T18:13:40.491972: step 31821, loss 0.153729, acc 0.921875
2017-03-02T18:13:40.563609: step 31822, loss 0.122862, acc 0.953125
2017-03-02T18:13:40.640957: step 31823, loss 0.199899, acc 0.921875
2017-03-02T18:13:40.707490: step 31824, loss 0.0631905, acc 0.984375
2017-03-02T18:13:40.778506: step 31825, loss 0.171718, acc 0.921875
2017-03-02T18:13:40.849405: step 31826, loss 0.141062, acc 0.9375
2017-03-02T18:13:40.934800: step 31827, loss 0.199271, acc 0.890625
2017-03-02T18:13:41.006813: step 31828, loss 0.121104, acc 0.9375
2017-03-02T18:13:41.083239: step 31829, loss 0.18863, acc 0.890625
2017-03-02T18:13:41.156150: step 31830, loss 0.0793704, acc 0.96875
2017-03-02T18:13:41.226118: step 31831, loss 0.25353, acc 0.875
2017-03-02T18:13:41.300501: step 31832, loss 0.104146, acc 0.953125
2017-03-02T18:13:41.367085: step 31833, loss 0.263877, acc 0.875
2017-03-02T18:13:41.437454: step 31834, loss 0.107, acc 0.9375
2017-03-02T18:13:41.512703: step 31835, loss 0.0503396, acc 0.984375
2017-03-02T18:13:41.587479: step 31836, loss 0.0878633, acc 0.984375
2017-03-02T18:13:41.655391: step 31837, loss 0.0987754, acc 0.9375
2017-03-02T18:13:41.738807: step 31838, loss 0.15448, acc 0.9375
2017-03-02T18:13:41.811617: step 31839, loss 0.178198, acc 0.921875
2017-03-02T18:13:41.883715: step 31840, loss 0.163518, acc 0.9375
2017-03-02T18:13:41.956135: step 31841, loss 0.226113, acc 0.890625
2017-03-02T18:13:42.027611: step 31842, loss 0.115917, acc 0.953125
2017-03-02T18:13:42.095794: step 31843, loss 0.150788, acc 0.890625
2017-03-02T18:13:42.163813: step 31844, loss 0.155689, acc 0.90625
2017-03-02T18:13:42.240112: step 31845, loss 0.173545, acc 0.953125
2017-03-02T18:13:42.314470: step 31846, loss 0.166789, acc 0.921875
2017-03-02T18:13:42.390254: step 31847, loss 0.158793, acc 0.9375
2017-03-02T18:13:42.476790: step 31848, loss 0.0781527, acc 0.953125
2017-03-02T18:13:42.545858: step 31849, loss 0.208373, acc 0.921875
2017-03-02T18:13:42.620717: step 31850, loss 0.118224, acc 0.9375
2017-03-02T18:13:42.691018: step 31851, loss 0.257448, acc 0.90625
2017-03-02T18:13:42.765648: step 31852, loss 0.124639, acc 0.953125
2017-03-02T18:13:42.834074: step 31853, loss 0.064209, acc 0.984375
2017-03-02T18:13:42.906516: step 31854, loss 0.152747, acc 0.90625
2017-03-02T18:13:42.977092: step 31855, loss 0.269728, acc 0.875
2017-03-02T18:13:43.049407: step 31856, loss 0.087836, acc 0.9375
2017-03-02T18:13:43.121720: step 31857, loss 0.204668, acc 0.921875
2017-03-02T18:13:43.193840: step 31858, loss 0.228354, acc 0.921875
2017-03-02T18:13:43.269574: step 31859, loss 0.228645, acc 0.890625
2017-03-02T18:13:43.338748: step 31860, loss 0.190646, acc 0.921875
2017-03-02T18:13:43.409993: step 31861, loss 0.134804, acc 0.953125
2017-03-02T18:13:43.486454: step 31862, loss 0.174041, acc 0.9375
2017-03-02T18:13:43.555768: step 31863, loss 0.0735997, acc 0.953125
2017-03-02T18:13:43.628402: step 31864, loss 0.263585, acc 0.828125
2017-03-02T18:13:43.731819: step 31865, loss 0.143437, acc 0.890625
2017-03-02T18:13:43.802387: step 31866, loss 0.13515, acc 0.953125
2017-03-02T18:13:43.871457: step 31867, loss 0.0978585, acc 0.96875
2017-03-02T18:13:43.945595: step 31868, loss 0.254811, acc 0.875
2017-03-02T18:13:44.025806: step 31869, loss 0.163334, acc 0.921875
2017-03-02T18:13:44.096709: step 31870, loss 0.277309, acc 0.921875
2017-03-02T18:13:44.162739: step 31871, loss 0.117035, acc 0.96875
2017-03-02T18:13:44.232825: step 31872, loss 0.16767, acc 0.90625
2017-03-02T18:13:44.305564: step 31873, loss 0.107369, acc 0.953125
2017-03-02T18:13:44.378321: step 31874, loss 0.0961902, acc 0.921875
2017-03-02T18:13:44.450089: step 31875, loss 0.148971, acc 0.96875
2017-03-02T18:13:44.534160: step 31876, loss 0.134047, acc 0.9375
2017-03-02T18:13:44.608065: step 31877, loss 0.111778, acc 0.953125
2017-03-02T18:13:44.678566: step 31878, loss 0.141865, acc 0.921875
2017-03-02T18:13:44.756747: step 31879, loss 0.0918552, acc 0.96875
2017-03-02T18:13:44.826109: step 31880, loss 0.126762, acc 0.953125
2017-03-02T18:13:44.891249: step 31881, loss 0.144849, acc 0.9375
2017-03-02T18:13:44.957264: step 31882, loss 0.230986, acc 0.890625
2017-03-02T18:13:45.032977: step 31883, loss 0.0671599, acc 0.96875
2017-03-02T18:13:45.099217: step 31884, loss 0.130666, acc 0.96875
2017-03-02T18:13:45.179753: step 31885, loss 0.184408, acc 0.921875
2017-03-02T18:13:45.266895: step 31886, loss 0.14397, acc 0.9375
2017-03-02T18:13:45.338994: step 31887, loss 0.21496, acc 0.875
2017-03-02T18:13:45.416145: step 31888, loss 0.0612542, acc 0.96875
2017-03-02T18:13:45.485617: step 31889, loss 0.177723, acc 0.921875
2017-03-02T18:13:45.553678: step 31890, loss 0.165217, acc 0.953125
2017-03-02T18:13:45.626563: step 31891, loss 0.0623206, acc 0.96875
2017-03-02T18:13:45.701671: step 31892, loss 0.13505, acc 0.953125
2017-03-02T18:13:45.773235: step 31893, loss 0.139953, acc 0.953125
2017-03-02T18:13:45.845675: step 31894, loss 0.12014, acc 0.96875
2017-03-02T18:13:45.925238: step 31895, loss 0.101944, acc 0.953125
2017-03-02T18:13:45.995353: step 31896, loss 0.221255, acc 0.90625
2017-03-02T18:13:46.068972: step 31897, loss 0.195592, acc 0.890625
2017-03-02T18:13:46.150266: step 31898, loss 0.162311, acc 0.921875
2017-03-02T18:13:46.218825: step 31899, loss 0.119441, acc 0.9375
2017-03-02T18:13:46.297485: step 31900, loss 0.211255, acc 0.90625

Evaluation:
2017-03-02T18:13:46.331333: step 31900, loss 4.27205, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-31900

2017-03-02T18:13:46.796773: step 31901, loss 0.114977, acc 0.921875
2017-03-02T18:13:46.870888: step 31902, loss 0.0679708, acc 0.984375
2017-03-02T18:13:46.939257: step 31903, loss 0.114902, acc 0.9375
2017-03-02T18:13:47.014292: step 31904, loss 0.13806, acc 0.921875
2017-03-02T18:13:47.089284: step 31905, loss 0.126801, acc 0.9375
2017-03-02T18:13:47.172109: step 31906, loss 0.0508414, acc 0.984375
2017-03-02T18:13:47.244663: step 31907, loss 0.13713, acc 0.953125
2017-03-02T18:13:47.324458: step 31908, loss 0.102039, acc 0.96875
2017-03-02T18:13:47.400762: step 31909, loss 0.0788548, acc 0.953125
2017-03-02T18:13:47.475427: step 31910, loss 0.0814751, acc 0.9375
2017-03-02T18:13:47.543255: step 31911, loss 0.204293, acc 0.875
2017-03-02T18:13:47.632674: step 31912, loss 0.188251, acc 0.890625
2017-03-02T18:13:47.704395: step 31913, loss 0.0990319, acc 0.96875
2017-03-02T18:13:47.776323: step 31914, loss 0.145169, acc 0.921875
2017-03-02T18:13:47.847790: step 31915, loss 0.143702, acc 0.921875
2017-03-02T18:13:47.923116: step 31916, loss 0.0853902, acc 0.984375
2017-03-02T18:13:48.005266: step 31917, loss 0.157264, acc 0.9375
2017-03-02T18:13:48.088421: step 31918, loss 0.109537, acc 0.96875
2017-03-02T18:13:48.161686: step 31919, loss 0.0799718, acc 0.96875
2017-03-02T18:13:48.234254: step 31920, loss 0.219404, acc 0.90625
2017-03-02T18:13:48.313036: step 31921, loss 0.185707, acc 0.921875
2017-03-02T18:13:48.386643: step 31922, loss 0.180593, acc 0.953125
2017-03-02T18:13:48.461018: step 31923, loss 0.149305, acc 0.9375
2017-03-02T18:13:48.532336: step 31924, loss 0.0921924, acc 0.984375
2017-03-02T18:13:48.605791: step 31925, loss 0.20516, acc 0.90625
2017-03-02T18:13:48.679997: step 31926, loss 0.28973, acc 0.875
2017-03-02T18:13:48.756914: step 31927, loss 0.06557, acc 0.984375
2017-03-02T18:13:48.842590: step 31928, loss 0.10764, acc 0.96875
2017-03-02T18:13:48.912389: step 31929, loss 0.248366, acc 0.875
2017-03-02T18:13:48.977945: step 31930, loss 0.182761, acc 0.921875
2017-03-02T18:13:49.049358: step 31931, loss 0.153009, acc 0.953125
2017-03-02T18:13:49.123178: step 31932, loss 0.119929, acc 0.953125
2017-03-02T18:13:49.200321: step 31933, loss 0.28438, acc 0.875
2017-03-02T18:13:49.279640: step 31934, loss 0.0709159, acc 0.984375
2017-03-02T18:13:49.365502: step 31935, loss 0.13905, acc 0.9375
2017-03-02T18:13:49.432397: step 31936, loss 0.143091, acc 0.9375
2017-03-02T18:13:49.508103: step 31937, loss 0.107617, acc 0.953125
2017-03-02T18:13:49.578635: step 31938, loss 0.115269, acc 0.953125
2017-03-02T18:13:49.649607: step 31939, loss 0.0650541, acc 0.96875
2017-03-02T18:13:49.720833: step 31940, loss 0.241997, acc 0.90625
2017-03-02T18:13:49.796003: step 31941, loss 0.0949867, acc 0.953125
2017-03-02T18:13:49.865904: step 31942, loss 0.192486, acc 0.921875
2017-03-02T18:13:49.938718: step 31943, loss 0.147542, acc 0.953125
2017-03-02T18:13:50.008958: step 31944, loss 0.2629, acc 0.875
2017-03-02T18:13:50.079879: step 31945, loss 0.121841, acc 0.984375
2017-03-02T18:13:50.150562: step 31946, loss 0.0764555, acc 0.953125
2017-03-02T18:13:50.225652: step 31947, loss 0.198164, acc 0.859375
2017-03-02T18:13:50.294847: step 31948, loss 0.177155, acc 1
2017-03-02T18:13:50.369452: step 31949, loss 0.135175, acc 0.921875
2017-03-02T18:13:50.443829: step 31950, loss 0.184529, acc 0.9375
2017-03-02T18:13:50.519186: step 31951, loss 0.251615, acc 0.875
2017-03-02T18:13:50.597817: step 31952, loss 0.155743, acc 0.9375
2017-03-02T18:13:50.674790: step 31953, loss 0.197926, acc 0.875
2017-03-02T18:13:50.751871: step 31954, loss 0.146502, acc 0.953125
2017-03-02T18:13:50.838170: step 31955, loss 0.148226, acc 0.9375
2017-03-02T18:13:50.910388: step 31956, loss 0.276299, acc 0.875
2017-03-02T18:13:50.981044: step 31957, loss 0.105755, acc 0.953125
2017-03-02T18:13:51.051657: step 31958, loss 0.11936, acc 0.921875
2017-03-02T18:13:51.129531: step 31959, loss 0.218089, acc 0.90625
2017-03-02T18:13:51.243508: step 31960, loss 0.153067, acc 0.921875
2017-03-02T18:13:51.310016: step 31961, loss 0.246514, acc 0.875
2017-03-02T18:13:51.385556: step 31962, loss 0.0866907, acc 0.9375
2017-03-02T18:13:51.458972: step 31963, loss 0.107766, acc 0.9375
2017-03-02T18:13:51.529790: step 31964, loss 0.161073, acc 0.890625
2017-03-02T18:13:51.595715: step 31965, loss 0.0934567, acc 0.96875
2017-03-02T18:13:51.661882: step 31966, loss 0.112291, acc 0.9375
2017-03-02T18:13:51.742949: step 31967, loss 0.0736029, acc 0.984375
2017-03-02T18:13:51.828880: step 31968, loss 0.110018, acc 0.953125
2017-03-02T18:13:51.900774: step 31969, loss 0.0859306, acc 0.96875
2017-03-02T18:13:51.987625: step 31970, loss 0.202717, acc 0.890625
2017-03-02T18:13:52.059717: step 31971, loss 0.22961, acc 0.90625
2017-03-02T18:13:52.133097: step 31972, loss 0.121144, acc 0.953125
2017-03-02T18:13:52.204335: step 31973, loss 0.192209, acc 0.890625
2017-03-02T18:13:52.281473: step 31974, loss 0.0823898, acc 0.9375
2017-03-02T18:13:52.352067: step 31975, loss 0.125087, acc 0.9375
2017-03-02T18:13:52.425937: step 31976, loss 0.100502, acc 0.96875
2017-03-02T18:13:52.494629: step 31977, loss 0.15045, acc 0.921875
2017-03-02T18:13:52.568128: step 31978, loss 0.150122, acc 0.921875
2017-03-02T18:13:52.641474: step 31979, loss 0.105915, acc 0.953125
2017-03-02T18:13:52.717918: step 31980, loss 0.195267, acc 0.921875
2017-03-02T18:13:52.790002: step 31981, loss 0.208114, acc 0.921875
2017-03-02T18:13:52.860994: step 31982, loss 0.0744296, acc 0.96875
2017-03-02T18:13:52.935472: step 31983, loss 0.110197, acc 0.9375
2017-03-02T18:13:53.010817: step 31984, loss 0.0843647, acc 0.96875
2017-03-02T18:13:53.080295: step 31985, loss 0.230256, acc 0.890625
2017-03-02T18:13:53.148452: step 31986, loss 0.263068, acc 0.828125
2017-03-02T18:13:53.215992: step 31987, loss 0.0971547, acc 0.9375
2017-03-02T18:13:53.290388: step 31988, loss 0.155042, acc 0.921875
2017-03-02T18:13:53.363430: step 31989, loss 0.129591, acc 0.984375
2017-03-02T18:13:53.433955: step 31990, loss 0.108743, acc 0.9375
2017-03-02T18:13:53.504649: step 31991, loss 0.203144, acc 0.890625
2017-03-02T18:13:53.576542: step 31992, loss 0.0887627, acc 0.96875
2017-03-02T18:13:53.644780: step 31993, loss 0.122651, acc 0.953125
2017-03-02T18:13:53.717263: step 31994, loss 0.192749, acc 0.921875
2017-03-02T18:13:53.788209: step 31995, loss 0.133794, acc 0.9375
2017-03-02T18:13:53.858069: step 31996, loss 0.0525676, acc 0.984375
2017-03-02T18:13:53.928492: step 31997, loss 0.233981, acc 0.875
2017-03-02T18:13:53.997015: step 31998, loss 0.11293, acc 0.953125
2017-03-02T18:13:54.064839: step 31999, loss 0.0924365, acc 0.953125
2017-03-02T18:13:54.135699: step 32000, loss 0.248285, acc 0.875

Evaluation:
2017-03-02T18:13:54.168048: step 32000, loss 4.19675, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32000

2017-03-02T18:13:54.633252: step 32001, loss 0.137909, acc 0.921875
2017-03-02T18:13:54.707535: step 32002, loss 0.173619, acc 0.921875
2017-03-02T18:13:54.774789: step 32003, loss 0.130616, acc 0.9375
2017-03-02T18:13:54.850732: step 32004, loss 0.0868381, acc 0.953125
2017-03-02T18:13:54.921472: step 32005, loss 0.155437, acc 0.96875
2017-03-02T18:13:54.997363: step 32006, loss 0.144667, acc 0.953125
2017-03-02T18:13:55.075044: step 32007, loss 0.145594, acc 0.921875
2017-03-02T18:13:55.145516: step 32008, loss 0.0617863, acc 0.984375
2017-03-02T18:13:55.216205: step 32009, loss 0.210482, acc 0.953125
2017-03-02T18:13:55.290827: step 32010, loss 0.130651, acc 0.953125
2017-03-02T18:13:55.370674: step 32011, loss 0.114564, acc 0.953125
2017-03-02T18:13:55.445511: step 32012, loss 0.165075, acc 0.90625
2017-03-02T18:13:55.510422: step 32013, loss 0.105909, acc 0.96875
2017-03-02T18:13:55.585881: step 32014, loss 0.0727907, acc 0.96875
2017-03-02T18:13:55.653432: step 32015, loss 0.110452, acc 0.9375
2017-03-02T18:13:55.729670: step 32016, loss 0.1713, acc 0.90625
2017-03-02T18:13:55.802020: step 32017, loss 0.0920833, acc 0.96875
2017-03-02T18:13:55.869955: step 32018, loss 0.268434, acc 0.90625
2017-03-02T18:13:55.936917: step 32019, loss 0.0529505, acc 0.984375
2017-03-02T18:13:56.015648: step 32020, loss 0.20376, acc 0.921875
2017-03-02T18:13:56.095232: step 32021, loss 0.129401, acc 0.9375
2017-03-02T18:13:56.165124: step 32022, loss 0.139551, acc 0.953125
2017-03-02T18:13:56.242321: step 32023, loss 0.113159, acc 0.921875
2017-03-02T18:13:56.318378: step 32024, loss 0.160345, acc 0.90625
2017-03-02T18:13:56.395398: step 32025, loss 0.136473, acc 0.9375
2017-03-02T18:13:56.468595: step 32026, loss 0.248057, acc 0.90625
2017-03-02T18:13:56.539240: step 32027, loss 0.113704, acc 0.953125
2017-03-02T18:13:56.611901: step 32028, loss 0.116428, acc 0.96875
2017-03-02T18:13:56.684063: step 32029, loss 0.17914, acc 0.90625
2017-03-02T18:13:56.762225: step 32030, loss 0.0866354, acc 0.953125
2017-03-02T18:13:56.837688: step 32031, loss 0.167797, acc 0.921875
2017-03-02T18:13:56.910691: step 32032, loss 0.100242, acc 0.96875
2017-03-02T18:13:56.984484: step 32033, loss 0.167413, acc 0.9375
2017-03-02T18:13:57.057421: step 32034, loss 0.129715, acc 0.96875
2017-03-02T18:13:57.135175: step 32035, loss 0.120651, acc 0.921875
2017-03-02T18:13:57.212232: step 32036, loss 0.0721482, acc 0.96875
2017-03-02T18:13:57.284658: step 32037, loss 0.0483194, acc 0.984375
2017-03-02T18:13:57.353263: step 32038, loss 0.323228, acc 0.828125
2017-03-02T18:13:57.415190: step 32039, loss 0.208629, acc 0.890625
2017-03-02T18:13:57.486444: step 32040, loss 0.111732, acc 0.9375
2017-03-02T18:13:57.558947: step 32041, loss 0.213529, acc 0.875
2017-03-02T18:13:57.626187: step 32042, loss 0.12789, acc 0.96875
2017-03-02T18:13:57.702452: step 32043, loss 0.131715, acc 0.9375
2017-03-02T18:13:57.775756: step 32044, loss 0.227732, acc 0.921875
2017-03-02T18:13:57.851246: step 32045, loss 0.140177, acc 0.90625
2017-03-02T18:13:57.925939: step 32046, loss 0.125871, acc 0.953125
2017-03-02T18:13:58.024195: step 32047, loss 0.0887309, acc 0.953125
2017-03-02T18:13:58.096494: step 32048, loss 0.219932, acc 0.875
2017-03-02T18:13:58.168086: step 32049, loss 0.23194, acc 0.890625
2017-03-02T18:13:58.240011: step 32050, loss 0.149085, acc 0.9375
2017-03-02T18:13:58.333251: step 32051, loss 0.106483, acc 0.9375
2017-03-02T18:13:58.416734: step 32052, loss 0.095958, acc 0.96875
2017-03-02T18:13:58.491490: step 32053, loss 0.0876664, acc 0.984375
2017-03-02T18:13:58.570768: step 32054, loss 0.139154, acc 0.9375
2017-03-02T18:13:58.639344: step 32055, loss 0.120232, acc 0.953125
2017-03-02T18:13:58.703135: step 32056, loss 0.169303, acc 0.890625
2017-03-02T18:13:58.777366: step 32057, loss 0.206856, acc 0.90625
2017-03-02T18:13:58.851237: step 32058, loss 0.223144, acc 0.90625
2017-03-02T18:13:58.925089: step 32059, loss 0.170518, acc 0.921875
2017-03-02T18:13:59.004978: step 32060, loss 0.153735, acc 0.921875
2017-03-02T18:13:59.078625: step 32061, loss 0.132982, acc 0.953125
2017-03-02T18:13:59.152092: step 32062, loss 0.211052, acc 0.90625
2017-03-02T18:13:59.223275: step 32063, loss 0.107841, acc 0.96875
2017-03-02T18:13:59.292396: step 32064, loss 0.170868, acc 0.890625
2017-03-02T18:13:59.361514: step 32065, loss 0.114237, acc 0.921875
2017-03-02T18:13:59.431883: step 32066, loss 0.136331, acc 0.9375
2017-03-02T18:13:59.503870: step 32067, loss 0.0799379, acc 0.96875
2017-03-02T18:13:59.580554: step 32068, loss 0.20311, acc 0.921875
2017-03-02T18:13:59.656497: step 32069, loss 0.163128, acc 0.921875
2017-03-02T18:13:59.742086: step 32070, loss 0.189848, acc 0.890625
2017-03-02T18:13:59.813225: step 32071, loss 0.122557, acc 0.9375
2017-03-02T18:13:59.890635: step 32072, loss 0.202342, acc 0.90625
2017-03-02T18:13:59.957173: step 32073, loss 0.071901, acc 0.984375
2017-03-02T18:14:00.022207: step 32074, loss 0.156888, acc 0.921875
2017-03-02T18:14:00.096464: step 32075, loss 0.141196, acc 0.953125
2017-03-02T18:14:00.172437: step 32076, loss 0.15523, acc 0.921875
2017-03-02T18:14:00.247270: step 32077, loss 0.0798348, acc 0.953125
2017-03-02T18:14:00.322973: step 32078, loss 0.146936, acc 0.90625
2017-03-02T18:14:00.406027: step 32079, loss 0.140187, acc 0.96875
2017-03-02T18:14:00.478165: step 32080, loss 0.0883214, acc 0.96875
2017-03-02T18:14:00.551182: step 32081, loss 0.0991841, acc 0.9375
2017-03-02T18:14:00.625227: step 32082, loss 0.314959, acc 0.890625
2017-03-02T18:14:00.694766: step 32083, loss 0.0790218, acc 0.96875
2017-03-02T18:14:00.758642: step 32084, loss 0.133009, acc 0.96875
2017-03-02T18:14:00.832972: step 32085, loss 0.205702, acc 0.921875
2017-03-02T18:14:00.908697: step 32086, loss 0.153856, acc 0.921875
2017-03-02T18:14:00.985184: step 32087, loss 0.109957, acc 0.953125
2017-03-02T18:14:01.059293: step 32088, loss 0.189806, acc 0.90625
2017-03-02T18:14:01.128854: step 32089, loss 0.118911, acc 0.9375
2017-03-02T18:14:01.207757: step 32090, loss 0.101583, acc 0.9375
2017-03-02T18:14:01.298189: step 32091, loss 0.213202, acc 0.90625
2017-03-02T18:14:01.361865: step 32092, loss 0.114555, acc 0.96875
2017-03-02T18:14:01.435828: step 32093, loss 0.0828727, acc 0.96875
2017-03-02T18:14:01.510502: step 32094, loss 0.148943, acc 0.9375
2017-03-02T18:14:01.579189: step 32095, loss 0.199556, acc 0.90625
2017-03-02T18:14:01.651600: step 32096, loss 0.125742, acc 0.953125
2017-03-02T18:14:01.724187: step 32097, loss 0.0447485, acc 1
2017-03-02T18:14:01.805515: step 32098, loss 0.293296, acc 0.859375
2017-03-02T18:14:01.885340: step 32099, loss 0.0636053, acc 0.96875
2017-03-02T18:14:01.962349: step 32100, loss 0.120531, acc 0.953125

Evaluation:
2017-03-02T18:14:01.993043: step 32100, loss 4.30384, acc 0.638789

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32100

2017-03-02T18:14:02.480989: step 32101, loss 0.160083, acc 0.921875
2017-03-02T18:14:02.553917: step 32102, loss 0.186604, acc 0.953125
2017-03-02T18:14:02.630667: step 32103, loss 0.147407, acc 0.90625
2017-03-02T18:14:02.706868: step 32104, loss 0.0863878, acc 0.953125
2017-03-02T18:14:02.783505: step 32105, loss 0.0882495, acc 0.96875
2017-03-02T18:14:02.851447: step 32106, loss 0.21457, acc 0.921875
2017-03-02T18:14:02.922640: step 32107, loss 0.137589, acc 0.96875
2017-03-02T18:14:02.997408: step 32108, loss 0.265662, acc 0.859375
2017-03-02T18:14:03.067881: step 32109, loss 0.164162, acc 0.90625
2017-03-02T18:14:03.141070: step 32110, loss 0.0348853, acc 1
2017-03-02T18:14:03.211675: step 32111, loss 0.0858309, acc 0.96875
2017-03-02T18:14:03.283645: step 32112, loss 0.0989837, acc 0.953125
2017-03-02T18:14:03.359286: step 32113, loss 0.116012, acc 0.96875
2017-03-02T18:14:03.427275: step 32114, loss 0.0638348, acc 0.96875
2017-03-02T18:14:03.497052: step 32115, loss 0.236928, acc 0.90625
2017-03-02T18:14:03.569339: step 32116, loss 0.119106, acc 0.9375
2017-03-02T18:14:03.641293: step 32117, loss 0.114672, acc 0.953125
2017-03-02T18:14:03.707480: step 32118, loss 0.128788, acc 0.9375
2017-03-02T18:14:03.781074: step 32119, loss 0.0606082, acc 0.984375
2017-03-02T18:14:03.847167: step 32120, loss 0.104875, acc 0.953125
2017-03-02T18:14:03.919836: step 32121, loss 0.0833381, acc 0.953125
2017-03-02T18:14:03.989104: step 32122, loss 0.0306703, acc 1
2017-03-02T18:14:04.055133: step 32123, loss 0.0670209, acc 0.953125
2017-03-02T18:14:04.123759: step 32124, loss 0.285989, acc 0.875
2017-03-02T18:14:04.190331: step 32125, loss 0.127074, acc 0.953125
2017-03-02T18:14:04.264009: step 32126, loss 0.155732, acc 0.921875
2017-03-02T18:14:04.346921: step 32127, loss 0.202813, acc 0.9375
2017-03-02T18:14:04.424522: step 32128, loss 0.162243, acc 0.953125
2017-03-02T18:14:04.493884: step 32129, loss 0.252215, acc 0.890625
2017-03-02T18:14:04.566685: step 32130, loss 0.123165, acc 0.9375
2017-03-02T18:14:04.637027: step 32131, loss 0.0625152, acc 0.96875
2017-03-02T18:14:04.709593: step 32132, loss 0.173006, acc 0.90625
2017-03-02T18:14:04.773163: step 32133, loss 0.0694146, acc 0.96875
2017-03-02T18:14:04.844751: step 32134, loss 0.106433, acc 0.984375
2017-03-02T18:14:04.914769: step 32135, loss 0.189561, acc 0.90625
2017-03-02T18:14:04.987539: step 32136, loss 0.136668, acc 0.9375
2017-03-02T18:14:05.061892: step 32137, loss 0.0560657, acc 1
2017-03-02T18:14:05.134451: step 32138, loss 0.162652, acc 0.921875
2017-03-02T18:14:05.205447: step 32139, loss 0.123621, acc 0.96875
2017-03-02T18:14:05.279543: step 32140, loss 0.0925608, acc 0.9375
2017-03-02T18:14:05.353719: step 32141, loss 0.073081, acc 0.96875
2017-03-02T18:14:05.428340: step 32142, loss 0.134853, acc 0.953125
2017-03-02T18:14:05.497247: step 32143, loss 0.219043, acc 0.890625
2017-03-02T18:14:05.561811: step 32144, loss 0.327086, acc 0.75
2017-03-02T18:14:05.643988: step 32145, loss 0.129393, acc 0.953125
2017-03-02T18:14:05.718852: step 32146, loss 0.0940632, acc 0.9375
2017-03-02T18:14:05.788559: step 32147, loss 0.172538, acc 0.9375
2017-03-02T18:14:05.859650: step 32148, loss 0.0600388, acc 0.984375
2017-03-02T18:14:05.931695: step 32149, loss 0.0891276, acc 0.953125
2017-03-02T18:14:06.004038: step 32150, loss 0.113717, acc 0.921875
2017-03-02T18:14:06.075193: step 32151, loss 0.0545033, acc 0.984375
2017-03-02T18:14:06.157538: step 32152, loss 0.0844856, acc 0.953125
2017-03-02T18:14:06.233242: step 32153, loss 0.155979, acc 0.921875
2017-03-02T18:14:06.303695: step 32154, loss 0.0612908, acc 0.96875
2017-03-02T18:14:06.374813: step 32155, loss 0.21895, acc 0.875
2017-03-02T18:14:06.441195: step 32156, loss 0.213675, acc 0.890625
2017-03-02T18:14:06.514111: step 32157, loss 0.17022, acc 0.921875
2017-03-02T18:14:06.590025: step 32158, loss 0.176912, acc 0.953125
2017-03-02T18:14:06.667027: step 32159, loss 0.133535, acc 0.9375
2017-03-02T18:14:06.741748: step 32160, loss 0.159811, acc 0.9375
2017-03-02T18:14:06.816000: step 32161, loss 0.186814, acc 0.921875
2017-03-02T18:14:06.875020: step 32162, loss 0.17749, acc 0.9375
2017-03-02T18:14:06.938241: step 32163, loss 0.0820443, acc 0.953125
2017-03-02T18:14:07.001706: step 32164, loss 0.101241, acc 0.953125
2017-03-02T18:14:07.073524: step 32165, loss 0.119339, acc 0.9375
2017-03-02T18:14:07.145180: step 32166, loss 0.179695, acc 0.9375
2017-03-02T18:14:07.214902: step 32167, loss 0.157313, acc 0.984375
2017-03-02T18:14:07.287802: step 32168, loss 0.154902, acc 0.921875
2017-03-02T18:14:07.359512: step 32169, loss 0.14878, acc 0.953125
2017-03-02T18:14:07.433016: step 32170, loss 0.181527, acc 0.890625
2017-03-02T18:14:07.513900: step 32171, loss 0.104696, acc 0.96875
2017-03-02T18:14:07.577926: step 32172, loss 0.15548, acc 0.953125
2017-03-02T18:14:07.643502: step 32173, loss 0.10596, acc 0.9375
2017-03-02T18:14:07.717151: step 32174, loss 0.152022, acc 0.9375
2017-03-02T18:14:07.791517: step 32175, loss 0.170854, acc 0.90625
2017-03-02T18:14:07.860028: step 32176, loss 0.206017, acc 0.9375
2017-03-02T18:14:07.935955: step 32177, loss 0.146383, acc 0.96875
2017-03-02T18:14:08.009478: step 32178, loss 0.158692, acc 0.90625
2017-03-02T18:14:08.082245: step 32179, loss 0.117976, acc 0.953125
2017-03-02T18:14:08.144452: step 32180, loss 0.102846, acc 0.953125
2017-03-02T18:14:08.227583: step 32181, loss 0.243794, acc 0.859375
2017-03-02T18:14:08.297046: step 32182, loss 0.147426, acc 0.9375
2017-03-02T18:14:08.362382: step 32183, loss 0.0868122, acc 1
2017-03-02T18:14:08.447146: step 32184, loss 0.103825, acc 0.9375
2017-03-02T18:14:08.531686: step 32185, loss 0.137832, acc 0.984375
2017-03-02T18:14:08.604138: step 32186, loss 0.0898075, acc 0.953125
2017-03-02T18:14:08.679909: step 32187, loss 0.072991, acc 0.984375
2017-03-02T18:14:08.749099: step 32188, loss 0.12049, acc 0.9375
2017-03-02T18:14:08.822988: step 32189, loss 0.150749, acc 0.9375
2017-03-02T18:14:08.895954: step 32190, loss 0.147155, acc 0.921875
2017-03-02T18:14:08.965497: step 32191, loss 0.225162, acc 0.90625
2017-03-02T18:14:09.035376: step 32192, loss 0.117371, acc 0.921875
2017-03-02T18:14:09.104959: step 32193, loss 0.0228672, acc 1
2017-03-02T18:14:09.177912: step 32194, loss 0.158723, acc 0.921875
2017-03-02T18:14:09.254500: step 32195, loss 0.153184, acc 0.90625
2017-03-02T18:14:09.328583: step 32196, loss 0.100724, acc 0.9375
2017-03-02T18:14:09.403335: step 32197, loss 0.100527, acc 0.953125
2017-03-02T18:14:09.473364: step 32198, loss 0.104306, acc 0.96875
2017-03-02T18:14:09.545954: step 32199, loss 0.221765, acc 0.875
2017-03-02T18:14:09.624638: step 32200, loss 0.168066, acc 0.90625

Evaluation:
2017-03-02T18:14:09.658525: step 32200, loss 4.39175, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32200

2017-03-02T18:14:10.100025: step 32201, loss 0.0953833, acc 0.953125
2017-03-02T18:14:10.170205: step 32202, loss 0.142755, acc 0.921875
2017-03-02T18:14:10.241521: step 32203, loss 0.208389, acc 0.875
2017-03-02T18:14:10.315936: step 32204, loss 0.0615701, acc 0.96875
2017-03-02T18:14:10.384291: step 32205, loss 0.109301, acc 0.953125
2017-03-02T18:14:10.454415: step 32206, loss 0.124185, acc 0.9375
2017-03-02T18:14:10.524243: step 32207, loss 0.14079, acc 0.890625
2017-03-02T18:14:10.599031: step 32208, loss 0.141228, acc 0.9375
2017-03-02T18:14:10.671418: step 32209, loss 0.103203, acc 0.953125
2017-03-02T18:14:10.743014: step 32210, loss 0.103762, acc 0.9375
2017-03-02T18:14:10.815542: step 32211, loss 0.0941779, acc 0.953125
2017-03-02T18:14:10.886417: step 32212, loss 0.0687892, acc 0.984375
2017-03-02T18:14:10.965621: step 32213, loss 0.048657, acc 0.96875
2017-03-02T18:14:11.037109: step 32214, loss 0.126064, acc 0.9375
2017-03-02T18:14:11.102754: step 32215, loss 0.171855, acc 0.921875
2017-03-02T18:14:11.171223: step 32216, loss 0.156485, acc 0.9375
2017-03-02T18:14:11.244928: step 32217, loss 0.243369, acc 0.875
2017-03-02T18:14:11.320038: step 32218, loss 0.0600954, acc 0.96875
2017-03-02T18:14:11.390376: step 32219, loss 0.201558, acc 0.890625
2017-03-02T18:14:11.462025: step 32220, loss 0.156719, acc 0.9375
2017-03-02T18:14:11.529348: step 32221, loss 0.0189927, acc 1
2017-03-02T18:14:11.597799: step 32222, loss 0.162949, acc 0.890625
2017-03-02T18:14:11.670664: step 32223, loss 0.149972, acc 0.953125
2017-03-02T18:14:11.742216: step 32224, loss 0.130905, acc 0.96875
2017-03-02T18:14:11.810627: step 32225, loss 0.13042, acc 0.953125
2017-03-02T18:14:11.878151: step 32226, loss 0.134961, acc 0.9375
2017-03-02T18:14:11.948424: step 32227, loss 0.242469, acc 0.890625
2017-03-02T18:14:12.017155: step 32228, loss 0.144725, acc 0.9375
2017-03-02T18:14:12.098645: step 32229, loss 0.274171, acc 0.90625
2017-03-02T18:14:12.172790: step 32230, loss 0.148142, acc 0.921875
2017-03-02T18:14:12.242875: step 32231, loss 0.211045, acc 0.890625
2017-03-02T18:14:12.325670: step 32232, loss 0.292428, acc 0.890625
2017-03-02T18:14:12.400361: step 32233, loss 0.0862629, acc 0.96875
2017-03-02T18:14:12.466006: step 32234, loss 0.119607, acc 0.96875
2017-03-02T18:14:12.534721: step 32235, loss 0.170674, acc 0.90625
2017-03-02T18:14:12.609053: step 32236, loss 0.171835, acc 0.890625
2017-03-02T18:14:12.677312: step 32237, loss 0.104282, acc 0.953125
2017-03-02T18:14:12.751920: step 32238, loss 0.091619, acc 0.953125
2017-03-02T18:14:12.821974: step 32239, loss 0.171241, acc 0.9375
2017-03-02T18:14:12.895720: step 32240, loss 0.139416, acc 0.921875
2017-03-02T18:14:12.965817: step 32241, loss 0.12046, acc 0.953125
2017-03-02T18:14:13.039684: step 32242, loss 0.251153, acc 0.890625
2017-03-02T18:14:13.108906: step 32243, loss 0.0616605, acc 0.984375
2017-03-02T18:14:13.178611: step 32244, loss 0.0687667, acc 0.96875
2017-03-02T18:14:13.251509: step 32245, loss 0.132218, acc 0.953125
2017-03-02T18:14:13.319661: step 32246, loss 0.103924, acc 0.953125
2017-03-02T18:14:13.391955: step 32247, loss 0.113433, acc 0.953125
2017-03-02T18:14:13.468137: step 32248, loss 0.111494, acc 0.953125
2017-03-02T18:14:13.541033: step 32249, loss 0.171788, acc 0.9375
2017-03-02T18:14:13.612199: step 32250, loss 0.189434, acc 0.90625
2017-03-02T18:14:13.700219: step 32251, loss 0.132467, acc 0.921875
2017-03-02T18:14:13.763590: step 32252, loss 0.143394, acc 0.921875
2017-03-02T18:14:13.841036: step 32253, loss 0.10448, acc 0.96875
2017-03-02T18:14:13.912911: step 32254, loss 0.192053, acc 0.890625
2017-03-02T18:14:13.991389: step 32255, loss 0.0926966, acc 0.953125
2017-03-02T18:14:14.079756: step 32256, loss 0.230843, acc 0.921875
2017-03-02T18:14:14.153542: step 32257, loss 0.171407, acc 0.921875
2017-03-02T18:14:14.225015: step 32258, loss 0.189545, acc 0.9375
2017-03-02T18:14:14.289761: step 32259, loss 0.0993185, acc 0.9375
2017-03-02T18:14:14.356841: step 32260, loss 0.200851, acc 0.90625
2017-03-02T18:14:14.426957: step 32261, loss 0.321358, acc 0.828125
2017-03-02T18:14:14.497785: step 32262, loss 0.156158, acc 0.921875
2017-03-02T18:14:14.571971: step 32263, loss 0.0926597, acc 0.96875
2017-03-02T18:14:14.638315: step 32264, loss 0.0874638, acc 0.953125
2017-03-02T18:14:14.716008: step 32265, loss 0.127296, acc 0.953125
2017-03-02T18:14:14.788347: step 32266, loss 0.179318, acc 0.921875
2017-03-02T18:14:14.860529: step 32267, loss 0.157576, acc 0.90625
2017-03-02T18:14:14.929000: step 32268, loss 0.225968, acc 0.90625
2017-03-02T18:14:15.002095: step 32269, loss 0.223565, acc 0.890625
2017-03-02T18:14:15.077764: step 32270, loss 0.275003, acc 0.9375
2017-03-02T18:14:15.152522: step 32271, loss 0.136537, acc 0.9375
2017-03-02T18:14:15.227402: step 32272, loss 0.233728, acc 0.90625
2017-03-02T18:14:15.298149: step 32273, loss 0.128021, acc 0.953125
2017-03-02T18:14:15.366395: step 32274, loss 0.142983, acc 0.9375
2017-03-02T18:14:15.442971: step 32275, loss 0.126481, acc 0.953125
2017-03-02T18:14:15.520178: step 32276, loss 0.0901921, acc 0.96875
2017-03-02T18:14:15.592784: step 32277, loss 0.152299, acc 0.9375
2017-03-02T18:14:15.664392: step 32278, loss 0.127735, acc 0.90625
2017-03-02T18:14:15.753578: step 32279, loss 0.11997, acc 0.90625
2017-03-02T18:14:15.822966: step 32280, loss 0.155705, acc 0.921875
2017-03-02T18:14:15.894182: step 32281, loss 0.0919419, acc 0.96875
2017-03-02T18:14:15.965219: step 32282, loss 0.107412, acc 0.953125
2017-03-02T18:14:16.029720: step 32283, loss 0.101522, acc 0.96875
2017-03-02T18:14:16.099645: step 32284, loss 0.0891492, acc 0.984375
2017-03-02T18:14:16.167516: step 32285, loss 0.191915, acc 0.9375
2017-03-02T18:14:16.241551: step 32286, loss 0.17202, acc 0.921875
2017-03-02T18:14:16.313543: step 32287, loss 0.165447, acc 0.90625
2017-03-02T18:14:16.387603: step 32288, loss 0.140798, acc 0.921875
2017-03-02T18:14:16.456367: step 32289, loss 0.22432, acc 0.890625
2017-03-02T18:14:16.528737: step 32290, loss 0.11957, acc 0.9375
2017-03-02T18:14:16.610532: step 32291, loss 0.205812, acc 0.953125
2017-03-02T18:14:16.682763: step 32292, loss 0.109143, acc 0.953125
2017-03-02T18:14:16.762263: step 32293, loss 0.128457, acc 0.921875
2017-03-02T18:14:16.837746: step 32294, loss 0.140563, acc 0.953125
2017-03-02T18:14:16.912605: step 32295, loss 0.0761573, acc 0.96875
2017-03-02T18:14:16.989843: step 32296, loss 0.10777, acc 0.9375
2017-03-02T18:14:17.074224: step 32297, loss 0.0740382, acc 0.96875
2017-03-02T18:14:17.147574: step 32298, loss 0.0854918, acc 0.96875
2017-03-02T18:14:17.220756: step 32299, loss 0.132327, acc 0.921875
2017-03-02T18:14:17.297125: step 32300, loss 0.125127, acc 0.953125

Evaluation:
2017-03-02T18:14:17.327022: step 32300, loss 4.36442, acc 0.638789

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32300

2017-03-02T18:14:17.777324: step 32301, loss 0.0752484, acc 0.96875
2017-03-02T18:14:17.854010: step 32302, loss 0.0769502, acc 0.96875
2017-03-02T18:14:17.924796: step 32303, loss 0.0597273, acc 1
2017-03-02T18:14:17.996327: step 32304, loss 0.0810879, acc 0.96875
2017-03-02T18:14:18.062640: step 32305, loss 0.250774, acc 0.890625
2017-03-02T18:14:18.136967: step 32306, loss 0.10047, acc 0.953125
2017-03-02T18:14:18.207437: step 32307, loss 0.130974, acc 0.921875
2017-03-02T18:14:18.285481: step 32308, loss 0.0679076, acc 0.953125
2017-03-02T18:14:18.361316: step 32309, loss 0.182879, acc 0.890625
2017-03-02T18:14:18.441331: step 32310, loss 0.155422, acc 0.921875
2017-03-02T18:14:18.513106: step 32311, loss 0.264594, acc 0.90625
2017-03-02T18:14:18.583736: step 32312, loss 0.152391, acc 0.9375
2017-03-02T18:14:18.657708: step 32313, loss 0.209618, acc 0.921875
2017-03-02T18:14:18.728340: step 32314, loss 0.339132, acc 0.84375
2017-03-02T18:14:18.795402: step 32315, loss 0.17905, acc 0.9375
2017-03-02T18:14:18.860585: step 32316, loss 0.153774, acc 0.921875
2017-03-02T18:14:18.931723: step 32317, loss 0.117759, acc 0.9375
2017-03-02T18:14:19.006562: step 32318, loss 0.189356, acc 0.90625
2017-03-02T18:14:19.074420: step 32319, loss 0.220118, acc 0.921875
2017-03-02T18:14:19.161635: step 32320, loss 0.230376, acc 0.890625
2017-03-02T18:14:19.235934: step 32321, loss 0.181802, acc 0.9375
2017-03-02T18:14:19.319584: step 32322, loss 0.190269, acc 0.90625
2017-03-02T18:14:19.382976: step 32323, loss 0.151393, acc 0.9375
2017-03-02T18:14:19.456440: step 32324, loss 0.217769, acc 0.890625
2017-03-02T18:14:19.538820: step 32325, loss 0.129158, acc 0.9375
2017-03-02T18:14:19.607457: step 32326, loss 0.209568, acc 0.859375
2017-03-02T18:14:19.683510: step 32327, loss 0.160136, acc 0.9375
2017-03-02T18:14:19.758604: step 32328, loss 0.165629, acc 0.953125
2017-03-02T18:14:19.833631: step 32329, loss 0.0342545, acc 1
2017-03-02T18:14:19.906756: step 32330, loss 0.150252, acc 0.9375
2017-03-02T18:14:19.978012: step 32331, loss 0.176046, acc 0.9375
2017-03-02T18:14:20.047552: step 32332, loss 0.145554, acc 0.9375
2017-03-02T18:14:20.119822: step 32333, loss 0.102386, acc 0.953125
2017-03-02T18:14:20.185536: step 32334, loss 0.120851, acc 0.9375
2017-03-02T18:14:20.267938: step 32335, loss 0.131785, acc 0.953125
2017-03-02T18:14:20.340910: step 32336, loss 0.16411, acc 0.90625
2017-03-02T18:14:20.422395: step 32337, loss 0.162736, acc 0.9375
2017-03-02T18:14:20.507475: step 32338, loss 0.166643, acc 0.875
2017-03-02T18:14:20.590219: step 32339, loss 0.114825, acc 0.921875
2017-03-02T18:14:20.669041: step 32340, loss 1.19209e-07, acc 1
2017-03-02T18:14:20.735392: step 32341, loss 0.162425, acc 0.921875
2017-03-02T18:14:20.809227: step 32342, loss 0.136291, acc 0.9375
2017-03-02T18:14:20.884858: step 32343, loss 0.137747, acc 0.9375
2017-03-02T18:14:20.968837: step 32344, loss 0.140505, acc 0.90625
2017-03-02T18:14:21.037565: step 32345, loss 0.195936, acc 0.90625
2017-03-02T18:14:21.118025: step 32346, loss 0.19043, acc 0.921875
2017-03-02T18:14:21.187969: step 32347, loss 0.177288, acc 0.921875
2017-03-02T18:14:21.261893: step 32348, loss 0.143063, acc 0.953125
2017-03-02T18:14:21.338808: step 32349, loss 0.17832, acc 0.921875
2017-03-02T18:14:21.418721: step 32350, loss 0.171406, acc 0.921875
2017-03-02T18:14:21.487487: step 32351, loss 0.101609, acc 0.953125
2017-03-02T18:14:21.567916: step 32352, loss 0.160767, acc 0.890625
2017-03-02T18:14:21.644861: step 32353, loss 0.08628, acc 0.953125
2017-03-02T18:14:21.719755: step 32354, loss 0.115387, acc 0.953125
2017-03-02T18:14:21.794903: step 32355, loss 0.0468991, acc 0.984375
2017-03-02T18:14:21.875643: step 32356, loss 0.223734, acc 0.890625
2017-03-02T18:14:21.948534: step 32357, loss 0.101261, acc 0.953125
2017-03-02T18:14:22.023687: step 32358, loss 0.135673, acc 0.921875
2017-03-02T18:14:22.093467: step 32359, loss 0.122774, acc 0.953125
2017-03-02T18:14:22.158659: step 32360, loss 0.0792344, acc 0.984375
2017-03-02T18:14:22.231015: step 32361, loss 0.117236, acc 0.9375
2017-03-02T18:14:22.323418: step 32362, loss 0.128433, acc 0.9375
2017-03-02T18:14:22.396886: step 32363, loss 0.151309, acc 0.9375
2017-03-02T18:14:22.471386: step 32364, loss 0.142435, acc 0.9375
2017-03-02T18:14:22.548999: step 32365, loss 0.137509, acc 0.9375
2017-03-02T18:14:22.622229: step 32366, loss 0.0931547, acc 0.984375
2017-03-02T18:14:22.685951: step 32367, loss 0.116855, acc 0.953125
2017-03-02T18:14:22.765903: step 32368, loss 0.0822988, acc 0.96875
2017-03-02T18:14:22.845808: step 32369, loss 0.283183, acc 0.875
2017-03-02T18:14:22.915866: step 32370, loss 0.148773, acc 0.96875
2017-03-02T18:14:22.985395: step 32371, loss 0.0463151, acc 1
2017-03-02T18:14:23.060208: step 32372, loss 0.140828, acc 0.921875
2017-03-02T18:14:23.128391: step 32373, loss 0.149261, acc 0.96875
2017-03-02T18:14:23.198605: step 32374, loss 0.178663, acc 0.90625
2017-03-02T18:14:23.271484: step 32375, loss 0.265469, acc 0.859375
2017-03-02T18:14:23.344724: step 32376, loss 0.162356, acc 0.921875
2017-03-02T18:14:23.427862: step 32377, loss 0.116274, acc 0.953125
2017-03-02T18:14:23.504786: step 32378, loss 0.0432164, acc 0.96875
2017-03-02T18:14:23.568126: step 32379, loss 0.156271, acc 0.9375
2017-03-02T18:14:23.632472: step 32380, loss 0.11906, acc 0.921875
2017-03-02T18:14:23.712301: step 32381, loss 0.0605071, acc 0.984375
2017-03-02T18:14:23.787096: step 32382, loss 0.125451, acc 0.890625
2017-03-02T18:14:23.859746: step 32383, loss 0.136688, acc 0.953125
2017-03-02T18:14:23.934298: step 32384, loss 0.144492, acc 0.921875
2017-03-02T18:14:24.012339: step 32385, loss 0.125826, acc 0.9375
2017-03-02T18:14:24.089341: step 32386, loss 0.125775, acc 0.9375
2017-03-02T18:14:24.165216: step 32387, loss 0.112952, acc 0.9375
2017-03-02T18:14:24.234542: step 32388, loss 0.111453, acc 0.9375
2017-03-02T18:14:24.314067: step 32389, loss 0.0378336, acc 0.984375
2017-03-02T18:14:24.390783: step 32390, loss 0.113, acc 0.953125
2017-03-02T18:14:24.470749: step 32391, loss 0.166099, acc 0.953125
2017-03-02T18:14:24.545328: step 32392, loss 0.0577851, acc 0.984375
2017-03-02T18:14:24.617143: step 32393, loss 0.212966, acc 0.9375
2017-03-02T18:14:24.694007: step 32394, loss 0.111861, acc 0.953125
2017-03-02T18:14:24.768485: step 32395, loss 0.114915, acc 0.953125
2017-03-02T18:14:24.843998: step 32396, loss 0.12531, acc 0.9375
2017-03-02T18:14:24.914581: step 32397, loss 0.162025, acc 0.90625
2017-03-02T18:14:24.981106: step 32398, loss 0.119739, acc 0.96875
2017-03-02T18:14:25.048365: step 32399, loss 0.118054, acc 0.921875
2017-03-02T18:14:25.144554: step 32400, loss 0.178072, acc 0.890625

Evaluation:
2017-03-02T18:14:25.195196: step 32400, loss 4.37691, acc 0.630858

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32400

2017-03-02T18:14:25.694427: step 32401, loss 0.14536, acc 0.953125
2017-03-02T18:14:25.766375: step 32402, loss 0.0745479, acc 0.96875
2017-03-02T18:14:25.835902: step 32403, loss 0.254453, acc 0.859375
2017-03-02T18:14:25.913759: step 32404, loss 0.273187, acc 0.859375
2017-03-02T18:14:25.993422: step 32405, loss 0.192285, acc 0.921875
2017-03-02T18:14:26.065484: step 32406, loss 0.147412, acc 0.890625
2017-03-02T18:14:26.130017: step 32407, loss 0.0902499, acc 0.953125
2017-03-02T18:14:26.203384: step 32408, loss 0.217193, acc 0.921875
2017-03-02T18:14:26.278372: step 32409, loss 0.0601868, acc 0.96875
2017-03-02T18:14:26.347979: step 32410, loss 0.0842329, acc 0.96875
2017-03-02T18:14:26.415352: step 32411, loss 0.20561, acc 0.875
2017-03-02T18:14:26.497333: step 32412, loss 0.192593, acc 0.90625
2017-03-02T18:14:26.571036: step 32413, loss 0.110664, acc 0.96875
2017-03-02T18:14:26.659296: step 32414, loss 0.219621, acc 0.890625
2017-03-02T18:14:26.741337: step 32415, loss 0.216836, acc 0.90625
2017-03-02T18:14:26.831707: step 32416, loss 0.153258, acc 0.921875
2017-03-02T18:14:26.912426: step 32417, loss 0.0490252, acc 1
2017-03-02T18:14:26.983387: step 32418, loss 0.0940112, acc 0.953125
2017-03-02T18:14:27.053701: step 32419, loss 0.223309, acc 0.90625
2017-03-02T18:14:27.125975: step 32420, loss 0.125609, acc 0.9375
2017-03-02T18:14:27.199333: step 32421, loss 0.193313, acc 0.921875
2017-03-02T18:14:27.280228: step 32422, loss 0.154313, acc 0.9375
2017-03-02T18:14:27.354665: step 32423, loss 0.0730803, acc 0.984375
2017-03-02T18:14:27.426005: step 32424, loss 0.153179, acc 0.890625
2017-03-02T18:14:27.496590: step 32425, loss 0.145252, acc 0.953125
2017-03-02T18:14:27.566460: step 32426, loss 0.182088, acc 0.890625
2017-03-02T18:14:27.639849: step 32427, loss 0.170043, acc 0.921875
2017-03-02T18:14:27.705787: step 32428, loss 0.0931167, acc 0.96875
2017-03-02T18:14:27.771091: step 32429, loss 0.117831, acc 0.921875
2017-03-02T18:14:27.843498: step 32430, loss 0.0822191, acc 0.96875
2017-03-02T18:14:27.915741: step 32431, loss 0.131979, acc 0.890625
2017-03-02T18:14:27.982243: step 32432, loss 0.070751, acc 0.953125
2017-03-02T18:14:28.058096: step 32433, loss 0.197464, acc 0.90625
2017-03-02T18:14:28.129389: step 32434, loss 0.0720106, acc 0.96875
2017-03-02T18:14:28.205350: step 32435, loss 0.149696, acc 0.9375
2017-03-02T18:14:28.279491: step 32436, loss 0.187397, acc 0.921875
2017-03-02T18:14:28.357771: step 32437, loss 0.0640857, acc 0.96875
2017-03-02T18:14:28.432626: step 32438, loss 0.180037, acc 0.921875
2017-03-02T18:14:28.501395: step 32439, loss 0.14659, acc 0.9375
2017-03-02T18:14:28.584916: step 32440, loss 0.191476, acc 0.90625
2017-03-02T18:14:28.662444: step 32441, loss 0.110616, acc 0.9375
2017-03-02T18:14:28.746227: step 32442, loss 0.141675, acc 0.9375
2017-03-02T18:14:28.818010: step 32443, loss 0.164939, acc 0.921875
2017-03-02T18:14:28.888480: step 32444, loss 0.126277, acc 0.921875
2017-03-02T18:14:28.960898: step 32445, loss 0.140677, acc 0.953125
2017-03-02T18:14:29.030082: step 32446, loss 0.141396, acc 0.921875
2017-03-02T18:14:29.109209: step 32447, loss 0.148646, acc 0.9375
2017-03-02T18:14:29.183745: step 32448, loss 0.104253, acc 0.96875
2017-03-02T18:14:29.277969: step 32449, loss 0.218695, acc 0.90625
2017-03-02T18:14:29.352728: step 32450, loss 0.0700071, acc 0.953125
2017-03-02T18:14:29.426782: step 32451, loss 0.139063, acc 0.9375
2017-03-02T18:14:29.503738: step 32452, loss 0.153412, acc 0.9375
2017-03-02T18:14:29.576540: step 32453, loss 0.116686, acc 0.953125
2017-03-02T18:14:29.640184: step 32454, loss 0.144729, acc 0.984375
2017-03-02T18:14:29.719210: step 32455, loss 0.146886, acc 0.921875
2017-03-02T18:14:29.785680: step 32456, loss 0.124857, acc 0.953125
2017-03-02T18:14:29.850734: step 32457, loss 0.1103, acc 0.953125
2017-03-02T18:14:29.922716: step 32458, loss 0.0410347, acc 1
2017-03-02T18:14:29.988138: step 32459, loss 0.13241, acc 0.953125
2017-03-02T18:14:30.060944: step 32460, loss 0.0519, acc 0.984375
2017-03-02T18:14:30.127861: step 32461, loss 0.237303, acc 0.875
2017-03-02T18:14:30.199956: step 32462, loss 0.179898, acc 0.890625
2017-03-02T18:14:30.274365: step 32463, loss 0.115541, acc 0.953125
2017-03-02T18:14:30.352008: step 32464, loss 0.160995, acc 0.90625
2017-03-02T18:14:30.419295: step 32465, loss 0.126242, acc 0.921875
2017-03-02T18:14:30.491213: step 32466, loss 0.113567, acc 0.921875
2017-03-02T18:14:30.567113: step 32467, loss 0.134133, acc 0.96875
2017-03-02T18:14:30.646997: step 32468, loss 0.21743, acc 0.84375
2017-03-02T18:14:30.723047: step 32469, loss 0.114464, acc 0.9375
2017-03-02T18:14:30.808013: step 32470, loss 0.0806684, acc 0.953125
2017-03-02T18:14:30.882104: step 32471, loss 0.0900808, acc 0.953125
2017-03-02T18:14:30.954705: step 32472, loss 0.219818, acc 0.859375
2017-03-02T18:14:31.030594: step 32473, loss 0.180199, acc 0.9375
2017-03-02T18:14:31.099223: step 32474, loss 0.111287, acc 0.96875
2017-03-02T18:14:31.171553: step 32475, loss 0.115724, acc 0.953125
2017-03-02T18:14:31.238075: step 32476, loss 0.18305, acc 0.90625
2017-03-02T18:14:31.310558: step 32477, loss 0.204799, acc 0.875
2017-03-02T18:14:31.381372: step 32478, loss 0.0863551, acc 0.96875
2017-03-02T18:14:31.466842: step 32479, loss 0.145954, acc 0.9375
2017-03-02T18:14:31.538852: step 32480, loss 0.157157, acc 0.953125
2017-03-02T18:14:31.622305: step 32481, loss 0.103801, acc 0.953125
2017-03-02T18:14:31.699613: step 32482, loss 0.218628, acc 0.859375
2017-03-02T18:14:31.771005: step 32483, loss 0.0704959, acc 0.984375
2017-03-02T18:14:31.837405: step 32484, loss 0.176459, acc 0.9375
2017-03-02T18:14:31.906643: step 32485, loss 0.261209, acc 0.890625
2017-03-02T18:14:31.974589: step 32486, loss 0.169842, acc 0.9375
2017-03-02T18:14:32.059437: step 32487, loss 0.128675, acc 0.921875
2017-03-02T18:14:32.139937: step 32488, loss 0.122769, acc 0.9375
2017-03-02T18:14:32.227045: step 32489, loss 0.18053, acc 0.921875
2017-03-02T18:14:32.302042: step 32490, loss 0.140279, acc 0.9375
2017-03-02T18:14:32.371372: step 32491, loss 0.130225, acc 0.953125
2017-03-02T18:14:32.440848: step 32492, loss 0.193565, acc 0.90625
2017-03-02T18:14:32.507556: step 32493, loss 0.101716, acc 0.96875
2017-03-02T18:14:32.580976: step 32494, loss 0.0571754, acc 0.984375
2017-03-02T18:14:32.649110: step 32495, loss 0.115709, acc 0.9375
2017-03-02T18:14:32.718714: step 32496, loss 0.17833, acc 0.921875
2017-03-02T18:14:32.794626: step 32497, loss 0.106082, acc 0.9375
2017-03-02T18:14:32.863774: step 32498, loss 0.214958, acc 0.890625
2017-03-02T18:14:32.939372: step 32499, loss 0.184915, acc 0.921875
2017-03-02T18:14:33.008084: step 32500, loss 0.165855, acc 0.9375

Evaluation:
2017-03-02T18:14:33.044604: step 32500, loss 4.45724, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32500

2017-03-02T18:14:33.492721: step 32501, loss 0.11458, acc 0.953125
2017-03-02T18:14:33.569080: step 32502, loss 0.11283, acc 0.9375
2017-03-02T18:14:33.639577: step 32503, loss 0.132717, acc 0.9375
2017-03-02T18:14:33.732143: step 32504, loss 0.0856767, acc 0.953125
2017-03-02T18:14:33.805384: step 32505, loss 0.198003, acc 0.96875
2017-03-02T18:14:33.881218: step 32506, loss 0.18345, acc 0.9375
2017-03-02T18:14:33.950842: step 32507, loss 0.11962, acc 0.921875
2017-03-02T18:14:34.020658: step 32508, loss 0.164895, acc 0.9375
2017-03-02T18:14:34.093419: step 32509, loss 0.114043, acc 0.953125
2017-03-02T18:14:34.157337: step 32510, loss 0.229407, acc 0.890625
2017-03-02T18:14:34.230328: step 32511, loss 0.163906, acc 0.921875
2017-03-02T18:14:34.300978: step 32512, loss 0.0336006, acc 1
2017-03-02T18:14:34.366418: step 32513, loss 0.149537, acc 0.9375
2017-03-02T18:14:34.436962: step 32514, loss 0.106289, acc 0.9375
2017-03-02T18:14:34.509850: step 32515, loss 0.182051, acc 0.9375
2017-03-02T18:14:34.574602: step 32516, loss 0.139998, acc 0.9375
2017-03-02T18:14:34.644019: step 32517, loss 0.277075, acc 0.890625
2017-03-02T18:14:34.716469: step 32518, loss 0.122152, acc 0.953125
2017-03-02T18:14:34.800086: step 32519, loss 0.0936597, acc 0.9375
2017-03-02T18:14:34.878188: step 32520, loss 0.128219, acc 0.9375
2017-03-02T18:14:34.946015: step 32521, loss 0.130085, acc 0.9375
2017-03-02T18:14:35.019637: step 32522, loss 0.135227, acc 0.921875
2017-03-02T18:14:35.106796: step 32523, loss 0.30548, acc 0.90625
2017-03-02T18:14:35.178066: step 32524, loss 0.134809, acc 0.953125
2017-03-02T18:14:35.253503: step 32525, loss 0.178015, acc 0.890625
2017-03-02T18:14:35.323269: step 32526, loss 0.150334, acc 0.9375
2017-03-02T18:14:35.395054: step 32527, loss 0.218245, acc 0.90625
2017-03-02T18:14:35.465693: step 32528, loss 0.104157, acc 0.953125
2017-03-02T18:14:35.536821: step 32529, loss 0.12827, acc 0.9375
2017-03-02T18:14:35.612352: step 32530, loss 0.180916, acc 0.90625
2017-03-02T18:14:35.697746: step 32531, loss 0.211901, acc 0.859375
2017-03-02T18:14:35.770448: step 32532, loss 0.213962, acc 0.890625
2017-03-02T18:14:35.857687: step 32533, loss 0.272519, acc 0.875
2017-03-02T18:14:35.934734: step 32534, loss 0.21101, acc 0.90625
2017-03-02T18:14:36.008388: step 32535, loss 0.115054, acc 0.9375
2017-03-02T18:14:36.074557: step 32536, loss 0.08845, acc 1
2017-03-02T18:14:36.146966: step 32537, loss 0.202114, acc 0.90625
2017-03-02T18:14:36.218933: step 32538, loss 0.205971, acc 0.890625
2017-03-02T18:14:36.290469: step 32539, loss 0.147526, acc 0.9375
2017-03-02T18:14:36.365900: step 32540, loss 0.139265, acc 0.9375
2017-03-02T18:14:36.440652: step 32541, loss 0.164566, acc 0.921875
2017-03-02T18:14:36.512544: step 32542, loss 0.0610069, acc 0.984375
2017-03-02T18:14:36.584308: step 32543, loss 0.0936122, acc 0.953125
2017-03-02T18:14:36.654141: step 32544, loss 0.229214, acc 0.875
2017-03-02T18:14:36.724364: step 32545, loss 0.172987, acc 0.90625
2017-03-02T18:14:36.791434: step 32546, loss 0.087441, acc 0.9375
2017-03-02T18:14:36.866617: step 32547, loss 0.155864, acc 0.921875
2017-03-02T18:14:36.939226: step 32548, loss 0.0722783, acc 0.984375
2017-03-02T18:14:37.011725: step 32549, loss 0.137606, acc 0.90625
2017-03-02T18:14:37.081337: step 32550, loss 0.109941, acc 0.96875
2017-03-02T18:14:37.157184: step 32551, loss 0.120415, acc 0.9375
2017-03-02T18:14:37.231878: step 32552, loss 0.155687, acc 0.921875
2017-03-02T18:14:37.305643: step 32553, loss 0.120852, acc 0.953125
2017-03-02T18:14:37.371891: step 32554, loss 0.153014, acc 0.890625
2017-03-02T18:14:37.443075: step 32555, loss 0.0965573, acc 0.953125
2017-03-02T18:14:37.524100: step 32556, loss 0.0700322, acc 0.984375
2017-03-02T18:14:37.599487: step 32557, loss 0.0877391, acc 0.984375
2017-03-02T18:14:37.669771: step 32558, loss 0.150458, acc 0.9375
2017-03-02T18:14:37.742439: step 32559, loss 0.119861, acc 0.9375
2017-03-02T18:14:37.817808: step 32560, loss 0.0983516, acc 0.9375
2017-03-02T18:14:37.887486: step 32561, loss 0.0698676, acc 0.96875
2017-03-02T18:14:37.961096: step 32562, loss 0.0726562, acc 0.96875
2017-03-02T18:14:38.031359: step 32563, loss 0.167487, acc 0.953125
2017-03-02T18:14:38.108856: step 32564, loss 0.112034, acc 0.953125
2017-03-02T18:14:38.179283: step 32565, loss 0.140764, acc 0.9375
2017-03-02T18:14:38.253704: step 32566, loss 0.0543149, acc 0.984375
2017-03-02T18:14:38.331120: step 32567, loss 0.0974086, acc 0.953125
2017-03-02T18:14:38.406102: step 32568, loss 0.154031, acc 0.921875
2017-03-02T18:14:38.479627: step 32569, loss 0.200801, acc 0.9375
2017-03-02T18:14:38.554597: step 32570, loss 0.105171, acc 0.9375
2017-03-02T18:14:38.626920: step 32571, loss 0.10691, acc 0.96875
2017-03-02T18:14:38.702266: step 32572, loss 0.0805533, acc 0.953125
2017-03-02T18:14:38.767527: step 32573, loss 0.128058, acc 0.9375
2017-03-02T18:14:38.835038: step 32574, loss 0.263495, acc 0.890625
2017-03-02T18:14:38.908822: step 32575, loss 0.0844562, acc 0.953125
2017-03-02T18:14:38.978415: step 32576, loss 0.16506, acc 0.9375
2017-03-02T18:14:39.045650: step 32577, loss 0.194646, acc 0.890625
2017-03-02T18:14:39.114251: step 32578, loss 0.0775223, acc 0.96875
2017-03-02T18:14:39.188176: step 32579, loss 0.172194, acc 0.890625
2017-03-02T18:14:39.257727: step 32580, loss 0.234032, acc 0.890625
2017-03-02T18:14:39.329519: step 32581, loss 0.212477, acc 0.90625
2017-03-02T18:14:39.403298: step 32582, loss 0.0884148, acc 0.953125
2017-03-02T18:14:39.477180: step 32583, loss 0.167544, acc 0.9375
2017-03-02T18:14:39.544828: step 32584, loss 0.175341, acc 0.921875
2017-03-02T18:14:39.612979: step 32585, loss 0.0769665, acc 0.96875
2017-03-02T18:14:39.688082: step 32586, loss 0.0517597, acc 0.984375
2017-03-02T18:14:39.759747: step 32587, loss 0.0837867, acc 0.96875
2017-03-02T18:14:39.832807: step 32588, loss 0.112149, acc 0.9375
2017-03-02T18:14:39.910770: step 32589, loss 0.181916, acc 0.90625
2017-03-02T18:14:39.975695: step 32590, loss 0.138255, acc 0.9375
2017-03-02T18:14:40.050538: step 32591, loss 0.228765, acc 0.875
2017-03-02T18:14:40.124867: step 32592, loss 0.171516, acc 0.921875
2017-03-02T18:14:40.193235: step 32593, loss 0.0723988, acc 0.984375
2017-03-02T18:14:40.260428: step 32594, loss 0.155942, acc 0.96875
2017-03-02T18:14:40.328492: step 32595, loss 0.116028, acc 0.9375
2017-03-02T18:14:40.398530: step 32596, loss 0.0767991, acc 0.984375
2017-03-02T18:14:40.470286: step 32597, loss 0.126456, acc 0.96875
2017-03-02T18:14:40.544890: step 32598, loss 0.102446, acc 0.9375
2017-03-02T18:14:40.613733: step 32599, loss 0.187462, acc 0.9375
2017-03-02T18:14:40.688001: step 32600, loss 0.111489, acc 0.9375

Evaluation:
2017-03-02T18:14:40.721627: step 32600, loss 4.38099, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32600

2017-03-02T18:14:41.194948: step 32601, loss 0.106356, acc 0.9375
2017-03-02T18:14:41.267412: step 32602, loss 0.127643, acc 0.921875
2017-03-02T18:14:41.336405: step 32603, loss 0.120857, acc 0.9375
2017-03-02T18:14:41.407896: step 32604, loss 0.134728, acc 0.9375
2017-03-02T18:14:41.485092: step 32605, loss 0.150176, acc 0.921875
2017-03-02T18:14:41.549595: step 32606, loss 0.113766, acc 0.921875
2017-03-02T18:14:41.618069: step 32607, loss 0.067849, acc 0.96875
2017-03-02T18:14:41.708030: step 32608, loss 0.130537, acc 0.921875
2017-03-02T18:14:41.777573: step 32609, loss 0.176788, acc 0.921875
2017-03-02T18:14:41.849712: step 32610, loss 0.0910541, acc 0.984375
2017-03-02T18:14:41.919787: step 32611, loss 0.173359, acc 0.90625
2017-03-02T18:14:41.988012: step 32612, loss 0.0884407, acc 0.96875
2017-03-02T18:14:42.063216: step 32613, loss 0.210767, acc 0.875
2017-03-02T18:14:42.137587: step 32614, loss 0.221791, acc 0.875
2017-03-02T18:14:42.213817: step 32615, loss 0.179242, acc 0.921875
2017-03-02T18:14:42.280437: step 32616, loss 0.189024, acc 0.890625
2017-03-02T18:14:42.350918: step 32617, loss 0.0858728, acc 0.96875
2017-03-02T18:14:42.426285: step 32618, loss 0.141621, acc 0.953125
2017-03-02T18:14:42.498616: step 32619, loss 0.125776, acc 0.9375
2017-03-02T18:14:42.562420: step 32620, loss 0.173781, acc 0.953125
2017-03-02T18:14:42.637791: step 32621, loss 0.226713, acc 0.875
2017-03-02T18:14:42.712309: step 32622, loss 0.0996496, acc 0.984375
2017-03-02T18:14:42.791786: step 32623, loss 0.257104, acc 0.875
2017-03-02T18:14:42.871883: step 32624, loss 0.188838, acc 0.921875
2017-03-02T18:14:42.941928: step 32625, loss 0.172108, acc 0.90625
2017-03-02T18:14:43.012576: step 32626, loss 0.197749, acc 0.890625
2017-03-02T18:14:43.081997: step 32627, loss 0.154056, acc 0.921875
2017-03-02T18:14:43.155299: step 32628, loss 0.0998399, acc 0.9375
2017-03-02T18:14:43.231451: step 32629, loss 0.174923, acc 0.921875
2017-03-02T18:14:43.309491: step 32630, loss 0.158548, acc 0.9375
2017-03-02T18:14:43.382224: step 32631, loss 0.132742, acc 0.9375
2017-03-02T18:14:43.454905: step 32632, loss 0.138589, acc 0.90625
2017-03-02T18:14:43.530584: step 32633, loss 0.0811899, acc 0.984375
2017-03-02T18:14:43.604101: step 32634, loss 0.160281, acc 0.96875
2017-03-02T18:14:43.672415: step 32635, loss 0.149279, acc 0.921875
2017-03-02T18:14:43.762686: step 32636, loss 0.149701, acc 0.953125
2017-03-02T18:14:43.838564: step 32637, loss 0.157039, acc 0.953125
2017-03-02T18:14:43.913229: step 32638, loss 0.124615, acc 0.921875
2017-03-02T18:14:43.985621: step 32639, loss 0.129695, acc 0.9375
2017-03-02T18:14:44.056476: step 32640, loss 0.102062, acc 0.9375
2017-03-02T18:14:44.129423: step 32641, loss 0.237381, acc 0.859375
2017-03-02T18:14:44.201918: step 32642, loss 0.0492434, acc 0.984375
2017-03-02T18:14:44.275092: step 32643, loss 0.112919, acc 0.953125
2017-03-02T18:14:44.340956: step 32644, loss 0.145177, acc 0.90625
2017-03-02T18:14:44.411289: step 32645, loss 0.0524298, acc 0.96875
2017-03-02T18:14:44.492612: step 32646, loss 0.0929842, acc 0.953125
2017-03-02T18:14:44.564110: step 32647, loss 0.11741, acc 0.953125
2017-03-02T18:14:44.636222: step 32648, loss 0.425192, acc 0.8125
2017-03-02T18:14:44.701862: step 32649, loss 0.18231, acc 0.921875
2017-03-02T18:14:44.777570: step 32650, loss 0.12227, acc 0.953125
2017-03-02T18:14:44.846579: step 32651, loss 0.136585, acc 0.953125
2017-03-02T18:14:44.920610: step 32652, loss 0.148878, acc 0.9375
2017-03-02T18:14:44.984188: step 32653, loss 0.129024, acc 0.953125
2017-03-02T18:14:45.051682: step 32654, loss 0.178471, acc 0.90625
2017-03-02T18:14:45.130668: step 32655, loss 0.121028, acc 0.921875
2017-03-02T18:14:45.202632: step 32656, loss 0.117069, acc 0.921875
2017-03-02T18:14:45.284155: step 32657, loss 0.0799392, acc 0.96875
2017-03-02T18:14:45.363037: step 32658, loss 0.117931, acc 0.9375
2017-03-02T18:14:45.438811: step 32659, loss 0.101018, acc 0.953125
2017-03-02T18:14:45.508117: step 32660, loss 0.129887, acc 0.953125
2017-03-02T18:14:45.581884: step 32661, loss 0.115554, acc 0.96875
2017-03-02T18:14:45.662928: step 32662, loss 0.2421, acc 0.90625
2017-03-02T18:14:45.730155: step 32663, loss 0.212163, acc 0.890625
2017-03-02T18:14:45.797329: step 32664, loss 0.106085, acc 0.9375
2017-03-02T18:14:45.869114: step 32665, loss 0.204658, acc 0.890625
2017-03-02T18:14:45.952793: step 32666, loss 0.150945, acc 0.921875
2017-03-02T18:14:46.023003: step 32667, loss 0.223184, acc 0.890625
2017-03-02T18:14:46.108295: step 32668, loss 0.0724159, acc 0.96875
2017-03-02T18:14:46.180485: step 32669, loss 0.157099, acc 0.9375
2017-03-02T18:14:46.252408: step 32670, loss 0.186037, acc 0.9375
2017-03-02T18:14:46.329024: step 32671, loss 0.150489, acc 0.9375
2017-03-02T18:14:46.396704: step 32672, loss 0.0531834, acc 0.984375
2017-03-02T18:14:46.468478: step 32673, loss 0.12923, acc 0.921875
2017-03-02T18:14:46.540319: step 32674, loss 0.151733, acc 0.9375
2017-03-02T18:14:46.609891: step 32675, loss 0.175668, acc 0.9375
2017-03-02T18:14:46.682706: step 32676, loss 0.204705, acc 0.921875
2017-03-02T18:14:46.756370: step 32677, loss 0.0910331, acc 0.984375
2017-03-02T18:14:46.828435: step 32678, loss 0.128238, acc 0.953125
2017-03-02T18:14:46.904988: step 32679, loss 0.252833, acc 0.890625
2017-03-02T18:14:46.974751: step 32680, loss 0.154167, acc 0.9375
2017-03-02T18:14:47.043251: step 32681, loss 0.0874754, acc 0.953125
2017-03-02T18:14:47.119364: step 32682, loss 0.14605, acc 0.921875
2017-03-02T18:14:47.188776: step 32683, loss 0.0640743, acc 0.984375
2017-03-02T18:14:47.265637: step 32684, loss 0.184255, acc 0.890625
2017-03-02T18:14:47.335312: step 32685, loss 0.0993192, acc 0.953125
2017-03-02T18:14:47.406185: step 32686, loss 0.183959, acc 0.890625
2017-03-02T18:14:47.481637: step 32687, loss 0.187266, acc 0.9375
2017-03-02T18:14:47.558740: step 32688, loss 0.0977467, acc 0.9375
2017-03-02T18:14:47.629264: step 32689, loss 0.205756, acc 0.90625
2017-03-02T18:14:47.702478: step 32690, loss 0.171735, acc 0.890625
2017-03-02T18:14:47.773230: step 32691, loss 0.166508, acc 0.921875
2017-03-02T18:14:47.838828: step 32692, loss 0.160045, acc 0.9375
2017-03-02T18:14:47.906865: step 32693, loss 0.149204, acc 0.921875
2017-03-02T18:14:47.980637: step 32694, loss 0.245049, acc 0.90625
2017-03-02T18:14:48.053271: step 32695, loss 0.215366, acc 0.9375
2017-03-02T18:14:48.127666: step 32696, loss 0.169907, acc 0.921875
2017-03-02T18:14:48.198531: step 32697, loss 0.26775, acc 0.859375
2017-03-02T18:14:48.268626: step 32698, loss 0.18321, acc 0.9375
2017-03-02T18:14:48.341760: step 32699, loss 0.206315, acc 0.890625
2017-03-02T18:14:48.415311: step 32700, loss 0.154279, acc 0.890625

Evaluation:
2017-03-02T18:14:48.440231: step 32700, loss 4.51942, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32700

2017-03-02T18:14:48.893448: step 32701, loss 0.229322, acc 0.9375
2017-03-02T18:14:48.966865: step 32702, loss 0.107548, acc 0.96875
2017-03-02T18:14:49.038626: step 32703, loss 0.149242, acc 0.9375
2017-03-02T18:14:49.107885: step 32704, loss 0.114623, acc 0.9375
2017-03-02T18:14:49.175005: step 32705, loss 0.163301, acc 0.921875
2017-03-02T18:14:49.245831: step 32706, loss 0.127034, acc 0.9375
2017-03-02T18:14:49.321537: step 32707, loss 0.242495, acc 0.859375
2017-03-02T18:14:49.401655: step 32708, loss 0.181012, acc 0.875
2017-03-02T18:14:49.473320: step 32709, loss 0.195943, acc 0.921875
2017-03-02T18:14:49.563950: step 32710, loss 0.155971, acc 0.921875
2017-03-02T18:14:49.635844: step 32711, loss 0.201197, acc 0.875
2017-03-02T18:14:49.705501: step 32712, loss 0.0448143, acc 1
2017-03-02T18:14:49.786578: step 32713, loss 0.122749, acc 0.953125
2017-03-02T18:14:49.870785: step 32714, loss 0.101757, acc 0.953125
2017-03-02T18:14:49.932986: step 32715, loss 0.157518, acc 0.953125
2017-03-02T18:14:50.006361: step 32716, loss 0.166103, acc 0.90625
2017-03-02T18:14:50.080758: step 32717, loss 0.111618, acc 0.953125
2017-03-02T18:14:50.152859: step 32718, loss 0.0748421, acc 0.96875
2017-03-02T18:14:50.228798: step 32719, loss 0.135218, acc 0.96875
2017-03-02T18:14:50.299894: step 32720, loss 0.132147, acc 0.921875
2017-03-02T18:14:50.372470: step 32721, loss 0.103017, acc 0.9375
2017-03-02T18:14:50.450816: step 32722, loss 0.120702, acc 0.953125
2017-03-02T18:14:50.525939: step 32723, loss 0.197329, acc 0.9375
2017-03-02T18:14:50.594654: step 32724, loss 0.198828, acc 0.90625
2017-03-02T18:14:50.663817: step 32725, loss 0.0718821, acc 0.96875
2017-03-02T18:14:50.736166: step 32726, loss 0.215332, acc 0.890625
2017-03-02T18:14:50.810777: step 32727, loss 0.11495, acc 0.96875
2017-03-02T18:14:50.882089: step 32728, loss 0.237771, acc 0.90625
2017-03-02T18:14:50.954704: step 32729, loss 0.185315, acc 0.921875
2017-03-02T18:14:51.026462: step 32730, loss 0.145604, acc 0.984375
2017-03-02T18:14:51.099014: step 32731, loss 0.0676116, acc 0.96875
2017-03-02T18:14:51.177207: step 32732, loss 0.654888, acc 0.75
2017-03-02T18:14:51.244913: step 32733, loss 0.0771022, acc 0.953125
2017-03-02T18:14:51.314248: step 32734, loss 0.168044, acc 0.890625
2017-03-02T18:14:51.387767: step 32735, loss 0.113805, acc 0.953125
2017-03-02T18:14:51.460749: step 32736, loss 0.112056, acc 0.953125
2017-03-02T18:14:51.532383: step 32737, loss 0.108743, acc 0.96875
2017-03-02T18:14:51.604728: step 32738, loss 0.0577804, acc 0.96875
2017-03-02T18:14:51.678102: step 32739, loss 0.15543, acc 0.96875
2017-03-02T18:14:51.747040: step 32740, loss 0.171249, acc 0.890625
2017-03-02T18:14:51.814013: step 32741, loss 0.106609, acc 0.921875
2017-03-02T18:14:51.884214: step 32742, loss 0.135579, acc 0.9375
2017-03-02T18:14:51.960894: step 32743, loss 0.187646, acc 0.921875
2017-03-02T18:14:52.027956: step 32744, loss 0.176911, acc 0.875
2017-03-02T18:14:52.103318: step 32745, loss 0.175358, acc 0.90625
2017-03-02T18:14:52.181541: step 32746, loss 0.147743, acc 0.90625
2017-03-02T18:14:52.251889: step 32747, loss 0.237424, acc 0.90625
2017-03-02T18:14:52.335399: step 32748, loss 0.143421, acc 0.9375
2017-03-02T18:14:52.408659: step 32749, loss 0.219328, acc 0.9375
2017-03-02T18:14:52.478506: step 32750, loss 0.244014, acc 0.90625
2017-03-02T18:14:52.550285: step 32751, loss 0.155545, acc 0.9375
2017-03-02T18:14:52.620252: step 32752, loss 0.092641, acc 0.9375
2017-03-02T18:14:52.689241: step 32753, loss 0.0638089, acc 0.96875
2017-03-02T18:14:52.763003: step 32754, loss 0.1275, acc 0.984375
2017-03-02T18:14:52.841661: step 32755, loss 0.179415, acc 0.953125
2017-03-02T18:14:52.912067: step 32756, loss 0.160118, acc 0.9375
2017-03-02T18:14:52.989277: step 32757, loss 0.111803, acc 0.9375
2017-03-02T18:14:53.069450: step 32758, loss 0.174906, acc 0.890625
2017-03-02T18:14:53.143968: step 32759, loss 0.200892, acc 0.90625
2017-03-02T18:14:53.219561: step 32760, loss 0.11995, acc 0.953125
2017-03-02T18:14:53.288751: step 32761, loss 0.183013, acc 0.921875
2017-03-02T18:14:53.351812: step 32762, loss 0.249106, acc 0.875
2017-03-02T18:14:53.432226: step 32763, loss 0.150914, acc 0.921875
2017-03-02T18:14:53.503772: step 32764, loss 0.0520881, acc 0.984375
2017-03-02T18:14:53.572899: step 32765, loss 0.0803708, acc 0.953125
2017-03-02T18:14:53.642899: step 32766, loss 0.217465, acc 0.875
2017-03-02T18:14:53.714915: step 32767, loss 0.149246, acc 0.921875
2017-03-02T18:14:53.787296: step 32768, loss 0.164715, acc 0.90625
2017-03-02T18:14:53.863615: step 32769, loss 0.063088, acc 0.984375
2017-03-02T18:14:53.936871: step 32770, loss 0.059242, acc 0.96875
2017-03-02T18:14:54.005026: step 32771, loss 0.0898991, acc 0.984375
2017-03-02T18:14:54.070460: step 32772, loss 0.165139, acc 0.921875
2017-03-02T18:14:54.133880: step 32773, loss 0.316472, acc 0.859375
2017-03-02T18:14:54.209994: step 32774, loss 0.101982, acc 0.9375
2017-03-02T18:14:54.280722: step 32775, loss 0.21291, acc 0.859375
2017-03-02T18:14:54.353902: step 32776, loss 0.163935, acc 0.90625
2017-03-02T18:14:54.425140: step 32777, loss 0.22548, acc 0.875
2017-03-02T18:14:54.499684: step 32778, loss 0.0444076, acc 1
2017-03-02T18:14:54.571610: step 32779, loss 0.129538, acc 0.9375
2017-03-02T18:14:54.643543: step 32780, loss 0.144524, acc 0.9375
2017-03-02T18:14:54.714034: step 32781, loss 0.137691, acc 0.953125
2017-03-02T18:14:54.784313: step 32782, loss 0.145119, acc 0.9375
2017-03-02T18:14:54.860750: step 32783, loss 0.187832, acc 0.921875
2017-03-02T18:14:54.942395: step 32784, loss 0.0899128, acc 0.953125
2017-03-02T18:14:55.013361: step 32785, loss 0.28829, acc 0.921875
2017-03-02T18:14:55.092653: step 32786, loss 0.0928896, acc 0.984375
2017-03-02T18:14:55.172434: step 32787, loss 0.12409, acc 0.953125
2017-03-02T18:14:55.249719: step 32788, loss 0.144954, acc 0.90625
2017-03-02T18:14:55.319078: step 32789, loss 0.0931436, acc 0.953125
2017-03-02T18:14:55.395292: step 32790, loss 0.157589, acc 0.9375
2017-03-02T18:14:55.474941: step 32791, loss 0.220815, acc 0.921875
2017-03-02T18:14:55.537786: step 32792, loss 0.15396, acc 0.90625
2017-03-02T18:14:55.608922: step 32793, loss 0.143004, acc 0.953125
2017-03-02T18:14:55.677662: step 32794, loss 0.0710839, acc 0.953125
2017-03-02T18:14:55.753019: step 32795, loss 0.148042, acc 0.9375
2017-03-02T18:14:55.832847: step 32796, loss 0.145157, acc 0.9375
2017-03-02T18:14:55.899570: step 32797, loss 0.113384, acc 0.96875
2017-03-02T18:14:55.957721: step 32798, loss 0.147598, acc 0.921875
2017-03-02T18:14:56.029799: step 32799, loss 0.109511, acc 0.9375
2017-03-02T18:14:56.095001: step 32800, loss 0.184674, acc 0.921875

Evaluation:
2017-03-02T18:14:56.128990: step 32800, loss 4.45313, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32800

2017-03-02T18:14:56.553611: step 32801, loss 0.272714, acc 0.890625
2017-03-02T18:14:56.621029: step 32802, loss 0.122881, acc 0.9375
2017-03-02T18:14:56.699224: step 32803, loss 0.305804, acc 0.828125
2017-03-02T18:14:56.766148: step 32804, loss 0.161859, acc 0.9375
2017-03-02T18:14:56.835396: step 32805, loss 0.0799249, acc 0.953125
2017-03-02T18:14:56.911347: step 32806, loss 0.121674, acc 0.953125
2017-03-02T18:14:56.993486: step 32807, loss 0.140019, acc 0.890625
2017-03-02T18:14:57.071114: step 32808, loss 0.1966, acc 0.90625
2017-03-02T18:14:57.143733: step 32809, loss 0.117893, acc 0.9375
2017-03-02T18:14:57.220511: step 32810, loss 0.163592, acc 0.921875
2017-03-02T18:14:57.291660: step 32811, loss 0.0343099, acc 1
2017-03-02T18:14:57.365414: step 32812, loss 0.320055, acc 0.8125
2017-03-02T18:14:57.430334: step 32813, loss 0.172396, acc 0.90625
2017-03-02T18:14:57.501659: step 32814, loss 0.0745694, acc 0.96875
2017-03-02T18:14:57.572097: step 32815, loss 0.0547301, acc 0.96875
2017-03-02T18:14:57.642374: step 32816, loss 0.196345, acc 0.875
2017-03-02T18:14:57.716342: step 32817, loss 0.214662, acc 0.921875
2017-03-02T18:14:57.786122: step 32818, loss 0.176538, acc 0.921875
2017-03-02T18:14:57.858023: step 32819, loss 0.126586, acc 0.9375
2017-03-02T18:14:57.928211: step 32820, loss 0.177467, acc 0.921875
2017-03-02T18:14:58.002472: step 32821, loss 0.120449, acc 0.953125
2017-03-02T18:14:58.078765: step 32822, loss 0.197535, acc 0.875
2017-03-02T18:14:58.150251: step 32823, loss 0.0667603, acc 0.96875
2017-03-02T18:14:58.228059: step 32824, loss 0.12834, acc 0.953125
2017-03-02T18:14:58.295420: step 32825, loss 0.0885539, acc 0.9375
2017-03-02T18:14:58.369496: step 32826, loss 0.14456, acc 0.9375
2017-03-02T18:14:58.438336: step 32827, loss 0.16456, acc 0.9375
2017-03-02T18:14:58.514368: step 32828, loss 0.264313, acc 0.875
2017-03-02T18:14:58.586750: step 32829, loss 0.100494, acc 0.953125
2017-03-02T18:14:58.657360: step 32830, loss 0.124686, acc 0.953125
2017-03-02T18:14:58.732578: step 32831, loss 0.0530885, acc 0.984375
2017-03-02T18:14:58.804192: step 32832, loss 0.170484, acc 0.921875
2017-03-02T18:14:58.873129: step 32833, loss 0.0667682, acc 0.984375
2017-03-02T18:14:58.945882: step 32834, loss 0.159372, acc 0.90625
2017-03-02T18:14:59.018910: step 32835, loss 0.182176, acc 0.890625
2017-03-02T18:14:59.085445: step 32836, loss 0.11514, acc 0.953125
2017-03-02T18:14:59.158471: step 32837, loss 0.14094, acc 0.921875
2017-03-02T18:14:59.228315: step 32838, loss 0.16092, acc 0.9375
2017-03-02T18:14:59.305992: step 32839, loss 0.0443339, acc 0.96875
2017-03-02T18:14:59.383644: step 32840, loss 0.149734, acc 0.953125
2017-03-02T18:14:59.453397: step 32841, loss 0.192984, acc 0.921875
2017-03-02T18:14:59.525362: step 32842, loss 0.206634, acc 0.890625
2017-03-02T18:14:59.592902: step 32843, loss 0.0710867, acc 0.96875
2017-03-02T18:14:59.668506: step 32844, loss 0.132301, acc 0.953125
2017-03-02T18:14:59.750200: step 32845, loss 0.1251, acc 0.9375
2017-03-02T18:14:59.823888: step 32846, loss 0.113566, acc 0.96875
2017-03-02T18:14:59.894646: step 32847, loss 0.131647, acc 0.9375
2017-03-02T18:14:59.967707: step 32848, loss 0.109494, acc 0.9375
2017-03-02T18:15:00.039379: step 32849, loss 0.1283, acc 0.953125
2017-03-02T18:15:00.111278: step 32850, loss 0.0402996, acc 0.984375
2017-03-02T18:15:00.176236: step 32851, loss 0.113383, acc 0.96875
2017-03-02T18:15:00.241001: step 32852, loss 0.0657716, acc 0.96875
2017-03-02T18:15:00.309020: step 32853, loss 0.145349, acc 0.921875
2017-03-02T18:15:00.378991: step 32854, loss 0.201483, acc 0.90625
2017-03-02T18:15:00.456531: step 32855, loss 0.0607209, acc 0.984375
2017-03-02T18:15:00.529274: step 32856, loss 0.199046, acc 0.90625
2017-03-02T18:15:00.615682: step 32857, loss 0.120633, acc 0.921875
2017-03-02T18:15:00.689391: step 32858, loss 0.132041, acc 0.953125
2017-03-02T18:15:00.759211: step 32859, loss 0.202245, acc 0.96875
2017-03-02T18:15:00.842627: step 32860, loss 0.133893, acc 0.9375
2017-03-02T18:15:00.921584: step 32861, loss 0.124572, acc 0.953125
2017-03-02T18:15:00.988848: step 32862, loss 0.189678, acc 0.90625
2017-03-02T18:15:01.061198: step 32863, loss 0.0936155, acc 0.921875
2017-03-02T18:15:01.132030: step 32864, loss 0.148959, acc 0.921875
2017-03-02T18:15:01.213344: step 32865, loss 0.203988, acc 0.921875
2017-03-02T18:15:01.286948: step 32866, loss 0.196528, acc 0.90625
2017-03-02T18:15:01.362750: step 32867, loss 0.147379, acc 0.9375
2017-03-02T18:15:01.432202: step 32868, loss 0.10867, acc 0.9375
2017-03-02T18:15:01.511835: step 32869, loss 0.0530705, acc 0.96875
2017-03-02T18:15:01.573693: step 32870, loss 0.111262, acc 0.96875
2017-03-02T18:15:01.637366: step 32871, loss 0.134085, acc 0.96875
2017-03-02T18:15:01.701391: step 32872, loss 0.093616, acc 0.9375
2017-03-02T18:15:01.778276: step 32873, loss 0.196749, acc 0.890625
2017-03-02T18:15:01.844736: step 32874, loss 0.0677171, acc 0.96875
2017-03-02T18:15:01.943701: step 32875, loss 0.103379, acc 0.96875
2017-03-02T18:15:02.024839: step 32876, loss 0.052581, acc 0.96875
2017-03-02T18:15:02.094484: step 32877, loss 0.169958, acc 0.953125
2017-03-02T18:15:02.161891: step 32878, loss 0.141932, acc 0.90625
2017-03-02T18:15:02.231375: step 32879, loss 0.191422, acc 0.890625
2017-03-02T18:15:02.301488: step 32880, loss 0.0689618, acc 0.953125
2017-03-02T18:15:02.369706: step 32881, loss 0.193974, acc 0.9375
2017-03-02T18:15:02.447675: step 32882, loss 0.190988, acc 0.921875
2017-03-02T18:15:02.523163: step 32883, loss 0.173663, acc 0.90625
2017-03-02T18:15:02.594624: step 32884, loss 0.328752, acc 0.828125
2017-03-02T18:15:02.658626: step 32885, loss 0.102914, acc 0.921875
2017-03-02T18:15:02.743479: step 32886, loss 0.144954, acc 0.921875
2017-03-02T18:15:02.815296: step 32887, loss 0.318038, acc 0.84375
2017-03-02T18:15:02.890303: step 32888, loss 0.0731063, acc 0.96875
2017-03-02T18:15:02.963590: step 32889, loss 0.0536758, acc 0.96875
2017-03-02T18:15:03.034775: step 32890, loss 0.142027, acc 0.921875
2017-03-02T18:15:03.105122: step 32891, loss 0.11542, acc 0.96875
2017-03-02T18:15:03.178048: step 32892, loss 0.111219, acc 0.96875
2017-03-02T18:15:03.255908: step 32893, loss 0.204814, acc 0.9375
2017-03-02T18:15:03.325283: step 32894, loss 0.112601, acc 0.921875
2017-03-02T18:15:03.402714: step 32895, loss 0.156325, acc 0.90625
2017-03-02T18:15:03.473455: step 32896, loss 0.120969, acc 0.96875
2017-03-02T18:15:03.542500: step 32897, loss 0.149666, acc 0.90625
2017-03-02T18:15:03.628342: step 32898, loss 0.0328882, acc 1
2017-03-02T18:15:03.698657: step 32899, loss 0.141331, acc 0.921875
2017-03-02T18:15:03.769466: step 32900, loss 0.147999, acc 0.9375

Evaluation:
2017-03-02T18:15:03.806987: step 32900, loss 4.44852, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-32900

2017-03-02T18:15:05.563943: step 32901, loss 0.135593, acc 0.984375
2017-03-02T18:15:05.638442: step 32902, loss 0.108265, acc 0.9375
2017-03-02T18:15:05.709400: step 32903, loss 0.185852, acc 0.921875
2017-03-02T18:15:05.786599: step 32904, loss 0.0785387, acc 0.96875
2017-03-02T18:15:05.878686: step 32905, loss 0.157544, acc 0.921875
2017-03-02T18:15:05.948748: step 32906, loss 0.22326, acc 0.90625
2017-03-02T18:15:06.018125: step 32907, loss 0.156786, acc 0.921875
2017-03-02T18:15:06.087293: step 32908, loss 0.147162, acc 0.953125
2017-03-02T18:15:06.169848: step 32909, loss 0.24687, acc 0.875
2017-03-02T18:15:06.244235: step 32910, loss 0.163396, acc 0.921875
2017-03-02T18:15:06.321545: step 32911, loss 0.167737, acc 0.90625
2017-03-02T18:15:06.389200: step 32912, loss 0.201081, acc 0.921875
2017-03-02T18:15:06.461414: step 32913, loss 0.13408, acc 0.9375
2017-03-02T18:15:06.536585: step 32914, loss 0.130986, acc 0.953125
2017-03-02T18:15:06.607506: step 32915, loss 0.170659, acc 0.890625
2017-03-02T18:15:06.676613: step 32916, loss 0.0722072, acc 0.96875
2017-03-02T18:15:06.745449: step 32917, loss 0.217704, acc 0.921875
2017-03-02T18:15:06.818702: step 32918, loss 0.218491, acc 0.90625
2017-03-02T18:15:06.886627: step 32919, loss 0.137077, acc 0.9375
2017-03-02T18:15:06.958412: step 32920, loss 0.169518, acc 0.890625
2017-03-02T18:15:07.027751: step 32921, loss 0.2244, acc 0.921875
2017-03-02T18:15:07.095922: step 32922, loss 0.0881447, acc 0.96875
2017-03-02T18:15:07.175486: step 32923, loss 0.132731, acc 0.9375
2017-03-02T18:15:07.251319: step 32924, loss 0.0777256, acc 0.96875
2017-03-02T18:15:07.328146: step 32925, loss 0.0938391, acc 0.96875
2017-03-02T18:15:07.400695: step 32926, loss 0.059802, acc 0.984375
2017-03-02T18:15:07.479381: step 32927, loss 0.195576, acc 0.9375
2017-03-02T18:15:07.550524: step 32928, loss 0.107076, acc 1
2017-03-02T18:15:07.635344: step 32929, loss 0.107759, acc 0.921875
2017-03-02T18:15:07.712497: step 32930, loss 0.133609, acc 0.953125
2017-03-02T18:15:07.803520: step 32931, loss 0.0845042, acc 0.96875
2017-03-02T18:15:07.876698: step 32932, loss 0.0880404, acc 0.984375
2017-03-02T18:15:07.946548: step 32933, loss 0.122077, acc 0.90625
2017-03-02T18:15:08.034942: step 32934, loss 0.132696, acc 0.921875
2017-03-02T18:15:08.106709: step 32935, loss 0.146241, acc 0.953125
2017-03-02T18:15:08.171776: step 32936, loss 0.0721677, acc 0.96875
2017-03-02T18:15:08.246280: step 32937, loss 0.322101, acc 0.875
2017-03-02T18:15:08.345847: step 32938, loss 0.0562379, acc 0.984375
2017-03-02T18:15:08.419699: step 32939, loss 0.0673506, acc 0.984375
2017-03-02T18:15:08.490435: step 32940, loss 0.188325, acc 0.921875
2017-03-02T18:15:08.562013: step 32941, loss 0.200857, acc 0.921875
2017-03-02T18:15:08.633149: step 32942, loss 0.117955, acc 0.9375
2017-03-02T18:15:08.735487: step 32943, loss 0.246218, acc 0.921875
2017-03-02T18:15:08.805999: step 32944, loss 0.212455, acc 0.953125
2017-03-02T18:15:08.879996: step 32945, loss 0.161576, acc 0.875
2017-03-02T18:15:08.954964: step 32946, loss 0.192926, acc 0.875
2017-03-02T18:15:09.033104: step 32947, loss 0.123783, acc 0.9375
2017-03-02T18:15:09.106554: step 32948, loss 0.0773743, acc 0.96875
2017-03-02T18:15:09.171152: step 32949, loss 0.0827467, acc 0.984375
2017-03-02T18:15:09.238646: step 32950, loss 0.109462, acc 0.953125
2017-03-02T18:15:09.316655: step 32951, loss 0.181049, acc 0.921875
2017-03-02T18:15:09.390584: step 32952, loss 0.154743, acc 0.9375
2017-03-02T18:15:09.464661: step 32953, loss 0.0657145, acc 0.984375
2017-03-02T18:15:09.535118: step 32954, loss 0.109938, acc 0.953125
2017-03-02T18:15:09.608765: step 32955, loss 0.177062, acc 0.921875
2017-03-02T18:15:09.677998: step 32956, loss 0.263904, acc 0.84375
2017-03-02T18:15:09.742363: step 32957, loss 0.135903, acc 0.96875
2017-03-02T18:15:09.808837: step 32958, loss 0.189896, acc 0.890625
2017-03-02T18:15:09.878516: step 32959, loss 0.216384, acc 0.90625
2017-03-02T18:15:09.952841: step 32960, loss 0.208568, acc 0.90625
2017-03-02T18:15:10.045155: step 32961, loss 0.181771, acc 0.9375
2017-03-02T18:15:10.115107: step 32962, loss 0.108546, acc 0.96875
2017-03-02T18:15:10.190616: step 32963, loss 0.10181, acc 0.96875
2017-03-02T18:15:10.261822: step 32964, loss 0.177864, acc 0.921875
2017-03-02T18:15:10.336978: step 32965, loss 0.148067, acc 0.96875
2017-03-02T18:15:10.412341: step 32966, loss 0.143987, acc 0.921875
2017-03-02T18:15:10.480044: step 32967, loss 0.0622607, acc 0.96875
2017-03-02T18:15:10.555585: step 32968, loss 0.14779, acc 0.9375
2017-03-02T18:15:10.625391: step 32969, loss 0.0979127, acc 0.9375
2017-03-02T18:15:10.694847: step 32970, loss 0.0941401, acc 0.953125
2017-03-02T18:15:10.758632: step 32971, loss 0.120855, acc 0.921875
2017-03-02T18:15:10.820432: step 32972, loss 0.206709, acc 0.890625
2017-03-02T18:15:10.893228: step 32973, loss 0.136218, acc 0.953125
2017-03-02T18:15:10.953837: step 32974, loss 0.0991873, acc 0.96875
2017-03-02T18:15:11.032703: step 32975, loss 0.179246, acc 0.921875
2017-03-02T18:15:11.105554: step 32976, loss 0.124945, acc 0.90625
2017-03-02T18:15:11.174373: step 32977, loss 0.153239, acc 0.9375
2017-03-02T18:15:11.245294: step 32978, loss 0.170712, acc 0.921875
2017-03-02T18:15:11.317142: step 32979, loss 0.196928, acc 0.890625
2017-03-02T18:15:11.394096: step 32980, loss 0.111861, acc 0.953125
2017-03-02T18:15:11.459674: step 32981, loss 0.0970753, acc 0.90625
2017-03-02T18:15:11.534510: step 32982, loss 0.0938317, acc 0.953125
2017-03-02T18:15:11.608139: step 32983, loss 0.126186, acc 0.90625
2017-03-02T18:15:11.681357: step 32984, loss 0.192413, acc 0.90625
2017-03-02T18:15:11.753979: step 32985, loss 0.179264, acc 0.9375
2017-03-02T18:15:11.831399: step 32986, loss 0.100726, acc 0.953125
2017-03-02T18:15:11.907353: step 32987, loss 0.148124, acc 0.9375
2017-03-02T18:15:11.975988: step 32988, loss 0.0865535, acc 0.984375
2017-03-02T18:15:12.047380: step 32989, loss 0.05289, acc 0.984375
2017-03-02T18:15:12.122615: step 32990, loss 0.160416, acc 0.953125
2017-03-02T18:15:12.196524: step 32991, loss 0.0582808, acc 1
2017-03-02T18:15:12.283846: step 32992, loss 0.163607, acc 0.9375
2017-03-02T18:15:12.354109: step 32993, loss 0.203572, acc 0.921875
2017-03-02T18:15:12.426371: step 32994, loss 0.177233, acc 0.921875
2017-03-02T18:15:12.504110: step 32995, loss 0.149854, acc 0.90625
2017-03-02T18:15:12.570413: step 32996, loss 0.144363, acc 0.953125
2017-03-02T18:15:12.638644: step 32997, loss 0.141072, acc 0.953125
2017-03-02T18:15:12.703428: step 32998, loss 0.151791, acc 0.9375
2017-03-02T18:15:12.773499: step 32999, loss 0.161543, acc 0.875
2017-03-02T18:15:12.842369: step 33000, loss 0.205516, acc 0.921875

Evaluation:
2017-03-02T18:15:12.882602: step 33000, loss 4.38562, acc 0.627253

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33000

2017-03-02T18:15:13.333976: step 33001, loss 0.0886946, acc 0.953125
2017-03-02T18:15:13.413677: step 33002, loss 0.161113, acc 0.90625
2017-03-02T18:15:13.485806: step 33003, loss 0.142886, acc 0.953125
2017-03-02T18:15:13.554498: step 33004, loss 0.204663, acc 0.90625
2017-03-02T18:15:13.625372: step 33005, loss 0.110812, acc 0.984375
2017-03-02T18:15:13.698076: step 33006, loss 0.211398, acc 0.875
2017-03-02T18:15:13.776175: step 33007, loss 0.137571, acc 0.953125
2017-03-02T18:15:13.851804: step 33008, loss 0.0570741, acc 0.984375
2017-03-02T18:15:13.934145: step 33009, loss 0.178033, acc 0.9375
2017-03-02T18:15:14.010793: step 33010, loss 0.132035, acc 0.9375
2017-03-02T18:15:14.103901: step 33011, loss 0.11144, acc 0.9375
2017-03-02T18:15:14.176581: step 33012, loss 0.248232, acc 0.875
2017-03-02T18:15:14.246566: step 33013, loss 0.132935, acc 0.9375
2017-03-02T18:15:14.319385: step 33014, loss 0.0537131, acc 1
2017-03-02T18:15:14.392226: step 33015, loss 0.124478, acc 0.953125
2017-03-02T18:15:14.461461: step 33016, loss 0.0926516, acc 0.9375
2017-03-02T18:15:14.536388: step 33017, loss 0.176115, acc 0.9375
2017-03-02T18:15:14.609074: step 33018, loss 0.166503, acc 0.921875
2017-03-02T18:15:14.674927: step 33019, loss 0.181743, acc 0.890625
2017-03-02T18:15:14.740188: step 33020, loss 0.152396, acc 0.921875
2017-03-02T18:15:14.819650: step 33021, loss 0.0795925, acc 0.96875
2017-03-02T18:15:14.899229: step 33022, loss 0.146808, acc 0.953125
2017-03-02T18:15:14.976154: step 33023, loss 0.05323, acc 0.96875
2017-03-02T18:15:15.048345: step 33024, loss 0.159596, acc 0.890625
2017-03-02T18:15:15.122855: step 33025, loss 0.109675, acc 0.953125
2017-03-02T18:15:15.197328: step 33026, loss 0.122745, acc 0.953125
2017-03-02T18:15:15.270832: step 33027, loss 0.0694183, acc 0.984375
2017-03-02T18:15:15.341595: step 33028, loss 0.257513, acc 0.84375
2017-03-02T18:15:15.406648: step 33029, loss 0.152793, acc 0.9375
2017-03-02T18:15:15.475196: step 33030, loss 0.190822, acc 0.921875
2017-03-02T18:15:15.550661: step 33031, loss 0.125225, acc 0.9375
2017-03-02T18:15:15.620751: step 33032, loss 0.139259, acc 0.9375
2017-03-02T18:15:15.694472: step 33033, loss 0.0874658, acc 0.953125
2017-03-02T18:15:15.761375: step 33034, loss 0.120189, acc 0.9375
2017-03-02T18:15:15.837142: step 33035, loss 0.0915902, acc 0.9375
2017-03-02T18:15:15.913232: step 33036, loss 0.0746834, acc 0.96875
2017-03-02T18:15:15.987305: step 33037, loss 0.169899, acc 0.921875
2017-03-02T18:15:16.061808: step 33038, loss 0.220575, acc 0.921875
2017-03-02T18:15:16.153032: step 33039, loss 0.140639, acc 0.921875
2017-03-02T18:15:16.229483: step 33040, loss 0.152636, acc 0.953125
2017-03-02T18:15:16.301509: step 33041, loss 0.0989393, acc 0.953125
2017-03-02T18:15:16.374164: step 33042, loss 0.18515, acc 0.90625
2017-03-02T18:15:16.447345: step 33043, loss 0.13086, acc 0.921875
2017-03-02T18:15:16.520725: step 33044, loss 0.095237, acc 0.96875
2017-03-02T18:15:16.597853: step 33045, loss 0.0565746, acc 0.984375
2017-03-02T18:15:16.671209: step 33046, loss 0.135693, acc 0.921875
2017-03-02T18:15:16.740091: step 33047, loss 0.0806477, acc 0.953125
2017-03-02T18:15:16.810894: step 33048, loss 0.146212, acc 0.9375
2017-03-02T18:15:16.885529: step 33049, loss 0.210998, acc 0.90625
2017-03-02T18:15:16.960016: step 33050, loss 0.132188, acc 0.96875
2017-03-02T18:15:17.031445: step 33051, loss 0.219492, acc 0.890625
2017-03-02T18:15:17.106353: step 33052, loss 0.114942, acc 0.9375
2017-03-02T18:15:17.192510: step 33053, loss 0.231796, acc 0.890625
2017-03-02T18:15:17.262575: step 33054, loss 0.163531, acc 0.953125
2017-03-02T18:15:17.331748: step 33055, loss 0.232834, acc 0.890625
2017-03-02T18:15:17.399625: step 33056, loss 0.218614, acc 0.90625
2017-03-02T18:15:17.462711: step 33057, loss 0.147394, acc 0.921875
2017-03-02T18:15:17.533624: step 33058, loss 0.144507, acc 0.953125
2017-03-02T18:15:17.637662: step 33059, loss 0.113302, acc 0.953125
2017-03-02T18:15:17.725742: step 33060, loss 0.247686, acc 0.890625
2017-03-02T18:15:17.796633: step 33061, loss 0.0594964, acc 1
2017-03-02T18:15:17.874149: step 33062, loss 0.115441, acc 0.9375
2017-03-02T18:15:17.949521: step 33063, loss 0.128714, acc 0.9375
2017-03-02T18:15:18.025675: step 33064, loss 0.310104, acc 0.84375
2017-03-02T18:15:18.100999: step 33065, loss 0.137323, acc 0.90625
2017-03-02T18:15:18.170318: step 33066, loss 0.232453, acc 0.90625
2017-03-02T18:15:18.243605: step 33067, loss 0.0353463, acc 0.984375
2017-03-02T18:15:18.322105: step 33068, loss 0.205602, acc 0.890625
2017-03-02T18:15:18.401712: step 33069, loss 0.207178, acc 0.890625
2017-03-02T18:15:18.474068: step 33070, loss 0.154659, acc 0.921875
2017-03-02T18:15:18.546692: step 33071, loss 0.0712126, acc 0.953125
2017-03-02T18:15:18.617727: step 33072, loss 0.157612, acc 0.9375
2017-03-02T18:15:18.689781: step 33073, loss 0.0644423, acc 1
2017-03-02T18:15:18.760820: step 33074, loss 0.0600051, acc 0.96875
2017-03-02T18:15:18.836494: step 33075, loss 0.129685, acc 0.9375
2017-03-02T18:15:18.903522: step 33076, loss 0.160248, acc 0.90625
2017-03-02T18:15:18.977583: step 33077, loss 0.264072, acc 0.875
2017-03-02T18:15:19.056032: step 33078, loss 0.0717033, acc 0.984375
2017-03-02T18:15:19.127940: step 33079, loss 0.107624, acc 0.953125
2017-03-02T18:15:19.200832: step 33080, loss 0.269748, acc 0.84375
2017-03-02T18:15:19.275571: step 33081, loss 0.127959, acc 0.9375
2017-03-02T18:15:19.352602: step 33082, loss 0.210849, acc 0.921875
2017-03-02T18:15:19.426566: step 33083, loss 0.156179, acc 0.9375
2017-03-02T18:15:19.498240: step 33084, loss 0.099428, acc 0.96875
2017-03-02T18:15:19.567969: step 33085, loss 0.15678, acc 0.890625
2017-03-02T18:15:19.650744: step 33086, loss 0.0663426, acc 0.96875
2017-03-02T18:15:19.722422: step 33087, loss 0.13569, acc 0.96875
2017-03-02T18:15:19.796050: step 33088, loss 0.127344, acc 0.9375
2017-03-02T18:15:19.874887: step 33089, loss 0.223203, acc 0.90625
2017-03-02T18:15:19.949947: step 33090, loss 0.0625136, acc 0.96875
2017-03-02T18:15:20.021474: step 33091, loss 0.131978, acc 0.9375
2017-03-02T18:15:20.092791: step 33092, loss 0.171092, acc 0.921875
2017-03-02T18:15:20.166884: step 33093, loss 0.220122, acc 0.90625
2017-03-02T18:15:20.244973: step 33094, loss 0.225461, acc 0.921875
2017-03-02T18:15:20.318996: step 33095, loss 0.091176, acc 0.953125
2017-03-02T18:15:20.393160: step 33096, loss 0.0814839, acc 0.96875
2017-03-02T18:15:20.462687: step 33097, loss 0.118768, acc 0.953125
2017-03-02T18:15:20.536479: step 33098, loss 0.0810414, acc 0.953125
2017-03-02T18:15:20.609646: step 33099, loss 0.079977, acc 0.953125
2017-03-02T18:15:20.683261: step 33100, loss 0.0943287, acc 0.9375

Evaluation:
2017-03-02T18:15:20.720917: step 33100, loss 4.45621, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33100

2017-03-02T18:15:21.166341: step 33101, loss 0.0968208, acc 0.96875
2017-03-02T18:15:21.237918: step 33102, loss 0.0666168, acc 0.953125
2017-03-02T18:15:21.308483: step 33103, loss 0.0818785, acc 0.96875
2017-03-02T18:15:21.382234: step 33104, loss 0.199329, acc 0.921875
2017-03-02T18:15:21.464155: step 33105, loss 0.167816, acc 0.921875
2017-03-02T18:15:21.543187: step 33106, loss 0.152446, acc 0.890625
2017-03-02T18:15:21.603228: step 33107, loss 0.137607, acc 0.96875
2017-03-02T18:15:21.670432: step 33108, loss 0.152319, acc 0.921875
2017-03-02T18:15:21.740552: step 33109, loss 0.134009, acc 0.9375
2017-03-02T18:15:21.810827: step 33110, loss 0.1039, acc 0.9375
2017-03-02T18:15:21.886278: step 33111, loss 0.158102, acc 0.9375
2017-03-02T18:15:21.976046: step 33112, loss 0.261547, acc 0.875
2017-03-02T18:15:22.054959: step 33113, loss 0.147646, acc 0.953125
2017-03-02T18:15:22.125349: step 33114, loss 0.143074, acc 0.90625
2017-03-02T18:15:22.204785: step 33115, loss 0.072157, acc 0.953125
2017-03-02T18:15:22.277518: step 33116, loss 0.0857572, acc 0.953125
2017-03-02T18:15:22.348972: step 33117, loss 0.255753, acc 0.859375
2017-03-02T18:15:22.424354: step 33118, loss 0.110783, acc 0.96875
2017-03-02T18:15:22.494837: step 33119, loss 0.128103, acc 0.921875
2017-03-02T18:15:22.566601: step 33120, loss 0.19462, acc 0.921875
2017-03-02T18:15:22.639918: step 33121, loss 0.0643351, acc 0.984375
2017-03-02T18:15:22.712395: step 33122, loss 0.0947068, acc 0.9375
2017-03-02T18:15:22.781803: step 33123, loss 0.225654, acc 0.859375
2017-03-02T18:15:22.854157: step 33124, loss 1.7285e-05, acc 1
2017-03-02T18:15:22.939342: step 33125, loss 0.0776753, acc 0.953125
2017-03-02T18:15:23.007995: step 33126, loss 0.246352, acc 0.921875
2017-03-02T18:15:23.079512: step 33127, loss 0.0955207, acc 0.96875
2017-03-02T18:15:23.152545: step 33128, loss 0.162212, acc 0.90625
2017-03-02T18:15:23.244588: step 33129, loss 0.11209, acc 0.9375
2017-03-02T18:15:23.315959: step 33130, loss 0.0791491, acc 0.96875
2017-03-02T18:15:23.388842: step 33131, loss 0.117765, acc 0.953125
2017-03-02T18:15:23.461491: step 33132, loss 0.087172, acc 0.96875
2017-03-02T18:15:23.535216: step 33133, loss 0.202511, acc 0.890625
2017-03-02T18:15:23.605437: step 33134, loss 0.150311, acc 0.921875
2017-03-02T18:15:23.676402: step 33135, loss 0.164979, acc 0.9375
2017-03-02T18:15:23.763498: step 33136, loss 0.0965284, acc 0.984375
2017-03-02T18:15:23.834890: step 33137, loss 0.215028, acc 0.890625
2017-03-02T18:15:23.908229: step 33138, loss 0.279968, acc 0.921875
2017-03-02T18:15:23.983828: step 33139, loss 0.2281, acc 0.890625
2017-03-02T18:15:24.051886: step 33140, loss 0.205152, acc 0.90625
2017-03-02T18:15:24.123812: step 33141, loss 0.0573728, acc 0.96875
2017-03-02T18:15:24.197661: step 33142, loss 0.336963, acc 0.921875
2017-03-02T18:15:24.264141: step 33143, loss 0.174324, acc 0.90625
2017-03-02T18:15:24.337269: step 33144, loss 0.100853, acc 0.953125
2017-03-02T18:15:24.401637: step 33145, loss 0.100388, acc 0.953125
2017-03-02T18:15:24.473646: step 33146, loss 0.1035, acc 0.984375
2017-03-02T18:15:24.545278: step 33147, loss 0.112422, acc 0.984375
2017-03-02T18:15:24.624690: step 33148, loss 0.0892435, acc 0.953125
2017-03-02T18:15:24.697703: step 33149, loss 0.0959592, acc 0.9375
2017-03-02T18:15:24.777507: step 33150, loss 0.155487, acc 0.953125
2017-03-02T18:15:24.849620: step 33151, loss 0.195319, acc 0.90625
2017-03-02T18:15:24.921726: step 33152, loss 0.067991, acc 0.984375
2017-03-02T18:15:24.988567: step 33153, loss 0.107666, acc 0.953125
2017-03-02T18:15:25.059737: step 33154, loss 0.135782, acc 0.953125
2017-03-02T18:15:25.130514: step 33155, loss 0.0808467, acc 0.984375
2017-03-02T18:15:25.203544: step 33156, loss 0.0648924, acc 0.96875
2017-03-02T18:15:25.276442: step 33157, loss 0.150826, acc 0.921875
2017-03-02T18:15:25.344913: step 33158, loss 0.148886, acc 0.953125
2017-03-02T18:15:25.424936: step 33159, loss 0.091326, acc 0.953125
2017-03-02T18:15:25.495090: step 33160, loss 0.122497, acc 0.96875
2017-03-02T18:15:25.563458: step 33161, loss 0.21613, acc 0.921875
2017-03-02T18:15:25.638588: step 33162, loss 0.0661749, acc 0.984375
2017-03-02T18:15:25.708434: step 33163, loss 0.0983981, acc 0.953125
2017-03-02T18:15:25.779171: step 33164, loss 0.0780483, acc 0.984375
2017-03-02T18:15:25.863771: step 33165, loss 0.0861595, acc 0.984375
2017-03-02T18:15:25.935615: step 33166, loss 0.172228, acc 0.953125
2017-03-02T18:15:26.005582: step 33167, loss 0.185807, acc 0.9375
2017-03-02T18:15:26.080626: step 33168, loss 0.100885, acc 0.953125
2017-03-02T18:15:26.152915: step 33169, loss 0.1937, acc 0.90625
2017-03-02T18:15:26.224931: step 33170, loss 0.112027, acc 0.9375
2017-03-02T18:15:26.297164: step 33171, loss 0.114835, acc 0.9375
2017-03-02T18:15:26.367553: step 33172, loss 0.284726, acc 0.828125
2017-03-02T18:15:26.438107: step 33173, loss 0.245885, acc 0.921875
2017-03-02T18:15:26.514799: step 33174, loss 0.0766349, acc 0.984375
2017-03-02T18:15:26.584876: step 33175, loss 0.163082, acc 0.90625
2017-03-02T18:15:26.671508: step 33176, loss 0.103318, acc 0.953125
2017-03-02T18:15:26.747419: step 33177, loss 0.113434, acc 0.9375
2017-03-02T18:15:26.819641: step 33178, loss 0.0883852, acc 0.96875
2017-03-02T18:15:26.899806: step 33179, loss 0.0962463, acc 0.96875
2017-03-02T18:15:26.975719: step 33180, loss 0.0966232, acc 0.984375
2017-03-02T18:15:27.053153: step 33181, loss 0.268974, acc 0.859375
2017-03-02T18:15:27.132477: step 33182, loss 0.0961842, acc 0.96875
2017-03-02T18:15:27.205537: step 33183, loss 0.104683, acc 0.96875
2017-03-02T18:15:27.280623: step 33184, loss 0.0640653, acc 0.984375
2017-03-02T18:15:27.353504: step 33185, loss 0.15954, acc 0.921875
2017-03-02T18:15:27.437768: step 33186, loss 0.144901, acc 0.9375
2017-03-02T18:15:27.503888: step 33187, loss 0.201206, acc 0.890625
2017-03-02T18:15:27.583695: step 33188, loss 0.186584, acc 0.890625
2017-03-02T18:15:27.648612: step 33189, loss 0.125661, acc 0.9375
2017-03-02T18:15:27.722750: step 33190, loss 0.182829, acc 0.921875
2017-03-02T18:15:27.793943: step 33191, loss 0.155168, acc 0.921875
2017-03-02T18:15:27.885686: step 33192, loss 0.176446, acc 0.890625
2017-03-02T18:15:27.954112: step 33193, loss 0.18436, acc 0.921875
2017-03-02T18:15:28.026466: step 33194, loss 0.164224, acc 0.953125
2017-03-02T18:15:28.098637: step 33195, loss 0.0594993, acc 0.96875
2017-03-02T18:15:28.171107: step 33196, loss 0.189799, acc 0.90625
2017-03-02T18:15:28.235612: step 33197, loss 0.191612, acc 0.875
2017-03-02T18:15:28.320603: step 33198, loss 0.180375, acc 0.90625
2017-03-02T18:15:28.394216: step 33199, loss 0.162459, acc 0.921875
2017-03-02T18:15:28.461386: step 33200, loss 0.118028, acc 0.9375

Evaluation:
2017-03-02T18:15:28.493459: step 33200, loss 4.50214, acc 0.633742

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33200

2017-03-02T18:15:28.941400: step 33201, loss 0.104049, acc 0.953125
2017-03-02T18:15:29.025067: step 33202, loss 0.033432, acc 0.984375
2017-03-02T18:15:29.092775: step 33203, loss 0.306919, acc 0.921875
2017-03-02T18:15:29.167542: step 33204, loss 0.129404, acc 0.9375
2017-03-02T18:15:29.247386: step 33205, loss 0.306356, acc 0.859375
2017-03-02T18:15:29.337633: step 33206, loss 0.135458, acc 0.9375
2017-03-02T18:15:29.418084: step 33207, loss 0.0985334, acc 0.9375
2017-03-02T18:15:29.489309: step 33208, loss 0.206726, acc 0.90625
2017-03-02T18:15:29.563834: step 33209, loss 0.144028, acc 0.921875
2017-03-02T18:15:29.637799: step 33210, loss 0.218962, acc 0.90625
2017-03-02T18:15:29.709066: step 33211, loss 0.0496525, acc 0.984375
2017-03-02T18:15:29.774361: step 33212, loss 0.211908, acc 0.890625
2017-03-02T18:15:29.847179: step 33213, loss 0.106109, acc 0.9375
2017-03-02T18:15:29.920783: step 33214, loss 0.142664, acc 0.9375
2017-03-02T18:15:29.998190: step 33215, loss 0.136521, acc 0.953125
2017-03-02T18:15:30.068553: step 33216, loss 0.130961, acc 0.9375
2017-03-02T18:15:30.139813: step 33217, loss 0.236756, acc 0.890625
2017-03-02T18:15:30.210192: step 33218, loss 0.243209, acc 0.890625
2017-03-02T18:15:30.284128: step 33219, loss 0.0905371, acc 0.953125
2017-03-02T18:15:30.353242: step 33220, loss 0.154036, acc 0.9375
2017-03-02T18:15:30.425705: step 33221, loss 0.302054, acc 0.90625
2017-03-02T18:15:30.492725: step 33222, loss 0.195038, acc 0.90625
2017-03-02T18:15:30.558761: step 33223, loss 0.197364, acc 0.875
2017-03-02T18:15:30.629628: step 33224, loss 0.177237, acc 0.9375
2017-03-02T18:15:30.700863: step 33225, loss 0.132035, acc 0.953125
2017-03-02T18:15:30.773748: step 33226, loss 0.139209, acc 0.96875
2017-03-02T18:15:30.844092: step 33227, loss 0.104556, acc 0.921875
2017-03-02T18:15:30.917570: step 33228, loss 0.0752348, acc 0.96875
2017-03-02T18:15:30.991781: step 33229, loss 0.238039, acc 0.875
2017-03-02T18:15:31.066203: step 33230, loss 0.165325, acc 0.90625
2017-03-02T18:15:31.133174: step 33231, loss 0.128567, acc 0.9375
2017-03-02T18:15:31.199049: step 33232, loss 0.175731, acc 0.90625
2017-03-02T18:15:31.281159: step 33233, loss 0.0889435, acc 0.984375
2017-03-02T18:15:31.375695: step 33234, loss 0.0889216, acc 0.953125
2017-03-02T18:15:31.448485: step 33235, loss 0.144774, acc 0.9375
2017-03-02T18:15:31.514076: step 33236, loss 0.154801, acc 0.921875
2017-03-02T18:15:31.589950: step 33237, loss 0.198544, acc 0.9375
2017-03-02T18:15:31.662878: step 33238, loss 0.0534255, acc 0.984375
2017-03-02T18:15:31.750726: step 33239, loss 0.175498, acc 0.953125
2017-03-02T18:15:31.816894: step 33240, loss 0.400044, acc 0.84375
2017-03-02T18:15:31.886445: step 33241, loss 0.131432, acc 0.96875
2017-03-02T18:15:31.959320: step 33242, loss 0.159455, acc 0.921875
2017-03-02T18:15:32.029910: step 33243, loss 0.174269, acc 0.9375
2017-03-02T18:15:32.101477: step 33244, loss 0.0689928, acc 0.96875
2017-03-02T18:15:32.169062: step 33245, loss 0.133517, acc 0.921875
2017-03-02T18:15:32.235968: step 33246, loss 0.136368, acc 0.9375
2017-03-02T18:15:32.308903: step 33247, loss 0.0935006, acc 0.96875
2017-03-02T18:15:32.383412: step 33248, loss 0.100263, acc 0.953125
2017-03-02T18:15:32.470389: step 33249, loss 0.114646, acc 0.953125
2017-03-02T18:15:32.539384: step 33250, loss 0.19055, acc 0.921875
2017-03-02T18:15:32.609586: step 33251, loss 0.189439, acc 0.90625
2017-03-02T18:15:32.683204: step 33252, loss 0.168527, acc 0.890625
2017-03-02T18:15:32.755018: step 33253, loss 0.0248526, acc 1
2017-03-02T18:15:32.819876: step 33254, loss 0.224575, acc 0.90625
2017-03-02T18:15:32.895089: step 33255, loss 0.135327, acc 0.9375
2017-03-02T18:15:32.970757: step 33256, loss 0.210058, acc 0.90625
2017-03-02T18:15:33.048649: step 33257, loss 0.153238, acc 0.90625
2017-03-02T18:15:33.129950: step 33258, loss 0.180431, acc 0.921875
2017-03-02T18:15:33.198202: step 33259, loss 0.247552, acc 0.875
2017-03-02T18:15:33.265155: step 33260, loss 0.0711156, acc 0.96875
2017-03-02T18:15:33.346516: step 33261, loss 0.261362, acc 0.90625
2017-03-02T18:15:33.421680: step 33262, loss 0.129475, acc 0.9375
2017-03-02T18:15:33.498561: step 33263, loss 0.104648, acc 0.953125
2017-03-02T18:15:33.568907: step 33264, loss 0.168993, acc 0.890625
2017-03-02T18:15:33.640865: step 33265, loss 0.142954, acc 0.9375
2017-03-02T18:15:33.716167: step 33266, loss 0.184148, acc 0.921875
2017-03-02T18:15:33.789274: step 33267, loss 0.10543, acc 0.9375
2017-03-02T18:15:33.852965: step 33268, loss 0.0485957, acc 0.984375
2017-03-02T18:15:33.919664: step 33269, loss 0.150624, acc 0.9375
2017-03-02T18:15:33.988295: step 33270, loss 0.188872, acc 0.9375
2017-03-02T18:15:34.060418: step 33271, loss 0.126204, acc 0.921875
2017-03-02T18:15:34.132020: step 33272, loss 0.0876919, acc 0.96875
2017-03-02T18:15:34.230533: step 33273, loss 0.0955894, acc 0.953125
2017-03-02T18:15:34.301007: step 33274, loss 0.163461, acc 0.9375
2017-03-02T18:15:34.373209: step 33275, loss 0.0634491, acc 0.96875
2017-03-02T18:15:34.449473: step 33276, loss 0.10486, acc 0.984375
2017-03-02T18:15:34.525734: step 33277, loss 0.124677, acc 0.9375
2017-03-02T18:15:34.593188: step 33278, loss 0.15802, acc 0.9375
2017-03-02T18:15:34.664906: step 33279, loss 0.114837, acc 0.921875
2017-03-02T18:15:34.741341: step 33280, loss 0.188025, acc 0.90625
2017-03-02T18:15:34.817632: step 33281, loss 0.116705, acc 0.953125
2017-03-02T18:15:34.890104: step 33282, loss 0.14636, acc 0.921875
2017-03-02T18:15:34.972559: step 33283, loss 0.226871, acc 0.90625
2017-03-02T18:15:35.041927: step 33284, loss 0.0510017, acc 0.96875
2017-03-02T18:15:35.119174: step 33285, loss 0.0815585, acc 0.953125
2017-03-02T18:15:35.198192: step 33286, loss 0.250904, acc 0.90625
2017-03-02T18:15:35.278853: step 33287, loss 0.115488, acc 0.953125
2017-03-02T18:15:35.350039: step 33288, loss 0.109328, acc 0.921875
2017-03-02T18:15:35.436877: step 33289, loss 0.163613, acc 0.921875
2017-03-02T18:15:35.513683: step 33290, loss 0.173513, acc 0.9375
2017-03-02T18:15:35.592854: step 33291, loss 0.244652, acc 0.890625
2017-03-02T18:15:35.666170: step 33292, loss 0.0997228, acc 0.96875
2017-03-02T18:15:35.743134: step 33293, loss 0.0536551, acc 0.96875
2017-03-02T18:15:35.820228: step 33294, loss 0.143688, acc 0.9375
2017-03-02T18:15:35.902993: step 33295, loss 0.232892, acc 0.90625
2017-03-02T18:15:35.969782: step 33296, loss 0.0393745, acc 1
2017-03-02T18:15:36.035898: step 33297, loss 0.105631, acc 0.9375
2017-03-02T18:15:36.109651: step 33298, loss 0.216031, acc 0.90625
2017-03-02T18:15:36.181030: step 33299, loss 0.143913, acc 0.9375
2017-03-02T18:15:36.253068: step 33300, loss 0.0950348, acc 0.96875

Evaluation:
2017-03-02T18:15:36.294654: step 33300, loss 4.49997, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33300

2017-03-02T18:15:36.742635: step 33301, loss 0.125743, acc 0.9375
2017-03-02T18:15:36.813331: step 33302, loss 0.0906122, acc 0.9375
2017-03-02T18:15:36.900696: step 33303, loss 0.123243, acc 0.953125
2017-03-02T18:15:36.972032: step 33304, loss 0.175397, acc 0.890625
2017-03-02T18:15:37.039759: step 33305, loss 0.162402, acc 0.921875
2017-03-02T18:15:37.107844: step 33306, loss 0.126193, acc 0.921875
2017-03-02T18:15:37.178065: step 33307, loss 0.0874748, acc 0.9375
2017-03-02T18:15:37.250815: step 33308, loss 0.246204, acc 0.875
2017-03-02T18:15:37.317990: step 33309, loss 0.106945, acc 0.953125
2017-03-02T18:15:37.386533: step 33310, loss 0.134434, acc 0.9375
2017-03-02T18:15:37.454514: step 33311, loss 0.105977, acc 0.9375
2017-03-02T18:15:37.535525: step 33312, loss 0.132039, acc 0.9375
2017-03-02T18:15:37.609584: step 33313, loss 0.0774187, acc 0.96875
2017-03-02T18:15:37.693646: step 33314, loss 0.189045, acc 0.921875
2017-03-02T18:15:37.757780: step 33315, loss 0.173494, acc 0.90625
2017-03-02T18:15:37.834874: step 33316, loss 0.2514, acc 0.875
2017-03-02T18:15:37.908918: step 33317, loss 0.0671542, acc 0.96875
2017-03-02T18:15:37.980309: step 33318, loss 0.0876136, acc 0.953125
2017-03-02T18:15:38.052186: step 33319, loss 0.155618, acc 0.921875
2017-03-02T18:15:38.120545: step 33320, loss 1.99967e-05, acc 1
2017-03-02T18:15:38.197243: step 33321, loss 0.102215, acc 0.9375
2017-03-02T18:15:38.274243: step 33322, loss 0.15387, acc 0.921875
2017-03-02T18:15:38.347825: step 33323, loss 0.307146, acc 0.890625
2017-03-02T18:15:38.421819: step 33324, loss 0.229175, acc 0.859375
2017-03-02T18:15:38.500445: step 33325, loss 0.12567, acc 0.96875
2017-03-02T18:15:38.571717: step 33326, loss 0.102318, acc 0.9375
2017-03-02T18:15:38.644627: step 33327, loss 0.123181, acc 0.96875
2017-03-02T18:15:38.726045: step 33328, loss 0.141848, acc 0.953125
2017-03-02T18:15:38.791622: step 33329, loss 0.117654, acc 0.953125
2017-03-02T18:15:38.857319: step 33330, loss 0.169076, acc 0.90625
2017-03-02T18:15:38.932220: step 33331, loss 0.115298, acc 0.96875
2017-03-02T18:15:39.005200: step 33332, loss 0.143068, acc 0.9375
2017-03-02T18:15:39.108703: step 33333, loss 0.0845374, acc 0.96875
2017-03-02T18:15:39.176009: step 33334, loss 0.0854309, acc 0.96875
2017-03-02T18:15:39.247103: step 33335, loss 0.0984984, acc 0.953125
2017-03-02T18:15:39.318217: step 33336, loss 0.0947209, acc 0.953125
2017-03-02T18:15:39.392047: step 33337, loss 0.217013, acc 0.875
2017-03-02T18:15:39.457558: step 33338, loss 0.128296, acc 0.953125
2017-03-02T18:15:39.531366: step 33339, loss 0.18433, acc 0.90625
2017-03-02T18:15:39.608705: step 33340, loss 0.125217, acc 0.96875
2017-03-02T18:15:39.676087: step 33341, loss 0.0949323, acc 0.9375
2017-03-02T18:15:39.750707: step 33342, loss 0.17613, acc 0.921875
2017-03-02T18:15:39.832593: step 33343, loss 0.123635, acc 0.9375
2017-03-02T18:15:39.900573: step 33344, loss 0.18331, acc 0.953125
2017-03-02T18:15:39.974048: step 33345, loss 0.0955567, acc 0.953125
2017-03-02T18:15:40.048145: step 33346, loss 0.081999, acc 0.96875
2017-03-02T18:15:40.118639: step 33347, loss 0.138082, acc 0.921875
2017-03-02T18:15:40.182805: step 33348, loss 0.0738774, acc 0.96875
2017-03-02T18:15:40.251203: step 33349, loss 0.2364, acc 0.890625
2017-03-02T18:15:40.319337: step 33350, loss 0.100597, acc 0.96875
2017-03-02T18:15:40.397949: step 33351, loss 0.101037, acc 0.984375
2017-03-02T18:15:40.468589: step 33352, loss 0.192943, acc 0.921875
2017-03-02T18:15:40.549279: step 33353, loss 0.176249, acc 0.9375
2017-03-02T18:15:40.618409: step 33354, loss 0.244339, acc 0.890625
2017-03-02T18:15:40.690875: step 33355, loss 0.14271, acc 0.9375
2017-03-02T18:15:40.767623: step 33356, loss 0.0976566, acc 0.953125
2017-03-02T18:15:40.837466: step 33357, loss 0.150318, acc 0.953125
2017-03-02T18:15:40.901967: step 33358, loss 0.104078, acc 0.984375
2017-03-02T18:15:40.982376: step 33359, loss 0.170564, acc 0.921875
2017-03-02T18:15:41.054121: step 33360, loss 0.133724, acc 0.953125
2017-03-02T18:15:41.126176: step 33361, loss 0.0617613, acc 0.984375
2017-03-02T18:15:41.201156: step 33362, loss 0.195463, acc 0.921875
2017-03-02T18:15:41.270887: step 33363, loss 0.120635, acc 0.9375
2017-03-02T18:15:41.342264: step 33364, loss 0.0999266, acc 0.96875
2017-03-02T18:15:41.413726: step 33365, loss 0.18837, acc 0.921875
2017-03-02T18:15:41.485976: step 33366, loss 0.225662, acc 0.859375
2017-03-02T18:15:41.550657: step 33367, loss 0.0930019, acc 0.984375
2017-03-02T18:15:41.618890: step 33368, loss 0.156157, acc 0.921875
2017-03-02T18:15:41.707523: step 33369, loss 0.114215, acc 0.9375
2017-03-02T18:15:41.785337: step 33370, loss 0.131924, acc 0.96875
2017-03-02T18:15:41.859239: step 33371, loss 0.100258, acc 0.9375
2017-03-02T18:15:41.938780: step 33372, loss 0.217946, acc 0.90625
2017-03-02T18:15:42.016782: step 33373, loss 0.0675959, acc 0.984375
2017-03-02T18:15:42.095440: step 33374, loss 0.130328, acc 0.953125
2017-03-02T18:15:42.171487: step 33375, loss 0.113887, acc 0.9375
2017-03-02T18:15:42.235957: step 33376, loss 0.219328, acc 0.90625
2017-03-02T18:15:42.308449: step 33377, loss 0.0645608, acc 0.96875
2017-03-02T18:15:42.376363: step 33378, loss 0.176983, acc 0.953125
2017-03-02T18:15:42.448157: step 33379, loss 0.0864758, acc 0.96875
2017-03-02T18:15:42.532173: step 33380, loss 0.126048, acc 0.953125
2017-03-02T18:15:42.603243: step 33381, loss 0.12609, acc 0.921875
2017-03-02T18:15:42.680201: step 33382, loss 0.141394, acc 0.921875
2017-03-02T18:15:42.749118: step 33383, loss 0.128555, acc 0.953125
2017-03-02T18:15:42.826394: step 33384, loss 0.171467, acc 0.90625
2017-03-02T18:15:42.906777: step 33385, loss 0.146657, acc 0.90625
2017-03-02T18:15:42.979045: step 33386, loss 0.101464, acc 0.953125
2017-03-02T18:15:43.049977: step 33387, loss 0.216149, acc 0.921875
2017-03-02T18:15:43.127509: step 33388, loss 0.0993656, acc 0.9375
2017-03-02T18:15:43.202575: step 33389, loss 0.188331, acc 0.9375
2017-03-02T18:15:43.277378: step 33390, loss 0.0941407, acc 0.96875
2017-03-02T18:15:43.351982: step 33391, loss 0.0858847, acc 0.96875
2017-03-02T18:15:43.425047: step 33392, loss 0.121457, acc 0.9375
2017-03-02T18:15:43.489763: step 33393, loss 0.176627, acc 0.90625
2017-03-02T18:15:43.573109: step 33394, loss 0.123471, acc 0.9375
2017-03-02T18:15:43.645386: step 33395, loss 0.168309, acc 0.859375
2017-03-02T18:15:43.739590: step 33396, loss 0.202651, acc 0.921875
2017-03-02T18:15:43.814952: step 33397, loss 0.0862655, acc 0.96875
2017-03-02T18:15:43.885521: step 33398, loss 0.0896463, acc 0.984375
2017-03-02T18:15:43.955327: step 33399, loss 0.0500253, acc 0.984375
2017-03-02T18:15:44.031192: step 33400, loss 0.180493, acc 0.90625

Evaluation:
2017-03-02T18:15:44.067860: step 33400, loss 4.49949, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33400

2017-03-02T18:15:44.531844: step 33401, loss 0.095941, acc 0.953125
2017-03-02T18:15:44.606218: step 33402, loss 0.174057, acc 0.890625
2017-03-02T18:15:44.679849: step 33403, loss 0.0968265, acc 0.9375
2017-03-02T18:15:44.751213: step 33404, loss 0.0313134, acc 0.984375
2017-03-02T18:15:44.818598: step 33405, loss 0.131307, acc 0.9375
2017-03-02T18:15:44.892863: step 33406, loss 0.081276, acc 0.96875
2017-03-02T18:15:44.962922: step 33407, loss 0.14493, acc 0.96875
2017-03-02T18:15:45.030830: step 33408, loss 0.150667, acc 0.9375
2017-03-02T18:15:45.107024: step 33409, loss 0.128334, acc 0.9375
2017-03-02T18:15:45.183169: step 33410, loss 0.158188, acc 0.921875
2017-03-02T18:15:45.261849: step 33411, loss 0.0945232, acc 0.953125
2017-03-02T18:15:45.342749: step 33412, loss 0.260304, acc 0.890625
2017-03-02T18:15:45.418357: step 33413, loss 0.0690904, acc 0.96875
2017-03-02T18:15:45.494964: step 33414, loss 0.191575, acc 0.875
2017-03-02T18:15:45.568491: step 33415, loss 0.0821922, acc 0.96875
2017-03-02T18:15:45.642207: step 33416, loss 0.204278, acc 0.890625
2017-03-02T18:15:45.709926: step 33417, loss 0.0938434, acc 0.953125
2017-03-02T18:15:45.782568: step 33418, loss 0.196431, acc 0.90625
2017-03-02T18:15:45.856833: step 33419, loss 0.207944, acc 0.921875
2017-03-02T18:15:45.941609: step 33420, loss 0.115251, acc 0.953125
2017-03-02T18:15:46.014018: step 33421, loss 0.0826121, acc 0.96875
2017-03-02T18:15:46.086187: step 33422, loss 0.0695479, acc 0.984375
2017-03-02T18:15:46.158887: step 33423, loss 0.205537, acc 0.890625
2017-03-02T18:15:46.234700: step 33424, loss 0.154357, acc 0.90625
2017-03-02T18:15:46.319262: step 33425, loss 0.159676, acc 0.921875
2017-03-02T18:15:46.386770: step 33426, loss 0.181323, acc 0.921875
2017-03-02T18:15:46.454701: step 33427, loss 0.149797, acc 0.953125
2017-03-02T18:15:46.528731: step 33428, loss 0.139089, acc 0.921875
2017-03-02T18:15:46.600158: step 33429, loss 0.184926, acc 0.9375
2017-03-02T18:15:46.676577: step 33430, loss 0.145376, acc 0.921875
2017-03-02T18:15:46.750502: step 33431, loss 0.236791, acc 0.859375
2017-03-02T18:15:46.824501: step 33432, loss 0.157837, acc 0.921875
2017-03-02T18:15:46.900146: step 33433, loss 0.134512, acc 0.984375
2017-03-02T18:15:46.978924: step 33434, loss 0.15786, acc 0.921875
2017-03-02T18:15:47.043119: step 33435, loss 0.134166, acc 0.953125
2017-03-02T18:15:47.109272: step 33436, loss 0.101123, acc 0.953125
2017-03-02T18:15:47.179125: step 33437, loss 0.150065, acc 0.90625
2017-03-02T18:15:47.249433: step 33438, loss 0.110335, acc 0.96875
2017-03-02T18:15:47.319138: step 33439, loss 0.0917002, acc 0.9375
2017-03-02T18:15:47.390502: step 33440, loss 0.220816, acc 0.859375
2017-03-02T18:15:47.462125: step 33441, loss 0.212059, acc 0.890625
2017-03-02T18:15:47.543727: step 33442, loss 0.0859663, acc 0.9375
2017-03-02T18:15:47.618438: step 33443, loss 0.0974833, acc 0.953125
2017-03-02T18:15:47.696732: step 33444, loss 0.175146, acc 0.921875
2017-03-02T18:15:47.761203: step 33445, loss 0.123843, acc 0.96875
2017-03-02T18:15:47.827095: step 33446, loss 0.127534, acc 0.96875
2017-03-02T18:15:47.896968: step 33447, loss 0.140091, acc 0.9375
2017-03-02T18:15:47.962939: step 33448, loss 0.173639, acc 0.9375
2017-03-02T18:15:48.033904: step 33449, loss 0.137037, acc 0.953125
2017-03-02T18:15:48.109281: step 33450, loss 0.106823, acc 0.96875
2017-03-02T18:15:48.183714: step 33451, loss 0.200118, acc 0.90625
2017-03-02T18:15:48.253773: step 33452, loss 0.0991579, acc 0.9375
2017-03-02T18:15:48.325148: step 33453, loss 0.111139, acc 0.953125
2017-03-02T18:15:48.411935: step 33454, loss 0.118368, acc 0.9375
2017-03-02T18:15:48.476234: step 33455, loss 0.160913, acc 0.90625
2017-03-02T18:15:48.545140: step 33456, loss 0.099082, acc 0.953125
2017-03-02T18:15:48.621224: step 33457, loss 0.162127, acc 0.9375
2017-03-02T18:15:48.690400: step 33458, loss 0.149739, acc 0.953125
2017-03-02T18:15:48.755666: step 33459, loss 0.108967, acc 0.9375
2017-03-02T18:15:48.835299: step 33460, loss 0.130568, acc 0.96875
2017-03-02T18:15:48.907896: step 33461, loss 0.0705409, acc 0.96875
2017-03-02T18:15:48.975771: step 33462, loss 0.236505, acc 0.890625
2017-03-02T18:15:49.047483: step 33463, loss 0.222163, acc 0.90625
2017-03-02T18:15:49.124355: step 33464, loss 0.234231, acc 0.90625
2017-03-02T18:15:49.198913: step 33465, loss 0.146555, acc 0.9375
2017-03-02T18:15:49.273871: step 33466, loss 0.230243, acc 0.859375
2017-03-02T18:15:49.346974: step 33467, loss 0.287422, acc 0.921875
2017-03-02T18:15:49.418356: step 33468, loss 0.0956128, acc 0.984375
2017-03-02T18:15:49.493706: step 33469, loss 0.162498, acc 0.921875
2017-03-02T18:15:49.566446: step 33470, loss 0.236889, acc 0.90625
2017-03-02T18:15:49.633202: step 33471, loss 0.172991, acc 0.90625
2017-03-02T18:15:49.707132: step 33472, loss 0.135616, acc 0.9375
2017-03-02T18:15:49.778883: step 33473, loss 0.248369, acc 0.875
2017-03-02T18:15:49.849646: step 33474, loss 0.105448, acc 0.953125
2017-03-02T18:15:49.923602: step 33475, loss 0.206012, acc 0.90625
2017-03-02T18:15:50.000771: step 33476, loss 0.252539, acc 0.90625
2017-03-02T18:15:50.072974: step 33477, loss 0.124159, acc 0.90625
2017-03-02T18:15:50.147098: step 33478, loss 0.121892, acc 0.9375
2017-03-02T18:15:50.221182: step 33479, loss 0.168147, acc 0.890625
2017-03-02T18:15:50.293920: step 33480, loss 0.149341, acc 0.90625
2017-03-02T18:15:50.370714: step 33481, loss 0.155642, acc 0.921875
2017-03-02T18:15:50.443807: step 33482, loss 0.260649, acc 0.90625
2017-03-02T18:15:50.514455: step 33483, loss 0.105305, acc 0.953125
2017-03-02T18:15:50.582579: step 33484, loss 0.189358, acc 0.90625
2017-03-02T18:15:50.658985: step 33485, loss 0.129011, acc 0.9375
2017-03-02T18:15:50.732383: step 33486, loss 0.168029, acc 0.90625
2017-03-02T18:15:50.806995: step 33487, loss 0.143698, acc 0.9375
2017-03-02T18:15:50.884988: step 33488, loss 0.0842197, acc 0.96875
2017-03-02T18:15:50.957352: step 33489, loss 0.101079, acc 0.953125
2017-03-02T18:15:51.028725: step 33490, loss 0.134278, acc 0.921875
2017-03-02T18:15:51.103833: step 33491, loss 0.143267, acc 0.9375
2017-03-02T18:15:51.172702: step 33492, loss 0.126917, acc 0.953125
2017-03-02T18:15:51.248151: step 33493, loss 0.140134, acc 0.90625
2017-03-02T18:15:51.329622: step 33494, loss 0.077226, acc 0.953125
2017-03-02T18:15:51.401510: step 33495, loss 0.214838, acc 0.921875
2017-03-02T18:15:51.465398: step 33496, loss 0.0689428, acc 0.96875
2017-03-02T18:15:51.540537: step 33497, loss 0.241664, acc 0.890625
2017-03-02T18:15:51.615636: step 33498, loss 0.059498, acc 0.96875
2017-03-02T18:15:51.685721: step 33499, loss 0.108608, acc 0.953125
2017-03-02T18:15:51.780148: step 33500, loss 0.119829, acc 0.9375

Evaluation:
2017-03-02T18:15:51.811080: step 33500, loss 4.5038, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33500

2017-03-02T18:15:52.255995: step 33501, loss 0.210982, acc 0.921875
2017-03-02T18:15:52.326772: step 33502, loss 0.135755, acc 0.9375
2017-03-02T18:15:52.394150: step 33503, loss 0.100466, acc 0.96875
2017-03-02T18:15:52.464738: step 33504, loss 0.241482, acc 0.921875
2017-03-02T18:15:52.532235: step 33505, loss 0.0803523, acc 0.96875
2017-03-02T18:15:52.593576: step 33506, loss 0.10909, acc 0.9375
2017-03-02T18:15:52.661328: step 33507, loss 0.095347, acc 0.953125
2017-03-02T18:15:52.731906: step 33508, loss 0.094809, acc 0.921875
2017-03-02T18:15:52.806609: step 33509, loss 0.125838, acc 0.9375
2017-03-02T18:15:52.876857: step 33510, loss 0.099869, acc 0.9375
2017-03-02T18:15:52.948244: step 33511, loss 0.261984, acc 0.890625
2017-03-02T18:15:53.022389: step 33512, loss 0.213777, acc 0.90625
2017-03-02T18:15:53.096155: step 33513, loss 0.26889, acc 0.84375
2017-03-02T18:15:53.167082: step 33514, loss 0.162967, acc 0.921875
2017-03-02T18:15:53.234004: step 33515, loss 0.188186, acc 0.921875
2017-03-02T18:15:53.303457: step 33516, loss 0.065329, acc 1
2017-03-02T18:15:53.390825: step 33517, loss 0.0951975, acc 0.953125
2017-03-02T18:15:53.464127: step 33518, loss 0.0567361, acc 0.96875
2017-03-02T18:15:53.532488: step 33519, loss 0.18268, acc 0.9375
2017-03-02T18:15:53.612389: step 33520, loss 0.18251, acc 0.90625
2017-03-02T18:15:53.683277: step 33521, loss 0.113009, acc 0.96875
2017-03-02T18:15:53.751867: step 33522, loss 0.120105, acc 0.953125
2017-03-02T18:15:53.824746: step 33523, loss 0.191282, acc 0.921875
2017-03-02T18:15:53.906655: step 33524, loss 0.113343, acc 0.984375
2017-03-02T18:15:53.970393: step 33525, loss 0.260687, acc 0.875
2017-03-02T18:15:54.037000: step 33526, loss 0.112818, acc 0.9375
2017-03-02T18:15:54.112169: step 33527, loss 0.146854, acc 0.9375
2017-03-02T18:15:54.183965: step 33528, loss 0.139471, acc 0.921875
2017-03-02T18:15:54.258388: step 33529, loss 0.0806296, acc 0.984375
2017-03-02T18:15:54.328349: step 33530, loss 0.201497, acc 0.90625
2017-03-02T18:15:54.400541: step 33531, loss 0.121874, acc 0.953125
2017-03-02T18:15:54.473262: step 33532, loss 0.113513, acc 0.9375
2017-03-02T18:15:54.559682: step 33533, loss 0.112164, acc 0.96875
2017-03-02T18:15:54.630694: step 33534, loss 0.197757, acc 0.90625
2017-03-02T18:15:54.694387: step 33535, loss 0.118039, acc 0.9375
2017-03-02T18:15:54.770766: step 33536, loss 0.197457, acc 0.890625
2017-03-02T18:15:54.844466: step 33537, loss 0.0531383, acc 1
2017-03-02T18:15:54.916380: step 33538, loss 0.141518, acc 0.9375
2017-03-02T18:15:54.990396: step 33539, loss 0.192403, acc 0.859375
2017-03-02T18:15:55.061357: step 33540, loss 0.0794835, acc 0.953125
2017-03-02T18:15:55.130738: step 33541, loss 0.210088, acc 0.890625
2017-03-02T18:15:55.203255: step 33542, loss 0.0493499, acc 0.984375
2017-03-02T18:15:55.275280: step 33543, loss 0.162416, acc 0.921875
2017-03-02T18:15:55.346331: step 33544, loss 0.185583, acc 0.9375
2017-03-02T18:15:55.416537: step 33545, loss 0.0889169, acc 0.953125
2017-03-02T18:15:55.489220: step 33546, loss 0.0821342, acc 0.9375
2017-03-02T18:15:55.560984: step 33547, loss 0.140371, acc 0.9375
2017-03-02T18:15:55.634176: step 33548, loss 0.136505, acc 0.9375
2017-03-02T18:15:55.706563: step 33549, loss 0.058702, acc 0.984375
2017-03-02T18:15:55.784064: step 33550, loss 0.108395, acc 0.953125
2017-03-02T18:15:55.855049: step 33551, loss 0.184797, acc 0.921875
2017-03-02T18:15:55.926513: step 33552, loss 0.17252, acc 0.90625
2017-03-02T18:15:55.995663: step 33553, loss 0.135471, acc 0.9375
2017-03-02T18:15:56.098462: step 33554, loss 0.0972409, acc 0.953125
2017-03-02T18:15:56.173874: step 33555, loss 0.052697, acc 0.96875
2017-03-02T18:15:56.241684: step 33556, loss 0.0981191, acc 0.96875
2017-03-02T18:15:56.326312: step 33557, loss 0.152906, acc 0.953125
2017-03-02T18:15:56.403220: step 33558, loss 0.289129, acc 0.921875
2017-03-02T18:15:56.478965: step 33559, loss 0.0921667, acc 0.9375
2017-03-02T18:15:56.553608: step 33560, loss 0.0713854, acc 0.984375
2017-03-02T18:15:56.626856: step 33561, loss 0.114883, acc 0.9375
2017-03-02T18:15:56.693992: step 33562, loss 0.185458, acc 0.9375
2017-03-02T18:15:56.767086: step 33563, loss 0.1814, acc 0.90625
2017-03-02T18:15:56.838906: step 33564, loss 0.112986, acc 0.953125
2017-03-02T18:15:56.907929: step 33565, loss 0.159492, acc 0.9375
2017-03-02T18:15:56.978822: step 33566, loss 0.213837, acc 0.921875
2017-03-02T18:15:57.054776: step 33567, loss 0.178513, acc 0.921875
2017-03-02T18:15:57.134340: step 33568, loss 0.113957, acc 0.953125
2017-03-02T18:15:57.208395: step 33569, loss 0.140578, acc 0.921875
2017-03-02T18:15:57.279658: step 33570, loss 0.162087, acc 0.90625
2017-03-02T18:15:57.342790: step 33571, loss 0.116615, acc 0.9375
2017-03-02T18:15:57.406174: step 33572, loss 0.105497, acc 0.96875
2017-03-02T18:15:57.468749: step 33573, loss 0.209039, acc 0.90625
2017-03-02T18:15:57.542954: step 33574, loss 0.126789, acc 0.953125
2017-03-02T18:15:57.616928: step 33575, loss 0.191907, acc 0.890625
2017-03-02T18:15:57.703068: step 33576, loss 0.102912, acc 0.9375
2017-03-02T18:15:57.779811: step 33577, loss 0.168401, acc 0.9375
2017-03-02T18:15:57.852798: step 33578, loss 0.176605, acc 0.9375
2017-03-02T18:15:57.929142: step 33579, loss 0.218104, acc 0.875
2017-03-02T18:15:57.999789: step 33580, loss 0.0604458, acc 0.984375
2017-03-02T18:15:58.064433: step 33581, loss 0.185422, acc 0.890625
2017-03-02T18:15:58.131253: step 33582, loss 0.203195, acc 0.90625
2017-03-02T18:15:58.202085: step 33583, loss 0.0891932, acc 0.96875
2017-03-02T18:15:58.274494: step 33584, loss 0.15027, acc 0.953125
2017-03-02T18:15:58.347135: step 33585, loss 0.106668, acc 0.96875
2017-03-02T18:15:58.424787: step 33586, loss 0.234888, acc 0.890625
2017-03-02T18:15:58.498331: step 33587, loss 0.139866, acc 0.953125
2017-03-02T18:15:58.569408: step 33588, loss 0.198953, acc 0.90625
2017-03-02T18:15:58.631149: step 33589, loss 0.124683, acc 0.953125
2017-03-02T18:15:58.709413: step 33590, loss 0.088823, acc 0.96875
2017-03-02T18:15:58.789473: step 33591, loss 0.327934, acc 0.859375
2017-03-02T18:15:58.867731: step 33592, loss 0.211144, acc 0.9375
2017-03-02T18:15:58.958233: step 33593, loss 0.192689, acc 0.90625
2017-03-02T18:15:59.043845: step 33594, loss 0.130003, acc 0.921875
2017-03-02T18:15:59.124597: step 33595, loss 0.138626, acc 0.90625
2017-03-02T18:15:59.200764: step 33596, loss 0.127555, acc 0.953125
2017-03-02T18:15:59.268702: step 33597, loss 0.16518, acc 0.953125
2017-03-02T18:15:59.342319: step 33598, loss 0.0619632, acc 0.984375
2017-03-02T18:15:59.415892: step 33599, loss 0.188209, acc 0.859375
2017-03-02T18:15:59.485763: step 33600, loss 0.131397, acc 0.9375

Evaluation:
2017-03-02T18:15:59.525575: step 33600, loss 4.6183, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33600

2017-03-02T18:16:00.035641: step 33601, loss 0.113485, acc 0.921875
2017-03-02T18:16:00.098289: step 33602, loss 0.137241, acc 0.9375
2017-03-02T18:16:00.162267: step 33603, loss 0.177037, acc 0.921875
2017-03-02T18:16:00.230762: step 33604, loss 0.102994, acc 0.953125
2017-03-02T18:16:00.304611: step 33605, loss 0.153005, acc 0.9375
2017-03-02T18:16:00.379890: step 33606, loss 0.161948, acc 0.9375
2017-03-02T18:16:00.465880: step 33607, loss 0.156899, acc 0.9375
2017-03-02T18:16:00.535891: step 33608, loss 0.151689, acc 0.921875
2017-03-02T18:16:00.607317: step 33609, loss 0.118331, acc 0.96875
2017-03-02T18:16:00.687296: step 33610, loss 0.144532, acc 0.921875
2017-03-02T18:16:00.761181: step 33611, loss 0.143459, acc 0.9375
2017-03-02T18:16:00.825276: step 33612, loss 0.112706, acc 0.953125
2017-03-02T18:16:00.888582: step 33613, loss 0.132405, acc 0.9375
2017-03-02T18:16:00.971078: step 33614, loss 0.134331, acc 0.96875
2017-03-02T18:16:01.048916: step 33615, loss 0.124517, acc 0.9375
2017-03-02T18:16:01.120200: step 33616, loss 0.145109, acc 0.921875
2017-03-02T18:16:01.192576: step 33617, loss 0.122816, acc 0.9375
2017-03-02T18:16:01.279924: step 33618, loss 0.267214, acc 0.875
2017-03-02T18:16:01.349154: step 33619, loss 0.256798, acc 0.890625
2017-03-02T18:16:01.424945: step 33620, loss 0.18084, acc 0.921875
2017-03-02T18:16:01.489495: step 33621, loss 0.162066, acc 0.921875
2017-03-02T18:16:01.555827: step 33622, loss 0.120758, acc 0.9375
2017-03-02T18:16:01.627524: step 33623, loss 0.129683, acc 0.90625
2017-03-02T18:16:01.713581: step 33624, loss 0.102129, acc 0.953125
2017-03-02T18:16:01.782288: step 33625, loss 0.163498, acc 0.90625
2017-03-02T18:16:01.869599: step 33626, loss 0.379231, acc 0.90625
2017-03-02T18:16:01.946791: step 33627, loss 0.152099, acc 0.921875
2017-03-02T18:16:02.018710: step 33628, loss 0.221882, acc 0.875
2017-03-02T18:16:02.110533: step 33629, loss 0.226344, acc 0.859375
2017-03-02T18:16:02.176890: step 33630, loss 0.15502, acc 0.921875
2017-03-02T18:16:02.246871: step 33631, loss 0.127987, acc 0.9375
2017-03-02T18:16:02.319043: step 33632, loss 0.0944865, acc 0.9375
2017-03-02T18:16:02.391605: step 33633, loss 0.222156, acc 0.921875
2017-03-02T18:16:02.467311: step 33634, loss 0.181089, acc 0.921875
2017-03-02T18:16:02.540445: step 33635, loss 0.130676, acc 0.953125
2017-03-02T18:16:02.632783: step 33636, loss 0.0479802, acc 1
2017-03-02T18:16:02.705265: step 33637, loss 0.134542, acc 0.90625
2017-03-02T18:16:02.779861: step 33638, loss 0.25322, acc 0.890625
2017-03-02T18:16:02.853609: step 33639, loss 0.161091, acc 0.90625
2017-03-02T18:16:02.944166: step 33640, loss 0.102646, acc 0.9375
2017-03-02T18:16:03.018914: step 33641, loss 0.113488, acc 0.953125
2017-03-02T18:16:03.092095: step 33642, loss 0.188147, acc 0.90625
2017-03-02T18:16:03.169644: step 33643, loss 0.102769, acc 0.953125
2017-03-02T18:16:03.242423: step 33644, loss 0.0976088, acc 0.9375
2017-03-02T18:16:03.314581: step 33645, loss 0.127528, acc 0.9375
2017-03-02T18:16:03.386559: step 33646, loss 0.200628, acc 0.875
2017-03-02T18:16:03.464452: step 33647, loss 0.159869, acc 0.921875
2017-03-02T18:16:03.537640: step 33648, loss 0.140672, acc 0.921875
2017-03-02T18:16:03.609993: step 33649, loss 0.165417, acc 0.921875
2017-03-02T18:16:03.671547: step 33650, loss 0.148457, acc 0.921875
2017-03-02T18:16:03.736891: step 33651, loss 0.196558, acc 0.921875
2017-03-02T18:16:03.804562: step 33652, loss 0.108129, acc 0.953125
2017-03-02T18:16:03.871865: step 33653, loss 0.118756, acc 0.9375
2017-03-02T18:16:03.970160: step 33654, loss 0.136209, acc 0.9375
2017-03-02T18:16:04.044546: step 33655, loss 0.132521, acc 0.9375
2017-03-02T18:16:04.117464: step 33656, loss 0.0836104, acc 0.96875
2017-03-02T18:16:04.190575: step 33657, loss 0.132421, acc 0.953125
2017-03-02T18:16:04.264912: step 33658, loss 0.139706, acc 0.921875
2017-03-02T18:16:04.335765: step 33659, loss 0.0612885, acc 0.96875
2017-03-02T18:16:04.400759: step 33660, loss 0.104777, acc 0.921875
2017-03-02T18:16:04.469239: step 33661, loss 0.0802744, acc 0.96875
2017-03-02T18:16:04.539937: step 33662, loss 0.137265, acc 0.9375
2017-03-02T18:16:04.610296: step 33663, loss 0.0980332, acc 0.953125
2017-03-02T18:16:04.691890: step 33664, loss 0.197415, acc 0.90625
2017-03-02T18:16:04.765577: step 33665, loss 0.139583, acc 0.921875
2017-03-02T18:16:04.846498: step 33666, loss 0.0991893, acc 0.9375
2017-03-02T18:16:04.921721: step 33667, loss 0.0871084, acc 0.953125
2017-03-02T18:16:04.988850: step 33668, loss 0.168264, acc 0.890625
2017-03-02T18:16:05.052314: step 33669, loss 0.0803987, acc 0.984375
2017-03-02T18:16:05.125696: step 33670, loss 0.0320644, acc 1
2017-03-02T18:16:05.204708: step 33671, loss 0.122543, acc 0.953125
2017-03-02T18:16:05.277012: step 33672, loss 0.109157, acc 0.921875
2017-03-02T18:16:05.348624: step 33673, loss 0.118472, acc 0.953125
2017-03-02T18:16:05.424864: step 33674, loss 0.126524, acc 0.96875
2017-03-02T18:16:05.498667: step 33675, loss 0.129202, acc 0.9375
2017-03-02T18:16:05.574122: step 33676, loss 0.118529, acc 0.96875
2017-03-02T18:16:05.644768: step 33677, loss 0.131712, acc 0.9375
2017-03-02T18:16:05.713781: step 33678, loss 0.178062, acc 0.9375
2017-03-02T18:16:05.787102: step 33679, loss 0.240183, acc 0.84375
2017-03-02T18:16:05.858608: step 33680, loss 0.102378, acc 0.9375
2017-03-02T18:16:05.932501: step 33681, loss 0.195445, acc 0.90625
2017-03-02T18:16:06.006011: step 33682, loss 0.268517, acc 0.921875
2017-03-02T18:16:06.084649: step 33683, loss 0.152576, acc 0.953125
2017-03-02T18:16:06.162603: step 33684, loss 0.198069, acc 0.890625
2017-03-02T18:16:06.234456: step 33685, loss 0.114116, acc 0.953125
2017-03-02T18:16:06.301375: step 33686, loss 0.0796911, acc 0.96875
2017-03-02T18:16:06.370381: step 33687, loss 0.0991639, acc 0.96875
2017-03-02T18:16:06.433724: step 33688, loss 0.0776898, acc 0.96875
2017-03-02T18:16:06.502328: step 33689, loss 0.181068, acc 0.875
2017-03-02T18:16:06.570740: step 33690, loss 0.0815288, acc 0.96875
2017-03-02T18:16:06.640495: step 33691, loss 0.250794, acc 0.890625
2017-03-02T18:16:06.703903: step 33692, loss 0.12823, acc 0.921875
2017-03-02T18:16:06.777276: step 33693, loss 0.210644, acc 0.875
2017-03-02T18:16:06.844547: step 33694, loss 0.103821, acc 0.953125
2017-03-02T18:16:06.915856: step 33695, loss 0.200811, acc 0.90625
2017-03-02T18:16:06.993907: step 33696, loss 0.189088, acc 0.90625
2017-03-02T18:16:07.065374: step 33697, loss 0.0822345, acc 0.984375
2017-03-02T18:16:07.135105: step 33698, loss 0.0501492, acc 0.984375
2017-03-02T18:16:07.201880: step 33699, loss 0.168949, acc 0.90625
2017-03-02T18:16:07.273506: step 33700, loss 0.145378, acc 0.9375

Evaluation:
2017-03-02T18:16:07.308074: step 33700, loss 4.63657, acc 0.631579

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33700

2017-03-02T18:16:07.752594: step 33701, loss 0.0712921, acc 0.96875
2017-03-02T18:16:07.818498: step 33702, loss 0.187541, acc 0.921875
2017-03-02T18:16:07.889315: step 33703, loss 0.308493, acc 0.90625
2017-03-02T18:16:07.972493: step 33704, loss 0.0790376, acc 0.96875
2017-03-02T18:16:08.048570: step 33705, loss 0.07185, acc 0.984375
2017-03-02T18:16:08.121166: step 33706, loss 0.0747129, acc 0.953125
2017-03-02T18:16:08.198944: step 33707, loss 0.130395, acc 0.921875
2017-03-02T18:16:08.270715: step 33708, loss 0.0807496, acc 0.984375
2017-03-02T18:16:08.346659: step 33709, loss 0.242613, acc 0.890625
2017-03-02T18:16:08.418865: step 33710, loss 0.0376682, acc 0.984375
2017-03-02T18:16:08.483544: step 33711, loss 0.178863, acc 0.9375
2017-03-02T18:16:08.549712: step 33712, loss 0.00128109, acc 1
2017-03-02T18:16:08.639160: step 33713, loss 0.173803, acc 0.921875
2017-03-02T18:16:08.708429: step 33714, loss 0.174656, acc 0.921875
2017-03-02T18:16:08.781802: step 33715, loss 0.130376, acc 0.9375
2017-03-02T18:16:08.847401: step 33716, loss 0.128497, acc 0.953125
2017-03-02T18:16:08.925056: step 33717, loss 0.0877118, acc 0.96875
2017-03-02T18:16:08.993921: step 33718, loss 0.125607, acc 0.9375
2017-03-02T18:16:09.070182: step 33719, loss 0.148409, acc 0.921875
2017-03-02T18:16:09.136413: step 33720, loss 0.0969682, acc 0.96875
2017-03-02T18:16:09.207645: step 33721, loss 0.0997503, acc 0.96875
2017-03-02T18:16:09.279819: step 33722, loss 0.171792, acc 0.96875
2017-03-02T18:16:09.355921: step 33723, loss 0.196838, acc 0.90625
2017-03-02T18:16:09.427360: step 33724, loss 0.0740772, acc 0.984375
2017-03-02T18:16:09.500958: step 33725, loss 0.142669, acc 0.96875
2017-03-02T18:16:09.573583: step 33726, loss 0.129245, acc 0.96875
2017-03-02T18:16:09.651156: step 33727, loss 0.264651, acc 0.890625
2017-03-02T18:16:09.726702: step 33728, loss 0.0417095, acc 0.984375
2017-03-02T18:16:09.799960: step 33729, loss 0.204361, acc 0.90625
2017-03-02T18:16:09.867728: step 33730, loss 0.132951, acc 0.953125
2017-03-02T18:16:09.933506: step 33731, loss 0.123847, acc 0.921875
2017-03-02T18:16:10.002205: step 33732, loss 0.199564, acc 0.890625
2017-03-02T18:16:10.075303: step 33733, loss 0.0768986, acc 0.984375
2017-03-02T18:16:10.145398: step 33734, loss 0.0727425, acc 0.984375
2017-03-02T18:16:10.217274: step 33735, loss 0.0712972, acc 0.984375
2017-03-02T18:16:10.286926: step 33736, loss 0.166336, acc 0.90625
2017-03-02T18:16:10.351206: step 33737, loss 0.0990012, acc 0.953125
2017-03-02T18:16:10.421834: step 33738, loss 0.168885, acc 0.9375
2017-03-02T18:16:10.500996: step 33739, loss 0.21477, acc 0.890625
2017-03-02T18:16:10.573192: step 33740, loss 0.139007, acc 0.953125
2017-03-02T18:16:10.651205: step 33741, loss 0.10916, acc 0.953125
2017-03-02T18:16:10.741864: step 33742, loss 0.179743, acc 0.90625
2017-03-02T18:16:10.820458: step 33743, loss 0.187173, acc 0.921875
2017-03-02T18:16:10.895787: step 33744, loss 0.136539, acc 0.90625
2017-03-02T18:16:10.970089: step 33745, loss 0.151286, acc 0.9375
2017-03-02T18:16:11.052441: step 33746, loss 0.107923, acc 0.953125
2017-03-02T18:16:11.128015: step 33747, loss 0.067632, acc 0.96875
2017-03-02T18:16:11.198318: step 33748, loss 0.133034, acc 0.9375
2017-03-02T18:16:11.266766: step 33749, loss 0.0629117, acc 0.953125
2017-03-02T18:16:11.340655: step 33750, loss 0.129744, acc 0.953125
2017-03-02T18:16:11.419290: step 33751, loss 0.105959, acc 0.9375
2017-03-02T18:16:11.517084: step 33752, loss 0.0915352, acc 0.984375
2017-03-02T18:16:11.589654: step 33753, loss 0.305837, acc 0.859375
2017-03-02T18:16:11.657767: step 33754, loss 0.235343, acc 0.9375
2017-03-02T18:16:11.731703: step 33755, loss 0.201528, acc 0.875
2017-03-02T18:16:11.801655: step 33756, loss 0.184761, acc 0.90625
2017-03-02T18:16:11.879221: step 33757, loss 0.116516, acc 0.953125
2017-03-02T18:16:11.951597: step 33758, loss 0.143992, acc 0.921875
2017-03-02T18:16:12.026617: step 33759, loss 0.198391, acc 0.890625
2017-03-02T18:16:12.104869: step 33760, loss 0.0875555, acc 0.953125
2017-03-02T18:16:12.173834: step 33761, loss 0.0960749, acc 0.953125
2017-03-02T18:16:12.251669: step 33762, loss 0.140185, acc 0.9375
2017-03-02T18:16:12.322583: step 33763, loss 0.177497, acc 0.9375
2017-03-02T18:16:12.392285: step 33764, loss 0.12558, acc 0.96875
2017-03-02T18:16:12.463387: step 33765, loss 0.220773, acc 0.890625
2017-03-02T18:16:12.533571: step 33766, loss 0.111122, acc 0.953125
2017-03-02T18:16:12.601485: step 33767, loss 0.175524, acc 0.921875
2017-03-02T18:16:12.668532: step 33768, loss 0.0500466, acc 1
2017-03-02T18:16:12.738564: step 33769, loss 0.142184, acc 0.921875
2017-03-02T18:16:12.812424: step 33770, loss 0.201013, acc 0.921875
2017-03-02T18:16:12.885180: step 33771, loss 0.0822952, acc 0.96875
2017-03-02T18:16:12.964706: step 33772, loss 0.147623, acc 0.921875
2017-03-02T18:16:13.037272: step 33773, loss 0.0868027, acc 0.96875
2017-03-02T18:16:13.108774: step 33774, loss 0.159943, acc 0.9375
2017-03-02T18:16:13.180856: step 33775, loss 0.154433, acc 0.9375
2017-03-02T18:16:13.243542: step 33776, loss 0.0993183, acc 0.96875
2017-03-02T18:16:13.306422: step 33777, loss 0.16738, acc 0.921875
2017-03-02T18:16:13.374135: step 33778, loss 0.317372, acc 0.78125
2017-03-02T18:16:13.446714: step 33779, loss 0.216685, acc 0.90625
2017-03-02T18:16:13.511730: step 33780, loss 0.145766, acc 0.9375
2017-03-02T18:16:13.584663: step 33781, loss 0.24912, acc 0.875
2017-03-02T18:16:13.653665: step 33782, loss 0.216334, acc 0.921875
2017-03-02T18:16:13.726365: step 33783, loss 0.133481, acc 0.953125
2017-03-02T18:16:13.797490: step 33784, loss 0.193238, acc 0.890625
2017-03-02T18:16:13.875989: step 33785, loss 0.128133, acc 0.9375
2017-03-02T18:16:13.954904: step 33786, loss 0.125196, acc 0.96875
2017-03-02T18:16:14.026693: step 33787, loss 0.13224, acc 0.9375
2017-03-02T18:16:14.093184: step 33788, loss 0.100803, acc 0.953125
2017-03-02T18:16:14.163460: step 33789, loss 0.289985, acc 0.90625
2017-03-02T18:16:14.239436: step 33790, loss 0.225618, acc 0.90625
2017-03-02T18:16:14.308388: step 33791, loss 0.0764369, acc 0.9375
2017-03-02T18:16:14.396162: step 33792, loss 0.118746, acc 0.9375
2017-03-02T18:16:14.466358: step 33793, loss 0.141344, acc 0.9375
2017-03-02T18:16:14.537005: step 33794, loss 0.107709, acc 0.9375
2017-03-02T18:16:14.615744: step 33795, loss 0.140695, acc 0.9375
2017-03-02T18:16:14.682996: step 33796, loss 0.228556, acc 0.859375
2017-03-02T18:16:14.755862: step 33797, loss 0.0948905, acc 0.96875
2017-03-02T18:16:14.829123: step 33798, loss 0.0993051, acc 0.953125
2017-03-02T18:16:14.902173: step 33799, loss 0.171717, acc 0.9375
2017-03-02T18:16:14.986678: step 33800, loss 0.0641761, acc 0.96875

Evaluation:
2017-03-02T18:16:15.022358: step 33800, loss 4.59564, acc 0.635184

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33800

2017-03-02T18:16:15.472829: step 33801, loss 0.078585, acc 0.953125
2017-03-02T18:16:15.546703: step 33802, loss 0.124118, acc 0.9375
2017-03-02T18:16:15.625296: step 33803, loss 0.148453, acc 0.921875
2017-03-02T18:16:15.698363: step 33804, loss 0.14927, acc 0.96875
2017-03-02T18:16:15.762117: step 33805, loss 0.19392, acc 0.890625
2017-03-02T18:16:15.833577: step 33806, loss 0.180199, acc 0.890625
2017-03-02T18:16:15.905250: step 33807, loss 0.102505, acc 0.96875
2017-03-02T18:16:15.975652: step 33808, loss 0.101879, acc 0.953125
2017-03-02T18:16:16.041165: step 33809, loss 0.104698, acc 0.953125
2017-03-02T18:16:16.111798: step 33810, loss 0.13641, acc 0.9375
2017-03-02T18:16:16.180714: step 33811, loss 0.181257, acc 0.890625
2017-03-02T18:16:16.254020: step 33812, loss 0.130908, acc 0.921875
2017-03-02T18:16:16.328026: step 33813, loss 0.053841, acc 1
2017-03-02T18:16:16.398692: step 33814, loss 0.198247, acc 0.9375
2017-03-02T18:16:16.465637: step 33815, loss 0.0921302, acc 0.953125
2017-03-02T18:16:16.536780: step 33816, loss 0.188029, acc 0.890625
2017-03-02T18:16:16.610040: step 33817, loss 0.150967, acc 0.953125
2017-03-02T18:16:16.675523: step 33818, loss 0.126335, acc 0.9375
2017-03-02T18:16:16.746759: step 33819, loss 0.110126, acc 0.9375
2017-03-02T18:16:16.806285: step 33820, loss 0.134485, acc 0.9375
2017-03-02T18:16:16.883576: step 33821, loss 0.0852237, acc 0.953125
2017-03-02T18:16:16.952180: step 33822, loss 0.143413, acc 0.9375
2017-03-02T18:16:17.023548: step 33823, loss 0.197195, acc 0.90625
2017-03-02T18:16:17.093999: step 33824, loss 0.273101, acc 0.875
2017-03-02T18:16:17.169120: step 33825, loss 0.156613, acc 0.921875
2017-03-02T18:16:17.236928: step 33826, loss 0.140264, acc 0.953125
2017-03-02T18:16:17.310522: step 33827, loss 0.169362, acc 0.921875
2017-03-02T18:16:17.377566: step 33828, loss 0.158457, acc 0.90625
2017-03-02T18:16:17.436406: step 33829, loss 0.156966, acc 0.90625
2017-03-02T18:16:17.510234: step 33830, loss 0.0909188, acc 0.9375
2017-03-02T18:16:17.581690: step 33831, loss 0.178909, acc 0.921875
2017-03-02T18:16:17.652368: step 33832, loss 0.101825, acc 0.9375
2017-03-02T18:16:17.725325: step 33833, loss 0.1288, acc 0.921875
2017-03-02T18:16:17.797047: step 33834, loss 0.0743431, acc 0.96875
2017-03-02T18:16:17.872923: step 33835, loss 0.151619, acc 0.921875
2017-03-02T18:16:17.947868: step 33836, loss 0.1234, acc 0.921875
2017-03-02T18:16:18.020354: step 33837, loss 0.148085, acc 0.953125
2017-03-02T18:16:18.088076: step 33838, loss 0.229286, acc 0.90625
2017-03-02T18:16:18.176583: step 33839, loss 0.123155, acc 0.9375
2017-03-02T18:16:18.248911: step 33840, loss 0.149334, acc 0.921875
2017-03-02T18:16:18.317721: step 33841, loss 0.148179, acc 0.9375
2017-03-02T18:16:18.395649: step 33842, loss 0.121408, acc 0.9375
2017-03-02T18:16:18.469806: step 33843, loss 0.0924679, acc 0.921875
2017-03-02T18:16:18.548148: step 33844, loss 0.132152, acc 0.9375
2017-03-02T18:16:18.617880: step 33845, loss 0.2099, acc 0.921875
2017-03-02T18:16:18.688318: step 33846, loss 0.173511, acc 0.875
2017-03-02T18:16:18.759580: step 33847, loss 0.0424485, acc 1
2017-03-02T18:16:18.828514: step 33848, loss 0.126467, acc 0.96875
2017-03-02T18:16:18.901336: step 33849, loss 0.101944, acc 0.9375
2017-03-02T18:16:18.976404: step 33850, loss 0.104175, acc 0.984375
2017-03-02T18:16:19.048348: step 33851, loss 0.151948, acc 0.921875
2017-03-02T18:16:19.117215: step 33852, loss 0.208391, acc 0.921875
2017-03-02T18:16:19.192093: step 33853, loss 0.14476, acc 0.96875
2017-03-02T18:16:19.264074: step 33854, loss 0.087944, acc 0.984375
2017-03-02T18:16:19.335878: step 33855, loss 0.169823, acc 0.953125
2017-03-02T18:16:19.401386: step 33856, loss 0.114105, acc 0.953125
2017-03-02T18:16:19.466633: step 33857, loss 0.173106, acc 0.953125
2017-03-02T18:16:19.538009: step 33858, loss 0.10654, acc 0.953125
2017-03-02T18:16:19.612315: step 33859, loss 0.218954, acc 0.9375
2017-03-02T18:16:19.699936: step 33860, loss 0.313105, acc 0.84375
2017-03-02T18:16:19.772105: step 33861, loss 0.0724916, acc 0.96875
2017-03-02T18:16:19.841075: step 33862, loss 0.184274, acc 0.875
2017-03-02T18:16:19.909260: step 33863, loss 0.216851, acc 0.953125
2017-03-02T18:16:19.986205: step 33864, loss 0.198844, acc 0.875
2017-03-02T18:16:20.051694: step 33865, loss 0.0991026, acc 0.96875
2017-03-02T18:16:20.121221: step 33866, loss 0.130946, acc 0.921875
2017-03-02T18:16:20.199496: step 33867, loss 0.126364, acc 0.921875
2017-03-02T18:16:20.273608: step 33868, loss 0.0402453, acc 1
2017-03-02T18:16:20.360637: step 33869, loss 0.190397, acc 0.890625
2017-03-02T18:16:20.439241: step 33870, loss 0.152528, acc 0.890625
2017-03-02T18:16:20.496869: step 33871, loss 0.115012, acc 0.9375
2017-03-02T18:16:20.578751: step 33872, loss 0.185239, acc 0.921875
2017-03-02T18:16:20.646039: step 33873, loss 0.108903, acc 0.953125
2017-03-02T18:16:20.721652: step 33874, loss 0.0534104, acc 0.984375
2017-03-02T18:16:20.784549: step 33875, loss 0.164802, acc 0.875
2017-03-02T18:16:20.849327: step 33876, loss 0.110445, acc 0.9375
2017-03-02T18:16:20.919577: step 33877, loss 0.244126, acc 0.875
2017-03-02T18:16:20.992158: step 33878, loss 0.241351, acc 0.9375
2017-03-02T18:16:21.065276: step 33879, loss 0.0903485, acc 0.9375
2017-03-02T18:16:21.138385: step 33880, loss 0.081941, acc 0.984375
2017-03-02T18:16:21.210159: step 33881, loss 0.236785, acc 0.890625
2017-03-02T18:16:21.274680: step 33882, loss 0.167607, acc 0.890625
2017-03-02T18:16:21.355627: step 33883, loss 0.186581, acc 0.921875
2017-03-02T18:16:21.443854: step 33884, loss 0.165563, acc 0.921875
2017-03-02T18:16:21.517825: step 33885, loss 0.127634, acc 0.953125
2017-03-02T18:16:21.595967: step 33886, loss 0.137526, acc 0.890625
2017-03-02T18:16:21.665376: step 33887, loss 0.0500275, acc 0.984375
2017-03-02T18:16:21.727398: step 33888, loss 0.131138, acc 0.9375
2017-03-02T18:16:21.803648: step 33889, loss 0.160621, acc 0.9375
2017-03-02T18:16:21.871093: step 33890, loss 0.100009, acc 0.9375
2017-03-02T18:16:21.941801: step 33891, loss 0.192768, acc 0.9375
2017-03-02T18:16:22.019994: step 33892, loss 0.130081, acc 0.953125
2017-03-02T18:16:22.096108: step 33893, loss 0.102618, acc 0.984375
2017-03-02T18:16:22.161729: step 33894, loss 0.202537, acc 0.875
2017-03-02T18:16:22.230601: step 33895, loss 0.157668, acc 0.921875
2017-03-02T18:16:22.299643: step 33896, loss 0.081894, acc 0.96875
2017-03-02T18:16:22.370722: step 33897, loss 0.120462, acc 0.921875
2017-03-02T18:16:22.442593: step 33898, loss 0.0955516, acc 0.96875
2017-03-02T18:16:22.514623: step 33899, loss 0.0944007, acc 0.953125
2017-03-02T18:16:22.595442: step 33900, loss 0.15272, acc 0.953125

Evaluation:
2017-03-02T18:16:22.632253: step 33900, loss 4.61428, acc 0.638789

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-33900

2017-03-02T18:16:23.076893: step 33901, loss 0.223248, acc 0.90625
2017-03-02T18:16:23.150208: step 33902, loss 0.171107, acc 0.90625
2017-03-02T18:16:23.217932: step 33903, loss 0.143346, acc 0.953125
2017-03-02T18:16:23.285634: step 33904, loss 0.0805518, acc 0.96875
2017-03-02T18:16:23.355662: step 33905, loss 0.11341, acc 0.921875
2017-03-02T18:16:23.426563: step 33906, loss 0.158922, acc 0.921875
2017-03-02T18:16:23.504445: step 33907, loss 0.365626, acc 0.8125
2017-03-02T18:16:23.570389: step 33908, loss 0.571243, acc 0.75
2017-03-02T18:16:23.638566: step 33909, loss 0.11134, acc 0.953125
2017-03-02T18:16:23.720090: step 33910, loss 0.130349, acc 0.9375
2017-03-02T18:16:23.793160: step 33911, loss 0.114656, acc 0.9375
2017-03-02T18:16:23.874664: step 33912, loss 0.113038, acc 0.9375
2017-03-02T18:16:23.950563: step 33913, loss 0.0668221, acc 0.96875
2017-03-02T18:16:24.031317: step 33914, loss 0.161442, acc 0.9375
2017-03-02T18:16:24.101249: step 33915, loss 0.238406, acc 0.9375
2017-03-02T18:16:24.177025: step 33916, loss 0.130719, acc 0.9375
2017-03-02T18:16:24.246336: step 33917, loss 0.273808, acc 0.859375
2017-03-02T18:16:24.326647: step 33918, loss 0.0919933, acc 0.984375
2017-03-02T18:16:24.402633: step 33919, loss 0.131641, acc 0.9375
2017-03-02T18:16:24.474935: step 33920, loss 0.138135, acc 0.90625
2017-03-02T18:16:24.548376: step 33921, loss 0.120173, acc 0.96875
2017-03-02T18:16:24.635352: step 33922, loss 0.160864, acc 0.953125
2017-03-02T18:16:24.705570: step 33923, loss 0.105941, acc 0.96875
2017-03-02T18:16:24.774856: step 33924, loss 0.0564526, acc 0.984375
2017-03-02T18:16:24.852748: step 33925, loss 0.129066, acc 0.953125
2017-03-02T18:16:24.928837: step 33926, loss 0.0774555, acc 0.96875
2017-03-02T18:16:24.997931: step 33927, loss 0.078025, acc 0.96875
2017-03-02T18:16:25.071221: step 33928, loss 0.143496, acc 0.90625
2017-03-02T18:16:25.142254: step 33929, loss 0.0426616, acc 1
2017-03-02T18:16:25.214328: step 33930, loss 0.0624633, acc 0.96875
2017-03-02T18:16:25.288728: step 33931, loss 0.230189, acc 0.890625
2017-03-02T18:16:25.359606: step 33932, loss 0.181351, acc 0.890625
2017-03-02T18:16:25.430423: step 33933, loss 0.175563, acc 0.890625
2017-03-02T18:16:25.503857: step 33934, loss 0.206617, acc 0.875
2017-03-02T18:16:25.574537: step 33935, loss 0.133798, acc 0.984375
2017-03-02T18:16:25.640749: step 33936, loss 0.194028, acc 0.890625
2017-03-02T18:16:25.708174: step 33937, loss 0.0844068, acc 0.984375
2017-03-02T18:16:25.771930: step 33938, loss 0.0922762, acc 0.96875
2017-03-02T18:16:25.846889: step 33939, loss 0.0884776, acc 0.953125
2017-03-02T18:16:25.923029: step 33940, loss 0.13997, acc 0.953125
2017-03-02T18:16:25.994505: step 33941, loss 0.106572, acc 0.96875
2017-03-02T18:16:26.065719: step 33942, loss 0.202806, acc 0.890625
2017-03-02T18:16:26.133061: step 33943, loss 0.103727, acc 0.96875
2017-03-02T18:16:26.206242: step 33944, loss 0.140974, acc 0.953125
2017-03-02T18:16:26.276729: step 33945, loss 0.184621, acc 0.96875
2017-03-02T18:16:26.343597: step 33946, loss 0.11773, acc 0.921875
2017-03-02T18:16:26.419279: step 33947, loss 0.189083, acc 0.921875
2017-03-02T18:16:26.513946: step 33948, loss 0.155123, acc 0.96875
2017-03-02T18:16:26.588318: step 33949, loss 0.153068, acc 0.9375
2017-03-02T18:16:26.662286: step 33950, loss 0.190271, acc 0.890625
2017-03-02T18:16:26.730459: step 33951, loss 0.185273, acc 0.9375
2017-03-02T18:16:26.799798: step 33952, loss 0.158751, acc 0.9375
2017-03-02T18:16:26.872558: step 33953, loss 0.203514, acc 0.921875
2017-03-02T18:16:26.949029: step 33954, loss 0.126658, acc 0.921875
2017-03-02T18:16:27.017437: step 33955, loss 0.33735, acc 0.890625
2017-03-02T18:16:27.091024: step 33956, loss 0.272972, acc 0.921875
2017-03-02T18:16:27.161904: step 33957, loss 0.123193, acc 0.96875
2017-03-02T18:16:27.232419: step 33958, loss 0.151859, acc 0.9375
2017-03-02T18:16:27.303813: step 33959, loss 0.08741, acc 0.953125
2017-03-02T18:16:27.376026: step 33960, loss 0.210162, acc 0.875
2017-03-02T18:16:27.447066: step 33961, loss 0.217536, acc 0.875
2017-03-02T18:16:27.519612: step 33962, loss 0.0572444, acc 0.984375
2017-03-02T18:16:27.592564: step 33963, loss 0.25866, acc 0.859375
2017-03-02T18:16:27.663086: step 33964, loss 0.179255, acc 0.9375
2017-03-02T18:16:27.737266: step 33965, loss 0.181069, acc 0.90625
2017-03-02T18:16:27.806785: step 33966, loss 0.105523, acc 0.96875
2017-03-02T18:16:27.873771: step 33967, loss 0.108002, acc 0.96875
2017-03-02T18:16:27.949851: step 33968, loss 0.165287, acc 0.921875
2017-03-02T18:16:28.021122: step 33969, loss 0.0829113, acc 0.953125
2017-03-02T18:16:28.092726: step 33970, loss 0.18886, acc 0.90625
2017-03-02T18:16:28.165654: step 33971, loss 0.15138, acc 0.921875
2017-03-02T18:16:28.239113: step 33972, loss 0.168515, acc 0.890625
2017-03-02T18:16:28.311709: step 33973, loss 0.218095, acc 0.921875
2017-03-02T18:16:28.388156: step 33974, loss 0.0770737, acc 0.96875
2017-03-02T18:16:28.465329: step 33975, loss 0.174011, acc 0.90625
2017-03-02T18:16:28.539094: step 33976, loss 0.0974958, acc 0.96875
2017-03-02T18:16:28.617709: step 33977, loss 0.14545, acc 0.9375
2017-03-02T18:16:28.690306: step 33978, loss 0.0580349, acc 0.96875
2017-03-02T18:16:28.765865: step 33979, loss 0.169486, acc 0.90625
2017-03-02T18:16:28.839156: step 33980, loss 0.180816, acc 0.875
2017-03-02T18:16:28.916734: step 33981, loss 0.141305, acc 0.9375
2017-03-02T18:16:28.989489: step 33982, loss 0.113848, acc 0.984375
2017-03-02T18:16:29.054966: step 33983, loss 0.0976288, acc 0.984375
2017-03-02T18:16:29.130828: step 33984, loss 0.218559, acc 0.90625
2017-03-02T18:16:29.200122: step 33985, loss 0.161196, acc 0.90625
2017-03-02T18:16:29.271774: step 33986, loss 0.129199, acc 0.921875
2017-03-02T18:16:29.343186: step 33987, loss 0.118283, acc 0.9375
2017-03-02T18:16:29.415095: step 33988, loss 0.0997351, acc 0.96875
2017-03-02T18:16:29.481550: step 33989, loss 0.176901, acc 0.890625
2017-03-02T18:16:29.550583: step 33990, loss 0.14132, acc 0.9375
2017-03-02T18:16:29.624214: step 33991, loss 0.265929, acc 0.859375
2017-03-02T18:16:29.705536: step 33992, loss 0.106269, acc 0.9375
2017-03-02T18:16:29.773305: step 33993, loss 0.0827865, acc 0.953125
2017-03-02T18:16:29.840816: step 33994, loss 0.314227, acc 0.828125
2017-03-02T18:16:29.913126: step 33995, loss 0.153331, acc 0.921875
2017-03-02T18:16:29.984587: step 33996, loss 0.142968, acc 0.9375
2017-03-02T18:16:30.056178: step 33997, loss 0.153999, acc 0.921875
2017-03-02T18:16:30.127590: step 33998, loss 0.150276, acc 0.921875
2017-03-02T18:16:30.194167: step 33999, loss 0.171036, acc 0.921875
2017-03-02T18:16:30.269037: step 34000, loss 0.113494, acc 0.953125

Evaluation:
2017-03-02T18:16:30.307878: step 34000, loss 4.71077, acc 0.637347

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34000

2017-03-02T18:16:30.762478: step 34001, loss 0.18349, acc 0.90625
2017-03-02T18:16:30.836038: step 34002, loss 0.300812, acc 0.890625
2017-03-02T18:16:30.913213: step 34003, loss 0.120809, acc 0.953125
2017-03-02T18:16:30.982477: step 34004, loss 0.154976, acc 0.9375
2017-03-02T18:16:31.059486: step 34005, loss 0.0415998, acc 0.96875
2017-03-02T18:16:31.129383: step 34006, loss 0.103701, acc 0.9375
2017-03-02T18:16:31.195513: step 34007, loss 0.15075, acc 0.921875
2017-03-02T18:16:31.265375: step 34008, loss 0.124371, acc 0.921875
2017-03-02T18:16:31.337088: step 34009, loss 0.0961577, acc 0.953125
2017-03-02T18:16:31.410931: step 34010, loss 0.179556, acc 0.90625
2017-03-02T18:16:31.481873: step 34011, loss 0.21962, acc 0.90625
2017-03-02T18:16:31.553458: step 34012, loss 0.236954, acc 0.90625
2017-03-02T18:16:31.625639: step 34013, loss 0.0954319, acc 0.953125
2017-03-02T18:16:31.700211: step 34014, loss 0.0903446, acc 0.984375
2017-03-02T18:16:31.779717: step 34015, loss 0.035641, acc 0.984375
2017-03-02T18:16:31.848346: step 34016, loss 0.186488, acc 0.9375
2017-03-02T18:16:31.918125: step 34017, loss 0.0526565, acc 0.96875
2017-03-02T18:16:31.988654: step 34018, loss 0.0908793, acc 0.953125
2017-03-02T18:16:32.058188: step 34019, loss 0.129729, acc 0.953125
2017-03-02T18:16:32.130137: step 34020, loss 0.236144, acc 0.90625
2017-03-02T18:16:32.211940: step 34021, loss 0.229665, acc 0.90625
2017-03-02T18:16:32.282794: step 34022, loss 0.13349, acc 0.90625
2017-03-02T18:16:32.356429: step 34023, loss 0.0838625, acc 0.984375
2017-03-02T18:16:32.432720: step 34024, loss 0.0982198, acc 0.921875
2017-03-02T18:16:32.496759: step 34025, loss 0.236059, acc 0.859375
2017-03-02T18:16:32.567469: step 34026, loss 0.129958, acc 0.953125
2017-03-02T18:16:32.633659: step 34027, loss 0.261756, acc 0.84375
2017-03-02T18:16:32.706662: step 34028, loss 0.0423942, acc 0.984375
2017-03-02T18:16:32.776849: step 34029, loss 0.132787, acc 0.953125
2017-03-02T18:16:32.849525: step 34030, loss 0.123828, acc 0.96875
2017-03-02T18:16:32.925327: step 34031, loss 0.0828246, acc 0.96875
2017-03-02T18:16:32.994193: step 34032, loss 0.110652, acc 0.953125
2017-03-02T18:16:33.059776: step 34033, loss 0.129202, acc 0.921875
2017-03-02T18:16:33.132427: step 34034, loss 0.105604, acc 0.96875
2017-03-02T18:16:33.211208: step 34035, loss 0.183023, acc 0.921875
2017-03-02T18:16:33.281252: step 34036, loss 0.149426, acc 0.9375
2017-03-02T18:16:33.360511: step 34037, loss 0.178101, acc 0.921875
2017-03-02T18:16:33.434693: step 34038, loss 0.157399, acc 0.921875
2017-03-02T18:16:33.508251: step 34039, loss 0.114829, acc 0.9375
2017-03-02T18:16:33.595614: step 34040, loss 0.134398, acc 0.9375
2017-03-02T18:16:33.666018: step 34041, loss 0.188932, acc 0.890625
2017-03-02T18:16:33.741088: step 34042, loss 0.136668, acc 0.953125
2017-03-02T18:16:33.814482: step 34043, loss 0.124129, acc 0.9375
2017-03-02T18:16:33.883600: step 34044, loss 0.218586, acc 0.890625
2017-03-02T18:16:33.950458: step 34045, loss 0.190978, acc 0.9375
2017-03-02T18:16:34.025718: step 34046, loss 0.0683644, acc 0.984375
2017-03-02T18:16:34.097887: step 34047, loss 0.0827128, acc 0.96875
2017-03-02T18:16:34.169602: step 34048, loss 0.220693, acc 0.875
2017-03-02T18:16:34.241095: step 34049, loss 0.112502, acc 0.953125
2017-03-02T18:16:34.313867: step 34050, loss 0.160221, acc 0.90625
2017-03-02T18:16:34.383805: step 34051, loss 0.102876, acc 0.96875
2017-03-02T18:16:34.455025: step 34052, loss 0.163925, acc 0.953125
2017-03-02T18:16:34.532159: step 34053, loss 0.106989, acc 0.953125
2017-03-02T18:16:34.604120: step 34054, loss 0.18554, acc 0.90625
2017-03-02T18:16:34.671714: step 34055, loss 0.0500328, acc 1
2017-03-02T18:16:34.745165: step 34056, loss 0.155182, acc 0.90625
2017-03-02T18:16:34.829043: step 34057, loss 0.081342, acc 0.953125
2017-03-02T18:16:34.900784: step 34058, loss 0.151892, acc 0.953125
2017-03-02T18:16:34.977477: step 34059, loss 0.109208, acc 0.953125
2017-03-02T18:16:35.054003: step 34060, loss 0.0887583, acc 0.953125
2017-03-02T18:16:35.128558: step 34061, loss 0.171261, acc 0.921875
2017-03-02T18:16:35.205349: step 34062, loss 0.0552248, acc 0.984375
2017-03-02T18:16:35.283390: step 34063, loss 0.153538, acc 0.921875
2017-03-02T18:16:35.353822: step 34064, loss 0.140763, acc 0.9375
2017-03-02T18:16:35.427503: step 34065, loss 0.20269, acc 0.890625
2017-03-02T18:16:35.501061: step 34066, loss 0.152238, acc 0.921875
2017-03-02T18:16:35.575918: step 34067, loss 0.262897, acc 0.875
2017-03-02T18:16:35.641436: step 34068, loss 0.131505, acc 0.90625
2017-03-02T18:16:35.713494: step 34069, loss 0.0757259, acc 0.96875
2017-03-02T18:16:35.786710: step 34070, loss 0.202201, acc 0.9375
2017-03-02T18:16:35.861927: step 34071, loss 0.0415478, acc 0.984375
2017-03-02T18:16:35.970289: step 34072, loss 0.129002, acc 0.9375
2017-03-02T18:16:36.038584: step 34073, loss 0.0767509, acc 0.953125
2017-03-02T18:16:36.115146: step 34074, loss 0.276012, acc 0.875
2017-03-02T18:16:36.192663: step 34075, loss 0.13961, acc 0.9375
2017-03-02T18:16:36.269342: step 34076, loss 0.129166, acc 0.9375
2017-03-02T18:16:36.349125: step 34077, loss 0.0953739, acc 0.953125
2017-03-02T18:16:36.426024: step 34078, loss 0.0901257, acc 0.953125
2017-03-02T18:16:36.496389: step 34079, loss 0.177871, acc 0.921875
2017-03-02T18:16:36.572545: step 34080, loss 0.110099, acc 0.9375
2017-03-02T18:16:36.645294: step 34081, loss 0.109866, acc 0.953125
2017-03-02T18:16:36.733077: step 34082, loss 0.0734263, acc 1
2017-03-02T18:16:36.812402: step 34083, loss 0.189391, acc 0.90625
2017-03-02T18:16:36.888049: step 34084, loss 0.0888062, acc 0.96875
2017-03-02T18:16:36.959431: step 34085, loss 0.213422, acc 0.875
2017-03-02T18:16:37.033361: step 34086, loss 0.0708837, acc 0.96875
2017-03-02T18:16:37.106455: step 34087, loss 0.0922378, acc 0.9375
2017-03-02T18:16:37.178060: step 34088, loss 0.138094, acc 0.9375
2017-03-02T18:16:37.265411: step 34089, loss 0.103212, acc 0.96875
2017-03-02T18:16:37.333267: step 34090, loss 0.18964, acc 0.90625
2017-03-02T18:16:37.415152: step 34091, loss 0.118676, acc 0.953125
2017-03-02T18:16:37.495811: step 34092, loss 0.22494, acc 0.84375
2017-03-02T18:16:37.564910: step 34093, loss 0.143357, acc 0.953125
2017-03-02T18:16:37.638004: step 34094, loss 0.154199, acc 0.921875
2017-03-02T18:16:37.712626: step 34095, loss 0.316073, acc 0.859375
2017-03-02T18:16:37.783167: step 34096, loss 0.111363, acc 0.953125
2017-03-02T18:16:37.853862: step 34097, loss 0.132876, acc 0.921875
2017-03-02T18:16:37.927020: step 34098, loss 0.153005, acc 0.921875
2017-03-02T18:16:37.996365: step 34099, loss 0.0991327, acc 0.9375
2017-03-02T18:16:38.065466: step 34100, loss 0.144415, acc 0.90625

Evaluation:
2017-03-02T18:16:38.102354: step 34100, loss 4.69148, acc 0.638068

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34100

2017-03-02T18:16:38.572405: step 34101, loss 0.208126, acc 0.953125
2017-03-02T18:16:38.641648: step 34102, loss 0.113526, acc 0.96875
2017-03-02T18:16:38.707575: step 34103, loss 0.112084, acc 0.9375
2017-03-02T18:16:38.773488: step 34104, loss 0.0201914, acc 1
2017-03-02T18:16:38.846387: step 34105, loss 0.0820127, acc 0.984375
2017-03-02T18:16:38.916642: step 34106, loss 0.0986043, acc 0.96875
2017-03-02T18:16:39.002303: step 34107, loss 0.114544, acc 0.953125
2017-03-02T18:16:39.076381: step 34108, loss 0.16773, acc 0.9375
2017-03-02T18:16:39.147670: step 34109, loss 0.239076, acc 0.890625
2017-03-02T18:16:39.218556: step 34110, loss 0.153033, acc 0.953125
2017-03-02T18:16:39.291051: step 34111, loss 0.121022, acc 0.96875
2017-03-02T18:16:39.369151: step 34112, loss 0.133377, acc 0.96875
2017-03-02T18:16:39.435820: step 34113, loss 0.129309, acc 0.90625
2017-03-02T18:16:39.517021: step 34114, loss 0.0739418, acc 0.96875
2017-03-02T18:16:39.592362: step 34115, loss 0.213126, acc 0.90625
2017-03-02T18:16:39.664929: step 34116, loss 0.16663, acc 0.921875
2017-03-02T18:16:39.750274: step 34117, loss 0.187487, acc 0.90625
2017-03-02T18:16:39.837114: step 34118, loss 0.146701, acc 0.9375
2017-03-02T18:16:39.908826: step 34119, loss 0.201396, acc 0.90625
2017-03-02T18:16:39.979477: step 34120, loss 0.13828, acc 0.890625
2017-03-02T18:16:40.046845: step 34121, loss 0.0711951, acc 0.96875
2017-03-02T18:16:40.114904: step 34122, loss 0.0783547, acc 0.96875
2017-03-02T18:16:40.188970: step 34123, loss 0.0693685, acc 0.96875
2017-03-02T18:16:40.261649: step 34124, loss 0.166528, acc 0.921875
2017-03-02T18:16:40.341940: step 34125, loss 0.208264, acc 0.90625
2017-03-02T18:16:40.410118: step 34126, loss 0.173265, acc 0.9375
2017-03-02T18:16:40.479772: step 34127, loss 0.116702, acc 0.9375
2017-03-02T18:16:40.551298: step 34128, loss 0.122433, acc 0.953125
2017-03-02T18:16:40.626757: step 34129, loss 0.121179, acc 0.9375
2017-03-02T18:16:40.694561: step 34130, loss 0.141791, acc 0.953125
2017-03-02T18:16:40.774639: step 34131, loss 0.121211, acc 0.9375
2017-03-02T18:16:40.842829: step 34132, loss 0.170452, acc 0.90625
2017-03-02T18:16:40.913569: step 34133, loss 0.227207, acc 0.921875
2017-03-02T18:16:40.986301: step 34134, loss 0.170876, acc 0.9375
2017-03-02T18:16:41.062397: step 34135, loss 0.099546, acc 0.9375
2017-03-02T18:16:41.138235: step 34136, loss 0.117952, acc 0.953125
2017-03-02T18:16:41.220010: step 34137, loss 0.119742, acc 0.9375
2017-03-02T18:16:41.293722: step 34138, loss 0.130264, acc 0.9375
2017-03-02T18:16:41.367826: step 34139, loss 0.0826278, acc 0.96875
2017-03-02T18:16:41.437510: step 34140, loss 0.107385, acc 0.96875
2017-03-02T18:16:41.510295: step 34141, loss 0.0624961, acc 0.984375
2017-03-02T18:16:41.586133: step 34142, loss 0.252758, acc 0.90625
2017-03-02T18:16:41.658066: step 34143, loss 0.104119, acc 0.9375
2017-03-02T18:16:41.732208: step 34144, loss 0.0843977, acc 0.96875
2017-03-02T18:16:41.809614: step 34145, loss 0.103364, acc 0.953125
2017-03-02T18:16:41.882170: step 34146, loss 0.132943, acc 0.9375
2017-03-02T18:16:41.955073: step 34147, loss 0.148502, acc 0.90625
2017-03-02T18:16:42.030855: step 34148, loss 0.0834916, acc 0.953125
2017-03-02T18:16:42.101953: step 34149, loss 0.138179, acc 0.9375
2017-03-02T18:16:42.167206: step 34150, loss 0.28109, acc 0.890625
2017-03-02T18:16:42.248496: step 34151, loss 0.130596, acc 0.953125
2017-03-02T18:16:42.323275: step 34152, loss 0.100192, acc 0.9375
2017-03-02T18:16:42.396425: step 34153, loss 0.134912, acc 0.953125
2017-03-02T18:16:42.476230: step 34154, loss 0.147248, acc 0.890625
2017-03-02T18:16:42.546242: step 34155, loss 0.161946, acc 0.90625
2017-03-02T18:16:42.608760: step 34156, loss 0.146355, acc 0.90625
2017-03-02T18:16:42.675786: step 34157, loss 0.0399279, acc 1
2017-03-02T18:16:42.746064: step 34158, loss 0.187379, acc 0.9375
2017-03-02T18:16:42.813874: step 34159, loss 0.182002, acc 0.90625
2017-03-02T18:16:42.880806: step 34160, loss 0.0291873, acc 1
2017-03-02T18:16:42.955038: step 34161, loss 0.136842, acc 0.953125
2017-03-02T18:16:43.030357: step 34162, loss 0.151366, acc 0.9375
2017-03-02T18:16:43.111791: step 34163, loss 0.113829, acc 0.9375
2017-03-02T18:16:43.182723: step 34164, loss 0.167271, acc 0.921875
2017-03-02T18:16:43.259784: step 34165, loss 0.181136, acc 0.90625
2017-03-02T18:16:43.338265: step 34166, loss 0.113719, acc 0.953125
2017-03-02T18:16:43.409635: step 34167, loss 0.303364, acc 0.859375
2017-03-02T18:16:43.479200: step 34168, loss 0.0838359, acc 0.96875
2017-03-02T18:16:43.547578: step 34169, loss 0.114645, acc 0.953125
2017-03-02T18:16:43.629979: step 34170, loss 0.0662687, acc 0.984375
2017-03-02T18:16:43.701986: step 34171, loss 0.19894, acc 0.90625
2017-03-02T18:16:43.778977: step 34172, loss 0.139538, acc 0.9375
2017-03-02T18:16:43.855065: step 34173, loss 0.138732, acc 0.90625
2017-03-02T18:16:43.924893: step 34174, loss 0.183996, acc 0.953125
2017-03-02T18:16:43.993704: step 34175, loss 0.0650707, acc 0.96875
2017-03-02T18:16:44.070400: step 34176, loss 0.189604, acc 0.9375
2017-03-02T18:16:44.146383: step 34177, loss 0.114296, acc 0.921875
2017-03-02T18:16:44.230269: step 34178, loss 0.263597, acc 0.90625
2017-03-02T18:16:44.308117: step 34179, loss 0.11255, acc 0.9375
2017-03-02T18:16:44.381944: step 34180, loss 0.221699, acc 0.921875
2017-03-02T18:16:44.456655: step 34181, loss 0.188474, acc 0.921875
2017-03-02T18:16:44.527728: step 34182, loss 0.0809541, acc 0.96875
2017-03-02T18:16:44.599189: step 34183, loss 0.0870838, acc 0.96875
2017-03-02T18:16:44.669511: step 34184, loss 0.0648364, acc 0.984375
2017-03-02T18:16:44.746133: step 34185, loss 0.297623, acc 0.890625
2017-03-02T18:16:44.827240: step 34186, loss 0.0433504, acc 0.984375
2017-03-02T18:16:44.902598: step 34187, loss 0.202339, acc 0.9375
2017-03-02T18:16:44.974548: step 34188, loss 0.108985, acc 0.9375
2017-03-02T18:16:45.044722: step 34189, loss 0.132839, acc 0.9375
2017-03-02T18:16:45.124455: step 34190, loss 0.211088, acc 0.890625
2017-03-02T18:16:45.198465: step 34191, loss 0.268664, acc 0.875
2017-03-02T18:16:45.272233: step 34192, loss 0.198038, acc 0.90625
2017-03-02T18:16:45.334313: step 34193, loss 0.141783, acc 0.921875
2017-03-02T18:16:45.405642: step 34194, loss 0.0768486, acc 0.96875
2017-03-02T18:16:45.483647: step 34195, loss 0.117669, acc 0.9375
2017-03-02T18:16:45.555582: step 34196, loss 0.230301, acc 0.921875
2017-03-02T18:16:45.626038: step 34197, loss 0.160748, acc 0.90625
2017-03-02T18:16:45.701102: step 34198, loss 0.0954685, acc 0.96875
2017-03-02T18:16:45.771909: step 34199, loss 0.128818, acc 0.921875
2017-03-02T18:16:45.842840: step 34200, loss 0.220411, acc 0.859375

Evaluation:
2017-03-02T18:16:45.879509: step 34200, loss 4.65741, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34200

2017-03-02T18:16:46.339768: step 34201, loss 0.107142, acc 0.9375
2017-03-02T18:16:46.408617: step 34202, loss 0.209563, acc 0.953125
2017-03-02T18:16:46.481387: step 34203, loss 0.163292, acc 0.921875
2017-03-02T18:16:46.558592: step 34204, loss 0.0803252, acc 0.984375
2017-03-02T18:16:46.640251: step 34205, loss 0.161873, acc 0.90625
2017-03-02T18:16:46.707919: step 34206, loss 0.128516, acc 0.953125
2017-03-02T18:16:46.781018: step 34207, loss 0.153666, acc 0.90625
2017-03-02T18:16:46.867789: step 34208, loss 0.143383, acc 0.90625
2017-03-02T18:16:46.937669: step 34209, loss 0.146189, acc 0.9375
2017-03-02T18:16:47.005895: step 34210, loss 0.159146, acc 0.921875
2017-03-02T18:16:47.083276: step 34211, loss 0.0962598, acc 0.953125
2017-03-02T18:16:47.156684: step 34212, loss 0.187063, acc 0.890625
2017-03-02T18:16:47.231093: step 34213, loss 0.134949, acc 0.96875
2017-03-02T18:16:47.307610: step 34214, loss 0.0821017, acc 0.953125
2017-03-02T18:16:47.383907: step 34215, loss 0.0847715, acc 0.953125
2017-03-02T18:16:47.453706: step 34216, loss 0.0979573, acc 0.953125
2017-03-02T18:16:47.524810: step 34217, loss 0.13825, acc 0.921875
2017-03-02T18:16:47.595164: step 34218, loss 0.124722, acc 0.96875
2017-03-02T18:16:47.675391: step 34219, loss 0.120285, acc 0.953125
2017-03-02T18:16:47.750594: step 34220, loss 0.238757, acc 0.921875
2017-03-02T18:16:47.822651: step 34221, loss 0.146809, acc 0.9375
2017-03-02T18:16:47.914128: step 34222, loss 0.158633, acc 0.953125
2017-03-02T18:16:47.989504: step 34223, loss 0.153508, acc 0.890625
2017-03-02T18:16:48.072784: step 34224, loss 0.154647, acc 0.953125
2017-03-02T18:16:48.149554: step 34225, loss 0.170677, acc 0.90625
2017-03-02T18:16:48.224819: step 34226, loss 0.0953059, acc 0.96875
2017-03-02T18:16:48.294735: step 34227, loss 0.186945, acc 0.90625
2017-03-02T18:16:48.365017: step 34228, loss 0.0832472, acc 0.96875
2017-03-02T18:16:48.436607: step 34229, loss 0.0930117, acc 0.953125
2017-03-02T18:16:48.507623: step 34230, loss 0.0763996, acc 0.953125
2017-03-02T18:16:48.582836: step 34231, loss 0.134351, acc 0.953125
2017-03-02T18:16:48.653915: step 34232, loss 0.15649, acc 0.9375
2017-03-02T18:16:48.718367: step 34233, loss 0.116268, acc 0.9375
2017-03-02T18:16:48.785239: step 34234, loss 0.219097, acc 0.921875
2017-03-02T18:16:48.855772: step 34235, loss 0.0997981, acc 0.96875
2017-03-02T18:16:48.926943: step 34236, loss 0.243605, acc 0.890625
2017-03-02T18:16:48.989845: step 34237, loss 0.163185, acc 0.9375
2017-03-02T18:16:49.058683: step 34238, loss 0.0739903, acc 0.96875
2017-03-02T18:16:49.136628: step 34239, loss 0.177038, acc 0.90625
2017-03-02T18:16:49.221712: step 34240, loss 0.198029, acc 0.90625
2017-03-02T18:16:49.298756: step 34241, loss 0.0910077, acc 0.953125
2017-03-02T18:16:49.374933: step 34242, loss 0.143067, acc 0.921875
2017-03-02T18:16:49.454884: step 34243, loss 0.0983788, acc 0.953125
2017-03-02T18:16:49.526352: step 34244, loss 0.0839509, acc 0.96875
2017-03-02T18:16:49.600623: step 34245, loss 0.0601979, acc 0.984375
2017-03-02T18:16:49.668468: step 34246, loss 0.146524, acc 0.921875
2017-03-02T18:16:49.759555: step 34247, loss 0.269798, acc 0.890625
2017-03-02T18:16:49.839780: step 34248, loss 0.209125, acc 0.890625
2017-03-02T18:16:49.909838: step 34249, loss 0.138268, acc 0.9375
2017-03-02T18:16:49.984681: step 34250, loss 0.172027, acc 0.9375
2017-03-02T18:16:50.055580: step 34251, loss 0.17215, acc 0.90625
2017-03-02T18:16:50.129049: step 34252, loss 0.152132, acc 0.953125
2017-03-02T18:16:50.209907: step 34253, loss 0.142077, acc 0.953125
2017-03-02T18:16:50.286644: step 34254, loss 0.158385, acc 0.890625
2017-03-02T18:16:50.359324: step 34255, loss 0.124011, acc 0.96875
2017-03-02T18:16:50.429143: step 34256, loss 0.0851896, acc 0.953125
2017-03-02T18:16:50.500513: step 34257, loss 0.0579816, acc 0.984375
2017-03-02T18:16:50.573995: step 34258, loss 0.0566466, acc 0.984375
2017-03-02T18:16:50.644140: step 34259, loss 0.247723, acc 0.859375
2017-03-02T18:16:50.715665: step 34260, loss 0.0656402, acc 0.96875
2017-03-02T18:16:50.787226: step 34261, loss 0.253194, acc 0.890625
2017-03-02T18:16:50.859142: step 34262, loss 0.111541, acc 0.96875
2017-03-02T18:16:50.932790: step 34263, loss 0.149342, acc 0.953125
2017-03-02T18:16:51.014573: step 34264, loss 0.0813134, acc 0.96875
2017-03-02T18:16:51.089994: step 34265, loss 0.103359, acc 0.953125
2017-03-02T18:16:51.172293: step 34266, loss 0.101737, acc 0.953125
2017-03-02T18:16:51.245191: step 34267, loss 0.242067, acc 0.875
2017-03-02T18:16:51.334803: step 34268, loss 0.149406, acc 0.890625
2017-03-02T18:16:51.418468: step 34269, loss 0.102231, acc 0.953125
2017-03-02T18:16:51.480794: step 34270, loss 0.0566483, acc 1
2017-03-02T18:16:51.554874: step 34271, loss 0.20106, acc 0.90625
2017-03-02T18:16:51.627200: step 34272, loss 0.0678929, acc 0.984375
2017-03-02T18:16:51.704423: step 34273, loss 0.1867, acc 0.90625
2017-03-02T18:16:51.772779: step 34274, loss 0.333964, acc 0.828125
2017-03-02T18:16:51.841773: step 34275, loss 0.279271, acc 0.875
2017-03-02T18:16:51.919599: step 34276, loss 0.121912, acc 0.921875
2017-03-02T18:16:51.988789: step 34277, loss 0.221616, acc 0.90625
2017-03-02T18:16:52.067829: step 34278, loss 0.149156, acc 0.921875
2017-03-02T18:16:52.141012: step 34279, loss 0.130285, acc 0.921875
2017-03-02T18:16:52.216497: step 34280, loss 0.0604077, acc 0.96875
2017-03-02T18:16:52.293762: step 34281, loss 0.111982, acc 0.921875
2017-03-02T18:16:52.368622: step 34282, loss 0.209724, acc 0.90625
2017-03-02T18:16:52.435426: step 34283, loss 0.109537, acc 0.96875
2017-03-02T18:16:52.506563: step 34284, loss 0.171576, acc 0.96875
2017-03-02T18:16:52.576826: step 34285, loss 0.12407, acc 0.921875
2017-03-02T18:16:52.645277: step 34286, loss 0.319919, acc 0.828125
2017-03-02T18:16:52.719176: step 34287, loss 0.0981587, acc 0.96875
2017-03-02T18:16:52.790148: step 34288, loss 0.150376, acc 0.921875
2017-03-02T18:16:52.861216: step 34289, loss 0.0786697, acc 0.96875
2017-03-02T18:16:52.926460: step 34290, loss 0.0472873, acc 0.96875
2017-03-02T18:16:52.995706: step 34291, loss 0.196593, acc 0.921875
2017-03-02T18:16:53.074706: step 34292, loss 0.12283, acc 0.9375
2017-03-02T18:16:53.145825: step 34293, loss 0.198139, acc 0.90625
2017-03-02T18:16:53.210067: step 34294, loss 0.146291, acc 0.953125
2017-03-02T18:16:53.292411: step 34295, loss 0.190835, acc 0.90625
2017-03-02T18:16:53.364488: step 34296, loss 0.183399, acc 0.90625
2017-03-02T18:16:53.440045: step 34297, loss 0.0728102, acc 0.984375
2017-03-02T18:16:53.512329: step 34298, loss 0.0916867, acc 0.984375
2017-03-02T18:16:53.589985: step 34299, loss 0.170644, acc 0.890625
2017-03-02T18:16:53.659943: step 34300, loss 0.0187242, acc 1

Evaluation:
2017-03-02T18:16:53.696946: step 34300, loss 4.6556, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34300

2017-03-02T18:16:54.149704: step 34301, loss 0.263732, acc 0.890625
2017-03-02T18:16:54.219112: step 34302, loss 0.106693, acc 0.9375
2017-03-02T18:16:54.291977: step 34303, loss 0.167662, acc 0.90625
2017-03-02T18:16:54.365096: step 34304, loss 0.0552163, acc 0.984375
2017-03-02T18:16:54.446988: step 34305, loss 0.214377, acc 0.90625
2017-03-02T18:16:54.512514: step 34306, loss 0.184931, acc 0.921875
2017-03-02T18:16:54.579582: step 34307, loss 0.123825, acc 0.953125
2017-03-02T18:16:54.649245: step 34308, loss 0.173306, acc 0.90625
2017-03-02T18:16:54.722475: step 34309, loss 0.177835, acc 0.90625
2017-03-02T18:16:54.792786: step 34310, loss 0.197685, acc 0.90625
2017-03-02T18:16:54.867839: step 34311, loss 0.12038, acc 0.953125
2017-03-02T18:16:54.949848: step 34312, loss 0.176545, acc 0.9375
2017-03-02T18:16:55.029094: step 34313, loss 0.202789, acc 0.890625
2017-03-02T18:16:55.101964: step 34314, loss 0.240872, acc 0.875
2017-03-02T18:16:55.172383: step 34315, loss 0.0906383, acc 0.9375
2017-03-02T18:16:55.243250: step 34316, loss 0.136365, acc 0.9375
2017-03-02T18:16:55.313367: step 34317, loss 0.14714, acc 0.9375
2017-03-02T18:16:55.384562: step 34318, loss 0.153532, acc 0.9375
2017-03-02T18:16:55.458242: step 34319, loss 0.0596201, acc 0.984375
2017-03-02T18:16:55.531656: step 34320, loss 0.154685, acc 0.953125
2017-03-02T18:16:55.601310: step 34321, loss 0.103359, acc 0.96875
2017-03-02T18:16:55.668311: step 34322, loss 0.100105, acc 0.984375
2017-03-02T18:16:55.728577: step 34323, loss 0.108753, acc 0.953125
2017-03-02T18:16:55.802036: step 34324, loss 0.0963516, acc 0.953125
2017-03-02T18:16:55.871613: step 34325, loss 0.0640796, acc 0.984375
2017-03-02T18:16:55.932362: step 34326, loss 0.142904, acc 0.921875
2017-03-02T18:16:55.998547: step 34327, loss 0.103501, acc 0.953125
2017-03-02T18:16:56.071481: step 34328, loss 0.130238, acc 0.921875
2017-03-02T18:16:56.132345: step 34329, loss 0.0634191, acc 0.984375
2017-03-02T18:16:56.193165: step 34330, loss 0.117609, acc 0.9375
2017-03-02T18:16:56.272687: step 34331, loss 0.10522, acc 0.96875
2017-03-02T18:16:56.334815: step 34332, loss 0.119834, acc 0.9375
2017-03-02T18:16:56.404339: step 34333, loss 0.142415, acc 0.953125
2017-03-02T18:16:56.475333: step 34334, loss 0.126023, acc 0.96875
2017-03-02T18:16:56.551800: step 34335, loss 0.168699, acc 0.90625
2017-03-02T18:16:56.623183: step 34336, loss 0.0995256, acc 0.953125
2017-03-02T18:16:56.692524: step 34337, loss 0.128692, acc 0.9375
2017-03-02T18:16:56.762996: step 34338, loss 0.160813, acc 0.9375
2017-03-02T18:16:56.832993: step 34339, loss 0.178665, acc 0.921875
2017-03-02T18:16:56.910915: step 34340, loss 0.0587827, acc 0.984375
2017-03-02T18:16:56.984404: step 34341, loss 0.0976893, acc 0.921875
2017-03-02T18:16:57.053178: step 34342, loss 0.151945, acc 0.953125
2017-03-02T18:16:57.120902: step 34343, loss 0.200498, acc 0.90625
2017-03-02T18:16:57.197343: step 34344, loss 0.0946417, acc 0.984375
2017-03-02T18:16:57.274952: step 34345, loss 0.0233126, acc 0.984375
2017-03-02T18:16:57.343381: step 34346, loss 0.157873, acc 0.921875
2017-03-02T18:16:57.417003: step 34347, loss 0.135066, acc 0.921875
2017-03-02T18:16:57.495943: step 34348, loss 0.109982, acc 0.9375
2017-03-02T18:16:57.571997: step 34349, loss 0.189029, acc 0.921875
2017-03-02T18:16:57.648028: step 34350, loss 0.211342, acc 0.921875
2017-03-02T18:16:57.723963: step 34351, loss 0.139539, acc 0.96875
2017-03-02T18:16:57.796916: step 34352, loss 0.0905975, acc 0.96875
2017-03-02T18:16:57.868113: step 34353, loss 0.217509, acc 0.90625
2017-03-02T18:16:57.947606: step 34354, loss 0.193678, acc 0.90625
2017-03-02T18:16:58.019445: step 34355, loss 0.148576, acc 0.921875
2017-03-02T18:16:58.100661: step 34356, loss 0.12116, acc 0.953125
2017-03-02T18:16:58.172343: step 34357, loss 0.0698535, acc 0.953125
2017-03-02T18:16:58.252994: step 34358, loss 0.13497, acc 0.921875
2017-03-02T18:16:58.318114: step 34359, loss 0.134442, acc 0.953125
2017-03-02T18:16:58.403790: step 34360, loss 0.139162, acc 0.9375
2017-03-02T18:16:58.477709: step 34361, loss 0.0759527, acc 0.953125
2017-03-02T18:16:58.548586: step 34362, loss 0.184561, acc 0.90625
2017-03-02T18:16:58.629930: step 34363, loss 0.111891, acc 0.953125
2017-03-02T18:16:58.699151: step 34364, loss 0.175617, acc 0.921875
2017-03-02T18:16:58.765694: step 34365, loss 0.144195, acc 0.953125
2017-03-02T18:16:58.837448: step 34366, loss 0.188924, acc 0.921875
2017-03-02T18:16:58.915162: step 34367, loss 0.0915154, acc 0.9375
2017-03-02T18:16:59.006892: step 34368, loss 0.157112, acc 0.921875
2017-03-02T18:16:59.081463: step 34369, loss 0.160798, acc 0.9375
2017-03-02T18:16:59.147586: step 34370, loss 0.152015, acc 0.953125
2017-03-02T18:16:59.227558: step 34371, loss 0.115237, acc 0.953125
2017-03-02T18:16:59.302061: step 34372, loss 0.136903, acc 0.921875
2017-03-02T18:16:59.367501: step 34373, loss 0.124877, acc 0.953125
2017-03-02T18:16:59.436498: step 34374, loss 0.135655, acc 0.9375
2017-03-02T18:16:59.506245: step 34375, loss 0.172249, acc 0.921875
2017-03-02T18:16:59.573219: step 34376, loss 0.154927, acc 0.921875
2017-03-02T18:16:59.642796: step 34377, loss 0.100777, acc 0.96875
2017-03-02T18:16:59.714888: step 34378, loss 0.203819, acc 0.90625
2017-03-02T18:16:59.786555: step 34379, loss 0.217534, acc 0.875
2017-03-02T18:16:59.861905: step 34380, loss 0.222479, acc 0.890625
2017-03-02T18:16:59.930438: step 34381, loss 0.315067, acc 0.90625
2017-03-02T18:17:00.005074: step 34382, loss 0.176704, acc 0.9375
2017-03-02T18:17:00.076871: step 34383, loss 0.144933, acc 0.90625
2017-03-02T18:17:00.154985: step 34384, loss 0.151205, acc 0.953125
2017-03-02T18:17:00.219515: step 34385, loss 0.19004, acc 0.859375
2017-03-02T18:17:00.297551: step 34386, loss 0.150134, acc 0.953125
2017-03-02T18:17:00.367711: step 34387, loss 0.178572, acc 0.9375
2017-03-02T18:17:00.447182: step 34388, loss 0.0632936, acc 0.984375
2017-03-02T18:17:00.526435: step 34389, loss 0.158909, acc 0.9375
2017-03-02T18:17:00.633566: step 34390, loss 0.131372, acc 0.953125
2017-03-02T18:17:00.699598: step 34391, loss 0.21409, acc 0.875
2017-03-02T18:17:00.766529: step 34392, loss 0.0714216, acc 0.953125
2017-03-02T18:17:00.833350: step 34393, loss 0.125181, acc 0.953125
2017-03-02T18:17:00.904223: step 34394, loss 0.115369, acc 0.921875
2017-03-02T18:17:00.975143: step 34395, loss 0.21104, acc 0.921875
2017-03-02T18:17:01.052052: step 34396, loss 0.230413, acc 0.890625
2017-03-02T18:17:01.122547: step 34397, loss 0.13206, acc 0.9375
2017-03-02T18:17:01.194306: step 34398, loss 0.0592389, acc 0.96875
2017-03-02T18:17:01.254866: step 34399, loss 0.158546, acc 0.921875
2017-03-02T18:17:01.325169: step 34400, loss 0.108903, acc 0.953125

Evaluation:
2017-03-02T18:17:01.353044: step 34400, loss 4.738, acc 0.636626

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34400

2017-03-02T18:17:01.819045: step 34401, loss 0.140693, acc 0.921875
2017-03-02T18:17:01.906214: step 34402, loss 0.326587, acc 0.875
2017-03-02T18:17:01.979107: step 34403, loss 0.0517255, acc 0.984375
2017-03-02T18:17:02.049352: step 34404, loss 0.172822, acc 0.921875
2017-03-02T18:17:02.122755: step 34405, loss 0.223601, acc 0.875
2017-03-02T18:17:02.190609: step 34406, loss 0.09479, acc 0.984375
2017-03-02T18:17:02.260225: step 34407, loss 0.0910489, acc 0.984375
2017-03-02T18:17:02.334176: step 34408, loss 0.0687234, acc 1
2017-03-02T18:17:02.407572: step 34409, loss 0.149051, acc 0.953125
2017-03-02T18:17:02.484960: step 34410, loss 0.21379, acc 0.90625
2017-03-02T18:17:02.559500: step 34411, loss 0.114732, acc 0.953125
2017-03-02T18:17:02.632554: step 34412, loss 0.155411, acc 0.90625
2017-03-02T18:17:02.697450: step 34413, loss 0.126687, acc 0.9375
2017-03-02T18:17:02.765980: step 34414, loss 0.200827, acc 0.90625
2017-03-02T18:17:02.843992: step 34415, loss 0.125526, acc 0.921875
2017-03-02T18:17:02.917893: step 34416, loss 0.125402, acc 0.953125
2017-03-02T18:17:03.000845: step 34417, loss 0.150442, acc 0.9375
2017-03-02T18:17:03.064636: step 34418, loss 0.264621, acc 0.859375
2017-03-02T18:17:03.142985: step 34419, loss 0.254946, acc 0.890625
2017-03-02T18:17:03.215979: step 34420, loss 0.121292, acc 0.921875
2017-03-02T18:17:03.284431: step 34421, loss 0.161292, acc 0.921875
2017-03-02T18:17:03.376478: step 34422, loss 0.0856547, acc 0.96875
2017-03-02T18:17:03.445782: step 34423, loss 0.248352, acc 0.90625
2017-03-02T18:17:03.518120: step 34424, loss 0.0670925, acc 0.984375
2017-03-02T18:17:03.597329: step 34425, loss 0.161797, acc 0.9375
2017-03-02T18:17:03.674299: step 34426, loss 0.202304, acc 0.875
2017-03-02T18:17:03.745575: step 34427, loss 0.120582, acc 0.96875
2017-03-02T18:17:03.827638: step 34428, loss 0.0879327, acc 0.953125
2017-03-02T18:17:03.901786: step 34429, loss 0.0425592, acc 0.984375
2017-03-02T18:17:03.974999: step 34430, loss 0.119497, acc 0.953125
2017-03-02T18:17:04.047264: step 34431, loss 0.0796338, acc 0.9375
2017-03-02T18:17:04.120491: step 34432, loss 0.0645555, acc 0.96875
2017-03-02T18:17:04.190004: step 34433, loss 0.0890194, acc 0.953125
2017-03-02T18:17:04.255277: step 34434, loss 0.263267, acc 0.875
2017-03-02T18:17:04.323421: step 34435, loss 0.143944, acc 0.9375
2017-03-02T18:17:04.397725: step 34436, loss 0.163677, acc 0.953125
2017-03-02T18:17:04.472372: step 34437, loss 0.0763629, acc 0.96875
2017-03-02T18:17:04.541379: step 34438, loss 0.16417, acc 0.953125
2017-03-02T18:17:04.618400: step 34439, loss 0.107982, acc 0.96875
2017-03-02T18:17:04.698777: step 34440, loss 0.121611, acc 0.953125
2017-03-02T18:17:04.765215: step 34441, loss 0.271347, acc 0.828125
2017-03-02T18:17:04.835538: step 34442, loss 0.17826, acc 0.9375
2017-03-02T18:17:04.910165: step 34443, loss 0.231921, acc 0.890625
2017-03-02T18:17:04.986377: step 34444, loss 0.0919358, acc 0.953125
2017-03-02T18:17:05.059196: step 34445, loss 0.119011, acc 0.921875
2017-03-02T18:17:05.128920: step 34446, loss 0.213105, acc 0.90625
2017-03-02T18:17:05.200799: step 34447, loss 0.0931972, acc 0.9375
2017-03-02T18:17:05.275695: step 34448, loss 0.286791, acc 0.890625
2017-03-02T18:17:05.348162: step 34449, loss 0.102445, acc 0.953125
2017-03-02T18:17:05.437105: step 34450, loss 0.16712, acc 0.90625
2017-03-02T18:17:05.505353: step 34451, loss 0.200002, acc 0.90625
2017-03-02T18:17:05.574083: step 34452, loss 0.135923, acc 0.953125
2017-03-02T18:17:05.647628: step 34453, loss 0.0901085, acc 0.96875
2017-03-02T18:17:05.716611: step 34454, loss 0.0671666, acc 0.96875
2017-03-02T18:17:05.785285: step 34455, loss 0.140746, acc 0.9375
2017-03-02T18:17:05.860661: step 34456, loss 0.148896, acc 0.953125
2017-03-02T18:17:05.936733: step 34457, loss 0.126264, acc 0.9375
2017-03-02T18:17:06.013232: step 34458, loss 0.0879748, acc 0.953125
2017-03-02T18:17:06.085941: step 34459, loss 0.0702464, acc 0.96875
2017-03-02T18:17:06.162386: step 34460, loss 0.0947, acc 0.953125
2017-03-02T18:17:06.229833: step 34461, loss 0.164199, acc 0.921875
2017-03-02T18:17:06.302766: step 34462, loss 0.226388, acc 0.890625
2017-03-02T18:17:06.371979: step 34463, loss 0.198096, acc 0.890625
2017-03-02T18:17:06.446752: step 34464, loss 0.131641, acc 0.921875
2017-03-02T18:17:06.523738: step 34465, loss 0.0734154, acc 0.96875
2017-03-02T18:17:06.595394: step 34466, loss 0.124391, acc 0.9375
2017-03-02T18:17:06.679572: step 34467, loss 0.202477, acc 0.921875
2017-03-02T18:17:06.750372: step 34468, loss 0.103357, acc 0.9375
2017-03-02T18:17:06.824300: step 34469, loss 0.108457, acc 0.953125
2017-03-02T18:17:06.917545: step 34470, loss 0.141107, acc 0.921875
2017-03-02T18:17:06.987304: step 34471, loss 0.148628, acc 0.921875
2017-03-02T18:17:07.056909: step 34472, loss 0.111328, acc 0.96875
2017-03-02T18:17:07.130786: step 34473, loss 0.100624, acc 0.9375
2017-03-02T18:17:07.211729: step 34474, loss 0.0736195, acc 0.984375
2017-03-02T18:17:07.287850: step 34475, loss 0.223172, acc 0.9375
2017-03-02T18:17:07.361531: step 34476, loss 0.124214, acc 0.953125
2017-03-02T18:17:07.433474: step 34477, loss 0.0825339, acc 0.984375
2017-03-02T18:17:07.509769: step 34478, loss 0.140243, acc 0.9375
2017-03-02T18:17:07.581389: step 34479, loss 0.0645655, acc 0.96875
2017-03-02T18:17:07.649167: step 34480, loss 0.0861009, acc 0.953125
2017-03-02T18:17:07.720502: step 34481, loss 0.240518, acc 0.890625
2017-03-02T18:17:07.794686: step 34482, loss 0.100881, acc 0.9375
2017-03-02T18:17:07.867526: step 34483, loss 0.132017, acc 0.90625
2017-03-02T18:17:07.949776: step 34484, loss 0.186751, acc 0.890625
2017-03-02T18:17:08.023905: step 34485, loss 0.170616, acc 0.953125
2017-03-02T18:17:08.095046: step 34486, loss 0.150439, acc 0.96875
2017-03-02T18:17:08.157506: step 34487, loss 0.178041, acc 0.90625
2017-03-02T18:17:08.222594: step 34488, loss 0.206129, acc 0.875
2017-03-02T18:17:08.295601: step 34489, loss 0.23395, acc 0.84375
2017-03-02T18:17:08.369816: step 34490, loss 0.102189, acc 0.9375
2017-03-02T18:17:08.440101: step 34491, loss 0.201679, acc 0.90625
2017-03-02T18:17:08.513244: step 34492, loss 0.0899081, acc 0.953125
2017-03-02T18:17:08.586577: step 34493, loss 0.0464409, acc 0.96875
2017-03-02T18:17:08.660785: step 34494, loss 0.403908, acc 0.78125
2017-03-02T18:17:08.731108: step 34495, loss 0.183824, acc 0.890625
2017-03-02T18:17:08.812913: step 34496, loss 0.00171515, acc 1
2017-03-02T18:17:08.892223: step 34497, loss 0.100346, acc 0.9375
2017-03-02T18:17:08.958760: step 34498, loss 0.124198, acc 0.953125
2017-03-02T18:17:09.022638: step 34499, loss 0.0691991, acc 0.953125
2017-03-02T18:17:09.088533: step 34500, loss 0.185531, acc 0.921875

Evaluation:
2017-03-02T18:17:09.124676: step 34500, loss 4.68763, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34500

2017-03-02T18:17:09.602015: step 34501, loss 0.178964, acc 0.921875
2017-03-02T18:17:09.677625: step 34502, loss 0.290944, acc 0.875
2017-03-02T18:17:09.743006: step 34503, loss 0.179254, acc 0.9375
2017-03-02T18:17:09.813113: step 34504, loss 0.0635346, acc 0.984375
2017-03-02T18:17:09.887694: step 34505, loss 0.148263, acc 0.921875
2017-03-02T18:17:09.961414: step 34506, loss 0.164867, acc 0.90625
2017-03-02T18:17:10.036598: step 34507, loss 0.08822, acc 0.96875
2017-03-02T18:17:10.111220: step 34508, loss 0.0786244, acc 0.953125
2017-03-02T18:17:10.182426: step 34509, loss 0.228587, acc 0.875
2017-03-02T18:17:10.251275: step 34510, loss 0.0520496, acc 0.984375
2017-03-02T18:17:10.322874: step 34511, loss 0.106411, acc 0.96875
2017-03-02T18:17:10.390059: step 34512, loss 0.136666, acc 0.9375
2017-03-02T18:17:10.455146: step 34513, loss 0.122002, acc 0.921875
2017-03-02T18:17:10.526423: step 34514, loss 0.122997, acc 0.9375
2017-03-02T18:17:10.599351: step 34515, loss 0.140772, acc 0.90625
2017-03-02T18:17:10.671630: step 34516, loss 0.120047, acc 0.953125
2017-03-02T18:17:10.753217: step 34517, loss 0.11923, acc 0.9375
2017-03-02T18:17:10.827062: step 34518, loss 0.173025, acc 0.953125
2017-03-02T18:17:10.902195: step 34519, loss 0.195511, acc 0.875
2017-03-02T18:17:10.975010: step 34520, loss 0.0729137, acc 0.984375
2017-03-02T18:17:11.049640: step 34521, loss 0.188606, acc 0.890625
2017-03-02T18:17:11.122878: step 34522, loss 0.10139, acc 0.9375
2017-03-02T18:17:11.196237: step 34523, loss 0.143187, acc 0.9375
2017-03-02T18:17:11.268177: step 34524, loss 0.189647, acc 0.859375
2017-03-02T18:17:11.341465: step 34525, loss 0.214533, acc 0.875
2017-03-02T18:17:11.412678: step 34526, loss 0.147312, acc 0.9375
2017-03-02T18:17:11.485458: step 34527, loss 0.117586, acc 0.9375
2017-03-02T18:17:11.561482: step 34528, loss 0.166526, acc 0.921875
2017-03-02T18:17:11.640564: step 34529, loss 0.186917, acc 0.890625
2017-03-02T18:17:11.722729: step 34530, loss 0.0491178, acc 1
2017-03-02T18:17:11.789011: step 34531, loss 0.256763, acc 0.859375
2017-03-02T18:17:11.856707: step 34532, loss 0.136766, acc 0.9375
2017-03-02T18:17:11.928637: step 34533, loss 0.284197, acc 0.859375
2017-03-02T18:17:12.006023: step 34534, loss 0.124285, acc 0.921875
2017-03-02T18:17:12.077765: step 34535, loss 0.147374, acc 0.90625
2017-03-02T18:17:12.149000: step 34536, loss 0.216125, acc 0.890625
2017-03-02T18:17:12.232796: step 34537, loss 0.10718, acc 0.953125
2017-03-02T18:17:12.298015: step 34538, loss 0.122206, acc 0.921875
2017-03-02T18:17:12.377012: step 34539, loss 0.108131, acc 0.953125
2017-03-02T18:17:12.443703: step 34540, loss 0.116982, acc 0.9375
2017-03-02T18:17:12.515682: step 34541, loss 0.118429, acc 0.96875
2017-03-02T18:17:12.587012: step 34542, loss 0.161282, acc 0.921875
2017-03-02T18:17:12.659958: step 34543, loss 0.197712, acc 0.890625
2017-03-02T18:17:12.737704: step 34544, loss 0.138594, acc 0.90625
2017-03-02T18:17:12.810006: step 34545, loss 0.323238, acc 0.875
2017-03-02T18:17:12.880738: step 34546, loss 0.0633427, acc 0.984375
2017-03-02T18:17:12.948728: step 34547, loss 0.181653, acc 0.890625
2017-03-02T18:17:13.021856: step 34548, loss 0.126161, acc 0.90625
2017-03-02T18:17:13.089116: step 34549, loss 0.183899, acc 0.921875
2017-03-02T18:17:13.162074: step 34550, loss 0.197634, acc 0.890625
2017-03-02T18:17:13.231409: step 34551, loss 0.0868866, acc 0.96875
2017-03-02T18:17:13.310142: step 34552, loss 0.0998512, acc 0.9375
2017-03-02T18:17:13.383649: step 34553, loss 0.0925858, acc 0.984375
2017-03-02T18:17:13.455354: step 34554, loss 0.0933849, acc 0.953125
2017-03-02T18:17:13.530557: step 34555, loss 0.144704, acc 0.953125
2017-03-02T18:17:13.603530: step 34556, loss 0.0505208, acc 0.984375
2017-03-02T18:17:13.679832: step 34557, loss 0.284957, acc 0.90625
2017-03-02T18:17:13.756357: step 34558, loss 0.126198, acc 0.9375
2017-03-02T18:17:13.826761: step 34559, loss 0.229397, acc 0.875
2017-03-02T18:17:13.895760: step 34560, loss 0.0851456, acc 0.953125
2017-03-02T18:17:13.971596: step 34561, loss 0.059406, acc 0.96875
2017-03-02T18:17:14.043796: step 34562, loss 0.0860438, acc 0.96875
2017-03-02T18:17:14.119011: step 34563, loss 0.0775098, acc 0.96875
2017-03-02T18:17:14.194190: step 34564, loss 0.0974718, acc 0.96875
2017-03-02T18:17:14.263541: step 34565, loss 0.101738, acc 0.9375
2017-03-02T18:17:14.337578: step 34566, loss 0.0817689, acc 0.96875
2017-03-02T18:17:14.396837: step 34567, loss 0.303294, acc 0.828125
2017-03-02T18:17:14.470374: step 34568, loss 0.190557, acc 0.90625
2017-03-02T18:17:14.540157: step 34569, loss 0.112434, acc 0.9375
2017-03-02T18:17:14.606670: step 34570, loss 0.128974, acc 0.9375
2017-03-02T18:17:14.679996: step 34571, loss 0.17375, acc 0.921875
2017-03-02T18:17:14.749693: step 34572, loss 0.168388, acc 0.953125
2017-03-02T18:17:14.849966: step 34573, loss 0.0771442, acc 0.984375
2017-03-02T18:17:14.915627: step 34574, loss 0.179197, acc 0.921875
2017-03-02T18:17:14.984865: step 34575, loss 0.16546, acc 0.890625
2017-03-02T18:17:15.068884: step 34576, loss 0.186919, acc 0.90625
2017-03-02T18:17:15.139167: step 34577, loss 0.176473, acc 0.921875
2017-03-02T18:17:15.217795: step 34578, loss 0.122152, acc 0.96875
2017-03-02T18:17:15.273810: step 34579, loss 0.167631, acc 0.890625
2017-03-02T18:17:15.345439: step 34580, loss 0.226026, acc 0.921875
2017-03-02T18:17:15.416253: step 34581, loss 0.107899, acc 0.921875
2017-03-02T18:17:15.494150: step 34582, loss 0.0813571, acc 0.96875
2017-03-02T18:17:15.584768: step 34583, loss 0.163191, acc 0.921875
2017-03-02T18:17:15.660465: step 34584, loss 0.0877957, acc 0.9375
2017-03-02T18:17:15.728312: step 34585, loss 0.101302, acc 0.96875
2017-03-02T18:17:15.796792: step 34586, loss 0.0773621, acc 0.96875
2017-03-02T18:17:15.879406: step 34587, loss 0.310588, acc 0.828125
2017-03-02T18:17:15.949574: step 34588, loss 0.166564, acc 0.9375
2017-03-02T18:17:16.015816: step 34589, loss 0.0966294, acc 0.953125
2017-03-02T18:17:16.093233: step 34590, loss 0.0493714, acc 0.984375
2017-03-02T18:17:16.164156: step 34591, loss 0.141436, acc 0.953125
2017-03-02T18:17:16.230144: step 34592, loss 0.129759, acc 0.9375
2017-03-02T18:17:16.292759: step 34593, loss 0.108274, acc 0.96875
2017-03-02T18:17:16.362513: step 34594, loss 0.157755, acc 0.9375
2017-03-02T18:17:16.442372: step 34595, loss 0.200826, acc 0.9375
2017-03-02T18:17:16.514329: step 34596, loss 0.0924428, acc 0.96875
2017-03-02T18:17:16.586579: step 34597, loss 0.156003, acc 0.953125
2017-03-02T18:17:16.657641: step 34598, loss 0.184232, acc 0.921875
2017-03-02T18:17:16.723638: step 34599, loss 0.209471, acc 0.890625
2017-03-02T18:17:16.788148: step 34600, loss 0.12665, acc 0.921875

Evaluation:
2017-03-02T18:17:16.822978: step 34600, loss 4.74551, acc 0.635184

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34600

2017-03-02T18:17:17.290506: step 34601, loss 0.101924, acc 0.9375
2017-03-02T18:17:17.351922: step 34602, loss 0.263379, acc 0.875
2017-03-02T18:17:17.426000: step 34603, loss 0.107377, acc 0.90625
2017-03-02T18:17:17.498686: step 34604, loss 0.131981, acc 0.9375
2017-03-02T18:17:17.570292: step 34605, loss 0.121574, acc 0.9375
2017-03-02T18:17:17.653826: step 34606, loss 0.126403, acc 0.9375
2017-03-02T18:17:17.728344: step 34607, loss 0.209598, acc 0.875
2017-03-02T18:17:17.803784: step 34608, loss 0.195122, acc 0.90625
2017-03-02T18:17:17.876459: step 34609, loss 0.115667, acc 0.953125
2017-03-02T18:17:17.950660: step 34610, loss 0.107847, acc 0.96875
2017-03-02T18:17:18.018282: step 34611, loss 0.177057, acc 0.90625
2017-03-02T18:17:18.092301: step 34612, loss 0.0878423, acc 0.953125
2017-03-02T18:17:18.169729: step 34613, loss 0.230956, acc 0.890625
2017-03-02T18:17:18.242848: step 34614, loss 0.142686, acc 0.9375
2017-03-02T18:17:18.329374: step 34615, loss 0.080495, acc 0.96875
2017-03-02T18:17:18.401559: step 34616, loss 0.0872982, acc 0.96875
2017-03-02T18:17:18.474407: step 34617, loss 0.161619, acc 0.9375
2017-03-02T18:17:18.547018: step 34618, loss 0.163322, acc 0.9375
2017-03-02T18:17:18.625310: step 34619, loss 0.175583, acc 0.9375
2017-03-02T18:17:18.697818: step 34620, loss 0.10222, acc 0.96875
2017-03-02T18:17:18.766690: step 34621, loss 0.104873, acc 0.953125
2017-03-02T18:17:18.841686: step 34622, loss 0.183123, acc 0.9375
2017-03-02T18:17:18.932112: step 34623, loss 0.131112, acc 0.9375
2017-03-02T18:17:19.005492: step 34624, loss 0.171713, acc 0.9375
2017-03-02T18:17:19.079880: step 34625, loss 0.0851705, acc 0.984375
2017-03-02T18:17:19.151594: step 34626, loss 0.22474, acc 0.890625
2017-03-02T18:17:19.221761: step 34627, loss 0.127644, acc 0.953125
2017-03-02T18:17:19.290112: step 34628, loss 0.21449, acc 0.921875
2017-03-02T18:17:19.357210: step 34629, loss 0.206721, acc 0.890625
2017-03-02T18:17:19.424838: step 34630, loss 0.201691, acc 0.9375
2017-03-02T18:17:19.492418: step 34631, loss 0.0756975, acc 0.9375
2017-03-02T18:17:19.569718: step 34632, loss 0.0738169, acc 0.9375
2017-03-02T18:17:19.631124: step 34633, loss 0.128421, acc 0.953125
2017-03-02T18:17:19.698105: step 34634, loss 0.124473, acc 0.953125
2017-03-02T18:17:19.772771: step 34635, loss 0.276578, acc 0.890625
2017-03-02T18:17:19.837054: step 34636, loss 0.147463, acc 0.9375
2017-03-02T18:17:19.908243: step 34637, loss 0.180087, acc 0.921875
2017-03-02T18:17:19.981682: step 34638, loss 0.166908, acc 0.90625
2017-03-02T18:17:20.058499: step 34639, loss 0.150395, acc 0.90625
2017-03-02T18:17:20.127735: step 34640, loss 0.0990504, acc 0.984375
2017-03-02T18:17:20.193904: step 34641, loss 0.163233, acc 0.9375
2017-03-02T18:17:20.262882: step 34642, loss 0.132983, acc 0.953125
2017-03-02T18:17:20.332109: step 34643, loss 0.116868, acc 0.921875
2017-03-02T18:17:20.395676: step 34644, loss 0.130269, acc 0.953125
2017-03-02T18:17:20.472283: step 34645, loss 0.0589632, acc 1
2017-03-02T18:17:20.547207: step 34646, loss 0.122704, acc 0.921875
2017-03-02T18:17:20.619473: step 34647, loss 0.129736, acc 0.953125
2017-03-02T18:17:20.692576: step 34648, loss 0.145301, acc 0.9375
2017-03-02T18:17:20.761190: step 34649, loss 0.149052, acc 0.90625
2017-03-02T18:17:20.824936: step 34650, loss 0.0598896, acc 0.984375
2017-03-02T18:17:20.894478: step 34651, loss 0.206663, acc 0.890625
2017-03-02T18:17:20.968852: step 34652, loss 0.135745, acc 0.921875
2017-03-02T18:17:21.042161: step 34653, loss 0.068552, acc 0.96875
2017-03-02T18:17:21.117513: step 34654, loss 0.174395, acc 0.9375
2017-03-02T18:17:21.193037: step 34655, loss 0.137909, acc 0.9375
2017-03-02T18:17:21.270591: step 34656, loss 0.0903242, acc 0.984375
2017-03-02T18:17:21.341284: step 34657, loss 0.144048, acc 0.921875
2017-03-02T18:17:21.419290: step 34658, loss 0.0592632, acc 0.96875
2017-03-02T18:17:21.486092: step 34659, loss 0.210704, acc 0.84375
2017-03-02T18:17:21.556737: step 34660, loss 0.1562, acc 0.9375
2017-03-02T18:17:21.628833: step 34661, loss 0.239525, acc 0.921875
2017-03-02T18:17:21.707797: step 34662, loss 0.0727576, acc 0.96875
2017-03-02T18:17:21.793281: step 34663, loss 0.213113, acc 0.890625
2017-03-02T18:17:21.874483: step 34664, loss 0.0720651, acc 0.96875
2017-03-02T18:17:21.948421: step 34665, loss 0.221492, acc 0.90625
2017-03-02T18:17:22.024400: step 34666, loss 0.114484, acc 0.96875
2017-03-02T18:17:22.118334: step 34667, loss 0.035398, acc 0.984375
2017-03-02T18:17:22.189505: step 34668, loss 0.0892173, acc 0.984375
2017-03-02T18:17:22.252448: step 34669, loss 0.118394, acc 0.953125
2017-03-02T18:17:22.319797: step 34670, loss 0.0923811, acc 0.953125
2017-03-02T18:17:22.386371: step 34671, loss 0.209583, acc 0.90625
2017-03-02T18:17:22.461036: step 34672, loss 0.206985, acc 0.90625
2017-03-02T18:17:22.529784: step 34673, loss 0.101897, acc 0.984375
2017-03-02T18:17:22.607402: step 34674, loss 0.19034, acc 0.90625
2017-03-02T18:17:22.683168: step 34675, loss 0.150556, acc 0.90625
2017-03-02T18:17:22.754928: step 34676, loss 0.106709, acc 0.984375
2017-03-02T18:17:22.821385: step 34677, loss 0.245021, acc 0.921875
2017-03-02T18:17:22.892403: step 34678, loss 0.100912, acc 0.96875
2017-03-02T18:17:22.968648: step 34679, loss 0.104182, acc 0.96875
2017-03-02T18:17:23.055695: step 34680, loss 0.206454, acc 0.90625
2017-03-02T18:17:23.134400: step 34681, loss 0.176026, acc 0.90625
2017-03-02T18:17:23.217327: step 34682, loss 0.184006, acc 0.90625
2017-03-02T18:17:23.286349: step 34683, loss 0.177457, acc 0.921875
2017-03-02T18:17:23.358092: step 34684, loss 0.0737879, acc 0.96875
2017-03-02T18:17:23.429303: step 34685, loss 0.120449, acc 0.921875
2017-03-02T18:17:23.494770: step 34686, loss 0.11139, acc 0.953125
2017-03-02T18:17:23.561839: step 34687, loss 0.0739866, acc 0.96875
2017-03-02T18:17:23.629604: step 34688, loss 0.114815, acc 0.953125
2017-03-02T18:17:23.706285: step 34689, loss 0.0690859, acc 0.96875
2017-03-02T18:17:23.784999: step 34690, loss 0.123708, acc 0.953125
2017-03-02T18:17:23.866062: step 34691, loss 0.149445, acc 0.90625
2017-03-02T18:17:23.931523: step 34692, loss 0.333129, acc 0.75
2017-03-02T18:17:24.015421: step 34693, loss 0.0958289, acc 0.96875
2017-03-02T18:17:24.091037: step 34694, loss 0.146229, acc 0.921875
2017-03-02T18:17:24.161694: step 34695, loss 0.176143, acc 0.921875
2017-03-02T18:17:24.226397: step 34696, loss 0.161061, acc 0.90625
2017-03-02T18:17:24.309592: step 34697, loss 0.162251, acc 0.90625
2017-03-02T18:17:24.385716: step 34698, loss 0.194704, acc 0.890625
2017-03-02T18:17:24.463538: step 34699, loss 0.118643, acc 0.9375
2017-03-02T18:17:24.544913: step 34700, loss 0.140171, acc 0.9375

Evaluation:
2017-03-02T18:17:24.580448: step 34700, loss 4.73035, acc 0.637347

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34700

2017-03-02T18:17:25.036154: step 34701, loss 0.175293, acc 0.90625
2017-03-02T18:17:25.109782: step 34702, loss 0.0594753, acc 0.984375
2017-03-02T18:17:25.181768: step 34703, loss 0.130528, acc 0.953125
2017-03-02T18:17:25.248027: step 34704, loss 0.18328, acc 0.921875
2017-03-02T18:17:25.314811: step 34705, loss 0.220664, acc 0.890625
2017-03-02T18:17:25.385911: step 34706, loss 0.0942922, acc 0.953125
2017-03-02T18:17:25.456395: step 34707, loss 0.166507, acc 0.921875
2017-03-02T18:17:25.531145: step 34708, loss 0.153397, acc 0.921875
2017-03-02T18:17:25.610958: step 34709, loss 0.10663, acc 0.921875
2017-03-02T18:17:25.690918: step 34710, loss 0.155193, acc 0.890625
2017-03-02T18:17:25.762347: step 34711, loss 0.084563, acc 0.953125
2017-03-02T18:17:25.838409: step 34712, loss 0.141759, acc 0.90625
2017-03-02T18:17:25.903814: step 34713, loss 0.125757, acc 0.96875
2017-03-02T18:17:25.976716: step 34714, loss 0.146369, acc 0.96875
2017-03-02T18:17:26.052844: step 34715, loss 0.172921, acc 0.9375
2017-03-02T18:17:26.130740: step 34716, loss 0.106081, acc 0.921875
2017-03-02T18:17:26.209992: step 34717, loss 0.153433, acc 0.890625
2017-03-02T18:17:26.280962: step 34718, loss 0.117099, acc 0.96875
2017-03-02T18:17:26.346357: step 34719, loss 0.0690112, acc 0.96875
2017-03-02T18:17:26.418046: step 34720, loss 0.267448, acc 0.859375
2017-03-02T18:17:26.491189: step 34721, loss 0.101712, acc 0.921875
2017-03-02T18:17:26.566033: step 34722, loss 0.121427, acc 0.90625
2017-03-02T18:17:26.640635: step 34723, loss 0.0976314, acc 0.953125
2017-03-02T18:17:26.714766: step 34724, loss 0.175725, acc 0.90625
2017-03-02T18:17:26.786715: step 34725, loss 0.0839177, acc 0.9375
2017-03-02T18:17:26.863480: step 34726, loss 0.156094, acc 0.921875
2017-03-02T18:17:26.936557: step 34727, loss 0.214727, acc 0.90625
2017-03-02T18:17:27.015517: step 34728, loss 0.120907, acc 0.953125
2017-03-02T18:17:27.083715: step 34729, loss 0.0754912, acc 0.96875
2017-03-02T18:17:27.159545: step 34730, loss 0.149371, acc 0.953125
2017-03-02T18:17:27.232240: step 34731, loss 0.0760053, acc 0.984375
2017-03-02T18:17:27.308997: step 34732, loss 0.140249, acc 0.9375
2017-03-02T18:17:27.386504: step 34733, loss 0.194295, acc 0.9375
2017-03-02T18:17:27.472383: step 34734, loss 0.102247, acc 0.953125
2017-03-02T18:17:27.544984: step 34735, loss 0.135979, acc 0.9375
2017-03-02T18:17:27.615400: step 34736, loss 0.171099, acc 0.921875
2017-03-02T18:17:27.685713: step 34737, loss 0.0991621, acc 0.9375
2017-03-02T18:17:27.771636: step 34738, loss 0.228546, acc 0.84375
2017-03-02T18:17:27.842636: step 34739, loss 0.114926, acc 0.9375
2017-03-02T18:17:27.914247: step 34740, loss 0.0968555, acc 0.96875
2017-03-02T18:17:27.988467: step 34741, loss 0.183359, acc 0.9375
2017-03-02T18:17:28.059709: step 34742, loss 0.179507, acc 0.90625
2017-03-02T18:17:28.134767: step 34743, loss 0.0883403, acc 0.953125
2017-03-02T18:17:28.205662: step 34744, loss 0.157445, acc 0.921875
2017-03-02T18:17:28.284080: step 34745, loss 0.040816, acc 0.984375
2017-03-02T18:17:28.350184: step 34746, loss 0.118427, acc 0.953125
2017-03-02T18:17:28.418354: step 34747, loss 0.06751, acc 0.96875
2017-03-02T18:17:28.489726: step 34748, loss 0.209185, acc 0.9375
2017-03-02T18:17:28.560406: step 34749, loss 0.195388, acc 0.890625
2017-03-02T18:17:28.627542: step 34750, loss 0.0921566, acc 0.953125
2017-03-02T18:17:28.700144: step 34751, loss 0.0634709, acc 0.96875
2017-03-02T18:17:28.776473: step 34752, loss 0.102279, acc 0.953125
2017-03-02T18:17:28.852426: step 34753, loss 0.130166, acc 0.921875
2017-03-02T18:17:28.922783: step 34754, loss 0.217496, acc 0.859375
2017-03-02T18:17:28.998205: step 34755, loss 0.165598, acc 0.921875
2017-03-02T18:17:29.069291: step 34756, loss 0.0815729, acc 0.953125
2017-03-02T18:17:29.138602: step 34757, loss 0.155946, acc 0.96875
2017-03-02T18:17:29.212750: step 34758, loss 0.110511, acc 0.9375
2017-03-02T18:17:29.283867: step 34759, loss 0.107793, acc 0.96875
2017-03-02T18:17:29.356764: step 34760, loss 0.14165, acc 0.9375
2017-03-02T18:17:29.431460: step 34761, loss 0.199279, acc 0.875
2017-03-02T18:17:29.495360: step 34762, loss 0.0883792, acc 0.96875
2017-03-02T18:17:29.557346: step 34763, loss 0.166891, acc 0.9375
2017-03-02T18:17:29.638785: step 34764, loss 0.0864167, acc 0.984375
2017-03-02T18:17:29.701976: step 34765, loss 0.186147, acc 0.921875
2017-03-02T18:17:29.771467: step 34766, loss 0.162078, acc 0.953125
2017-03-02T18:17:29.839942: step 34767, loss 0.306374, acc 0.859375
2017-03-02T18:17:29.911894: step 34768, loss 0.112409, acc 0.9375
2017-03-02T18:17:29.988539: step 34769, loss 0.0727687, acc 0.984375
2017-03-02T18:17:30.064289: step 34770, loss 0.121426, acc 0.96875
2017-03-02T18:17:30.139947: step 34771, loss 0.130667, acc 0.921875
2017-03-02T18:17:30.211973: step 34772, loss 0.211883, acc 0.921875
2017-03-02T18:17:30.281922: step 34773, loss 0.150686, acc 0.953125
2017-03-02T18:17:30.361224: step 34774, loss 0.159393, acc 0.9375
2017-03-02T18:17:30.431174: step 34775, loss 0.248327, acc 0.890625
2017-03-02T18:17:30.501704: step 34776, loss 0.135796, acc 0.96875
2017-03-02T18:17:30.572298: step 34777, loss 0.0707104, acc 0.984375
2017-03-02T18:17:30.641474: step 34778, loss 0.147919, acc 0.921875
2017-03-02T18:17:30.718340: step 34779, loss 0.173293, acc 0.90625
2017-03-02T18:17:30.793598: step 34780, loss 0.132875, acc 0.953125
2017-03-02T18:17:30.868109: step 34781, loss 0.107754, acc 0.953125
2017-03-02T18:17:30.937600: step 34782, loss 0.111375, acc 0.9375
2017-03-02T18:17:31.010937: step 34783, loss 0.0978032, acc 0.953125
2017-03-02T18:17:31.083896: step 34784, loss 0.10355, acc 0.953125
2017-03-02T18:17:31.147867: step 34785, loss 0.0690588, acc 0.984375
2017-03-02T18:17:31.214273: step 34786, loss 0.284588, acc 0.875
2017-03-02T18:17:31.288338: step 34787, loss 0.133416, acc 0.921875
2017-03-02T18:17:31.364039: step 34788, loss 0.0863911, acc 0.9375
2017-03-02T18:17:31.437482: step 34789, loss 0.170221, acc 0.90625
2017-03-02T18:17:31.515346: step 34790, loss 0.165585, acc 0.890625
2017-03-02T18:17:31.588836: step 34791, loss 0.158056, acc 0.90625
2017-03-02T18:17:31.662129: step 34792, loss 0.153252, acc 0.921875
2017-03-02T18:17:31.733808: step 34793, loss 0.184164, acc 0.921875
2017-03-02T18:17:31.804228: step 34794, loss 0.156106, acc 0.921875
2017-03-02T18:17:31.868891: step 34795, loss 0.147316, acc 0.921875
2017-03-02T18:17:31.940495: step 34796, loss 0.0853359, acc 0.9375
2017-03-02T18:17:32.011888: step 34797, loss 0.155943, acc 0.96875
2017-03-02T18:17:32.082934: step 34798, loss 0.161084, acc 0.921875
2017-03-02T18:17:32.159703: step 34799, loss 0.078237, acc 0.96875
2017-03-02T18:17:32.234119: step 34800, loss 0.104297, acc 0.953125

Evaluation:
2017-03-02T18:17:32.274927: step 34800, loss 4.76438, acc 0.631579

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34800

2017-03-02T18:17:32.822280: step 34801, loss 0.0719626, acc 0.984375
2017-03-02T18:17:32.898056: step 34802, loss 0.08316, acc 0.984375
2017-03-02T18:17:32.978439: step 34803, loss 0.155318, acc 0.921875
2017-03-02T18:17:33.052774: step 34804, loss 0.134697, acc 0.921875
2017-03-02T18:17:33.125446: step 34805, loss 0.0639049, acc 0.953125
2017-03-02T18:17:33.199093: step 34806, loss 0.14772, acc 0.9375
2017-03-02T18:17:33.272150: step 34807, loss 0.0318858, acc 1
2017-03-02T18:17:33.340841: step 34808, loss 0.119703, acc 0.953125
2017-03-02T18:17:33.414451: step 34809, loss 0.165113, acc 0.953125
2017-03-02T18:17:33.492177: step 34810, loss 0.0648497, acc 0.984375
2017-03-02T18:17:33.569050: step 34811, loss 0.242565, acc 0.875
2017-03-02T18:17:33.640096: step 34812, loss 0.138289, acc 0.9375
2017-03-02T18:17:33.711300: step 34813, loss 0.0538038, acc 0.984375
2017-03-02T18:17:33.778258: step 34814, loss 0.180501, acc 0.9375
2017-03-02T18:17:33.854063: step 34815, loss 0.183614, acc 0.9375
2017-03-02T18:17:33.918748: step 34816, loss 0.155072, acc 0.921875
2017-03-02T18:17:33.987788: step 34817, loss 0.134493, acc 0.9375
2017-03-02T18:17:34.059273: step 34818, loss 0.172143, acc 0.90625
2017-03-02T18:17:34.131967: step 34819, loss 0.242109, acc 0.875
2017-03-02T18:17:34.208086: step 34820, loss 0.203569, acc 0.890625
2017-03-02T18:17:34.283503: step 34821, loss 0.152856, acc 0.9375
2017-03-02T18:17:34.361090: step 34822, loss 0.234176, acc 0.921875
2017-03-02T18:17:34.434440: step 34823, loss 0.151423, acc 0.890625
2017-03-02T18:17:34.505522: step 34824, loss 0.101884, acc 0.9375
2017-03-02T18:17:34.579797: step 34825, loss 0.252988, acc 0.875
2017-03-02T18:17:34.665953: step 34826, loss 0.213445, acc 0.890625
2017-03-02T18:17:34.744575: step 34827, loss 0.134432, acc 0.96875
2017-03-02T18:17:34.811322: step 34828, loss 0.149833, acc 0.953125
2017-03-02T18:17:34.879793: step 34829, loss 0.172321, acc 0.921875
2017-03-02T18:17:34.952530: step 34830, loss 0.124046, acc 0.953125
2017-03-02T18:17:35.021549: step 34831, loss 0.0854633, acc 0.953125
2017-03-02T18:17:35.093135: step 34832, loss 0.0415697, acc 0.984375
2017-03-02T18:17:35.168910: step 34833, loss 0.130646, acc 0.96875
2017-03-02T18:17:35.241492: step 34834, loss 0.220772, acc 0.875
2017-03-02T18:17:35.307908: step 34835, loss 0.170699, acc 0.90625
2017-03-02T18:17:35.376851: step 34836, loss 0.165756, acc 0.9375
2017-03-02T18:17:35.462149: step 34837, loss 0.129305, acc 0.96875
2017-03-02T18:17:35.538451: step 34838, loss 0.102546, acc 0.96875
2017-03-02T18:17:35.609704: step 34839, loss 0.195109, acc 0.90625
2017-03-02T18:17:35.684779: step 34840, loss 0.0340731, acc 0.984375
2017-03-02T18:17:35.763729: step 34841, loss 0.143137, acc 0.953125
2017-03-02T18:17:35.833257: step 34842, loss 0.0899715, acc 0.921875
2017-03-02T18:17:35.906047: step 34843, loss 0.163143, acc 0.90625
2017-03-02T18:17:35.981642: step 34844, loss 0.234046, acc 0.890625
2017-03-02T18:17:36.054355: step 34845, loss 0.14626, acc 0.9375
2017-03-02T18:17:36.123232: step 34846, loss 0.113454, acc 0.9375
2017-03-02T18:17:36.204760: step 34847, loss 0.25239, acc 0.859375
2017-03-02T18:17:36.278343: step 34848, loss 0.103283, acc 0.953125
2017-03-02T18:17:36.350272: step 34849, loss 0.208982, acc 0.9375
2017-03-02T18:17:36.422321: step 34850, loss 0.102896, acc 0.9375
2017-03-02T18:17:36.495649: step 34851, loss 0.185527, acc 0.921875
2017-03-02T18:17:36.569672: step 34852, loss 0.190539, acc 0.875
2017-03-02T18:17:36.646820: step 34853, loss 0.169523, acc 0.9375
2017-03-02T18:17:36.713891: step 34854, loss 0.126539, acc 0.9375
2017-03-02T18:17:36.776248: step 34855, loss 0.124385, acc 0.9375
2017-03-02T18:17:36.846475: step 34856, loss 0.142019, acc 0.9375
2017-03-02T18:17:36.917553: step 34857, loss 0.117497, acc 0.9375
2017-03-02T18:17:36.990225: step 34858, loss 0.12652, acc 0.9375
2017-03-02T18:17:37.062006: step 34859, loss 0.121794, acc 0.921875
2017-03-02T18:17:37.135484: step 34860, loss 0.160031, acc 0.90625
2017-03-02T18:17:37.207176: step 34861, loss 0.191548, acc 0.921875
2017-03-02T18:17:37.277334: step 34862, loss 0.23145, acc 0.921875
2017-03-02T18:17:37.350613: step 34863, loss 0.102747, acc 0.9375
2017-03-02T18:17:37.422176: step 34864, loss 0.0812697, acc 0.984375
2017-03-02T18:17:37.491575: step 34865, loss 0.127944, acc 0.921875
2017-03-02T18:17:37.569893: step 34866, loss 0.131315, acc 0.9375
2017-03-02T18:17:37.638058: step 34867, loss 0.101675, acc 0.984375
2017-03-02T18:17:37.711412: step 34868, loss 0.195263, acc 0.890625
2017-03-02T18:17:37.789397: step 34869, loss 0.158805, acc 0.9375
2017-03-02T18:17:37.850090: step 34870, loss 0.256874, acc 0.890625
2017-03-02T18:17:37.923145: step 34871, loss 0.224746, acc 0.890625
2017-03-02T18:17:37.997270: step 34872, loss 0.161477, acc 0.96875
2017-03-02T18:17:38.064547: step 34873, loss 0.0801338, acc 0.96875
2017-03-02T18:17:38.129929: step 34874, loss 0.0643339, acc 0.96875
2017-03-02T18:17:38.199808: step 34875, loss 0.138543, acc 0.90625
2017-03-02T18:17:38.275328: step 34876, loss 0.198006, acc 0.921875
2017-03-02T18:17:38.353412: step 34877, loss 0.137599, acc 0.9375
2017-03-02T18:17:38.429322: step 34878, loss 0.112239, acc 0.96875
2017-03-02T18:17:38.502893: step 34879, loss 0.309293, acc 0.890625
2017-03-02T18:17:38.571756: step 34880, loss 0.240013, acc 0.921875
2017-03-02T18:17:38.644247: step 34881, loss 0.11903, acc 0.953125
2017-03-02T18:17:38.714001: step 34882, loss 0.119397, acc 0.96875
2017-03-02T18:17:38.785480: step 34883, loss 0.0832067, acc 0.953125
2017-03-02T18:17:38.852893: step 34884, loss 0.107299, acc 0.953125
2017-03-02T18:17:38.928610: step 34885, loss 0.170373, acc 0.890625
2017-03-02T18:17:39.001412: step 34886, loss 0.17203, acc 0.90625
2017-03-02T18:17:39.061812: step 34887, loss 0.0887446, acc 0.953125
2017-03-02T18:17:39.145773: step 34888, loss 0.0334739, acc 1
2017-03-02T18:17:39.220860: step 34889, loss 0.101546, acc 0.9375
2017-03-02T18:17:39.295183: step 34890, loss 0.20781, acc 0.875
2017-03-02T18:17:39.366334: step 34891, loss 0.127938, acc 0.96875
2017-03-02T18:17:39.435852: step 34892, loss 0.0995983, acc 0.9375
2017-03-02T18:17:39.506974: step 34893, loss 0.0977816, acc 0.953125
2017-03-02T18:17:39.578489: step 34894, loss 0.0580615, acc 0.96875
2017-03-02T18:17:39.649842: step 34895, loss 0.129107, acc 0.9375
2017-03-02T18:17:39.723289: step 34896, loss 0.138746, acc 0.96875
2017-03-02T18:17:39.795190: step 34897, loss 0.165045, acc 0.9375
2017-03-02T18:17:39.868147: step 34898, loss 0.179555, acc 0.90625
2017-03-02T18:17:39.939514: step 34899, loss 0.0259709, acc 1
2017-03-02T18:17:40.013978: step 34900, loss 0.0928589, acc 0.984375

Evaluation:
2017-03-02T18:17:40.051185: step 34900, loss 4.8192, acc 0.645999

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-34900

2017-03-02T18:17:40.548290: step 34901, loss 0.112127, acc 0.96875
2017-03-02T18:17:40.623941: step 34902, loss 0.0829715, acc 0.953125
2017-03-02T18:17:40.696941: step 34903, loss 0.256697, acc 0.90625
2017-03-02T18:17:40.770578: step 34904, loss 0.0850061, acc 0.953125
2017-03-02T18:17:40.838409: step 34905, loss 0.132183, acc 0.921875
2017-03-02T18:17:40.907966: step 34906, loss 0.122381, acc 0.953125
2017-03-02T18:17:40.983625: step 34907, loss 0.108291, acc 0.953125
2017-03-02T18:17:41.055383: step 34908, loss 0.130762, acc 0.921875
2017-03-02T18:17:41.131645: step 34909, loss 0.130904, acc 0.9375
2017-03-02T18:17:41.203962: step 34910, loss 0.211516, acc 0.921875
2017-03-02T18:17:41.276934: step 34911, loss 0.235366, acc 0.90625
2017-03-02T18:17:41.350504: step 34912, loss 0.0728019, acc 0.984375
2017-03-02T18:17:41.419162: step 34913, loss 0.0998831, acc 0.96875
2017-03-02T18:17:41.487089: step 34914, loss 0.203402, acc 0.890625
2017-03-02T18:17:41.553197: step 34915, loss 0.0715943, acc 0.984375
2017-03-02T18:17:41.622941: step 34916, loss 0.227579, acc 0.90625
2017-03-02T18:17:41.693080: step 34917, loss 0.0957014, acc 0.96875
2017-03-02T18:17:41.774877: step 34918, loss 0.0900439, acc 0.96875
2017-03-02T18:17:41.846501: step 34919, loss 0.145942, acc 0.953125
2017-03-02T18:17:41.929432: step 34920, loss 0.137076, acc 0.953125
2017-03-02T18:17:42.002498: step 34921, loss 0.0793541, acc 0.96875
2017-03-02T18:17:42.074888: step 34922, loss 0.184532, acc 0.90625
2017-03-02T18:17:42.152202: step 34923, loss 0.0878983, acc 0.953125
2017-03-02T18:17:42.227031: step 34924, loss 0.13519, acc 0.921875
2017-03-02T18:17:42.309215: step 34925, loss 0.213346, acc 0.859375
2017-03-02T18:17:42.389744: step 34926, loss 0.160671, acc 0.9375
2017-03-02T18:17:42.460037: step 34927, loss 0.219203, acc 0.890625
2017-03-02T18:17:42.526236: step 34928, loss 0.117858, acc 0.921875
2017-03-02T18:17:42.603582: step 34929, loss 0.112296, acc 0.9375
2017-03-02T18:17:42.677022: step 34930, loss 0.0494767, acc 0.984375
2017-03-02T18:17:42.749957: step 34931, loss 0.131068, acc 0.921875
2017-03-02T18:17:42.827306: step 34932, loss 0.137756, acc 0.9375
2017-03-02T18:17:42.894554: step 34933, loss 0.0810747, acc 0.96875
2017-03-02T18:17:42.958874: step 34934, loss 0.234292, acc 0.890625
2017-03-02T18:17:43.032437: step 34935, loss 0.0556869, acc 0.984375
2017-03-02T18:17:43.105766: step 34936, loss 0.0505095, acc 0.96875
2017-03-02T18:17:43.196901: step 34937, loss 0.136923, acc 0.953125
2017-03-02T18:17:43.266004: step 34938, loss 0.183068, acc 0.90625
2017-03-02T18:17:43.337073: step 34939, loss 0.11652, acc 0.921875
2017-03-02T18:17:43.408164: step 34940, loss 0.0779254, acc 0.96875
2017-03-02T18:17:43.479519: step 34941, loss 0.155593, acc 0.9375
2017-03-02T18:17:43.549080: step 34942, loss 0.163026, acc 0.9375
2017-03-02T18:17:43.618119: step 34943, loss 0.170199, acc 0.921875
2017-03-02T18:17:43.694281: step 34944, loss 0.103641, acc 0.953125
2017-03-02T18:17:43.767862: step 34945, loss 0.0730784, acc 0.9375
2017-03-02T18:17:43.839541: step 34946, loss 0.140566, acc 0.921875
2017-03-02T18:17:43.916219: step 34947, loss 0.119881, acc 0.953125
2017-03-02T18:17:43.987288: step 34948, loss 0.27767, acc 0.9375
2017-03-02T18:17:44.060948: step 34949, loss 0.0472693, acc 1
2017-03-02T18:17:44.131193: step 34950, loss 0.186271, acc 0.921875
2017-03-02T18:17:44.213984: step 34951, loss 0.120613, acc 0.953125
2017-03-02T18:17:44.283306: step 34952, loss 0.162746, acc 0.890625
2017-03-02T18:17:44.349356: step 34953, loss 0.141202, acc 0.90625
2017-03-02T18:17:44.431386: step 34954, loss 0.144609, acc 0.90625
2017-03-02T18:17:44.502158: step 34955, loss 0.119389, acc 0.921875
2017-03-02T18:17:44.584463: step 34956, loss 0.132742, acc 0.9375
2017-03-02T18:17:44.660594: step 34957, loss 0.13904, acc 0.921875
2017-03-02T18:17:44.741365: step 34958, loss 0.261782, acc 0.890625
2017-03-02T18:17:44.811206: step 34959, loss 0.165614, acc 0.9375
2017-03-02T18:17:44.884514: step 34960, loss 0.165501, acc 0.9375
2017-03-02T18:17:44.950280: step 34961, loss 0.103598, acc 0.953125
2017-03-02T18:17:45.017228: step 34962, loss 0.160512, acc 0.9375
2017-03-02T18:17:45.085643: step 34963, loss 0.15788, acc 0.921875
2017-03-02T18:17:45.156813: step 34964, loss 0.177959, acc 0.9375
2017-03-02T18:17:45.228564: step 34965, loss 0.146796, acc 0.9375
2017-03-02T18:17:45.291208: step 34966, loss 0.109881, acc 0.96875
2017-03-02T18:17:45.362812: step 34967, loss 0.224845, acc 0.90625
2017-03-02T18:17:45.430336: step 34968, loss 0.141058, acc 0.953125
2017-03-02T18:17:45.497527: step 34969, loss 0.275322, acc 0.859375
2017-03-02T18:17:45.567591: step 34970, loss 0.21756, acc 0.890625
2017-03-02T18:17:45.632150: step 34971, loss 0.0891462, acc 0.984375
2017-03-02T18:17:45.696008: step 34972, loss 0.0604877, acc 0.984375
2017-03-02T18:17:45.764343: step 34973, loss 0.147816, acc 0.921875
2017-03-02T18:17:45.840772: step 34974, loss 0.188032, acc 0.90625
2017-03-02T18:17:45.915773: step 34975, loss 0.0571495, acc 0.96875
2017-03-02T18:17:45.988166: step 34976, loss 0.145429, acc 0.9375
2017-03-02T18:17:46.061218: step 34977, loss 0.175638, acc 0.90625
2017-03-02T18:17:46.135907: step 34978, loss 0.218339, acc 0.90625
2017-03-02T18:17:46.204640: step 34979, loss 0.10496, acc 0.953125
2017-03-02T18:17:46.285638: step 34980, loss 0.143968, acc 0.9375
2017-03-02T18:17:46.356702: step 34981, loss 0.0916793, acc 0.984375
2017-03-02T18:17:46.430368: step 34982, loss 0.0735383, acc 0.953125
2017-03-02T18:17:46.526628: step 34983, loss 0.18745, acc 0.921875
2017-03-02T18:17:46.603944: step 34984, loss 0.0943341, acc 0.953125
2017-03-02T18:17:46.693812: step 34985, loss 0.169817, acc 0.90625
2017-03-02T18:17:46.762495: step 34986, loss 0.148135, acc 0.953125
2017-03-02T18:17:46.841457: step 34987, loss 0.0884539, acc 0.984375
2017-03-02T18:17:46.921004: step 34988, loss 0.111355, acc 0.953125
2017-03-02T18:17:47.001840: step 34989, loss 0.237418, acc 0.875
2017-03-02T18:17:47.074944: step 34990, loss 0.14486, acc 0.90625
2017-03-02T18:17:47.144664: step 34991, loss 0.111567, acc 0.96875
2017-03-02T18:17:47.215144: step 34992, loss 0.241254, acc 0.890625
2017-03-02T18:17:47.280453: step 34993, loss 0.159276, acc 0.9375
2017-03-02T18:17:47.351402: step 34994, loss 0.328799, acc 0.859375
2017-03-02T18:17:47.447601: step 34995, loss 0.237987, acc 0.921875
2017-03-02T18:17:47.527191: step 34996, loss 0.131484, acc 0.9375
2017-03-02T18:17:47.601362: step 34997, loss 0.120869, acc 0.96875
2017-03-02T18:17:47.681147: step 34998, loss 0.095159, acc 0.953125
2017-03-02T18:17:47.745082: step 34999, loss 0.20633, acc 0.890625
2017-03-02T18:17:47.810233: step 35000, loss 0.219225, acc 0.890625

Evaluation:
2017-03-02T18:17:47.843769: step 35000, loss 4.79602, acc 0.648161

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35000

2017-03-02T18:17:48.286275: step 35001, loss 0.18648, acc 0.953125
2017-03-02T18:17:48.363749: step 35002, loss 0.15617, acc 0.921875
2017-03-02T18:17:48.437007: step 35003, loss 0.13608, acc 0.921875
2017-03-02T18:17:48.508663: step 35004, loss 0.0477357, acc 0.984375
2017-03-02T18:17:48.583640: step 35005, loss 0.0970566, acc 0.96875
2017-03-02T18:17:48.653912: step 35006, loss 0.0951658, acc 0.953125
2017-03-02T18:17:48.731718: step 35007, loss 0.339257, acc 0.859375
2017-03-02T18:17:48.805419: step 35008, loss 0.160994, acc 0.921875
2017-03-02T18:17:48.885158: step 35009, loss 0.143064, acc 0.90625
2017-03-02T18:17:48.960311: step 35010, loss 0.0475083, acc 0.96875
2017-03-02T18:17:49.031238: step 35011, loss 0.114666, acc 0.953125
2017-03-02T18:17:49.100466: step 35012, loss 0.117159, acc 0.9375
2017-03-02T18:17:49.170004: step 35013, loss 0.183981, acc 0.875
2017-03-02T18:17:49.240440: step 35014, loss 0.142194, acc 0.921875
2017-03-02T18:17:49.312965: step 35015, loss 0.193585, acc 0.890625
2017-03-02T18:17:49.376890: step 35016, loss 0.12911, acc 0.9375
2017-03-02T18:17:49.449108: step 35017, loss 0.144446, acc 0.890625
2017-03-02T18:17:49.520422: step 35018, loss 0.100776, acc 0.953125
2017-03-02T18:17:49.589469: step 35019, loss 0.25447, acc 0.90625
2017-03-02T18:17:49.662277: step 35020, loss 0.181319, acc 0.921875
2017-03-02T18:17:49.727908: step 35021, loss 0.0437015, acc 0.984375
2017-03-02T18:17:49.795474: step 35022, loss 0.251971, acc 0.90625
2017-03-02T18:17:49.864608: step 35023, loss 0.146419, acc 0.953125
2017-03-02T18:17:49.942156: step 35024, loss 0.25152, acc 0.890625
2017-03-02T18:17:50.021624: step 35025, loss 0.107555, acc 0.953125
2017-03-02T18:17:50.095676: step 35026, loss 0.0902138, acc 0.984375
2017-03-02T18:17:50.168327: step 35027, loss 0.0594494, acc 0.984375
2017-03-02T18:17:50.228601: step 35028, loss 0.259491, acc 0.859375
2017-03-02T18:17:50.303488: step 35029, loss 0.15118, acc 0.90625
2017-03-02T18:17:50.379864: step 35030, loss 0.148812, acc 0.953125
2017-03-02T18:17:50.447743: step 35031, loss 0.153332, acc 0.9375
2017-03-02T18:17:50.514604: step 35032, loss 0.208324, acc 0.890625
2017-03-02T18:17:50.588026: step 35033, loss 0.0796755, acc 0.953125
2017-03-02T18:17:50.659428: step 35034, loss 0.0523675, acc 0.96875
2017-03-02T18:17:50.730013: step 35035, loss 0.126688, acc 0.9375
2017-03-02T18:17:50.801921: step 35036, loss 0.184734, acc 0.921875
2017-03-02T18:17:50.873541: step 35037, loss 0.118958, acc 0.953125
2017-03-02T18:17:50.941980: step 35038, loss 0.16559, acc 0.921875
2017-03-02T18:17:51.010437: step 35039, loss 0.201551, acc 0.921875
2017-03-02T18:17:51.087004: step 35040, loss 0.154029, acc 0.921875
2017-03-02T18:17:51.159615: step 35041, loss 0.0776675, acc 0.953125
2017-03-02T18:17:51.237244: step 35042, loss 0.0692354, acc 0.953125
2017-03-02T18:17:51.300092: step 35043, loss 0.104693, acc 0.953125
2017-03-02T18:17:51.368959: step 35044, loss 0.185214, acc 0.96875
2017-03-02T18:17:51.444636: step 35045, loss 0.0943911, acc 0.96875
2017-03-02T18:17:51.522964: step 35046, loss 0.104276, acc 0.953125
2017-03-02T18:17:51.604232: step 35047, loss 0.112149, acc 0.96875
2017-03-02T18:17:51.684131: step 35048, loss 0.175408, acc 0.921875
2017-03-02T18:17:51.758112: step 35049, loss 0.169661, acc 0.921875
2017-03-02T18:17:51.824617: step 35050, loss 0.150893, acc 0.90625
2017-03-02T18:17:51.886776: step 35051, loss 0.178705, acc 0.90625
2017-03-02T18:17:51.966069: step 35052, loss 0.0702975, acc 0.96875
2017-03-02T18:17:52.043467: step 35053, loss 0.144427, acc 0.9375
2017-03-02T18:17:52.133126: step 35054, loss 0.104692, acc 0.953125
2017-03-02T18:17:52.216358: step 35055, loss 0.188372, acc 0.921875
2017-03-02T18:17:52.286845: step 35056, loss 0.224566, acc 0.921875
2017-03-02T18:17:52.359290: step 35057, loss 0.191118, acc 0.890625
2017-03-02T18:17:52.430557: step 35058, loss 0.167366, acc 0.9375
2017-03-02T18:17:52.507541: step 35059, loss 0.188495, acc 0.90625
2017-03-02T18:17:52.576136: step 35060, loss 0.273355, acc 0.890625
2017-03-02T18:17:52.654014: step 35061, loss 0.166562, acc 0.921875
2017-03-02T18:17:52.723011: step 35062, loss 0.206275, acc 0.890625
2017-03-02T18:17:52.797160: step 35063, loss 0.0910731, acc 0.96875
2017-03-02T18:17:52.867455: step 35064, loss 0.186995, acc 0.890625
2017-03-02T18:17:52.943747: step 35065, loss 0.159566, acc 0.953125
2017-03-02T18:17:53.012332: step 35066, loss 0.175867, acc 0.90625
2017-03-02T18:17:53.086059: step 35067, loss 0.230167, acc 0.90625
2017-03-02T18:17:53.164138: step 35068, loss 0.100208, acc 0.96875
2017-03-02T18:17:53.243199: step 35069, loss 0.0975835, acc 0.96875
2017-03-02T18:17:53.309260: step 35070, loss 0.151874, acc 0.953125
2017-03-02T18:17:53.392956: step 35071, loss 0.116145, acc 0.9375
2017-03-02T18:17:53.473517: step 35072, loss 0.185067, acc 0.9375
2017-03-02T18:17:53.560842: step 35073, loss 0.126441, acc 0.953125
2017-03-02T18:17:53.631951: step 35074, loss 0.123675, acc 0.9375
2017-03-02T18:17:53.708753: step 35075, loss 0.125462, acc 0.96875
2017-03-02T18:17:53.782912: step 35076, loss 0.110086, acc 0.953125
2017-03-02T18:17:53.854595: step 35077, loss 0.158002, acc 0.921875
2017-03-02T18:17:53.926727: step 35078, loss 0.174429, acc 0.9375
2017-03-02T18:17:53.995530: step 35079, loss 0.228362, acc 0.859375
2017-03-02T18:17:54.077020: step 35080, loss 0.106382, acc 0.953125
2017-03-02T18:17:54.148690: step 35081, loss 0.19043, acc 0.9375
2017-03-02T18:17:54.218624: step 35082, loss 0.10041, acc 0.953125
2017-03-02T18:17:54.291777: step 35083, loss 0.124725, acc 0.921875
2017-03-02T18:17:54.357312: step 35084, loss 0, acc 1
2017-03-02T18:17:54.433195: step 35085, loss 0.0976565, acc 0.984375
2017-03-02T18:17:54.523543: step 35086, loss 0.0279415, acc 0.984375
2017-03-02T18:17:54.591787: step 35087, loss 0.0863749, acc 0.984375
2017-03-02T18:17:54.667906: step 35088, loss 0.0478728, acc 0.984375
2017-03-02T18:17:54.738185: step 35089, loss 0.0659866, acc 0.984375
2017-03-02T18:17:54.808696: step 35090, loss 0.11808, acc 0.9375
2017-03-02T18:17:54.882105: step 35091, loss 0.132334, acc 0.90625
2017-03-02T18:17:54.952090: step 35092, loss 0.144007, acc 0.921875
2017-03-02T18:17:55.024683: step 35093, loss 0.13321, acc 0.921875
2017-03-02T18:17:55.103143: step 35094, loss 0.157229, acc 0.90625
2017-03-02T18:17:55.186298: step 35095, loss 0.123201, acc 0.953125
2017-03-02T18:17:55.253302: step 35096, loss 0.135954, acc 0.96875
2017-03-02T18:17:55.319321: step 35097, loss 0.153223, acc 0.9375
2017-03-02T18:17:55.401926: step 35098, loss 0.106805, acc 0.9375
2017-03-02T18:17:55.466935: step 35099, loss 0.145741, acc 0.9375
2017-03-02T18:17:55.549053: step 35100, loss 0.0594956, acc 0.984375

Evaluation:
2017-03-02T18:17:55.589660: step 35100, loss 4.86078, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35100

2017-03-02T18:17:56.042355: step 35101, loss 0.0861917, acc 0.96875
2017-03-02T18:17:56.121588: step 35102, loss 0.117421, acc 0.921875
2017-03-02T18:17:56.197452: step 35103, loss 0.0992114, acc 0.96875
2017-03-02T18:17:56.271178: step 35104, loss 0.137706, acc 0.9375
2017-03-02T18:17:56.344290: step 35105, loss 0.167916, acc 0.90625
2017-03-02T18:17:56.417323: step 35106, loss 0.267724, acc 0.921875
2017-03-02T18:17:56.492118: step 35107, loss 0.102743, acc 0.96875
2017-03-02T18:17:56.565845: step 35108, loss 0.306775, acc 0.84375
2017-03-02T18:17:56.632900: step 35109, loss 0.0517268, acc 1
2017-03-02T18:17:56.703448: step 35110, loss 0.107018, acc 0.96875
2017-03-02T18:17:56.781557: step 35111, loss 0.159441, acc 0.921875
2017-03-02T18:17:56.858318: step 35112, loss 0.113347, acc 0.9375
2017-03-02T18:17:56.924333: step 35113, loss 0.173697, acc 0.9375
2017-03-02T18:17:57.003238: step 35114, loss 0.0983151, acc 0.9375
2017-03-02T18:17:57.077899: step 35115, loss 0.189373, acc 0.921875
2017-03-02T18:17:57.150246: step 35116, loss 0.136552, acc 0.96875
2017-03-02T18:17:57.223346: step 35117, loss 0.183035, acc 0.890625
2017-03-02T18:17:57.297217: step 35118, loss 0.173011, acc 0.890625
2017-03-02T18:17:57.364282: step 35119, loss 0.130041, acc 0.953125
2017-03-02T18:17:57.435259: step 35120, loss 0.0971058, acc 0.96875
2017-03-02T18:17:57.517196: step 35121, loss 0.111136, acc 0.9375
2017-03-02T18:17:57.592466: step 35122, loss 0.108283, acc 0.921875
2017-03-02T18:17:57.665313: step 35123, loss 0.113298, acc 0.9375
2017-03-02T18:17:57.737613: step 35124, loss 0.130946, acc 0.96875
2017-03-02T18:17:57.809435: step 35125, loss 0.113527, acc 0.96875
2017-03-02T18:17:57.881545: step 35126, loss 0.145923, acc 0.9375
2017-03-02T18:17:57.961149: step 35127, loss 0.187692, acc 0.890625
2017-03-02T18:17:58.027287: step 35128, loss 0.199263, acc 0.9375
2017-03-02T18:17:58.097773: step 35129, loss 0.111132, acc 0.96875
2017-03-02T18:17:58.170866: step 35130, loss 0.135128, acc 0.953125
2017-03-02T18:17:58.240223: step 35131, loss 0.182072, acc 0.9375
2017-03-02T18:17:58.309308: step 35132, loss 0.215529, acc 0.890625
2017-03-02T18:17:58.380874: step 35133, loss 0.172022, acc 0.890625
2017-03-02T18:17:58.454050: step 35134, loss 0.284019, acc 0.84375
2017-03-02T18:17:58.532172: step 35135, loss 0.0980377, acc 0.953125
2017-03-02T18:17:58.598650: step 35136, loss 0.0481969, acc 1
2017-03-02T18:17:58.671050: step 35137, loss 0.155084, acc 0.9375
2017-03-02T18:17:58.739753: step 35138, loss 0.120829, acc 0.921875
2017-03-02T18:17:58.815874: step 35139, loss 0.206994, acc 0.90625
2017-03-02T18:17:58.887206: step 35140, loss 0.127667, acc 0.921875
2017-03-02T18:17:58.967235: step 35141, loss 0.117497, acc 0.9375
2017-03-02T18:17:59.036720: step 35142, loss 0.104636, acc 0.9375
2017-03-02T18:17:59.107320: step 35143, loss 0.149797, acc 0.921875
2017-03-02T18:17:59.182399: step 35144, loss 0.0986178, acc 0.953125
2017-03-02T18:17:59.258996: step 35145, loss 0.0908317, acc 0.953125
2017-03-02T18:17:59.339955: step 35146, loss 0.256904, acc 0.90625
2017-03-02T18:17:59.410933: step 35147, loss 0.115963, acc 0.9375
2017-03-02T18:17:59.477478: step 35148, loss 0.152616, acc 0.9375
2017-03-02T18:17:59.549555: step 35149, loss 0.0967332, acc 0.984375
2017-03-02T18:17:59.622560: step 35150, loss 0.179569, acc 0.890625
2017-03-02T18:17:59.705995: step 35151, loss 0.153703, acc 0.9375
2017-03-02T18:17:59.776483: step 35152, loss 0.0547273, acc 0.984375
2017-03-02T18:17:59.854433: step 35153, loss 0.115029, acc 0.9375
2017-03-02T18:17:59.932761: step 35154, loss 0.126775, acc 0.9375
2017-03-02T18:18:00.005808: step 35155, loss 0.151423, acc 0.921875
2017-03-02T18:18:00.072919: step 35156, loss 0.0845386, acc 0.953125
2017-03-02T18:18:00.143102: step 35157, loss 0.0695887, acc 0.9375
2017-03-02T18:18:00.220306: step 35158, loss 0.121011, acc 0.9375
2017-03-02T18:18:00.292790: step 35159, loss 0.120384, acc 0.96875
2017-03-02T18:18:00.371156: step 35160, loss 0.139164, acc 0.9375
2017-03-02T18:18:00.437939: step 35161, loss 0.156367, acc 0.9375
2017-03-02T18:18:00.511284: step 35162, loss 0.103748, acc 0.9375
2017-03-02T18:18:00.584411: step 35163, loss 0.194982, acc 0.890625
2017-03-02T18:18:00.656357: step 35164, loss 0.300543, acc 0.890625
2017-03-02T18:18:00.748901: step 35165, loss 0.14526, acc 0.9375
2017-03-02T18:18:00.820525: step 35166, loss 0.105778, acc 0.953125
2017-03-02T18:18:00.890283: step 35167, loss 0.189736, acc 0.890625
2017-03-02T18:18:00.964699: step 35168, loss 0.0776616, acc 0.984375
2017-03-02T18:18:01.024728: step 35169, loss 0.139057, acc 0.953125
2017-03-02T18:18:01.096879: step 35170, loss 0.232274, acc 0.875
2017-03-02T18:18:01.175048: step 35171, loss 0.214488, acc 0.90625
2017-03-02T18:18:01.250494: step 35172, loss 0.215314, acc 0.890625
2017-03-02T18:18:01.330410: step 35173, loss 0.119194, acc 0.9375
2017-03-02T18:18:01.403003: step 35174, loss 0.120011, acc 0.953125
2017-03-02T18:18:01.475798: step 35175, loss 0.131274, acc 0.953125
2017-03-02T18:18:01.550428: step 35176, loss 0.172027, acc 0.90625
2017-03-02T18:18:01.620275: step 35177, loss 0.0891513, acc 0.9375
2017-03-02T18:18:01.691957: step 35178, loss 0.114863, acc 0.921875
2017-03-02T18:18:01.765667: step 35179, loss 0.0631316, acc 0.984375
2017-03-02T18:18:01.839217: step 35180, loss 0.0897089, acc 0.96875
2017-03-02T18:18:01.907202: step 35181, loss 0.144946, acc 0.953125
2017-03-02T18:18:01.981534: step 35182, loss 0.149832, acc 0.921875
2017-03-02T18:18:02.057614: step 35183, loss 0.250089, acc 0.890625
2017-03-02T18:18:02.128578: step 35184, loss 0.0772296, acc 0.96875
2017-03-02T18:18:02.199511: step 35185, loss 0.127028, acc 0.921875
2017-03-02T18:18:02.270403: step 35186, loss 0.076813, acc 0.953125
2017-03-02T18:18:02.340732: step 35187, loss 0.215915, acc 0.90625
2017-03-02T18:18:02.415884: step 35188, loss 0.125973, acc 0.9375
2017-03-02T18:18:02.486007: step 35189, loss 0.121812, acc 0.9375
2017-03-02T18:18:02.560088: step 35190, loss 0.115648, acc 0.953125
2017-03-02T18:18:02.642956: step 35191, loss 0.150352, acc 0.921875
2017-03-02T18:18:02.715166: step 35192, loss 0.183563, acc 0.953125
2017-03-02T18:18:02.789292: step 35193, loss 0.202175, acc 0.890625
2017-03-02T18:18:02.856420: step 35194, loss 0.0871636, acc 0.96875
2017-03-02T18:18:02.931665: step 35195, loss 0.186201, acc 0.9375
2017-03-02T18:18:02.997041: step 35196, loss 0.0972646, acc 0.96875
2017-03-02T18:18:03.079151: step 35197, loss 0.0935008, acc 0.96875
2017-03-02T18:18:03.159364: step 35198, loss 0.141099, acc 0.9375
2017-03-02T18:18:03.249283: step 35199, loss 0.0985269, acc 0.984375
2017-03-02T18:18:03.326367: step 35200, loss 0.167246, acc 0.9375

Evaluation:
2017-03-02T18:18:03.367213: step 35200, loss 4.95145, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35200

2017-03-02T18:18:03.826421: step 35201, loss 0.187276, acc 0.875
2017-03-02T18:18:03.902500: step 35202, loss 0.112159, acc 0.953125
2017-03-02T18:18:03.984974: step 35203, loss 0.192969, acc 0.890625
2017-03-02T18:18:04.064802: step 35204, loss 0.111628, acc 0.953125
2017-03-02T18:18:04.156373: step 35205, loss 0.139985, acc 0.9375
2017-03-02T18:18:04.223700: step 35206, loss 0.168286, acc 0.890625
2017-03-02T18:18:04.294039: step 35207, loss 0.250465, acc 0.875
2017-03-02T18:18:04.358759: step 35208, loss 0.17904, acc 0.921875
2017-03-02T18:18:04.430732: step 35209, loss 0.247468, acc 0.875
2017-03-02T18:18:04.536847: step 35210, loss 0.131909, acc 0.953125
2017-03-02T18:18:04.629884: step 35211, loss 0.167876, acc 0.9375
2017-03-02T18:18:04.695238: step 35212, loss 0.165839, acc 0.9375
2017-03-02T18:18:04.771392: step 35213, loss 0.12635, acc 0.9375
2017-03-02T18:18:04.850451: step 35214, loss 0.223349, acc 0.921875
2017-03-02T18:18:04.916544: step 35215, loss 0.154871, acc 0.953125
2017-03-02T18:18:04.985639: step 35216, loss 0.141492, acc 0.921875
2017-03-02T18:18:05.055670: step 35217, loss 0.189017, acc 0.90625
2017-03-02T18:18:05.127481: step 35218, loss 0.134352, acc 0.953125
2017-03-02T18:18:05.199658: step 35219, loss 0.144835, acc 0.953125
2017-03-02T18:18:05.273401: step 35220, loss 0.252616, acc 0.875
2017-03-02T18:18:05.344192: step 35221, loss 0.202269, acc 0.921875
2017-03-02T18:18:05.419656: step 35222, loss 0.200154, acc 0.890625
2017-03-02T18:18:05.498831: step 35223, loss 0.0457504, acc 0.984375
2017-03-02T18:18:05.570838: step 35224, loss 0.077577, acc 0.96875
2017-03-02T18:18:05.640104: step 35225, loss 0.210337, acc 0.9375
2017-03-02T18:18:05.706875: step 35226, loss 0.144376, acc 0.9375
2017-03-02T18:18:05.776637: step 35227, loss 0.0617828, acc 0.96875
2017-03-02T18:18:05.848608: step 35228, loss 0.188956, acc 0.9375
2017-03-02T18:18:05.924170: step 35229, loss 0.0802017, acc 0.96875
2017-03-02T18:18:05.992666: step 35230, loss 0.0886599, acc 0.984375
2017-03-02T18:18:06.062992: step 35231, loss 0.148012, acc 0.921875
2017-03-02T18:18:06.137325: step 35232, loss 0.120462, acc 0.9375
2017-03-02T18:18:06.198367: step 35233, loss 0.211049, acc 0.90625
2017-03-02T18:18:06.271239: step 35234, loss 0.173477, acc 0.9375
2017-03-02T18:18:06.344003: step 35235, loss 0.133366, acc 0.921875
2017-03-02T18:18:06.411122: step 35236, loss 0.101212, acc 0.96875
2017-03-02T18:18:06.483835: step 35237, loss 0.183517, acc 0.890625
2017-03-02T18:18:06.554471: step 35238, loss 0.0680022, acc 0.984375
2017-03-02T18:18:06.626944: step 35239, loss 0.102636, acc 0.96875
2017-03-02T18:18:06.699310: step 35240, loss 0.205851, acc 0.890625
2017-03-02T18:18:06.769934: step 35241, loss 0.199498, acc 0.890625
2017-03-02T18:18:06.843376: step 35242, loss 0.143117, acc 0.875
2017-03-02T18:18:06.919197: step 35243, loss 0.209608, acc 0.90625
2017-03-02T18:18:06.989883: step 35244, loss 0.0632011, acc 0.96875
2017-03-02T18:18:07.062764: step 35245, loss 0.0842324, acc 0.96875
2017-03-02T18:18:07.134692: step 35246, loss 0.092811, acc 0.96875
2017-03-02T18:18:07.213775: step 35247, loss 0.175415, acc 0.9375
2017-03-02T18:18:07.287520: step 35248, loss 0.230075, acc 0.9375
2017-03-02T18:18:07.373600: step 35249, loss 0.103012, acc 0.953125
2017-03-02T18:18:07.458227: step 35250, loss 0.167075, acc 0.9375
2017-03-02T18:18:07.525761: step 35251, loss 0.163732, acc 0.90625
2017-03-02T18:18:07.607645: step 35252, loss 0.0902299, acc 0.953125
2017-03-02T18:18:07.675639: step 35253, loss 0.0628827, acc 0.96875
2017-03-02T18:18:07.745751: step 35254, loss 0.113798, acc 0.921875
2017-03-02T18:18:07.816847: step 35255, loss 0.08681, acc 0.96875
2017-03-02T18:18:07.891157: step 35256, loss 0.154325, acc 0.9375
2017-03-02T18:18:07.962216: step 35257, loss 0.113459, acc 0.953125
2017-03-02T18:18:08.034255: step 35258, loss 0.181886, acc 0.9375
2017-03-02T18:18:08.104144: step 35259, loss 0.138903, acc 0.9375
2017-03-02T18:18:08.195038: step 35260, loss 0.0737821, acc 0.984375
2017-03-02T18:18:08.279740: step 35261, loss 0.112697, acc 0.953125
2017-03-02T18:18:08.357700: step 35262, loss 0.130938, acc 0.921875
2017-03-02T18:18:08.429638: step 35263, loss 0.103851, acc 0.9375
2017-03-02T18:18:08.503998: step 35264, loss 0.159296, acc 0.921875
2017-03-02T18:18:08.577899: step 35265, loss 0.182307, acc 0.90625
2017-03-02T18:18:08.645201: step 35266, loss 0.121086, acc 0.9375
2017-03-02T18:18:08.717177: step 35267, loss 0.211636, acc 0.90625
2017-03-02T18:18:08.784784: step 35268, loss 0.0869203, acc 0.953125
2017-03-02T18:18:08.860181: step 35269, loss 0.134159, acc 0.953125
2017-03-02T18:18:08.936643: step 35270, loss 0.0700548, acc 0.953125
2017-03-02T18:18:09.007049: step 35271, loss 0.123913, acc 0.9375
2017-03-02T18:18:09.076421: step 35272, loss 0.112704, acc 0.953125
2017-03-02T18:18:09.150228: step 35273, loss 0.157047, acc 0.921875
2017-03-02T18:18:09.221743: step 35274, loss 0.235293, acc 0.90625
2017-03-02T18:18:09.292698: step 35275, loss 0.154618, acc 0.9375
2017-03-02T18:18:09.363470: step 35276, loss 0.313762, acc 0.890625
2017-03-02T18:18:09.435000: step 35277, loss 0.308993, acc 0.90625
2017-03-02T18:18:09.514606: step 35278, loss 0.17232, acc 0.890625
2017-03-02T18:18:09.579261: step 35279, loss 0.196022, acc 0.90625
2017-03-02T18:18:09.648883: step 35280, loss 0.262815, acc 0.75
2017-03-02T18:18:09.724177: step 35281, loss 0.149518, acc 0.921875
2017-03-02T18:18:09.794223: step 35282, loss 0.117256, acc 0.96875
2017-03-02T18:18:09.876699: step 35283, loss 0.0538625, acc 0.984375
2017-03-02T18:18:09.949284: step 35284, loss 0.11666, acc 0.953125
2017-03-02T18:18:10.029010: step 35285, loss 0.233473, acc 0.90625
2017-03-02T18:18:10.099353: step 35286, loss 0.0855072, acc 0.96875
2017-03-02T18:18:10.176631: step 35287, loss 0.111814, acc 0.921875
2017-03-02T18:18:10.255350: step 35288, loss 0.150676, acc 0.921875
2017-03-02T18:18:10.335816: step 35289, loss 0.179666, acc 0.921875
2017-03-02T18:18:10.408803: step 35290, loss 0.0936454, acc 0.953125
2017-03-02T18:18:10.484372: step 35291, loss 0.100593, acc 0.96875
2017-03-02T18:18:10.556575: step 35292, loss 0.121787, acc 0.953125
2017-03-02T18:18:10.627581: step 35293, loss 0.10154, acc 0.953125
2017-03-02T18:18:10.698007: step 35294, loss 0.217027, acc 0.84375
2017-03-02T18:18:10.782056: step 35295, loss 0.138368, acc 0.96875
2017-03-02T18:18:10.857559: step 35296, loss 0.166223, acc 0.9375
2017-03-02T18:18:10.920999: step 35297, loss 0.0876142, acc 0.953125
2017-03-02T18:18:10.996053: step 35298, loss 0.154593, acc 0.921875
2017-03-02T18:18:11.067952: step 35299, loss 0.167781, acc 0.890625
2017-03-02T18:18:11.137097: step 35300, loss 0.118735, acc 0.9375

Evaluation:
2017-03-02T18:18:11.165753: step 35300, loss 4.86434, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35300

2017-03-02T18:18:11.610094: step 35301, loss 0.192026, acc 0.90625
2017-03-02T18:18:11.676974: step 35302, loss 0.17949, acc 0.9375
2017-03-02T18:18:11.762536: step 35303, loss 0.115193, acc 0.953125
2017-03-02T18:18:11.827696: step 35304, loss 0.101217, acc 0.9375
2017-03-02T18:18:11.897680: step 35305, loss 0.0911758, acc 0.96875
2017-03-02T18:18:11.967207: step 35306, loss 0.132744, acc 0.921875
2017-03-02T18:18:12.042637: step 35307, loss 0.0709996, acc 0.96875
2017-03-02T18:18:12.121554: step 35308, loss 0.149563, acc 0.90625
2017-03-02T18:18:12.193465: step 35309, loss 0.245387, acc 0.921875
2017-03-02T18:18:12.272193: step 35310, loss 0.26956, acc 0.890625
2017-03-02T18:18:12.347891: step 35311, loss 0.121062, acc 0.9375
2017-03-02T18:18:12.422059: step 35312, loss 0.150797, acc 0.90625
2017-03-02T18:18:12.489901: step 35313, loss 0.127682, acc 0.953125
2017-03-02T18:18:12.565967: step 35314, loss 0.0637367, acc 0.984375
2017-03-02T18:18:12.642744: step 35315, loss 0.18779, acc 0.9375
2017-03-02T18:18:12.714082: step 35316, loss 0.122739, acc 0.953125
2017-03-02T18:18:12.783426: step 35317, loss 0.101805, acc 0.953125
2017-03-02T18:18:12.855243: step 35318, loss 0.247246, acc 0.90625
2017-03-02T18:18:12.930027: step 35319, loss 0.262995, acc 0.859375
2017-03-02T18:18:13.008451: step 35320, loss 0.178235, acc 0.921875
2017-03-02T18:18:13.088791: step 35321, loss 0.18121, acc 0.921875
2017-03-02T18:18:13.153006: step 35322, loss 0.0685834, acc 0.96875
2017-03-02T18:18:13.223138: step 35323, loss 0.106609, acc 0.953125
2017-03-02T18:18:13.288773: step 35324, loss 0.136743, acc 0.953125
2017-03-02T18:18:13.360706: step 35325, loss 0.114659, acc 0.953125
2017-03-02T18:18:13.431072: step 35326, loss 0.0685514, acc 0.984375
2017-03-02T18:18:13.501245: step 35327, loss 0.274315, acc 0.890625
2017-03-02T18:18:13.573293: step 35328, loss 0.1253, acc 0.921875
2017-03-02T18:18:13.645149: step 35329, loss 0.0911809, acc 0.96875
2017-03-02T18:18:13.719965: step 35330, loss 0.140152, acc 0.96875
2017-03-02T18:18:13.788144: step 35331, loss 0.0688756, acc 0.96875
2017-03-02T18:18:13.864683: step 35332, loss 0.25973, acc 0.890625
2017-03-02T18:18:13.952719: step 35333, loss 0.231431, acc 0.859375
2017-03-02T18:18:14.048655: step 35334, loss 0.1039, acc 0.953125
2017-03-02T18:18:14.120079: step 35335, loss 0.156505, acc 0.921875
2017-03-02T18:18:14.192961: step 35336, loss 0.199501, acc 0.890625
2017-03-02T18:18:14.261637: step 35337, loss 0.212983, acc 0.921875
2017-03-02T18:18:14.330556: step 35338, loss 0.20776, acc 0.921875
2017-03-02T18:18:14.400572: step 35339, loss 0.214961, acc 0.890625
2017-03-02T18:18:14.473763: step 35340, loss 0.0681572, acc 0.96875
2017-03-02T18:18:14.536951: step 35341, loss 0.0494011, acc 0.984375
2017-03-02T18:18:14.601736: step 35342, loss 0.108988, acc 0.953125
2017-03-02T18:18:14.669485: step 35343, loss 0.185552, acc 0.9375
2017-03-02T18:18:14.740524: step 35344, loss 0.131097, acc 0.953125
2017-03-02T18:18:14.810418: step 35345, loss 0.0911863, acc 0.984375
2017-03-02T18:18:14.879963: step 35346, loss 0.147989, acc 0.9375
2017-03-02T18:18:14.951471: step 35347, loss 0.23115, acc 0.921875
2017-03-02T18:18:15.025811: step 35348, loss 0.136419, acc 0.9375
2017-03-02T18:18:15.102568: step 35349, loss 0.118526, acc 0.953125
2017-03-02T18:18:15.172887: step 35350, loss 0.161582, acc 0.953125
2017-03-02T18:18:15.241286: step 35351, loss 0.186027, acc 0.953125
2017-03-02T18:18:15.301485: step 35352, loss 0.145156, acc 0.921875
2017-03-02T18:18:15.371906: step 35353, loss 0.121984, acc 0.921875
2017-03-02T18:18:15.452140: step 35354, loss 0.211329, acc 0.90625
2017-03-02T18:18:15.522374: step 35355, loss 0.243946, acc 0.9375
2017-03-02T18:18:15.592722: step 35356, loss 0.246593, acc 0.90625
2017-03-02T18:18:15.664310: step 35357, loss 0.232341, acc 0.890625
2017-03-02T18:18:15.733967: step 35358, loss 0.134535, acc 0.9375
2017-03-02T18:18:15.808132: step 35359, loss 0.108524, acc 0.9375
2017-03-02T18:18:15.881468: step 35360, loss 0.118137, acc 0.953125
2017-03-02T18:18:15.961123: step 35361, loss 0.166714, acc 0.921875
2017-03-02T18:18:16.033082: step 35362, loss 0.120386, acc 0.96875
2017-03-02T18:18:16.106288: step 35363, loss 0.141837, acc 0.9375
2017-03-02T18:18:16.182052: step 35364, loss 0.108215, acc 0.96875
2017-03-02T18:18:16.257843: step 35365, loss 0.227575, acc 0.875
2017-03-02T18:18:16.324141: step 35366, loss 0.143836, acc 0.96875
2017-03-02T18:18:16.394679: step 35367, loss 0.151067, acc 0.921875
2017-03-02T18:18:16.470484: step 35368, loss 0.137159, acc 0.953125
2017-03-02T18:18:16.540614: step 35369, loss 0.130617, acc 0.953125
2017-03-02T18:18:16.611540: step 35370, loss 0.227072, acc 0.875
2017-03-02T18:18:16.681841: step 35371, loss 0.179581, acc 0.9375
2017-03-02T18:18:16.761837: step 35372, loss 0.179213, acc 0.90625
2017-03-02T18:18:16.835862: step 35373, loss 0.136415, acc 0.953125
2017-03-02T18:18:16.914589: step 35374, loss 0.114216, acc 0.953125
2017-03-02T18:18:16.985479: step 35375, loss 0.0535744, acc 0.96875
2017-03-02T18:18:17.059110: step 35376, loss 0.152373, acc 0.953125
2017-03-02T18:18:17.128826: step 35377, loss 0.0982627, acc 0.9375
2017-03-02T18:18:17.210868: step 35378, loss 0.12193, acc 0.9375
2017-03-02T18:18:17.285379: step 35379, loss 0.0679619, acc 0.953125
2017-03-02T18:18:17.359864: step 35380, loss 0.127293, acc 0.953125
2017-03-02T18:18:17.428012: step 35381, loss 0.103812, acc 0.9375
2017-03-02T18:18:17.505984: step 35382, loss 0.116177, acc 0.9375
2017-03-02T18:18:17.573452: step 35383, loss 0.136981, acc 0.921875
2017-03-02T18:18:17.649050: step 35384, loss 0.0984671, acc 0.984375
2017-03-02T18:18:17.721679: step 35385, loss 0.154944, acc 0.921875
2017-03-02T18:18:17.796919: step 35386, loss 0.261094, acc 0.859375
2017-03-02T18:18:17.870299: step 35387, loss 0.184611, acc 0.953125
2017-03-02T18:18:17.937139: step 35388, loss 0.21422, acc 0.875
2017-03-02T18:18:18.008199: step 35389, loss 0.186649, acc 0.875
2017-03-02T18:18:18.075746: step 35390, loss 0.197808, acc 0.921875
2017-03-02T18:18:18.148199: step 35391, loss 0.194831, acc 0.921875
2017-03-02T18:18:18.218879: step 35392, loss 0.149416, acc 0.921875
2017-03-02T18:18:18.293104: step 35393, loss 0.070507, acc 1
2017-03-02T18:18:18.365191: step 35394, loss 0.149689, acc 0.890625
2017-03-02T18:18:18.435094: step 35395, loss 0.112326, acc 0.9375
2017-03-02T18:18:18.513791: step 35396, loss 0.203581, acc 0.921875
2017-03-02T18:18:18.585291: step 35397, loss 0.0740035, acc 0.96875
2017-03-02T18:18:18.654133: step 35398, loss 0.138005, acc 0.9375
2017-03-02T18:18:18.718743: step 35399, loss 0.119538, acc 0.90625
2017-03-02T18:18:18.787560: step 35400, loss 0.117107, acc 0.921875

Evaluation:
2017-03-02T18:18:18.823273: step 35400, loss 5.13376, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35400

2017-03-02T18:18:19.267627: step 35401, loss 0.122045, acc 0.953125
2017-03-02T18:18:19.334446: step 35402, loss 0.112686, acc 0.96875
2017-03-02T18:18:19.406945: step 35403, loss 0.175274, acc 0.9375
2017-03-02T18:18:19.475039: step 35404, loss 0.0744453, acc 0.96875
2017-03-02T18:18:19.548856: step 35405, loss 0.146277, acc 0.90625
2017-03-02T18:18:19.624700: step 35406, loss 0.206512, acc 0.921875
2017-03-02T18:18:19.730336: step 35407, loss 0.132436, acc 0.9375
2017-03-02T18:18:19.807820: step 35408, loss 0.0284098, acc 1
2017-03-02T18:18:19.878281: step 35409, loss 0.0907419, acc 0.96875
2017-03-02T18:18:19.947320: step 35410, loss 0.18541, acc 0.90625
2017-03-02T18:18:20.020005: step 35411, loss 0.161341, acc 0.953125
2017-03-02T18:18:20.093638: step 35412, loss 0.0832369, acc 0.953125
2017-03-02T18:18:20.161981: step 35413, loss 0.132376, acc 0.90625
2017-03-02T18:18:20.256878: step 35414, loss 0.106399, acc 0.96875
2017-03-02T18:18:20.337742: step 35415, loss 0.127651, acc 0.9375
2017-03-02T18:18:20.400454: step 35416, loss 0.25148, acc 0.859375
2017-03-02T18:18:20.476113: step 35417, loss 0.230497, acc 0.875
2017-03-02T18:18:20.551537: step 35418, loss 0.100044, acc 0.953125
2017-03-02T18:18:20.622372: step 35419, loss 0.139361, acc 0.953125
2017-03-02T18:18:20.695650: step 35420, loss 0.215852, acc 0.9375
2017-03-02T18:18:20.765078: step 35421, loss 0.141284, acc 0.921875
2017-03-02T18:18:20.847868: step 35422, loss 0.213824, acc 0.890625
2017-03-02T18:18:20.921013: step 35423, loss 0.147124, acc 0.921875
2017-03-02T18:18:20.997644: step 35424, loss 0.214374, acc 0.875
2017-03-02T18:18:21.071250: step 35425, loss 0.118234, acc 0.96875
2017-03-02T18:18:21.142857: step 35426, loss 0.0893166, acc 0.96875
2017-03-02T18:18:21.218805: step 35427, loss 0.159001, acc 0.921875
2017-03-02T18:18:21.290191: step 35428, loss 0.0439342, acc 1
2017-03-02T18:18:21.365047: step 35429, loss 0.243376, acc 0.9375
2017-03-02T18:18:21.442320: step 35430, loss 0.121166, acc 0.9375
2017-03-02T18:18:21.511131: step 35431, loss 0.123468, acc 0.921875
2017-03-02T18:18:21.581088: step 35432, loss 0.0799284, acc 0.953125
2017-03-02T18:18:21.654058: step 35433, loss 0.226263, acc 0.90625
2017-03-02T18:18:21.728246: step 35434, loss 0.127227, acc 0.953125
2017-03-02T18:18:21.802474: step 35435, loss 0.0388049, acc 0.984375
2017-03-02T18:18:21.877886: step 35436, loss 0.131855, acc 0.9375
2017-03-02T18:18:21.954946: step 35437, loss 0.0638816, acc 0.96875
2017-03-02T18:18:22.021841: step 35438, loss 0.164437, acc 0.9375
2017-03-02T18:18:22.095140: step 35439, loss 0.101778, acc 0.96875
2017-03-02T18:18:22.168368: step 35440, loss 0.112932, acc 0.90625
2017-03-02T18:18:22.263608: step 35441, loss 0.11017, acc 0.953125
2017-03-02T18:18:22.336794: step 35442, loss 0.111767, acc 0.953125
2017-03-02T18:18:22.413337: step 35443, loss 0.086329, acc 0.953125
2017-03-02T18:18:22.482820: step 35444, loss 0.190647, acc 0.9375
2017-03-02T18:18:22.550380: step 35445, loss 0.210679, acc 0.953125
2017-03-02T18:18:22.624524: step 35446, loss 0.112579, acc 0.953125
2017-03-02T18:18:22.696284: step 35447, loss 0.249238, acc 0.84375
2017-03-02T18:18:22.766963: step 35448, loss 0.178296, acc 0.90625
2017-03-02T18:18:22.842237: step 35449, loss 0.11695, acc 0.90625
2017-03-02T18:18:22.915649: step 35450, loss 0.120336, acc 0.953125
2017-03-02T18:18:22.990521: step 35451, loss 0.208346, acc 0.875
2017-03-02T18:18:23.067358: step 35452, loss 0.180792, acc 0.90625
2017-03-02T18:18:23.142795: step 35453, loss 0.0961065, acc 0.96875
2017-03-02T18:18:23.228068: step 35454, loss 0.109428, acc 0.96875
2017-03-02T18:18:23.296708: step 35455, loss 0.128645, acc 0.96875
2017-03-02T18:18:23.367778: step 35456, loss 0.110955, acc 0.9375
2017-03-02T18:18:23.443394: step 35457, loss 0.248067, acc 0.84375
2017-03-02T18:18:23.512479: step 35458, loss 0.151329, acc 0.953125
2017-03-02T18:18:23.582470: step 35459, loss 0.08173, acc 0.984375
2017-03-02T18:18:23.654526: step 35460, loss 0.0952941, acc 0.9375
2017-03-02T18:18:23.732216: step 35461, loss 0.0655917, acc 0.96875
2017-03-02T18:18:23.807798: step 35462, loss 0.203527, acc 0.890625
2017-03-02T18:18:23.883510: step 35463, loss 0.0569781, acc 0.984375
2017-03-02T18:18:23.957943: step 35464, loss 0.0824135, acc 0.9375
2017-03-02T18:18:24.033995: step 35465, loss 0.137543, acc 0.921875
2017-03-02T18:18:24.109198: step 35466, loss 0.160377, acc 0.921875
2017-03-02T18:18:24.180410: step 35467, loss 0.234147, acc 0.890625
2017-03-02T18:18:24.255292: step 35468, loss 0.0819447, acc 0.9375
2017-03-02T18:18:24.328820: step 35469, loss 0.0748041, acc 0.984375
2017-03-02T18:18:24.416528: step 35470, loss 0.109915, acc 0.953125
2017-03-02T18:18:24.497813: step 35471, loss 0.13383, acc 0.921875
2017-03-02T18:18:24.571484: step 35472, loss 0.266266, acc 0.890625
2017-03-02T18:18:24.641663: step 35473, loss 0.133463, acc 0.921875
2017-03-02T18:18:24.712079: step 35474, loss 0.122186, acc 0.953125
2017-03-02T18:18:24.784698: step 35475, loss 0.1699, acc 0.921875
2017-03-02T18:18:24.852932: step 35476, loss 0.00165332, acc 1
2017-03-02T18:18:24.928981: step 35477, loss 0.0849936, acc 0.96875
2017-03-02T18:18:25.000229: step 35478, loss 0.145003, acc 0.9375
2017-03-02T18:18:25.069638: step 35479, loss 0.0983268, acc 0.96875
2017-03-02T18:18:25.141091: step 35480, loss 0.100665, acc 0.9375
2017-03-02T18:18:25.216758: step 35481, loss 0.137425, acc 0.921875
2017-03-02T18:18:25.287973: step 35482, loss 0.0762668, acc 0.984375
2017-03-02T18:18:25.359731: step 35483, loss 0.0385812, acc 1
2017-03-02T18:18:25.432562: step 35484, loss 0.16607, acc 0.9375
2017-03-02T18:18:25.506428: step 35485, loss 0.120937, acc 0.953125
2017-03-02T18:18:25.575568: step 35486, loss 0.0986241, acc 0.984375
2017-03-02T18:18:25.640598: step 35487, loss 0.31345, acc 0.859375
2017-03-02T18:18:25.712674: step 35488, loss 0.11694, acc 0.921875
2017-03-02T18:18:25.780499: step 35489, loss 0.081044, acc 0.96875
2017-03-02T18:18:25.846016: step 35490, loss 0.130671, acc 0.96875
2017-03-02T18:18:25.911153: step 35491, loss 0.153976, acc 0.9375
2017-03-02T18:18:25.977367: step 35492, loss 0.127502, acc 0.953125
2017-03-02T18:18:26.038889: step 35493, loss 0.166147, acc 0.9375
2017-03-02T18:18:26.113410: step 35494, loss 0.128106, acc 0.9375
2017-03-02T18:18:26.179486: step 35495, loss 0.120858, acc 0.953125
2017-03-02T18:18:26.250996: step 35496, loss 0.0854658, acc 0.96875
2017-03-02T18:18:26.314272: step 35497, loss 0.111851, acc 0.953125
2017-03-02T18:18:26.381457: step 35498, loss 0.180548, acc 0.90625
2017-03-02T18:18:26.451124: step 35499, loss 0.0953645, acc 0.96875
2017-03-02T18:18:26.523275: step 35500, loss 0.133532, acc 0.921875

Evaluation:
2017-03-02T18:18:26.559754: step 35500, loss 5.12614, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35500

2017-03-02T18:18:27.016109: step 35501, loss 0.156604, acc 0.921875
2017-03-02T18:18:27.088534: step 35502, loss 0.14523, acc 0.9375
2017-03-02T18:18:27.166912: step 35503, loss 0.202228, acc 0.90625
2017-03-02T18:18:27.241215: step 35504, loss 0.118496, acc 0.9375
2017-03-02T18:18:27.314443: step 35505, loss 0.180257, acc 0.90625
2017-03-02T18:18:27.389381: step 35506, loss 0.157818, acc 0.9375
2017-03-02T18:18:27.463374: step 35507, loss 0.11208, acc 0.953125
2017-03-02T18:18:27.535153: step 35508, loss 0.169687, acc 0.890625
2017-03-02T18:18:27.602243: step 35509, loss 0.072835, acc 0.984375
2017-03-02T18:18:27.675294: step 35510, loss 0.0613314, acc 0.96875
2017-03-02T18:18:27.745721: step 35511, loss 0.161024, acc 0.921875
2017-03-02T18:18:27.816432: step 35512, loss 0.102276, acc 0.9375
2017-03-02T18:18:27.887521: step 35513, loss 0.200195, acc 0.890625
2017-03-02T18:18:27.954653: step 35514, loss 0.225649, acc 0.953125
2017-03-02T18:18:28.034214: step 35515, loss 0.148316, acc 0.90625
2017-03-02T18:18:28.115671: step 35516, loss 0.108371, acc 0.953125
2017-03-02T18:18:28.192766: step 35517, loss 0.18197, acc 0.921875
2017-03-02T18:18:28.263966: step 35518, loss 0.183787, acc 0.921875
2017-03-02T18:18:28.333959: step 35519, loss 0.132313, acc 0.9375
2017-03-02T18:18:28.400290: step 35520, loss 0.127457, acc 0.953125
2017-03-02T18:18:28.471366: step 35521, loss 0.0704066, acc 0.984375
2017-03-02T18:18:28.539229: step 35522, loss 0.157804, acc 0.921875
2017-03-02T18:18:28.612499: step 35523, loss 0.126485, acc 0.9375
2017-03-02T18:18:28.683455: step 35524, loss 0.137329, acc 0.953125
2017-03-02T18:18:28.754784: step 35525, loss 0.136925, acc 0.921875
2017-03-02T18:18:28.827176: step 35526, loss 0.069372, acc 1
2017-03-02T18:18:28.899082: step 35527, loss 0.143027, acc 0.9375
2017-03-02T18:18:28.975807: step 35528, loss 0.20361, acc 0.90625
2017-03-02T18:18:29.044160: step 35529, loss 0.0472402, acc 0.984375
2017-03-02T18:18:29.119956: step 35530, loss 0.26746, acc 0.90625
2017-03-02T18:18:29.185329: step 35531, loss 0.0861207, acc 0.96875
2017-03-02T18:18:29.253453: step 35532, loss 0.138151, acc 0.9375
2017-03-02T18:18:29.322216: step 35533, loss 0.0528656, acc 0.984375
2017-03-02T18:18:29.393612: step 35534, loss 0.162782, acc 0.921875
2017-03-02T18:18:29.458175: step 35535, loss 0.105069, acc 0.953125
2017-03-02T18:18:29.522382: step 35536, loss 0.143529, acc 0.9375
2017-03-02T18:18:29.594900: step 35537, loss 0.238299, acc 0.921875
2017-03-02T18:18:29.665743: step 35538, loss 0.154455, acc 0.921875
2017-03-02T18:18:29.733576: step 35539, loss 0.160736, acc 0.9375
2017-03-02T18:18:29.799067: step 35540, loss 0.175362, acc 0.921875
2017-03-02T18:18:29.872343: step 35541, loss 0.155645, acc 0.96875
2017-03-02T18:18:29.947731: step 35542, loss 0.160575, acc 0.90625
2017-03-02T18:18:30.019585: step 35543, loss 0.154069, acc 0.9375
2017-03-02T18:18:30.089123: step 35544, loss 0.153066, acc 0.921875
2017-03-02T18:18:30.161387: step 35545, loss 0.269403, acc 0.90625
2017-03-02T18:18:30.228691: step 35546, loss 0.176503, acc 0.953125
2017-03-02T18:18:30.311356: step 35547, loss 0.164821, acc 0.9375
2017-03-02T18:18:30.379470: step 35548, loss 0.142883, acc 0.953125
2017-03-02T18:18:30.448923: step 35549, loss 0.0658938, acc 0.96875
2017-03-02T18:18:30.524040: step 35550, loss 0.148678, acc 0.921875
2017-03-02T18:18:30.601980: step 35551, loss 0.161913, acc 0.9375
2017-03-02T18:18:30.678237: step 35552, loss 0.114201, acc 0.9375
2017-03-02T18:18:30.750669: step 35553, loss 0.0941542, acc 0.96875
2017-03-02T18:18:30.849932: step 35554, loss 0.13039, acc 0.953125
2017-03-02T18:18:30.925275: step 35555, loss 0.176538, acc 0.890625
2017-03-02T18:18:30.997669: step 35556, loss 0.111011, acc 0.953125
2017-03-02T18:18:31.071092: step 35557, loss 0.149653, acc 0.90625
2017-03-02T18:18:31.142524: step 35558, loss 0.172388, acc 0.921875
2017-03-02T18:18:31.212418: step 35559, loss 0.227359, acc 0.9375
2017-03-02T18:18:31.286335: step 35560, loss 0.149106, acc 0.9375
2017-03-02T18:18:31.358630: step 35561, loss 0.123706, acc 0.921875
2017-03-02T18:18:31.432500: step 35562, loss 0.21058, acc 0.90625
2017-03-02T18:18:31.503998: step 35563, loss 0.0437398, acc 1
2017-03-02T18:18:31.583104: step 35564, loss 0.0911832, acc 0.96875
2017-03-02T18:18:31.663405: step 35565, loss 0.225915, acc 0.890625
2017-03-02T18:18:31.742326: step 35566, loss 0.12756, acc 0.9375
2017-03-02T18:18:31.812928: step 35567, loss 0.16217, acc 0.9375
2017-03-02T18:18:31.889782: step 35568, loss 0.221344, acc 0.875
2017-03-02T18:18:31.962673: step 35569, loss 0.158981, acc 0.890625
2017-03-02T18:18:32.032397: step 35570, loss 0.132079, acc 0.953125
2017-03-02T18:18:32.106207: step 35571, loss 0.146418, acc 0.90625
2017-03-02T18:18:32.176251: step 35572, loss 0.0652576, acc 0.96875
2017-03-02T18:18:32.246410: step 35573, loss 0.165869, acc 0.90625
2017-03-02T18:18:32.332620: step 35574, loss 0.203125, acc 0.890625
2017-03-02T18:18:32.404587: step 35575, loss 0.211062, acc 0.875
2017-03-02T18:18:32.473706: step 35576, loss 0.141641, acc 0.90625
2017-03-02T18:18:32.539199: step 35577, loss 0.0917183, acc 0.953125
2017-03-02T18:18:32.609485: step 35578, loss 0.0817504, acc 0.984375
2017-03-02T18:18:32.681643: step 35579, loss 0.17915, acc 0.90625
2017-03-02T18:18:32.763942: step 35580, loss 0.219565, acc 0.921875
2017-03-02T18:18:32.836188: step 35581, loss 0.160594, acc 0.9375
2017-03-02T18:18:32.903914: step 35582, loss 0.0962483, acc 0.984375
2017-03-02T18:18:32.969639: step 35583, loss 0.148838, acc 0.953125
2017-03-02T18:18:33.039695: step 35584, loss 0.147156, acc 0.9375
2017-03-02T18:18:33.115460: step 35585, loss 0.0444503, acc 0.984375
2017-03-02T18:18:33.186708: step 35586, loss 0.106273, acc 0.9375
2017-03-02T18:18:33.256845: step 35587, loss 0.0901522, acc 0.953125
2017-03-02T18:18:33.332053: step 35588, loss 0.135613, acc 0.96875
2017-03-02T18:18:33.403419: step 35589, loss 0.0617175, acc 1
2017-03-02T18:18:33.495557: step 35590, loss 0.0955242, acc 0.953125
2017-03-02T18:18:33.572846: step 35591, loss 0.101434, acc 0.9375
2017-03-02T18:18:33.654700: step 35592, loss 0.221421, acc 0.875
2017-03-02T18:18:33.738692: step 35593, loss 0.0750986, acc 0.953125
2017-03-02T18:18:33.811111: step 35594, loss 0.170159, acc 0.921875
2017-03-02T18:18:33.879727: step 35595, loss 0.136632, acc 0.9375
2017-03-02T18:18:33.952499: step 35596, loss 0.171757, acc 0.890625
2017-03-02T18:18:34.027232: step 35597, loss 0.129911, acc 0.9375
2017-03-02T18:18:34.097758: step 35598, loss 0.143215, acc 0.921875
2017-03-02T18:18:34.182333: step 35599, loss 0.152021, acc 0.9375
2017-03-02T18:18:34.254645: step 35600, loss 0.128454, acc 0.921875

Evaluation:
2017-03-02T18:18:34.290361: step 35600, loss 5.16506, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35600

2017-03-02T18:18:34.744406: step 35601, loss 0.0691524, acc 0.953125
2017-03-02T18:18:34.818571: step 35602, loss 0.167994, acc 0.90625
2017-03-02T18:18:34.896015: step 35603, loss 0.17982, acc 0.890625
2017-03-02T18:18:34.971953: step 35604, loss 0.164412, acc 0.90625
2017-03-02T18:18:35.043715: step 35605, loss 0.150089, acc 0.921875
2017-03-02T18:18:35.119170: step 35606, loss 0.141273, acc 0.921875
2017-03-02T18:18:35.183705: step 35607, loss 0.227469, acc 0.890625
2017-03-02T18:18:35.259907: step 35608, loss 0.140986, acc 0.9375
2017-03-02T18:18:35.334403: step 35609, loss 0.152227, acc 0.921875
2017-03-02T18:18:35.409514: step 35610, loss 0.226619, acc 0.890625
2017-03-02T18:18:35.484120: step 35611, loss 0.107224, acc 0.953125
2017-03-02T18:18:35.557727: step 35612, loss 0.0908626, acc 0.953125
2017-03-02T18:18:35.629519: step 35613, loss 0.192621, acc 0.953125
2017-03-02T18:18:35.702628: step 35614, loss 0.129179, acc 0.96875
2017-03-02T18:18:35.771853: step 35615, loss 0.102211, acc 0.953125
2017-03-02T18:18:35.844561: step 35616, loss 0.211332, acc 0.9375
2017-03-02T18:18:35.909220: step 35617, loss 0.152575, acc 0.9375
2017-03-02T18:18:35.980442: step 35618, loss 0.0907347, acc 0.953125
2017-03-02T18:18:36.074595: step 35619, loss 0.0717037, acc 0.96875
2017-03-02T18:18:36.147494: step 35620, loss 0.191645, acc 0.90625
2017-03-02T18:18:36.224541: step 35621, loss 0.140763, acc 0.90625
2017-03-02T18:18:36.296840: step 35622, loss 0.0901584, acc 0.984375
2017-03-02T18:18:36.366358: step 35623, loss 0.175159, acc 0.90625
2017-03-02T18:18:36.443108: step 35624, loss 0.153069, acc 0.96875
2017-03-02T18:18:36.525421: step 35625, loss 0.12177, acc 0.96875
2017-03-02T18:18:36.594113: step 35626, loss 0.126707, acc 0.9375
2017-03-02T18:18:36.663165: step 35627, loss 0.183877, acc 0.90625
2017-03-02T18:18:36.740222: step 35628, loss 0.122388, acc 0.953125
2017-03-02T18:18:36.809099: step 35629, loss 0.177637, acc 0.921875
2017-03-02T18:18:36.885509: step 35630, loss 0.0638472, acc 0.953125
2017-03-02T18:18:36.953180: step 35631, loss 0.119279, acc 0.9375
2017-03-02T18:18:37.023211: step 35632, loss 0.140649, acc 0.9375
2017-03-02T18:18:37.087719: step 35633, loss 0.151744, acc 0.90625
2017-03-02T18:18:37.157370: step 35634, loss 0.216537, acc 0.890625
2017-03-02T18:18:37.228167: step 35635, loss 0.120134, acc 0.921875
2017-03-02T18:18:37.297075: step 35636, loss 0.123054, acc 0.921875
2017-03-02T18:18:37.377563: step 35637, loss 0.117521, acc 0.9375
2017-03-02T18:18:37.451234: step 35638, loss 0.238586, acc 0.890625
2017-03-02T18:18:37.525702: step 35639, loss 0.0743807, acc 0.921875
2017-03-02T18:18:37.606007: step 35640, loss 0.258003, acc 0.859375
2017-03-02T18:18:37.699468: step 35641, loss 0.163665, acc 0.90625
2017-03-02T18:18:37.769324: step 35642, loss 0.175931, acc 0.9375
2017-03-02T18:18:37.851056: step 35643, loss 0.243396, acc 0.875
2017-03-02T18:18:37.918647: step 35644, loss 0.1777, acc 0.890625
2017-03-02T18:18:37.984990: step 35645, loss 0.128238, acc 0.921875
2017-03-02T18:18:38.053619: step 35646, loss 0.126162, acc 0.9375
2017-03-02T18:18:38.124890: step 35647, loss 0.196636, acc 0.921875
2017-03-02T18:18:38.197373: step 35648, loss 0.110934, acc 0.9375
2017-03-02T18:18:38.270535: step 35649, loss 0.234143, acc 0.890625
2017-03-02T18:18:38.341368: step 35650, loss 0.207625, acc 0.921875
2017-03-02T18:18:38.423917: step 35651, loss 0.140961, acc 0.9375
2017-03-02T18:18:38.500797: step 35652, loss 0.215378, acc 0.90625
2017-03-02T18:18:38.572670: step 35653, loss 0.0989801, acc 0.953125
2017-03-02T18:18:38.644736: step 35654, loss 0.0558796, acc 0.96875
2017-03-02T18:18:38.717359: step 35655, loss 0.0780682, acc 0.953125
2017-03-02T18:18:38.797146: step 35656, loss 0.134096, acc 0.9375
2017-03-02T18:18:38.870718: step 35657, loss 0.247718, acc 0.890625
2017-03-02T18:18:38.942708: step 35658, loss 0.194188, acc 0.890625
2017-03-02T18:18:39.018244: step 35659, loss 0.103295, acc 0.9375
2017-03-02T18:18:39.088785: step 35660, loss 0.0904848, acc 0.9375
2017-03-02T18:18:39.158901: step 35661, loss 0.0537562, acc 0.984375
2017-03-02T18:18:39.229383: step 35662, loss 0.12204, acc 0.953125
2017-03-02T18:18:39.308984: step 35663, loss 0.108059, acc 0.96875
2017-03-02T18:18:39.377049: step 35664, loss 0.0670544, acc 0.953125
2017-03-02T18:18:39.446774: step 35665, loss 0.156097, acc 0.921875
2017-03-02T18:18:39.522467: step 35666, loss 0.179888, acc 0.921875
2017-03-02T18:18:39.596748: step 35667, loss 0.0687845, acc 0.984375
2017-03-02T18:18:39.676968: step 35668, loss 0.242224, acc 0.890625
2017-03-02T18:18:39.752670: step 35669, loss 0.158869, acc 0.953125
2017-03-02T18:18:39.833647: step 35670, loss 0.164014, acc 0.90625
2017-03-02T18:18:39.909273: step 35671, loss 0.189579, acc 0.921875
2017-03-02T18:18:39.979760: step 35672, loss 0.0329898, acc 1
2017-03-02T18:18:40.047164: step 35673, loss 0.12043, acc 0.9375
2017-03-02T18:18:40.112679: step 35674, loss 0.158617, acc 0.921875
2017-03-02T18:18:40.184246: step 35675, loss 0.174518, acc 0.90625
2017-03-02T18:18:40.251873: step 35676, loss 0.0825861, acc 0.9375
2017-03-02T18:18:40.322811: step 35677, loss 0.133846, acc 0.9375
2017-03-02T18:18:40.394311: step 35678, loss 0.229215, acc 0.921875
2017-03-02T18:18:40.472456: step 35679, loss 0.059137, acc 0.984375
2017-03-02T18:18:40.543990: step 35680, loss 0.127801, acc 0.953125
2017-03-02T18:18:40.614147: step 35681, loss 0.16095, acc 0.875
2017-03-02T18:18:40.686079: step 35682, loss 0.105415, acc 0.921875
2017-03-02T18:18:40.753642: step 35683, loss 0.0674041, acc 0.96875
2017-03-02T18:18:40.819840: step 35684, loss 0.255666, acc 0.890625
2017-03-02T18:18:40.890512: step 35685, loss 0.217411, acc 0.875
2017-03-02T18:18:40.959258: step 35686, loss 0.132664, acc 0.9375
2017-03-02T18:18:41.032368: step 35687, loss 0.126909, acc 0.9375
2017-03-02T18:18:41.107076: step 35688, loss 0.191012, acc 0.921875
2017-03-02T18:18:41.182773: step 35689, loss 0.10907, acc 0.9375
2017-03-02T18:18:41.255577: step 35690, loss 0.100664, acc 0.953125
2017-03-02T18:18:41.330480: step 35691, loss 0.0843487, acc 0.96875
2017-03-02T18:18:41.400858: step 35692, loss 0.198774, acc 0.890625
2017-03-02T18:18:41.469548: step 35693, loss 0.101064, acc 0.953125
2017-03-02T18:18:41.536718: step 35694, loss 0.222365, acc 0.859375
2017-03-02T18:18:41.606602: step 35695, loss 0.117122, acc 0.96875
2017-03-02T18:18:41.679140: step 35696, loss 0.0791616, acc 0.953125
2017-03-02T18:18:41.751089: step 35697, loss 0.0920819, acc 0.9375
2017-03-02T18:18:41.822927: step 35698, loss 0.0978114, acc 0.953125
2017-03-02T18:18:41.899779: step 35699, loss 0.104113, acc 0.921875
2017-03-02T18:18:41.968153: step 35700, loss 0.144114, acc 0.921875

Evaluation:
2017-03-02T18:18:42.007647: step 35700, loss 5.32965, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35700

2017-03-02T18:18:42.457749: step 35701, loss 0.139897, acc 0.921875
2017-03-02T18:18:42.531018: step 35702, loss 0.0697299, acc 0.96875
2017-03-02T18:18:42.606671: step 35703, loss 0.158833, acc 0.921875
2017-03-02T18:18:42.677971: step 35704, loss 0.112229, acc 0.953125
2017-03-02T18:18:42.748701: step 35705, loss 0.0634022, acc 0.96875
2017-03-02T18:18:42.817195: step 35706, loss 0.151787, acc 0.90625
2017-03-02T18:18:42.888186: step 35707, loss 0.107045, acc 0.9375
2017-03-02T18:18:42.962552: step 35708, loss 0.0858568, acc 0.96875
2017-03-02T18:18:43.035167: step 35709, loss 0.190879, acc 0.921875
2017-03-02T18:18:43.108636: step 35710, loss 0.107727, acc 0.9375
2017-03-02T18:18:43.173110: step 35711, loss 0.141263, acc 0.921875
2017-03-02T18:18:43.248499: step 35712, loss 0.101021, acc 0.96875
2017-03-02T18:18:43.324711: step 35713, loss 0.149201, acc 0.9375
2017-03-02T18:18:43.402136: step 35714, loss 0.0892657, acc 0.96875
2017-03-02T18:18:43.469387: step 35715, loss 0.155215, acc 0.96875
2017-03-02T18:18:43.541143: step 35716, loss 0.0935982, acc 0.984375
2017-03-02T18:18:43.611301: step 35717, loss 0.238627, acc 0.921875
2017-03-02T18:18:43.685847: step 35718, loss 0.128531, acc 0.953125
2017-03-02T18:18:43.756380: step 35719, loss 0.135882, acc 0.90625
2017-03-02T18:18:43.832599: step 35720, loss 0.123395, acc 0.921875
2017-03-02T18:18:43.910556: step 35721, loss 0.0919143, acc 0.953125
2017-03-02T18:18:43.990199: step 35722, loss 0.125602, acc 0.953125
2017-03-02T18:18:44.059286: step 35723, loss 0.153747, acc 0.921875
2017-03-02T18:18:44.127675: step 35724, loss 0.132974, acc 0.921875
2017-03-02T18:18:44.200214: step 35725, loss 0.104405, acc 0.953125
2017-03-02T18:18:44.272731: step 35726, loss 0.201108, acc 0.9375
2017-03-02T18:18:44.344639: step 35727, loss 0.245163, acc 0.890625
2017-03-02T18:18:44.418466: step 35728, loss 0.223619, acc 0.875
2017-03-02T18:18:44.499288: step 35729, loss 0.0772213, acc 0.984375
2017-03-02T18:18:44.570679: step 35730, loss 0.14054, acc 0.9375
2017-03-02T18:18:44.644790: step 35731, loss 0.132262, acc 0.953125
2017-03-02T18:18:44.715856: step 35732, loss 0.200849, acc 0.890625
2017-03-02T18:18:44.809077: step 35733, loss 0.142822, acc 0.953125
2017-03-02T18:18:44.878790: step 35734, loss 0.196238, acc 0.9375
2017-03-02T18:18:44.947881: step 35735, loss 0.224786, acc 0.90625
2017-03-02T18:18:45.016252: step 35736, loss 0.0876536, acc 0.96875
2017-03-02T18:18:45.089148: step 35737, loss 0.143277, acc 0.90625
2017-03-02T18:18:45.160449: step 35738, loss 0.25412, acc 0.875
2017-03-02T18:18:45.232530: step 35739, loss 0.19083, acc 0.9375
2017-03-02T18:18:45.303511: step 35740, loss 0.129072, acc 0.921875
2017-03-02T18:18:45.381483: step 35741, loss 0.120237, acc 0.953125
2017-03-02T18:18:45.449395: step 35742, loss 0.237384, acc 0.890625
2017-03-02T18:18:45.517459: step 35743, loss 0.249724, acc 0.859375
2017-03-02T18:18:45.578089: step 35744, loss 0.120519, acc 0.953125
2017-03-02T18:18:45.647208: step 35745, loss 0.138458, acc 0.9375
2017-03-02T18:18:45.723751: step 35746, loss 0.135001, acc 0.9375
2017-03-02T18:18:45.796598: step 35747, loss 0.101985, acc 0.953125
2017-03-02T18:18:45.885237: step 35748, loss 0.0865917, acc 0.984375
2017-03-02T18:18:45.958163: step 35749, loss 0.190951, acc 0.875
2017-03-02T18:18:46.031217: step 35750, loss 0.172978, acc 0.9375
2017-03-02T18:18:46.103270: step 35751, loss 0.200669, acc 0.90625
2017-03-02T18:18:46.187416: step 35752, loss 0.126309, acc 0.9375
2017-03-02T18:18:46.262064: step 35753, loss 0.076767, acc 0.96875
2017-03-02T18:18:46.326597: step 35754, loss 0.188863, acc 0.90625
2017-03-02T18:18:46.386371: step 35755, loss 0.169754, acc 0.921875
2017-03-02T18:18:46.459013: step 35756, loss 0.0854148, acc 0.984375
2017-03-02T18:18:46.535415: step 35757, loss 0.120806, acc 0.96875
2017-03-02T18:18:46.618034: step 35758, loss 0.0503522, acc 0.984375
2017-03-02T18:18:46.719529: step 35759, loss 0.207903, acc 0.890625
2017-03-02T18:18:46.814678: step 35760, loss 0.210111, acc 0.890625
2017-03-02T18:18:46.892477: step 35761, loss 0.119091, acc 0.9375
2017-03-02T18:18:46.962180: step 35762, loss 0.13759, acc 0.921875
2017-03-02T18:18:47.031299: step 35763, loss 0.150692, acc 0.9375
2017-03-02T18:18:47.105911: step 35764, loss 0.150663, acc 0.859375
2017-03-02T18:18:47.179464: step 35765, loss 0.103662, acc 0.953125
2017-03-02T18:18:47.259153: step 35766, loss 0.0947937, acc 0.96875
2017-03-02T18:18:47.334384: step 35767, loss 0.158938, acc 0.921875
2017-03-02T18:18:47.412046: step 35768, loss 0.256911, acc 0.90625
2017-03-02T18:18:47.486227: step 35769, loss 0.215295, acc 0.90625
2017-03-02T18:18:47.561565: step 35770, loss 0.0608343, acc 0.96875
2017-03-02T18:18:47.632554: step 35771, loss 0.125107, acc 0.921875
2017-03-02T18:18:47.698785: step 35772, loss 0.240906, acc 0.921875
2017-03-02T18:18:47.768861: step 35773, loss 0.130417, acc 0.96875
2017-03-02T18:18:47.837768: step 35774, loss 0.145, acc 0.953125
2017-03-02T18:18:47.907497: step 35775, loss 0.129667, acc 0.921875
2017-03-02T18:18:47.976610: step 35776, loss 0.12224, acc 0.953125
2017-03-02T18:18:48.044451: step 35777, loss 0.164977, acc 0.921875
2017-03-02T18:18:48.114057: step 35778, loss 0.0642379, acc 0.953125
2017-03-02T18:18:48.194992: step 35779, loss 0.195026, acc 0.90625
2017-03-02T18:18:48.267014: step 35780, loss 0.266315, acc 0.90625
2017-03-02T18:18:48.348140: step 35781, loss 0.0855916, acc 0.953125
2017-03-02T18:18:48.425372: step 35782, loss 0.163936, acc 0.921875
2017-03-02T18:18:48.495036: step 35783, loss 0.136009, acc 0.9375
2017-03-02T18:18:48.567269: step 35784, loss 0.167239, acc 0.9375
2017-03-02T18:18:48.641486: step 35785, loss 0.0647372, acc 0.984375
2017-03-02T18:18:48.713315: step 35786, loss 0.0887218, acc 0.96875
2017-03-02T18:18:48.787006: step 35787, loss 0.160963, acc 0.9375
2017-03-02T18:18:48.866624: step 35788, loss 0.155617, acc 0.921875
2017-03-02T18:18:48.940230: step 35789, loss 0.107775, acc 0.9375
2017-03-02T18:18:49.019582: step 35790, loss 0.192644, acc 0.890625
2017-03-02T18:18:49.093306: step 35791, loss 0.0618217, acc 0.984375
2017-03-02T18:18:49.164818: step 35792, loss 0.153145, acc 0.9375
2017-03-02T18:18:49.243094: step 35793, loss 0.136876, acc 0.9375
2017-03-02T18:18:49.313706: step 35794, loss 0.113999, acc 0.9375
2017-03-02T18:18:49.384191: step 35795, loss 0.11798, acc 0.953125
2017-03-02T18:18:49.458773: step 35796, loss 0.0901036, acc 0.9375
2017-03-02T18:18:49.539530: step 35797, loss 0.102414, acc 0.96875
2017-03-02T18:18:49.611660: step 35798, loss 0.148542, acc 0.953125
2017-03-02T18:18:49.683118: step 35799, loss 0.154202, acc 0.9375
2017-03-02T18:18:49.754316: step 35800, loss 0.134452, acc 0.9375

Evaluation:
2017-03-02T18:18:49.793560: step 35800, loss 5.2509, acc 0.638789

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35800

2017-03-02T18:18:50.247148: step 35801, loss 0.200897, acc 0.890625
2017-03-02T18:18:50.321296: step 35802, loss 0.260627, acc 0.875
2017-03-02T18:18:50.393454: step 35803, loss 0.139313, acc 0.9375
2017-03-02T18:18:50.466754: step 35804, loss 0.10578, acc 0.9375
2017-03-02T18:18:50.537888: step 35805, loss 0.0865713, acc 0.9375
2017-03-02T18:18:50.622892: step 35806, loss 0.100829, acc 0.96875
2017-03-02T18:18:50.692887: step 35807, loss 0.0552831, acc 0.984375
2017-03-02T18:18:50.764739: step 35808, loss 0.148805, acc 0.9375
2017-03-02T18:18:50.838996: step 35809, loss 0.169656, acc 0.90625
2017-03-02T18:18:50.908179: step 35810, loss 0.186728, acc 0.921875
2017-03-02T18:18:50.985537: step 35811, loss 0.199354, acc 0.921875
2017-03-02T18:18:51.059339: step 35812, loss 0.0855746, acc 0.953125
2017-03-02T18:18:51.133181: step 35813, loss 0.231002, acc 0.890625
2017-03-02T18:18:51.209338: step 35814, loss 0.119714, acc 0.953125
2017-03-02T18:18:51.288843: step 35815, loss 0.263651, acc 0.875
2017-03-02T18:18:51.362605: step 35816, loss 0.192722, acc 0.921875
2017-03-02T18:18:51.430893: step 35817, loss 0.226961, acc 0.890625
2017-03-02T18:18:51.505638: step 35818, loss 0.0733988, acc 0.953125
2017-03-02T18:18:51.571764: step 35819, loss 0.171459, acc 0.9375
2017-03-02T18:18:51.643583: step 35820, loss 0.12177, acc 0.9375
2017-03-02T18:18:51.722222: step 35821, loss 0.0799108, acc 0.953125
2017-03-02T18:18:51.784787: step 35822, loss 0.171753, acc 0.921875
2017-03-02T18:18:51.853541: step 35823, loss 0.238534, acc 0.859375
2017-03-02T18:18:51.925393: step 35824, loss 0.286876, acc 0.890625
2017-03-02T18:18:52.016514: step 35825, loss 0.242232, acc 0.90625
2017-03-02T18:18:52.091372: step 35826, loss 0.136736, acc 0.90625
2017-03-02T18:18:52.164703: step 35827, loss 0.193052, acc 0.90625
2017-03-02T18:18:52.246379: step 35828, loss 0.140063, acc 0.953125
2017-03-02T18:18:52.320739: step 35829, loss 0.10353, acc 0.96875
2017-03-02T18:18:52.394427: step 35830, loss 0.251411, acc 0.875
2017-03-02T18:18:52.459056: step 35831, loss 0.135565, acc 0.90625
2017-03-02T18:18:52.526799: step 35832, loss 0.0796283, acc 0.96875
2017-03-02T18:18:52.595774: step 35833, loss 0.103382, acc 0.96875
2017-03-02T18:18:52.670666: step 35834, loss 0.168007, acc 0.90625
2017-03-02T18:18:52.730929: step 35835, loss 0.0711732, acc 0.96875
2017-03-02T18:18:52.804721: step 35836, loss 0.0329969, acc 1
2017-03-02T18:18:52.876496: step 35837, loss 0.0942701, acc 0.9375
2017-03-02T18:18:52.948129: step 35838, loss 0.129174, acc 0.9375
2017-03-02T18:18:53.018267: step 35839, loss 0.139424, acc 0.921875
2017-03-02T18:18:53.081937: step 35840, loss 0.141296, acc 0.953125
2017-03-02T18:18:53.158410: step 35841, loss 0.125422, acc 0.96875
2017-03-02T18:18:53.225102: step 35842, loss 0.166587, acc 0.953125
2017-03-02T18:18:53.295711: step 35843, loss 0.0770217, acc 0.953125
2017-03-02T18:18:53.364964: step 35844, loss 0.109212, acc 0.96875
2017-03-02T18:18:53.439552: step 35845, loss 0.169753, acc 0.921875
2017-03-02T18:18:53.523943: step 35846, loss 0.135392, acc 0.9375
2017-03-02T18:18:53.597996: step 35847, loss 0.0539262, acc 0.96875
2017-03-02T18:18:53.677895: step 35848, loss 0.209635, acc 0.90625
2017-03-02T18:18:53.754469: step 35849, loss 0.159959, acc 0.921875
2017-03-02T18:18:53.837251: step 35850, loss 0.118822, acc 0.953125
2017-03-02T18:18:53.908331: step 35851, loss 0.168073, acc 0.90625
2017-03-02T18:18:53.973446: step 35852, loss 0.215766, acc 0.859375
2017-03-02T18:18:54.051188: step 35853, loss 0.0830815, acc 0.96875
2017-03-02T18:18:54.128418: step 35854, loss 0.169393, acc 0.9375
2017-03-02T18:18:54.199331: step 35855, loss 0.072833, acc 0.96875
2017-03-02T18:18:54.276524: step 35856, loss 0.0882717, acc 0.953125
2017-03-02T18:18:54.351375: step 35857, loss 0.0688261, acc 0.984375
2017-03-02T18:18:54.424054: step 35858, loss 0.12275, acc 0.9375
2017-03-02T18:18:54.506300: step 35859, loss 0.103459, acc 0.953125
2017-03-02T18:18:54.575350: step 35860, loss 0.0830808, acc 0.96875
2017-03-02T18:18:54.641629: step 35861, loss 0.104094, acc 0.96875
2017-03-02T18:18:54.718759: step 35862, loss 0.295362, acc 0.90625
2017-03-02T18:18:54.793982: step 35863, loss 0.0806966, acc 0.96875
2017-03-02T18:18:54.867442: step 35864, loss 0.219254, acc 0.9375
2017-03-02T18:18:54.947626: step 35865, loss 0.166391, acc 0.9375
2017-03-02T18:18:55.020600: step 35866, loss 0.160051, acc 0.9375
2017-03-02T18:18:55.093761: step 35867, loss 0.0606877, acc 1
2017-03-02T18:18:55.170445: step 35868, loss 0.000868272, acc 1
2017-03-02T18:18:55.250426: step 35869, loss 0.102699, acc 0.953125
2017-03-02T18:18:55.312702: step 35870, loss 0.109571, acc 0.9375
2017-03-02T18:18:55.393414: step 35871, loss 0.0438966, acc 0.984375
2017-03-02T18:18:55.468602: step 35872, loss 0.142256, acc 0.90625
2017-03-02T18:18:55.543192: step 35873, loss 0.0952339, acc 0.984375
2017-03-02T18:18:55.613981: step 35874, loss 0.146185, acc 0.921875
2017-03-02T18:18:55.683888: step 35875, loss 0.051936, acc 1
2017-03-02T18:18:55.755062: step 35876, loss 0.176649, acc 0.953125
2017-03-02T18:18:55.829240: step 35877, loss 0.111094, acc 0.953125
2017-03-02T18:18:55.898607: step 35878, loss 0.151577, acc 0.90625
2017-03-02T18:18:55.970380: step 35879, loss 0.236208, acc 0.875
2017-03-02T18:18:56.033047: step 35880, loss 0.153046, acc 0.921875
2017-03-02T18:18:56.102230: step 35881, loss 0.113015, acc 0.953125
2017-03-02T18:18:56.174192: step 35882, loss 0.140424, acc 0.9375
2017-03-02T18:18:56.243966: step 35883, loss 0.138675, acc 0.953125
2017-03-02T18:18:56.318155: step 35884, loss 0.244817, acc 0.875
2017-03-02T18:18:56.391405: step 35885, loss 0.0931872, acc 0.953125
2017-03-02T18:18:56.467130: step 35886, loss 0.199516, acc 0.875
2017-03-02T18:18:56.546517: step 35887, loss 0.139339, acc 0.9375
2017-03-02T18:18:56.612420: step 35888, loss 0.2421, acc 0.90625
2017-03-02T18:18:56.680002: step 35889, loss 0.10923, acc 0.921875
2017-03-02T18:18:56.757079: step 35890, loss 0.0995442, acc 0.953125
2017-03-02T18:18:56.832160: step 35891, loss 0.155937, acc 0.96875
2017-03-02T18:18:56.906633: step 35892, loss 0.25779, acc 0.890625
2017-03-02T18:18:56.984521: step 35893, loss 0.10589, acc 0.953125
2017-03-02T18:18:57.058539: step 35894, loss 0.0952777, acc 0.953125
2017-03-02T18:18:57.126731: step 35895, loss 0.0808692, acc 0.984375
2017-03-02T18:18:57.208190: step 35896, loss 0.160793, acc 0.90625
2017-03-02T18:18:57.295727: step 35897, loss 0.0740002, acc 0.96875
2017-03-02T18:18:57.365441: step 35898, loss 0.128327, acc 0.921875
2017-03-02T18:18:57.434492: step 35899, loss 0.147178, acc 0.953125
2017-03-02T18:18:57.509771: step 35900, loss 0.158122, acc 0.921875

Evaluation:
2017-03-02T18:18:57.546318: step 35900, loss 5.32404, acc 0.630858

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-35900

2017-03-02T18:18:57.988413: step 35901, loss 0.195606, acc 0.90625
2017-03-02T18:18:58.056955: step 35902, loss 0.202524, acc 0.890625
2017-03-02T18:18:58.122312: step 35903, loss 0.149896, acc 0.921875
2017-03-02T18:18:58.196212: step 35904, loss 0.0981337, acc 0.9375
2017-03-02T18:18:58.265593: step 35905, loss 0.114122, acc 0.953125
2017-03-02T18:18:58.337483: step 35906, loss 0.240323, acc 0.890625
2017-03-02T18:18:58.403867: step 35907, loss 0.182017, acc 0.9375
2017-03-02T18:18:58.477631: step 35908, loss 0.140917, acc 0.9375
2017-03-02T18:18:58.552512: step 35909, loss 0.138281, acc 0.953125
2017-03-02T18:18:58.632222: step 35910, loss 0.130671, acc 0.921875
2017-03-02T18:18:58.697028: step 35911, loss 0.181787, acc 0.90625
2017-03-02T18:18:58.765130: step 35912, loss 0.138926, acc 0.9375
2017-03-02T18:18:58.839247: step 35913, loss 0.0952198, acc 0.984375
2017-03-02T18:18:58.911032: step 35914, loss 0.0775031, acc 0.984375
2017-03-02T18:18:58.999847: step 35915, loss 0.101632, acc 0.96875
2017-03-02T18:18:59.069673: step 35916, loss 0.209647, acc 0.9375
2017-03-02T18:18:59.139257: step 35917, loss 0.130734, acc 0.90625
2017-03-02T18:18:59.218962: step 35918, loss 0.133525, acc 0.921875
2017-03-02T18:18:59.291776: step 35919, loss 0.150726, acc 0.90625
2017-03-02T18:18:59.366929: step 35920, loss 0.109332, acc 0.921875
2017-03-02T18:18:59.432451: step 35921, loss 0.186692, acc 0.9375
2017-03-02T18:18:59.506991: step 35922, loss 0.0964306, acc 0.96875
2017-03-02T18:18:59.582995: step 35923, loss 0.168363, acc 0.90625
2017-03-02T18:18:59.654572: step 35924, loss 0.0770163, acc 0.96875
2017-03-02T18:18:59.746520: step 35925, loss 0.145842, acc 0.921875
2017-03-02T18:18:59.815372: step 35926, loss 0.0528152, acc 0.984375
2017-03-02T18:18:59.895072: step 35927, loss 0.184101, acc 0.890625
2017-03-02T18:18:59.965157: step 35928, loss 0.205108, acc 0.890625
2017-03-02T18:19:00.044973: step 35929, loss 0.137047, acc 0.921875
2017-03-02T18:19:00.114672: step 35930, loss 0.0584449, acc 0.984375
2017-03-02T18:19:00.186308: step 35931, loss 0.131467, acc 0.9375
2017-03-02T18:19:00.257799: step 35932, loss 0.127832, acc 0.921875
2017-03-02T18:19:00.331211: step 35933, loss 0.113306, acc 0.953125
2017-03-02T18:19:00.407832: step 35934, loss 0.194744, acc 0.90625
2017-03-02T18:19:00.483642: step 35935, loss 0.184322, acc 0.921875
2017-03-02T18:19:00.553625: step 35936, loss 0.140187, acc 0.953125
2017-03-02T18:19:00.625270: step 35937, loss 0.157471, acc 0.90625
2017-03-02T18:19:00.693976: step 35938, loss 0.0677627, acc 0.984375
2017-03-02T18:19:00.762170: step 35939, loss 0.194752, acc 0.953125
2017-03-02T18:19:00.830274: step 35940, loss 0.0920233, acc 0.96875
2017-03-02T18:19:00.905148: step 35941, loss 0.0619947, acc 0.984375
2017-03-02T18:19:00.978047: step 35942, loss 0.12578, acc 0.9375
2017-03-02T18:19:01.049555: step 35943, loss 0.1059, acc 0.984375
2017-03-02T18:19:01.120777: step 35944, loss 0.146092, acc 0.921875
2017-03-02T18:19:01.197478: step 35945, loss 0.186818, acc 0.890625
2017-03-02T18:19:01.274528: step 35946, loss 0.302555, acc 0.875
2017-03-02T18:19:01.353580: step 35947, loss 0.117015, acc 0.953125
2017-03-02T18:19:01.418664: step 35948, loss 0.196291, acc 0.875
2017-03-02T18:19:01.490187: step 35949, loss 0.154688, acc 0.9375
2017-03-02T18:19:01.565993: step 35950, loss 0.142446, acc 0.90625
2017-03-02T18:19:01.649710: step 35951, loss 0.118179, acc 0.9375
2017-03-02T18:19:01.722306: step 35952, loss 0.0763672, acc 0.96875
2017-03-02T18:19:01.790750: step 35953, loss 0.180013, acc 0.9375
2017-03-02T18:19:01.861576: step 35954, loss 0.193138, acc 0.953125
2017-03-02T18:19:01.936091: step 35955, loss 0.239779, acc 0.859375
2017-03-02T18:19:02.012803: step 35956, loss 0.236458, acc 0.875
2017-03-02T18:19:02.093196: step 35957, loss 0.111002, acc 0.96875
2017-03-02T18:19:02.163800: step 35958, loss 0.195343, acc 0.9375
2017-03-02T18:19:02.235978: step 35959, loss 0.0458417, acc 0.984375
2017-03-02T18:19:02.309176: step 35960, loss 0.105772, acc 0.953125
2017-03-02T18:19:02.571305: step 35961, loss 0.150014, acc 0.9375
2017-03-02T18:19:02.642478: step 35962, loss 0.0692653, acc 0.953125
2017-03-02T18:19:02.722968: step 35963, loss 0.090419, acc 0.953125
2017-03-02T18:19:02.798499: step 35964, loss 0.0525263, acc 0.984375
2017-03-02T18:19:02.870229: step 35965, loss 0.127546, acc 0.953125
2017-03-02T18:19:02.934308: step 35966, loss 0.21292, acc 0.90625
2017-03-02T18:19:03.013031: step 35967, loss 0.15364, acc 0.890625
2017-03-02T18:19:03.098813: step 35968, loss 0.130257, acc 0.953125
2017-03-02T18:19:03.177168: step 35969, loss 0.130772, acc 0.953125
2017-03-02T18:19:03.245458: step 35970, loss 0.174965, acc 0.9375
2017-03-02T18:19:03.313569: step 35971, loss 0.178932, acc 0.921875
2017-03-02T18:19:03.394615: step 35972, loss 0.207744, acc 0.90625
2017-03-02T18:19:03.477008: step 35973, loss 0.135327, acc 0.9375
2017-03-02T18:19:03.544006: step 35974, loss 0.213535, acc 0.921875
2017-03-02T18:19:03.616589: step 35975, loss 0.228048, acc 0.90625
2017-03-02T18:19:03.682896: step 35976, loss 0.183194, acc 0.921875
2017-03-02T18:19:03.766459: step 35977, loss 0.237298, acc 0.875
2017-03-02T18:19:03.851615: step 35978, loss 0.27204, acc 0.890625
2017-03-02T18:19:03.925834: step 35979, loss 0.127098, acc 0.9375
2017-03-02T18:19:04.008515: step 35980, loss 0.0740566, acc 0.96875
2017-03-02T18:19:04.074889: step 35981, loss 0.0648946, acc 0.953125
2017-03-02T18:19:04.140494: step 35982, loss 0.0414985, acc 1
2017-03-02T18:19:04.210034: step 35983, loss 0.141725, acc 0.9375
2017-03-02T18:19:04.281816: step 35984, loss 0.131701, acc 0.9375
2017-03-02T18:19:04.350543: step 35985, loss 0.113775, acc 0.96875
2017-03-02T18:19:04.420588: step 35986, loss 0.179938, acc 0.890625
2017-03-02T18:19:04.493577: step 35987, loss 0.0915111, acc 0.953125
2017-03-02T18:19:04.569746: step 35988, loss 0.128143, acc 0.953125
2017-03-02T18:19:04.638771: step 35989, loss 0.0670429, acc 0.984375
2017-03-02T18:19:04.709830: step 35990, loss 0.145076, acc 0.9375
2017-03-02T18:19:04.784151: step 35991, loss 0.117742, acc 0.953125
2017-03-02T18:19:04.856673: step 35992, loss 0.196748, acc 0.90625
2017-03-02T18:19:04.925764: step 35993, loss 0.151058, acc 0.9375
2017-03-02T18:19:04.995885: step 35994, loss 0.0454411, acc 1
2017-03-02T18:19:05.066781: step 35995, loss 0.137157, acc 0.9375
2017-03-02T18:19:05.138505: step 35996, loss 0.168972, acc 0.90625
2017-03-02T18:19:05.224491: step 35997, loss 0.0963112, acc 0.9375
2017-03-02T18:19:05.295314: step 35998, loss 0.108628, acc 0.921875
2017-03-02T18:19:05.366634: step 35999, loss 0.151251, acc 0.953125
2017-03-02T18:19:05.438187: step 36000, loss 0.102991, acc 0.96875

Evaluation:
2017-03-02T18:19:05.466000: step 36000, loss 5.35194, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36000

2017-03-02T18:19:05.907495: step 36001, loss 0.183439, acc 0.90625
2017-03-02T18:19:05.977707: step 36002, loss 0.0824888, acc 0.984375
2017-03-02T18:19:06.048836: step 36003, loss 0.179479, acc 0.921875
2017-03-02T18:19:06.108755: step 36004, loss 0.152801, acc 0.921875
2017-03-02T18:19:06.170565: step 36005, loss 0.0931595, acc 0.953125
2017-03-02T18:19:06.255142: step 36006, loss 0.141448, acc 0.953125
2017-03-02T18:19:06.320137: step 36007, loss 0.0496061, acc 0.984375
2017-03-02T18:19:06.397893: step 36008, loss 0.190858, acc 0.9375
2017-03-02T18:19:06.471085: step 36009, loss 0.15085, acc 0.9375
2017-03-02T18:19:06.545681: step 36010, loss 0.0932869, acc 0.96875
2017-03-02T18:19:06.612192: step 36011, loss 0.23983, acc 0.875
2017-03-02T18:19:06.687062: step 36012, loss 0.0877518, acc 0.96875
2017-03-02T18:19:06.759659: step 36013, loss 0.0142891, acc 1
2017-03-02T18:19:06.837174: step 36014, loss 0.141088, acc 0.96875
2017-03-02T18:19:06.912346: step 36015, loss 0.198854, acc 0.921875
2017-03-02T18:19:06.979309: step 36016, loss 0.146386, acc 0.90625
2017-03-02T18:19:07.049523: step 36017, loss 0.160101, acc 0.921875
2017-03-02T18:19:07.120180: step 36018, loss 0.196526, acc 0.9375
2017-03-02T18:19:07.194994: step 36019, loss 0.116993, acc 0.9375
2017-03-02T18:19:07.271335: step 36020, loss 0.234405, acc 0.859375
2017-03-02T18:19:07.346751: step 36021, loss 0.134386, acc 0.90625
2017-03-02T18:19:07.429079: step 36022, loss 0.221948, acc 0.9375
2017-03-02T18:19:07.513923: step 36023, loss 0.203103, acc 0.921875
2017-03-02T18:19:07.588312: step 36024, loss 0.0681355, acc 0.96875
2017-03-02T18:19:07.671490: step 36025, loss 0.154296, acc 0.9375
2017-03-02T18:19:07.744707: step 36026, loss 0.225591, acc 0.921875
2017-03-02T18:19:07.819863: step 36027, loss 0.156714, acc 0.890625
2017-03-02T18:19:07.902040: step 36028, loss 0.122052, acc 0.9375
2017-03-02T18:19:07.973475: step 36029, loss 0.18743, acc 0.890625
2017-03-02T18:19:08.046891: step 36030, loss 0.100558, acc 0.953125
2017-03-02T18:19:08.117757: step 36031, loss 0.153183, acc 0.9375
2017-03-02T18:19:08.189905: step 36032, loss 0.107445, acc 0.9375
2017-03-02T18:19:08.260134: step 36033, loss 0.192552, acc 0.9375
2017-03-02T18:19:08.330085: step 36034, loss 0.168677, acc 0.953125
2017-03-02T18:19:08.399913: step 36035, loss 0.294836, acc 0.875
2017-03-02T18:19:08.471911: step 36036, loss 0.121675, acc 0.9375
2017-03-02T18:19:08.542623: step 36037, loss 0.101496, acc 0.96875
2017-03-02T18:19:08.616535: step 36038, loss 0.222207, acc 0.859375
2017-03-02T18:19:08.694054: step 36039, loss 0.166296, acc 0.9375
2017-03-02T18:19:08.764248: step 36040, loss 0.171269, acc 0.9375
2017-03-02T18:19:08.838236: step 36041, loss 0.142695, acc 0.921875
2017-03-02T18:19:08.904540: step 36042, loss 0.130325, acc 0.9375
2017-03-02T18:19:08.968864: step 36043, loss 0.0625686, acc 0.96875
2017-03-02T18:19:09.037540: step 36044, loss 0.18126, acc 0.875
2017-03-02T18:19:09.109900: step 36045, loss 0.158013, acc 0.90625
2017-03-02T18:19:09.194204: step 36046, loss 0.0562837, acc 0.96875
2017-03-02T18:19:09.278673: step 36047, loss 0.0793536, acc 0.96875
2017-03-02T18:19:09.354017: step 36048, loss 0.122533, acc 0.953125
2017-03-02T18:19:09.424431: step 36049, loss 0.153259, acc 0.921875
2017-03-02T18:19:09.498752: step 36050, loss 0.162275, acc 0.90625
2017-03-02T18:19:09.579831: step 36051, loss 0.202301, acc 0.9375
2017-03-02T18:19:09.647441: step 36052, loss 0.0691251, acc 0.953125
2017-03-02T18:19:09.712689: step 36053, loss 0.0927039, acc 0.96875
2017-03-02T18:19:09.780811: step 36054, loss 0.0419047, acc 0.984375
2017-03-02T18:19:09.853140: step 36055, loss 0.232358, acc 0.859375
2017-03-02T18:19:09.928957: step 36056, loss 0.252677, acc 0.90625
2017-03-02T18:19:10.001205: step 36057, loss 0.131069, acc 0.9375
2017-03-02T18:19:10.072542: step 36058, loss 0.266358, acc 0.84375
2017-03-02T18:19:10.153769: step 36059, loss 0.168403, acc 0.9375
2017-03-02T18:19:10.230256: step 36060, loss 0.180795, acc 0.9375
2017-03-02T18:19:10.312536: step 36061, loss 0.139526, acc 0.90625
2017-03-02T18:19:10.392873: step 36062, loss 0.180467, acc 0.9375
2017-03-02T18:19:10.458865: step 36063, loss 0.23772, acc 0.84375
2017-03-02T18:19:10.528829: step 36064, loss 0.00208167, acc 1
2017-03-02T18:19:10.602276: step 36065, loss 0.192266, acc 0.921875
2017-03-02T18:19:10.690864: step 36066, loss 0.0780683, acc 0.96875
2017-03-02T18:19:10.766883: step 36067, loss 0.172879, acc 0.90625
2017-03-02T18:19:10.846662: step 36068, loss 0.143399, acc 0.90625
2017-03-02T18:19:10.924423: step 36069, loss 0.0654154, acc 1
2017-03-02T18:19:10.998155: step 36070, loss 0.175828, acc 0.9375
2017-03-02T18:19:11.066817: step 36071, loss 0.0478814, acc 0.984375
2017-03-02T18:19:11.135318: step 36072, loss 0.188703, acc 0.890625
2017-03-02T18:19:11.210461: step 36073, loss 0.0697812, acc 0.96875
2017-03-02T18:19:11.288553: step 36074, loss 0.0694338, acc 0.984375
2017-03-02T18:19:11.366249: step 36075, loss 0.10919, acc 0.953125
2017-03-02T18:19:11.441707: step 36076, loss 0.116304, acc 0.953125
2017-03-02T18:19:11.513704: step 36077, loss 0.149193, acc 0.921875
2017-03-02T18:19:11.585811: step 36078, loss 0.138824, acc 0.953125
2017-03-02T18:19:11.646967: step 36079, loss 0.0425761, acc 0.984375
2017-03-02T18:19:11.716951: step 36080, loss 0.131606, acc 0.921875
2017-03-02T18:19:11.791531: step 36081, loss 0.0567003, acc 1
2017-03-02T18:19:11.858543: step 36082, loss 0.137984, acc 0.9375
2017-03-02T18:19:11.929670: step 36083, loss 0.25069, acc 0.890625
2017-03-02T18:19:11.999201: step 36084, loss 0.121567, acc 0.9375
2017-03-02T18:19:12.077421: step 36085, loss 0.102325, acc 0.953125
2017-03-02T18:19:12.147544: step 36086, loss 0.0618736, acc 0.953125
2017-03-02T18:19:12.233049: step 36087, loss 0.257329, acc 0.859375
2017-03-02T18:19:12.303784: step 36088, loss 0.19497, acc 0.9375
2017-03-02T18:19:12.379554: step 36089, loss 0.171577, acc 0.90625
2017-03-02T18:19:12.450391: step 36090, loss 0.131026, acc 0.921875
2017-03-02T18:19:12.515834: step 36091, loss 0.14503, acc 0.90625
2017-03-02T18:19:12.588824: step 36092, loss 0.10935, acc 0.953125
2017-03-02T18:19:12.667216: step 36093, loss 0.179255, acc 0.890625
2017-03-02T18:19:12.740438: step 36094, loss 0.222152, acc 0.890625
2017-03-02T18:19:12.809423: step 36095, loss 0.119188, acc 0.96875
2017-03-02T18:19:12.880883: step 36096, loss 0.183542, acc 0.90625
2017-03-02T18:19:12.950599: step 36097, loss 0.124929, acc 0.9375
2017-03-02T18:19:13.025962: step 36098, loss 0.0422587, acc 0.984375
2017-03-02T18:19:13.097080: step 36099, loss 0.150617, acc 0.953125
2017-03-02T18:19:13.164747: step 36100, loss 0.132528, acc 0.90625

Evaluation:
2017-03-02T18:19:13.196002: step 36100, loss 5.23582, acc 0.63951

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36100

2017-03-02T18:19:13.688141: step 36101, loss 0.143493, acc 0.96875
2017-03-02T18:19:13.757275: step 36102, loss 0.0444993, acc 0.984375
2017-03-02T18:19:13.822848: step 36103, loss 0.0905949, acc 0.953125
2017-03-02T18:19:13.889214: step 36104, loss 0.204201, acc 0.90625
2017-03-02T18:19:13.960015: step 36105, loss 0.19031, acc 0.90625
2017-03-02T18:19:14.031341: step 36106, loss 0.109105, acc 0.953125
2017-03-02T18:19:14.099306: step 36107, loss 0.0551593, acc 0.984375
2017-03-02T18:19:14.173419: step 36108, loss 0.106086, acc 0.9375
2017-03-02T18:19:14.252134: step 36109, loss 0.0861649, acc 0.9375
2017-03-02T18:19:14.329307: step 36110, loss 0.0871037, acc 0.953125
2017-03-02T18:19:14.403549: step 36111, loss 0.151533, acc 0.9375
2017-03-02T18:19:14.478174: step 36112, loss 0.124112, acc 0.9375
2017-03-02T18:19:14.553064: step 36113, loss 0.218467, acc 0.90625
2017-03-02T18:19:14.621196: step 36114, loss 0.141862, acc 0.921875
2017-03-02T18:19:14.691718: step 36115, loss 0.254331, acc 0.9375
2017-03-02T18:19:14.763025: step 36116, loss 0.0829728, acc 0.96875
2017-03-02T18:19:14.835662: step 36117, loss 0.0456731, acc 1
2017-03-02T18:19:14.909331: step 36118, loss 0.121394, acc 0.953125
2017-03-02T18:19:14.989800: step 36119, loss 0.125084, acc 0.953125
2017-03-02T18:19:15.095302: step 36120, loss 0.0898144, acc 0.953125
2017-03-02T18:19:15.163488: step 36121, loss 0.169648, acc 0.921875
2017-03-02T18:19:15.232705: step 36122, loss 0.237905, acc 0.875
2017-03-02T18:19:15.302131: step 36123, loss 0.256516, acc 0.875
2017-03-02T18:19:15.376551: step 36124, loss 0.182284, acc 0.9375
2017-03-02T18:19:15.467032: step 36125, loss 0.153561, acc 0.90625
2017-03-02T18:19:15.545916: step 36126, loss 0.152256, acc 0.90625
2017-03-02T18:19:15.616846: step 36127, loss 0.132623, acc 0.9375
2017-03-02T18:19:15.701590: step 36128, loss 0.0549166, acc 1
2017-03-02T18:19:15.777200: step 36129, loss 0.213944, acc 0.90625
2017-03-02T18:19:15.847558: step 36130, loss 0.118572, acc 0.953125
2017-03-02T18:19:15.914910: step 36131, loss 0.190964, acc 0.875
2017-03-02T18:19:15.982392: step 36132, loss 0.172406, acc 0.921875
2017-03-02T18:19:16.054888: step 36133, loss 0.188927, acc 0.921875
2017-03-02T18:19:16.126362: step 36134, loss 0.0579278, acc 0.96875
2017-03-02T18:19:16.201263: step 36135, loss 0.151138, acc 0.921875
2017-03-02T18:19:16.280055: step 36136, loss 0.126586, acc 0.953125
2017-03-02T18:19:16.350983: step 36137, loss 0.111878, acc 0.96875
2017-03-02T18:19:16.420544: step 36138, loss 0.20568, acc 0.921875
2017-03-02T18:19:16.504844: step 36139, loss 0.132022, acc 0.953125
2017-03-02T18:19:16.573077: step 36140, loss 0.17373, acc 0.921875
2017-03-02T18:19:16.645683: step 36141, loss 0.155909, acc 0.90625
2017-03-02T18:19:16.719107: step 36142, loss 0.0844723, acc 0.96875
2017-03-02T18:19:16.793319: step 36143, loss 0.137656, acc 0.953125
2017-03-02T18:19:16.864291: step 36144, loss 0.125406, acc 0.921875
2017-03-02T18:19:16.936302: step 36145, loss 0.197246, acc 0.921875
2017-03-02T18:19:17.011799: step 36146, loss 0.117594, acc 0.921875
2017-03-02T18:19:17.091127: step 36147, loss 0.145597, acc 0.9375
2017-03-02T18:19:17.164618: step 36148, loss 0.134745, acc 0.96875
2017-03-02T18:19:17.230774: step 36149, loss 0.176773, acc 0.9375
2017-03-02T18:19:17.304436: step 36150, loss 0.083244, acc 0.953125
2017-03-02T18:19:17.372327: step 36151, loss 0.0542578, acc 0.984375
2017-03-02T18:19:17.444922: step 36152, loss 0.185219, acc 0.90625
2017-03-02T18:19:17.525207: step 36153, loss 0.0917443, acc 0.96875
2017-03-02T18:19:17.591670: step 36154, loss 0.232184, acc 0.90625
2017-03-02T18:19:17.670041: step 36155, loss 0.108396, acc 0.984375
2017-03-02T18:19:17.742328: step 36156, loss 0.0937186, acc 0.96875
2017-03-02T18:19:17.814241: step 36157, loss 0.149899, acc 0.90625
2017-03-02T18:19:17.890006: step 36158, loss 0.117262, acc 0.96875
2017-03-02T18:19:17.963472: step 36159, loss 0.086546, acc 0.96875
2017-03-02T18:19:18.040652: step 36160, loss 0.229182, acc 0.921875
2017-03-02T18:19:18.105448: step 36161, loss 0.400149, acc 0.828125
2017-03-02T18:19:18.178391: step 36162, loss 0.204101, acc 0.890625
2017-03-02T18:19:18.245890: step 36163, loss 0.121393, acc 0.9375
2017-03-02T18:19:18.317014: step 36164, loss 0.152351, acc 0.90625
2017-03-02T18:19:18.396206: step 36165, loss 0.174372, acc 0.953125
2017-03-02T18:19:18.476434: step 36166, loss 0.148025, acc 0.9375
2017-03-02T18:19:18.550552: step 36167, loss 0.0503331, acc 0.984375
2017-03-02T18:19:18.614619: step 36168, loss 0.134787, acc 0.953125
2017-03-02T18:19:18.685341: step 36169, loss 0.19798, acc 0.90625
2017-03-02T18:19:18.757905: step 36170, loss 0.164419, acc 0.9375
2017-03-02T18:19:18.827787: step 36171, loss 0.172116, acc 0.921875
2017-03-02T18:19:18.903098: step 36172, loss 0.0900118, acc 0.96875
2017-03-02T18:19:18.973657: step 36173, loss 0.109389, acc 0.953125
2017-03-02T18:19:19.047147: step 36174, loss 0.149128, acc 0.90625
2017-03-02T18:19:19.118771: step 36175, loss 0.176197, acc 0.90625
2017-03-02T18:19:19.192175: step 36176, loss 0.123054, acc 0.9375
2017-03-02T18:19:19.264925: step 36177, loss 0.191646, acc 0.890625
2017-03-02T18:19:19.336238: step 36178, loss 0.210394, acc 0.921875
2017-03-02T18:19:19.403098: step 36179, loss 0.143786, acc 0.9375
2017-03-02T18:19:19.480206: step 36180, loss 0.0890125, acc 0.953125
2017-03-02T18:19:19.546992: step 36181, loss 0.109245, acc 0.9375
2017-03-02T18:19:19.620851: step 36182, loss 0.186459, acc 0.90625
2017-03-02T18:19:19.692216: step 36183, loss 0.0900711, acc 0.9375
2017-03-02T18:19:19.776043: step 36184, loss 0.115066, acc 0.953125
2017-03-02T18:19:19.852059: step 36185, loss 0.125467, acc 0.9375
2017-03-02T18:19:19.924759: step 36186, loss 0.0177727, acc 1
2017-03-02T18:19:19.990209: step 36187, loss 0.191537, acc 0.921875
2017-03-02T18:19:20.058947: step 36188, loss 0.172203, acc 0.859375
2017-03-02T18:19:20.136634: step 36189, loss 0.100843, acc 0.96875
2017-03-02T18:19:20.207944: step 36190, loss 0.230901, acc 0.9375
2017-03-02T18:19:20.283723: step 36191, loss 0.114523, acc 0.96875
2017-03-02T18:19:20.356109: step 36192, loss 0.236932, acc 0.859375
2017-03-02T18:19:20.429665: step 36193, loss 0.0798371, acc 0.9375
2017-03-02T18:19:20.503906: step 36194, loss 0.122745, acc 0.9375
2017-03-02T18:19:20.575891: step 36195, loss 0.106899, acc 0.96875
2017-03-02T18:19:20.644803: step 36196, loss 0.157234, acc 0.890625
2017-03-02T18:19:20.715423: step 36197, loss 0.147632, acc 0.9375
2017-03-02T18:19:20.784083: step 36198, loss 0.273379, acc 0.84375
2017-03-02T18:19:20.859705: step 36199, loss 0.0537556, acc 0.96875
2017-03-02T18:19:20.966972: step 36200, loss 0.14775, acc 0.9375

Evaluation:
2017-03-02T18:19:20.999946: step 36200, loss 5.2404, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36200

2017-03-02T18:19:21.450967: step 36201, loss 0.155089, acc 0.921875
2017-03-02T18:19:21.531099: step 36202, loss 0.113296, acc 0.9375
2017-03-02T18:19:21.603688: step 36203, loss 0.136605, acc 0.921875
2017-03-02T18:19:21.676714: step 36204, loss 0.123778, acc 0.9375
2017-03-02T18:19:21.749412: step 36205, loss 0.141133, acc 0.9375
2017-03-02T18:19:21.821948: step 36206, loss 0.201059, acc 0.921875
2017-03-02T18:19:21.894489: step 36207, loss 0.157101, acc 0.9375
2017-03-02T18:19:21.973727: step 36208, loss 0.0816046, acc 0.953125
2017-03-02T18:19:22.042667: step 36209, loss 0.10425, acc 0.953125
2017-03-02T18:19:22.106709: step 36210, loss 0.16171, acc 0.9375
2017-03-02T18:19:22.179082: step 36211, loss 0.116432, acc 0.9375
2017-03-02T18:19:22.255916: step 36212, loss 0.157486, acc 0.921875
2017-03-02T18:19:22.324353: step 36213, loss 0.227635, acc 0.921875
2017-03-02T18:19:22.397224: step 36214, loss 0.0966299, acc 0.9375
2017-03-02T18:19:22.476436: step 36215, loss 0.189247, acc 0.875
2017-03-02T18:19:22.541060: step 36216, loss 0.212359, acc 0.953125
2017-03-02T18:19:22.615259: step 36217, loss 0.16589, acc 0.953125
2017-03-02T18:19:22.701428: step 36218, loss 0.0858814, acc 0.953125
2017-03-02T18:19:22.771430: step 36219, loss 0.141489, acc 0.921875
2017-03-02T18:19:22.841983: step 36220, loss 0.129658, acc 0.90625
2017-03-02T18:19:22.915887: step 36221, loss 0.113754, acc 0.96875
2017-03-02T18:19:22.986732: step 36222, loss 0.17805, acc 0.921875
2017-03-02T18:19:23.072210: step 36223, loss 0.143224, acc 0.921875
2017-03-02T18:19:23.146886: step 36224, loss 0.0936336, acc 0.953125
2017-03-02T18:19:23.223445: step 36225, loss 0.108596, acc 0.953125
2017-03-02T18:19:23.298136: step 36226, loss 0.180448, acc 0.890625
2017-03-02T18:19:23.366920: step 36227, loss 0.117875, acc 0.96875
2017-03-02T18:19:23.438993: step 36228, loss 0.144123, acc 0.953125
2017-03-02T18:19:23.517622: step 36229, loss 0.219891, acc 0.90625
2017-03-02T18:19:23.592534: step 36230, loss 0.203841, acc 0.90625
2017-03-02T18:19:23.674625: step 36231, loss 0.340743, acc 0.859375
2017-03-02T18:19:23.746646: step 36232, loss 0.185987, acc 0.90625
2017-03-02T18:19:23.817994: step 36233, loss 0.18519, acc 0.953125
2017-03-02T18:19:23.893985: step 36234, loss 0.0810108, acc 0.96875
2017-03-02T18:19:23.965811: step 36235, loss 0.171773, acc 0.90625
2017-03-02T18:19:24.044736: step 36236, loss 0.196329, acc 0.90625
2017-03-02T18:19:24.125195: step 36237, loss 0.0540811, acc 0.96875
2017-03-02T18:19:24.196220: step 36238, loss 0.25274, acc 0.890625
2017-03-02T18:19:24.277337: step 36239, loss 0.12056, acc 0.96875
2017-03-02T18:19:24.351727: step 36240, loss 0.137085, acc 0.9375
2017-03-02T18:19:24.423947: step 36241, loss 0.174393, acc 0.90625
2017-03-02T18:19:24.511296: step 36242, loss 0.108124, acc 0.96875
2017-03-02T18:19:24.590141: step 36243, loss 0.144537, acc 0.921875
2017-03-02T18:19:24.661217: step 36244, loss 0.210477, acc 0.9375
2017-03-02T18:19:24.741944: step 36245, loss 0.0812678, acc 0.96875
2017-03-02T18:19:24.801195: step 36246, loss 0.12053, acc 0.9375
2017-03-02T18:19:24.867517: step 36247, loss 0.0943036, acc 0.96875
2017-03-02T18:19:24.939352: step 36248, loss 0.186481, acc 0.890625
2017-03-02T18:19:25.023036: step 36249, loss 0.107271, acc 0.96875
2017-03-02T18:19:25.097806: step 36250, loss 0.129541, acc 0.953125
2017-03-02T18:19:25.177908: step 36251, loss 0.0614433, acc 0.984375
2017-03-02T18:19:25.258389: step 36252, loss 0.135667, acc 0.9375
2017-03-02T18:19:25.330502: step 36253, loss 0.169819, acc 0.921875
2017-03-02T18:19:25.397926: step 36254, loss 0.220086, acc 0.875
2017-03-02T18:19:25.469254: step 36255, loss 0.217427, acc 0.9375
2017-03-02T18:19:25.536709: step 36256, loss 0.084716, acc 0.9375
2017-03-02T18:19:25.612153: step 36257, loss 0.192207, acc 0.90625
2017-03-02T18:19:25.683317: step 36258, loss 0.095932, acc 0.9375
2017-03-02T18:19:25.757932: step 36259, loss 0.165159, acc 0.921875
2017-03-02T18:19:25.829290: step 36260, loss 0.0504454, acc 1
2017-03-02T18:19:25.900842: step 36261, loss 0.073747, acc 0.96875
2017-03-02T18:19:25.972679: step 36262, loss 0.13473, acc 0.953125
2017-03-02T18:19:26.051374: step 36263, loss 0.116399, acc 0.953125
2017-03-02T18:19:26.127180: step 36264, loss 0.11214, acc 0.9375
2017-03-02T18:19:26.198150: step 36265, loss 0.0863855, acc 0.9375
2017-03-02T18:19:26.267210: step 36266, loss 0.181596, acc 0.875
2017-03-02T18:19:26.334096: step 36267, loss 0.171864, acc 0.90625
2017-03-02T18:19:26.397997: step 36268, loss 0.164296, acc 0.90625
2017-03-02T18:19:26.468088: step 36269, loss 0.0918962, acc 0.9375
2017-03-02T18:19:26.539160: step 36270, loss 0.117188, acc 0.953125
2017-03-02T18:19:26.623479: step 36271, loss 0.147317, acc 0.9375
2017-03-02T18:19:26.692947: step 36272, loss 0.112018, acc 0.953125
2017-03-02T18:19:26.757511: step 36273, loss 0.199963, acc 0.90625
2017-03-02T18:19:26.825685: step 36274, loss 0.131302, acc 0.921875
2017-03-02T18:19:26.895622: step 36275, loss 0.100298, acc 0.96875
2017-03-02T18:19:26.965543: step 36276, loss 0.179182, acc 0.921875
2017-03-02T18:19:27.041968: step 36277, loss 0.0661628, acc 1
2017-03-02T18:19:27.115735: step 36278, loss 0.126737, acc 0.9375
2017-03-02T18:19:27.194156: step 36279, loss 0.179134, acc 0.875
2017-03-02T18:19:27.266949: step 36280, loss 0.174493, acc 0.890625
2017-03-02T18:19:27.338637: step 36281, loss 0.13598, acc 0.953125
2017-03-02T18:19:27.417009: step 36282, loss 0.0938558, acc 0.953125
2017-03-02T18:19:27.498472: step 36283, loss 0.170166, acc 0.921875
2017-03-02T18:19:27.562678: step 36284, loss 0.233673, acc 0.890625
2017-03-02T18:19:27.629318: step 36285, loss 0.0855542, acc 0.96875
2017-03-02T18:19:27.702891: step 36286, loss 0.129444, acc 0.96875
2017-03-02T18:19:27.774312: step 36287, loss 0.10488, acc 0.953125
2017-03-02T18:19:27.847472: step 36288, loss 0.142077, acc 0.953125
2017-03-02T18:19:27.917707: step 36289, loss 0.111369, acc 0.984375
2017-03-02T18:19:27.990286: step 36290, loss 0.101495, acc 0.953125
2017-03-02T18:19:28.058189: step 36291, loss 0.223276, acc 0.921875
2017-03-02T18:19:28.124479: step 36292, loss 0.120565, acc 0.9375
2017-03-02T18:19:28.199304: step 36293, loss 0.0913967, acc 0.953125
2017-03-02T18:19:28.260871: step 36294, loss 0.0762275, acc 0.9375
2017-03-02T18:19:28.329886: step 36295, loss 0.202464, acc 0.90625
2017-03-02T18:19:28.398744: step 36296, loss 0.125428, acc 0.96875
2017-03-02T18:19:28.466814: step 36297, loss 0.148696, acc 0.921875
2017-03-02T18:19:28.541840: step 36298, loss 0.0916416, acc 0.96875
2017-03-02T18:19:28.614452: step 36299, loss 0.0647523, acc 0.984375
2017-03-02T18:19:28.686877: step 36300, loss 0.097293, acc 0.96875

Evaluation:
2017-03-02T18:19:28.723520: step 36300, loss 5.20864, acc 0.63951

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36300

2017-03-02T18:19:29.175721: step 36301, loss 0.155534, acc 0.9375
2017-03-02T18:19:29.251492: step 36302, loss 0.0776773, acc 0.96875
2017-03-02T18:19:29.324200: step 36303, loss 0.166402, acc 0.890625
2017-03-02T18:19:29.393181: step 36304, loss 0.156709, acc 0.9375
2017-03-02T18:19:29.462175: step 36305, loss 0.088468, acc 0.984375
2017-03-02T18:19:29.537354: step 36306, loss 0.230802, acc 0.875
2017-03-02T18:19:29.611995: step 36307, loss 0.102352, acc 0.953125
2017-03-02T18:19:29.679643: step 36308, loss 0.22012, acc 0.875
2017-03-02T18:19:29.751109: step 36309, loss 0.198009, acc 0.890625
2017-03-02T18:19:29.832039: step 36310, loss 0.209385, acc 0.90625
2017-03-02T18:19:29.902968: step 36311, loss 0.154933, acc 0.9375
2017-03-02T18:19:29.968316: step 36312, loss 0.100505, acc 0.984375
2017-03-02T18:19:30.035622: step 36313, loss 0.0753871, acc 0.984375
2017-03-02T18:19:30.111163: step 36314, loss 0.12387, acc 0.953125
2017-03-02T18:19:30.183426: step 36315, loss 0.0906342, acc 0.96875
2017-03-02T18:19:30.286334: step 36316, loss 0.0884648, acc 0.96875
2017-03-02T18:19:30.357220: step 36317, loss 0.0621195, acc 0.96875
2017-03-02T18:19:30.456185: step 36318, loss 0.177766, acc 0.9375
2017-03-02T18:19:30.539202: step 36319, loss 0.164638, acc 0.90625
2017-03-02T18:19:30.615457: step 36320, loss 0.0794418, acc 0.984375
2017-03-02T18:19:30.702131: step 36321, loss 0.1413, acc 0.921875
2017-03-02T18:19:30.773415: step 36322, loss 0.143198, acc 0.90625
2017-03-02T18:19:30.859668: step 36323, loss 0.249812, acc 0.859375
2017-03-02T18:19:30.924803: step 36324, loss 0.114226, acc 0.9375
2017-03-02T18:19:30.993793: step 36325, loss 0.121323, acc 0.953125
2017-03-02T18:19:31.061722: step 36326, loss 0.108639, acc 0.9375
2017-03-02T18:19:31.124792: step 36327, loss 0.154962, acc 0.921875
2017-03-02T18:19:31.202497: step 36328, loss 0.119081, acc 0.921875
2017-03-02T18:19:31.279413: step 36329, loss 0.137604, acc 0.90625
2017-03-02T18:19:31.356237: step 36330, loss 0.0808932, acc 0.96875
2017-03-02T18:19:31.428934: step 36331, loss 0.0994977, acc 0.953125
2017-03-02T18:19:31.503804: step 36332, loss 0.310832, acc 0.90625
2017-03-02T18:19:31.576446: step 36333, loss 0.110356, acc 0.953125
2017-03-02T18:19:31.655422: step 36334, loss 0.11925, acc 0.9375
2017-03-02T18:19:31.735511: step 36335, loss 0.194574, acc 0.90625
2017-03-02T18:19:31.803373: step 36336, loss 0.270182, acc 0.84375
2017-03-02T18:19:31.875218: step 36337, loss 0.112388, acc 0.921875
2017-03-02T18:19:31.954644: step 36338, loss 0.0824165, acc 0.96875
2017-03-02T18:19:32.031010: step 36339, loss 0.128866, acc 0.9375
2017-03-02T18:19:32.101187: step 36340, loss 0.08149, acc 0.96875
2017-03-02T18:19:32.173146: step 36341, loss 0.100333, acc 0.953125
2017-03-02T18:19:32.246331: step 36342, loss 0.187607, acc 0.921875
2017-03-02T18:19:32.321839: step 36343, loss 0.331929, acc 0.875
2017-03-02T18:19:32.394723: step 36344, loss 0.167793, acc 0.9375
2017-03-02T18:19:32.469661: step 36345, loss 0.165995, acc 0.890625
2017-03-02T18:19:32.537244: step 36346, loss 0.201327, acc 0.875
2017-03-02T18:19:32.639319: step 36347, loss 0.044058, acc 0.984375
2017-03-02T18:19:32.716576: step 36348, loss 0.12156, acc 0.953125
2017-03-02T18:19:32.790416: step 36349, loss 0.172167, acc 0.921875
2017-03-02T18:19:32.865112: step 36350, loss 0.108044, acc 0.953125
2017-03-02T18:19:32.933646: step 36351, loss 0.228834, acc 0.84375
2017-03-02T18:19:33.009233: step 36352, loss 0.0633105, acc 0.984375
2017-03-02T18:19:33.084994: step 36353, loss 0.0614542, acc 0.96875
2017-03-02T18:19:33.157507: step 36354, loss 0.151252, acc 0.953125
2017-03-02T18:19:33.228837: step 36355, loss 0.156362, acc 0.9375
2017-03-02T18:19:33.302057: step 36356, loss 0.175787, acc 0.921875
2017-03-02T18:19:33.380558: step 36357, loss 0.152365, acc 0.9375
2017-03-02T18:19:33.453274: step 36358, loss 0.125607, acc 0.921875
2017-03-02T18:19:33.524332: step 36359, loss 0.0782445, acc 0.96875
2017-03-02T18:19:33.615519: step 36360, loss 0.0766371, acc 0.984375
2017-03-02T18:19:33.683228: step 36361, loss 0.156033, acc 0.90625
2017-03-02T18:19:33.756926: step 36362, loss 0.103681, acc 0.9375
2017-03-02T18:19:33.834108: step 36363, loss 0.160624, acc 0.9375
2017-03-02T18:19:33.905495: step 36364, loss 0.213062, acc 0.890625
2017-03-02T18:19:33.980218: step 36365, loss 0.120782, acc 0.9375
2017-03-02T18:19:34.050905: step 36366, loss 0.174179, acc 0.96875
2017-03-02T18:19:34.124816: step 36367, loss 0.130224, acc 0.9375
2017-03-02T18:19:34.200805: step 36368, loss 0.141058, acc 0.9375
2017-03-02T18:19:34.305521: step 36369, loss 0.102917, acc 0.9375
2017-03-02T18:19:34.381319: step 36370, loss 0.122866, acc 0.921875
2017-03-02T18:19:34.455046: step 36371, loss 0.0860395, acc 0.953125
2017-03-02T18:19:34.522859: step 36372, loss 0.17622, acc 0.90625
2017-03-02T18:19:34.604024: step 36373, loss 0.165418, acc 0.9375
2017-03-02T18:19:34.687035: step 36374, loss 0.105306, acc 0.9375
2017-03-02T18:19:34.758066: step 36375, loss 0.171544, acc 0.921875
2017-03-02T18:19:34.827644: step 36376, loss 0.0853628, acc 0.9375
2017-03-02T18:19:34.897784: step 36377, loss 0.162384, acc 0.90625
2017-03-02T18:19:34.980606: step 36378, loss 0.139601, acc 0.921875
2017-03-02T18:19:35.066278: step 36379, loss 0.114078, acc 0.953125
2017-03-02T18:19:35.139156: step 36380, loss 0.199196, acc 0.890625
2017-03-02T18:19:35.208668: step 36381, loss 0.0591594, acc 1
2017-03-02T18:19:35.275343: step 36382, loss 0.0689324, acc 0.984375
2017-03-02T18:19:35.344803: step 36383, loss 0.215841, acc 0.9375
2017-03-02T18:19:35.415009: step 36384, loss 0.191726, acc 0.90625
2017-03-02T18:19:35.488314: step 36385, loss 0.110137, acc 0.9375
2017-03-02T18:19:35.559959: step 36386, loss 0.214829, acc 0.890625
2017-03-02T18:19:35.626576: step 36387, loss 0.249267, acc 0.890625
2017-03-02T18:19:35.700085: step 36388, loss 0.267848, acc 0.90625
2017-03-02T18:19:35.779080: step 36389, loss 0.170566, acc 0.90625
2017-03-02T18:19:35.842422: step 36390, loss 0.172514, acc 0.9375
2017-03-02T18:19:35.913212: step 36391, loss 0.131941, acc 0.953125
2017-03-02T18:19:35.973270: step 36392, loss 0.255125, acc 0.890625
2017-03-02T18:19:36.059063: step 36393, loss 0.201115, acc 0.953125
2017-03-02T18:19:36.130460: step 36394, loss 0.0724293, acc 0.96875
2017-03-02T18:19:36.206959: step 36395, loss 0.120353, acc 0.9375
2017-03-02T18:19:36.277958: step 36396, loss 0.0919286, acc 0.953125
2017-03-02T18:19:36.363916: step 36397, loss 0.0947344, acc 0.953125
2017-03-02T18:19:36.434882: step 36398, loss 0.125248, acc 0.953125
2017-03-02T18:19:36.516946: step 36399, loss 0.100702, acc 0.96875
2017-03-02T18:19:36.589367: step 36400, loss 0.113353, acc 0.9375

Evaluation:
2017-03-02T18:19:36.621694: step 36400, loss 5.18486, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36400

2017-03-02T18:19:37.062052: step 36401, loss 0.0875823, acc 0.953125
2017-03-02T18:19:37.133102: step 36402, loss 0.243729, acc 0.875
2017-03-02T18:19:37.201004: step 36403, loss 0.134619, acc 0.921875
2017-03-02T18:19:37.277588: step 36404, loss 0.129709, acc 0.9375
2017-03-02T18:19:37.349040: step 36405, loss 0.231933, acc 0.890625
2017-03-02T18:19:37.426071: step 36406, loss 0.209344, acc 0.90625
2017-03-02T18:19:37.495017: step 36407, loss 0.143191, acc 0.921875
2017-03-02T18:19:37.569359: step 36408, loss 0.186022, acc 0.9375
2017-03-02T18:19:37.646362: step 36409, loss 0.161165, acc 0.9375
2017-03-02T18:19:37.714320: step 36410, loss 0.153338, acc 0.90625
2017-03-02T18:19:37.790097: step 36411, loss 0.185956, acc 0.90625
2017-03-02T18:19:37.864933: step 36412, loss 0.153155, acc 0.9375
2017-03-02T18:19:37.938673: step 36413, loss 0.177805, acc 0.9375
2017-03-02T18:19:38.002988: step 36414, loss 0.129063, acc 0.9375
2017-03-02T18:19:38.075398: step 36415, loss 0.159931, acc 0.90625
2017-03-02T18:19:38.147345: step 36416, loss 0.112672, acc 0.953125
2017-03-02T18:19:38.219333: step 36417, loss 0.140796, acc 0.921875
2017-03-02T18:19:38.287022: step 36418, loss 0.141009, acc 0.9375
2017-03-02T18:19:38.362093: step 36419, loss 0.151374, acc 0.90625
2017-03-02T18:19:38.440725: step 36420, loss 0.126416, acc 0.953125
2017-03-02T18:19:38.520755: step 36421, loss 0.197846, acc 0.921875
2017-03-02T18:19:38.584279: step 36422, loss 0.123946, acc 0.9375
2017-03-02T18:19:38.652580: step 36423, loss 0.0914231, acc 0.953125
2017-03-02T18:19:38.723657: step 36424, loss 0.125975, acc 0.9375
2017-03-02T18:19:38.794450: step 36425, loss 0.18156, acc 0.90625
2017-03-02T18:19:38.866267: step 36426, loss 0.198354, acc 0.921875
2017-03-02T18:19:38.943180: step 36427, loss 0.171807, acc 0.859375
2017-03-02T18:19:39.010875: step 36428, loss 0.146907, acc 0.90625
2017-03-02T18:19:39.085241: step 36429, loss 0.108861, acc 0.9375
2017-03-02T18:19:39.157507: step 36430, loss 0.148246, acc 0.9375
2017-03-02T18:19:39.231584: step 36431, loss 0.174685, acc 0.90625
2017-03-02T18:19:39.300600: step 36432, loss 0.101937, acc 0.953125
2017-03-02T18:19:39.368236: step 36433, loss 0.192704, acc 0.90625
2017-03-02T18:19:39.435788: step 36434, loss 0.0916532, acc 0.96875
2017-03-02T18:19:39.510162: step 36435, loss 0.21123, acc 0.921875
2017-03-02T18:19:39.590285: step 36436, loss 0.100878, acc 0.96875
2017-03-02T18:19:39.675204: step 36437, loss 0.139429, acc 0.953125
2017-03-02T18:19:39.756873: step 36438, loss 0.12789, acc 0.921875
2017-03-02T18:19:39.828679: step 36439, loss 0.212517, acc 0.921875
2017-03-02T18:19:39.903246: step 36440, loss 0.159899, acc 0.921875
2017-03-02T18:19:39.980597: step 36441, loss 0.151791, acc 0.921875
2017-03-02T18:19:40.050961: step 36442, loss 0.102993, acc 0.953125
2017-03-02T18:19:40.124590: step 36443, loss 0.101748, acc 0.96875
2017-03-02T18:19:40.193626: step 36444, loss 0.0954209, acc 0.9375
2017-03-02T18:19:40.262683: step 36445, loss 0.0728095, acc 0.984375
2017-03-02T18:19:40.334786: step 36446, loss 0.252977, acc 0.84375
2017-03-02T18:19:40.406387: step 36447, loss 0.192197, acc 0.921875
2017-03-02T18:19:40.481000: step 36448, loss 0.137868, acc 0.9375
2017-03-02T18:19:40.552773: step 36449, loss 0.123328, acc 0.953125
2017-03-02T18:19:40.623223: step 36450, loss 0.215718, acc 0.9375
2017-03-02T18:19:40.696889: step 36451, loss 0.194662, acc 0.890625
2017-03-02T18:19:40.760905: step 36452, loss 0.128423, acc 0.9375
2017-03-02T18:19:40.839593: step 36453, loss 0.135517, acc 0.953125
2017-03-02T18:19:40.909475: step 36454, loss 0.180477, acc 0.90625
2017-03-02T18:19:40.989115: step 36455, loss 0.105402, acc 0.953125
2017-03-02T18:19:41.056432: step 36456, loss 0.364879, acc 0.75
2017-03-02T18:19:41.137391: step 36457, loss 0.0821191, acc 0.96875
2017-03-02T18:19:41.212817: step 36458, loss 0.0732069, acc 0.96875
2017-03-02T18:19:41.277267: step 36459, loss 0.134912, acc 0.9375
2017-03-02T18:19:41.349292: step 36460, loss 0.161431, acc 0.921875
2017-03-02T18:19:41.420550: step 36461, loss 0.138064, acc 0.953125
2017-03-02T18:19:41.490073: step 36462, loss 0.137592, acc 0.9375
2017-03-02T18:19:41.558651: step 36463, loss 0.262031, acc 0.921875
2017-03-02T18:19:41.634891: step 36464, loss 0.108487, acc 0.96875
2017-03-02T18:19:41.706692: step 36465, loss 0.0844298, acc 0.9375
2017-03-02T18:19:41.777826: step 36466, loss 0.181816, acc 0.90625
2017-03-02T18:19:41.850122: step 36467, loss 0.137831, acc 0.9375
2017-03-02T18:19:41.921040: step 36468, loss 0.142176, acc 0.953125
2017-03-02T18:19:41.994045: step 36469, loss 0.209612, acc 0.875
2017-03-02T18:19:42.061684: step 36470, loss 0.132987, acc 0.953125
2017-03-02T18:19:42.129002: step 36471, loss 0.0674693, acc 0.96875
2017-03-02T18:19:42.210080: step 36472, loss 0.169897, acc 0.90625
2017-03-02T18:19:42.284302: step 36473, loss 0.0764829, acc 0.96875
2017-03-02T18:19:42.363054: step 36474, loss 0.0712681, acc 0.984375
2017-03-02T18:19:42.439775: step 36475, loss 0.211193, acc 0.921875
2017-03-02T18:19:42.515959: step 36476, loss 0.0761802, acc 0.96875
2017-03-02T18:19:42.597308: step 36477, loss 0.107512, acc 0.9375
2017-03-02T18:19:42.682192: step 36478, loss 0.133889, acc 0.9375
2017-03-02T18:19:42.754032: step 36479, loss 0.210583, acc 0.9375
2017-03-02T18:19:42.822734: step 36480, loss 0.115793, acc 0.96875
2017-03-02T18:19:42.891873: step 36481, loss 0.115955, acc 0.90625
2017-03-02T18:19:42.964988: step 36482, loss 0.244417, acc 0.890625
2017-03-02T18:19:43.037650: step 36483, loss 0.209196, acc 0.9375
2017-03-02T18:19:43.113653: step 36484, loss 0.269116, acc 0.859375
2017-03-02T18:19:43.192793: step 36485, loss 0.119338, acc 0.921875
2017-03-02T18:19:43.260457: step 36486, loss 0.131976, acc 0.953125
2017-03-02T18:19:43.341345: step 36487, loss 0.234494, acc 0.875
2017-03-02T18:19:43.410623: step 36488, loss 0.150246, acc 0.921875
2017-03-02T18:19:43.473209: step 36489, loss 0.0815172, acc 0.96875
2017-03-02T18:19:43.546671: step 36490, loss 0.201957, acc 0.90625
2017-03-02T18:19:43.622861: step 36491, loss 0.119799, acc 0.953125
2017-03-02T18:19:43.698439: step 36492, loss 0.101508, acc 0.953125
2017-03-02T18:19:43.769539: step 36493, loss 0.433667, acc 0.796875
2017-03-02T18:19:43.839093: step 36494, loss 0.14582, acc 0.96875
2017-03-02T18:19:43.916661: step 36495, loss 0.0943563, acc 0.953125
2017-03-02T18:19:43.985878: step 36496, loss 0.203678, acc 0.890625
2017-03-02T18:19:44.057771: step 36497, loss 0.0653649, acc 0.984375
2017-03-02T18:19:44.125889: step 36498, loss 0.152071, acc 0.90625
2017-03-02T18:19:44.191361: step 36499, loss 0.117524, acc 0.921875
2017-03-02T18:19:44.264729: step 36500, loss 0.249646, acc 0.875

Evaluation:
2017-03-02T18:19:44.298875: step 36500, loss 5.2592, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36500

2017-03-02T18:19:44.755404: step 36501, loss 0.143104, acc 0.9375
2017-03-02T18:19:44.823460: step 36502, loss 0.1224, acc 0.96875
2017-03-02T18:19:44.892859: step 36503, loss 0.107275, acc 0.953125
2017-03-02T18:19:44.973976: step 36504, loss 0.07235, acc 0.96875
2017-03-02T18:19:45.041592: step 36505, loss 0.0536768, acc 0.984375
2017-03-02T18:19:45.116636: step 36506, loss 0.0898712, acc 0.953125
2017-03-02T18:19:45.185548: step 36507, loss 0.125556, acc 0.9375
2017-03-02T18:19:45.268302: step 36508, loss 0.14389, acc 0.953125
2017-03-02T18:19:45.345182: step 36509, loss 0.0653454, acc 0.953125
2017-03-02T18:19:45.416759: step 36510, loss 0.206564, acc 0.9375
2017-03-02T18:19:45.483447: step 36511, loss 0.169119, acc 0.921875
2017-03-02T18:19:45.547607: step 36512, loss 0.165905, acc 0.90625
2017-03-02T18:19:45.631334: step 36513, loss 0.234087, acc 0.90625
2017-03-02T18:19:45.706749: step 36514, loss 0.102257, acc 0.953125
2017-03-02T18:19:45.772710: step 36515, loss 0.0580165, acc 0.984375
2017-03-02T18:19:45.851351: step 36516, loss 0.192794, acc 0.921875
2017-03-02T18:19:45.924982: step 36517, loss 0.170756, acc 0.921875
2017-03-02T18:19:45.998945: step 36518, loss 0.245359, acc 0.890625
2017-03-02T18:19:46.070789: step 36519, loss 0.157243, acc 0.9375
2017-03-02T18:19:46.139054: step 36520, loss 0.0823177, acc 0.96875
2017-03-02T18:19:46.202154: step 36521, loss 0.172885, acc 0.90625
2017-03-02T18:19:46.269833: step 36522, loss 0.0954647, acc 0.984375
2017-03-02T18:19:46.340706: step 36523, loss 0.0959664, acc 0.96875
2017-03-02T18:19:46.433876: step 36524, loss 0.0981224, acc 0.984375
2017-03-02T18:19:46.536691: step 36525, loss 0.134472, acc 0.953125
2017-03-02T18:19:46.611826: step 36526, loss 0.117237, acc 0.921875
2017-03-02T18:19:46.683388: step 36527, loss 0.114963, acc 0.9375
2017-03-02T18:19:46.756773: step 36528, loss 0.0867158, acc 0.96875
2017-03-02T18:19:46.833121: step 36529, loss 0.11936, acc 0.96875
2017-03-02T18:19:46.901722: step 36530, loss 0.0692413, acc 1
2017-03-02T18:19:46.972737: step 36531, loss 0.124603, acc 0.9375
2017-03-02T18:19:47.044521: step 36532, loss 0.175461, acc 0.9375
2017-03-02T18:19:47.122738: step 36533, loss 0.0924036, acc 0.953125
2017-03-02T18:19:47.199424: step 36534, loss 0.136627, acc 0.9375
2017-03-02T18:19:47.297499: step 36535, loss 0.188756, acc 0.921875
2017-03-02T18:19:47.364637: step 36536, loss 0.1406, acc 0.921875
2017-03-02T18:19:47.438685: step 36537, loss 0.084337, acc 0.96875
2017-03-02T18:19:47.509910: step 36538, loss 0.175522, acc 0.90625
2017-03-02T18:19:47.585843: step 36539, loss 0.0988753, acc 0.9375
2017-03-02T18:19:47.656221: step 36540, loss 0.162091, acc 0.9375
2017-03-02T18:19:47.727929: step 36541, loss 0.136343, acc 0.90625
2017-03-02T18:19:47.799332: step 36542, loss 0.184092, acc 0.90625
2017-03-02T18:19:47.885766: step 36543, loss 0.168851, acc 0.9375
2017-03-02T18:19:47.953016: step 36544, loss 0.0963828, acc 0.984375
2017-03-02T18:19:48.022805: step 36545, loss 0.192042, acc 0.921875
2017-03-02T18:19:48.090327: step 36546, loss 0.179359, acc 0.90625
2017-03-02T18:19:48.153932: step 36547, loss 0.0167252, acc 0.984375
2017-03-02T18:19:48.219178: step 36548, loss 0.0959534, acc 0.96875
2017-03-02T18:19:48.280944: step 36549, loss 0.025219, acc 0.984375
2017-03-02T18:19:48.358529: step 36550, loss 0.148581, acc 0.9375
2017-03-02T18:19:48.430681: step 36551, loss 0.124019, acc 0.96875
2017-03-02T18:19:48.502277: step 36552, loss 0.0458111, acc 0.984375
2017-03-02T18:19:48.585907: step 36553, loss 0.11776, acc 0.921875
2017-03-02T18:19:48.655974: step 36554, loss 0.24519, acc 0.875
2017-03-02T18:19:48.727035: step 36555, loss 0.113969, acc 0.96875
2017-03-02T18:19:48.796746: step 36556, loss 0.199313, acc 0.953125
2017-03-02T18:19:48.876847: step 36557, loss 0.199768, acc 0.90625
2017-03-02T18:19:48.941982: step 36558, loss 0.154173, acc 0.953125
2017-03-02T18:19:49.011171: step 36559, loss 0.221681, acc 0.9375
2017-03-02T18:19:49.074051: step 36560, loss 0.109999, acc 0.984375
2017-03-02T18:19:49.145220: step 36561, loss 0.063737, acc 0.96875
2017-03-02T18:19:49.216918: step 36562, loss 0.148765, acc 0.9375
2017-03-02T18:19:49.288642: step 36563, loss 0.10974, acc 0.953125
2017-03-02T18:19:49.358222: step 36564, loss 0.077225, acc 0.96875
2017-03-02T18:19:49.434828: step 36565, loss 0.2475, acc 0.9375
2017-03-02T18:19:49.515165: step 36566, loss 0.0772434, acc 0.953125
2017-03-02T18:19:49.595379: step 36567, loss 0.200494, acc 0.90625
2017-03-02T18:19:49.662581: step 36568, loss 0.0595471, acc 0.96875
2017-03-02T18:19:49.747237: step 36569, loss 0.211122, acc 0.921875
2017-03-02T18:19:49.819830: step 36570, loss 0.0965386, acc 0.953125
2017-03-02T18:19:49.893582: step 36571, loss 0.0834669, acc 0.9375
2017-03-02T18:19:49.971666: step 36572, loss 0.223757, acc 0.90625
2017-03-02T18:19:50.043158: step 36573, loss 0.206963, acc 0.859375
2017-03-02T18:19:50.123410: step 36574, loss 0.0457144, acc 0.96875
2017-03-02T18:19:50.199046: step 36575, loss 0.140465, acc 0.9375
2017-03-02T18:19:50.288308: step 36576, loss 0.114875, acc 0.9375
2017-03-02T18:19:50.353622: step 36577, loss 0.112178, acc 0.953125
2017-03-02T18:19:50.419849: step 36578, loss 0.138047, acc 0.921875
2017-03-02T18:19:50.491648: step 36579, loss 0.238925, acc 0.9375
2017-03-02T18:19:50.562846: step 36580, loss 0.0596951, acc 0.984375
2017-03-02T18:19:50.634737: step 36581, loss 0.194862, acc 0.921875
2017-03-02T18:19:50.705749: step 36582, loss 0.167092, acc 0.9375
2017-03-02T18:19:50.776788: step 36583, loss 0.250755, acc 0.875
2017-03-02T18:19:50.847499: step 36584, loss 0.40809, acc 0.828125
2017-03-02T18:19:50.919355: step 36585, loss 0.169318, acc 0.90625
2017-03-02T18:19:50.992801: step 36586, loss 0.185971, acc 0.9375
2017-03-02T18:19:51.059630: step 36587, loss 0.151494, acc 0.9375
2017-03-02T18:19:51.136732: step 36588, loss 0.0526003, acc 0.96875
2017-03-02T18:19:51.213829: step 36589, loss 0.18922, acc 0.90625
2017-03-02T18:19:51.286522: step 36590, loss 0.0471009, acc 0.984375
2017-03-02T18:19:51.362564: step 36591, loss 0.117045, acc 0.96875
2017-03-02T18:19:51.437633: step 36592, loss 0.117639, acc 0.953125
2017-03-02T18:19:51.510401: step 36593, loss 0.212209, acc 0.890625
2017-03-02T18:19:51.583595: step 36594, loss 0.168167, acc 0.9375
2017-03-02T18:19:51.655144: step 36595, loss 0.0519626, acc 0.96875
2017-03-02T18:19:51.731596: step 36596, loss 0.200833, acc 0.9375
2017-03-02T18:19:51.800154: step 36597, loss 0.0758064, acc 0.96875
2017-03-02T18:19:51.871196: step 36598, loss 0.156367, acc 0.90625
2017-03-02T18:19:51.948198: step 36599, loss 0.0720207, acc 0.953125
2017-03-02T18:19:52.014915: step 36600, loss 0.120073, acc 0.9375

Evaluation:
2017-03-02T18:19:52.054302: step 36600, loss 5.29423, acc 0.642394

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36600

2017-03-02T18:19:52.492015: step 36601, loss 0.243436, acc 0.921875
2017-03-02T18:19:52.554090: step 36602, loss 0.143286, acc 0.921875
2017-03-02T18:19:52.643978: step 36603, loss 0.133649, acc 0.921875
2017-03-02T18:19:52.721249: step 36604, loss 0.126883, acc 0.9375
2017-03-02T18:19:52.797183: step 36605, loss 0.21954, acc 0.90625
2017-03-02T18:19:52.861306: step 36606, loss 0.184584, acc 0.890625
2017-03-02T18:19:52.934453: step 36607, loss 0.269017, acc 0.890625
2017-03-02T18:19:53.002227: step 36608, loss 0.111023, acc 0.9375
2017-03-02T18:19:53.069132: step 36609, loss 0.0746726, acc 0.96875
2017-03-02T18:19:53.134735: step 36610, loss 0.120486, acc 0.953125
2017-03-02T18:19:53.210479: step 36611, loss 0.217695, acc 0.9375
2017-03-02T18:19:53.288338: step 36612, loss 0.103479, acc 0.953125
2017-03-02T18:19:53.354846: step 36613, loss 0.108272, acc 0.921875
2017-03-02T18:19:53.429554: step 36614, loss 0.153066, acc 0.953125
2017-03-02T18:19:53.520067: step 36615, loss 0.120383, acc 0.953125
2017-03-02T18:19:53.595731: step 36616, loss 0.112464, acc 0.9375
2017-03-02T18:19:53.672365: step 36617, loss 0.160507, acc 0.90625
2017-03-02T18:19:53.738936: step 36618, loss 0.163972, acc 0.9375
2017-03-02T18:19:53.818389: step 36619, loss 0.0944215, acc 0.96875
2017-03-02T18:19:53.898210: step 36620, loss 0.0970735, acc 0.9375
2017-03-02T18:19:53.975042: step 36621, loss 0.204757, acc 0.875
2017-03-02T18:19:54.049775: step 36622, loss 0.141082, acc 0.921875
2017-03-02T18:19:54.121716: step 36623, loss 0.250586, acc 0.921875
2017-03-02T18:19:54.197148: step 36624, loss 0.24452, acc 0.875
2017-03-02T18:19:54.272433: step 36625, loss 0.165762, acc 0.921875
2017-03-02T18:19:54.344171: step 36626, loss 0.0825135, acc 0.96875
2017-03-02T18:19:54.412548: step 36627, loss 0.123945, acc 0.921875
2017-03-02T18:19:54.479692: step 36628, loss 0.0990496, acc 0.953125
2017-03-02T18:19:54.552444: step 36629, loss 0.198201, acc 0.890625
2017-03-02T18:19:54.628368: step 36630, loss 0.119661, acc 0.953125
2017-03-02T18:19:54.700229: step 36631, loss 0.118915, acc 0.953125
2017-03-02T18:19:54.767070: step 36632, loss 0.197353, acc 0.921875
2017-03-02T18:19:54.844946: step 36633, loss 0.218229, acc 0.890625
2017-03-02T18:19:54.923117: step 36634, loss 0.135103, acc 0.921875
2017-03-02T18:19:54.997552: step 36635, loss 0.100646, acc 0.953125
2017-03-02T18:19:55.071313: step 36636, loss 0.126577, acc 0.921875
2017-03-02T18:19:55.138622: step 36637, loss 0.0955234, acc 0.9375
2017-03-02T18:19:55.235418: step 36638, loss 0.150729, acc 0.9375
2017-03-02T18:19:55.308771: step 36639, loss 0.235279, acc 0.875
2017-03-02T18:19:55.383237: step 36640, loss 0.143968, acc 0.921875
2017-03-02T18:19:55.466998: step 36641, loss 0.256478, acc 0.90625
2017-03-02T18:19:55.541379: step 36642, loss 0.111209, acc 0.953125
2017-03-02T18:19:55.616382: step 36643, loss 0.121862, acc 0.953125
2017-03-02T18:19:55.690634: step 36644, loss 0.134021, acc 0.921875
2017-03-02T18:19:55.781399: step 36645, loss 0.178936, acc 0.890625
2017-03-02T18:19:55.846052: step 36646, loss 0.136105, acc 0.921875
2017-03-02T18:19:55.908530: step 36647, loss 0.113434, acc 0.953125
2017-03-02T18:19:55.992515: step 36648, loss 0.110474, acc 0.953125
2017-03-02T18:19:56.068492: step 36649, loss 0.132028, acc 0.953125
2017-03-02T18:19:56.142981: step 36650, loss 0.136752, acc 0.9375
2017-03-02T18:19:56.215657: step 36651, loss 0.165189, acc 0.890625
2017-03-02T18:19:56.286887: step 36652, loss 0, acc 1
2017-03-02T18:19:56.358535: step 36653, loss 0.177202, acc 0.90625
2017-03-02T18:19:56.433078: step 36654, loss 0.126344, acc 0.953125
2017-03-02T18:19:56.503626: step 36655, loss 0.0829241, acc 0.984375
2017-03-02T18:19:56.570016: step 36656, loss 0.145815, acc 0.921875
2017-03-02T18:19:56.642105: step 36657, loss 0.152236, acc 0.921875
2017-03-02T18:19:56.714266: step 36658, loss 0.140389, acc 0.9375
2017-03-02T18:19:56.803510: step 36659, loss 0.248089, acc 0.90625
2017-03-02T18:19:56.872420: step 36660, loss 0.113061, acc 0.953125
2017-03-02T18:19:56.943769: step 36661, loss 0.0865356, acc 0.96875
2017-03-02T18:19:57.018966: step 36662, loss 0.109479, acc 0.953125
2017-03-02T18:19:57.088877: step 36663, loss 0.0757497, acc 0.96875
2017-03-02T18:19:57.161046: step 36664, loss 0.130513, acc 0.9375
2017-03-02T18:19:57.226971: step 36665, loss 0.191098, acc 0.890625
2017-03-02T18:19:57.293451: step 36666, loss 0.15651, acc 0.9375
2017-03-02T18:19:57.366114: step 36667, loss 0.168253, acc 0.890625
2017-03-02T18:19:57.444475: step 36668, loss 0.20702, acc 0.90625
2017-03-02T18:19:57.516667: step 36669, loss 0.218162, acc 0.890625
2017-03-02T18:19:57.585608: step 36670, loss 0.160051, acc 0.921875
2017-03-02T18:19:57.654569: step 36671, loss 0.117194, acc 0.96875
2017-03-02T18:19:57.726248: step 36672, loss 0.243831, acc 0.890625
2017-03-02T18:19:57.795621: step 36673, loss 0.0896231, acc 0.9375
2017-03-02T18:19:57.865992: step 36674, loss 0.134211, acc 0.9375
2017-03-02T18:19:57.938229: step 36675, loss 0.0743044, acc 0.953125
2017-03-02T18:19:58.019903: step 36676, loss 0.120521, acc 0.953125
2017-03-02T18:19:58.099648: step 36677, loss 0.173027, acc 0.90625
2017-03-02T18:19:58.177481: step 36678, loss 0.157749, acc 0.90625
2017-03-02T18:19:58.251365: step 36679, loss 0.117386, acc 0.953125
2017-03-02T18:19:58.326892: step 36680, loss 0.206463, acc 0.90625
2017-03-02T18:19:58.398321: step 36681, loss 0.225924, acc 0.859375
2017-03-02T18:19:58.475279: step 36682, loss 0.0917428, acc 0.96875
2017-03-02T18:19:58.544978: step 36683, loss 0.1111, acc 0.9375
2017-03-02T18:19:58.616494: step 36684, loss 0.0763255, acc 0.96875
2017-03-02T18:19:58.696571: step 36685, loss 0.23217, acc 0.890625
2017-03-02T18:19:58.767235: step 36686, loss 0.176188, acc 0.890625
2017-03-02T18:19:58.846505: step 36687, loss 0.073859, acc 0.96875
2017-03-02T18:19:58.925294: step 36688, loss 0.128816, acc 0.9375
2017-03-02T18:19:59.004782: step 36689, loss 0.0912153, acc 0.953125
2017-03-02T18:19:59.084527: step 36690, loss 0.129126, acc 0.9375
2017-03-02T18:19:59.154058: step 36691, loss 0.0941742, acc 0.96875
2017-03-02T18:19:59.226878: step 36692, loss 0.0725591, acc 0.984375
2017-03-02T18:19:59.296575: step 36693, loss 0.148157, acc 0.9375
2017-03-02T18:19:59.369288: step 36694, loss 0.0545716, acc 0.984375
2017-03-02T18:19:59.439264: step 36695, loss 0.283849, acc 0.875
2017-03-02T18:19:59.512092: step 36696, loss 0.124414, acc 0.9375
2017-03-02T18:19:59.589297: step 36697, loss 0.060585, acc 0.984375
2017-03-02T18:19:59.652846: step 36698, loss 0.0614263, acc 0.984375
2017-03-02T18:19:59.729251: step 36699, loss 0.192646, acc 0.9375
2017-03-02T18:19:59.800318: step 36700, loss 0.0925251, acc 0.96875

Evaluation:
2017-03-02T18:19:59.837359: step 36700, loss 5.32705, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36700

2017-03-02T18:20:00.288627: step 36701, loss 0.0791649, acc 0.9375
2017-03-02T18:20:00.360964: step 36702, loss 0.0444201, acc 0.984375
2017-03-02T18:20:00.451057: step 36703, loss 0.233427, acc 0.875
2017-03-02T18:20:00.524390: step 36704, loss 0.0907411, acc 0.96875
2017-03-02T18:20:00.600002: step 36705, loss 0.180283, acc 0.90625
2017-03-02T18:20:00.669670: step 36706, loss 0.166437, acc 0.921875
2017-03-02T18:20:00.735441: step 36707, loss 0.103913, acc 0.96875
2017-03-02T18:20:00.815375: step 36708, loss 0.231064, acc 0.890625
2017-03-02T18:20:00.888878: step 36709, loss 0.18496, acc 0.9375
2017-03-02T18:20:00.961774: step 36710, loss 0.0707098, acc 0.953125
2017-03-02T18:20:01.033526: step 36711, loss 0.150896, acc 0.90625
2017-03-02T18:20:01.105744: step 36712, loss 0.165921, acc 0.890625
2017-03-02T18:20:01.182223: step 36713, loss 0.15759, acc 0.953125
2017-03-02T18:20:01.256715: step 36714, loss 0.0889173, acc 0.96875
2017-03-02T18:20:01.330065: step 36715, loss 0.231399, acc 0.9375
2017-03-02T18:20:01.408745: step 36716, loss 0.194496, acc 0.953125
2017-03-02T18:20:01.486435: step 36717, loss 0.0745974, acc 0.953125
2017-03-02T18:20:01.561052: step 36718, loss 0.077083, acc 0.984375
2017-03-02T18:20:01.636787: step 36719, loss 0.113681, acc 0.9375
2017-03-02T18:20:01.711315: step 36720, loss 0.208297, acc 0.90625
2017-03-02T18:20:01.786952: step 36721, loss 0.155282, acc 0.9375
2017-03-02T18:20:01.858829: step 36722, loss 0.165902, acc 0.90625
2017-03-02T18:20:01.931242: step 36723, loss 0.0554925, acc 0.984375
2017-03-02T18:20:02.009363: step 36724, loss 0.0920829, acc 0.953125
2017-03-02T18:20:02.074238: step 36725, loss 0.222216, acc 0.953125
2017-03-02T18:20:02.140627: step 36726, loss 0.111242, acc 0.9375
2017-03-02T18:20:02.218226: step 36727, loss 0.16077, acc 0.890625
2017-03-02T18:20:02.287085: step 36728, loss 0.135604, acc 0.9375
2017-03-02T18:20:02.360044: step 36729, loss 0.117334, acc 0.9375
2017-03-02T18:20:02.424732: step 36730, loss 0.173204, acc 0.921875
2017-03-02T18:20:02.495849: step 36731, loss 0.142127, acc 0.921875
2017-03-02T18:20:02.555341: step 36732, loss 0.093105, acc 0.953125
2017-03-02T18:20:02.627190: step 36733, loss 0.15863, acc 0.921875
2017-03-02T18:20:02.694145: step 36734, loss 0.0928438, acc 0.96875
2017-03-02T18:20:02.762163: step 36735, loss 0.038726, acc 1
2017-03-02T18:20:02.833722: step 36736, loss 0.137031, acc 0.9375
2017-03-02T18:20:02.906966: step 36737, loss 0.209773, acc 0.921875
2017-03-02T18:20:02.992205: step 36738, loss 0.227775, acc 0.859375
2017-03-02T18:20:03.063725: step 36739, loss 0.136712, acc 0.9375
2017-03-02T18:20:03.136379: step 36740, loss 0.0938454, acc 0.96875
2017-03-02T18:20:03.209520: step 36741, loss 0.229516, acc 0.890625
2017-03-02T18:20:03.284162: step 36742, loss 0.0989749, acc 0.953125
2017-03-02T18:20:03.365014: step 36743, loss 0.13613, acc 0.9375
2017-03-02T18:20:03.434961: step 36744, loss 0.0889945, acc 0.96875
2017-03-02T18:20:03.499114: step 36745, loss 0.133632, acc 0.921875
2017-03-02T18:20:03.571156: step 36746, loss 0.17431, acc 0.9375
2017-03-02T18:20:03.648646: step 36747, loss 0.141648, acc 0.953125
2017-03-02T18:20:03.721961: step 36748, loss 0.127307, acc 0.9375
2017-03-02T18:20:03.794077: step 36749, loss 0.122628, acc 0.90625
2017-03-02T18:20:03.867052: step 36750, loss 0.108902, acc 0.96875
2017-03-02T18:20:03.941605: step 36751, loss 0.240746, acc 0.90625
2017-03-02T18:20:04.015637: step 36752, loss 0.175205, acc 0.921875
2017-03-02T18:20:04.081855: step 36753, loss 0.140081, acc 0.953125
2017-03-02T18:20:04.148582: step 36754, loss 0.0868868, acc 0.9375
2017-03-02T18:20:04.215742: step 36755, loss 0.108094, acc 0.96875
2017-03-02T18:20:04.285819: step 36756, loss 0.163534, acc 0.90625
2017-03-02T18:20:04.364468: step 36757, loss 0.137799, acc 0.921875
2017-03-02T18:20:04.443027: step 36758, loss 0.0404446, acc 0.984375
2017-03-02T18:20:04.517362: step 36759, loss 0.0990803, acc 0.953125
2017-03-02T18:20:04.587220: step 36760, loss 0.136787, acc 0.921875
2017-03-02T18:20:04.663270: step 36761, loss 0.188886, acc 0.921875
2017-03-02T18:20:04.732750: step 36762, loss 0.120384, acc 0.953125
2017-03-02T18:20:04.803271: step 36763, loss 0.144733, acc 0.921875
2017-03-02T18:20:04.868695: step 36764, loss 0.295618, acc 0.84375
2017-03-02T18:20:04.960467: step 36765, loss 0.155048, acc 0.90625
2017-03-02T18:20:05.032594: step 36766, loss 0.086159, acc 0.9375
2017-03-02T18:20:05.104630: step 36767, loss 0.179673, acc 0.9375
2017-03-02T18:20:05.180634: step 36768, loss 0.0901076, acc 0.953125
2017-03-02T18:20:05.259450: step 36769, loss 0.106064, acc 0.953125
2017-03-02T18:20:05.333339: step 36770, loss 0.171537, acc 0.921875
2017-03-02T18:20:05.424091: step 36771, loss 0.108223, acc 0.9375
2017-03-02T18:20:05.490981: step 36772, loss 0.0821878, acc 0.953125
2017-03-02T18:20:05.566825: step 36773, loss 0.0984956, acc 0.9375
2017-03-02T18:20:05.628568: step 36774, loss 0.110031, acc 0.96875
2017-03-02T18:20:05.694542: step 36775, loss 0.157255, acc 0.921875
2017-03-02T18:20:05.769758: step 36776, loss 0.141313, acc 0.921875
2017-03-02T18:20:05.842382: step 36777, loss 0.231122, acc 0.890625
2017-03-02T18:20:05.919151: step 36778, loss 0.323775, acc 0.828125
2017-03-02T18:20:05.994049: step 36779, loss 0.0535858, acc 0.96875
2017-03-02T18:20:06.067236: step 36780, loss 0.173144, acc 0.921875
2017-03-02T18:20:06.135906: step 36781, loss 0.184638, acc 0.9375
2017-03-02T18:20:06.203999: step 36782, loss 0.202976, acc 0.921875
2017-03-02T18:20:06.276916: step 36783, loss 0.168903, acc 0.90625
2017-03-02T18:20:06.350802: step 36784, loss 0.0588646, acc 0.984375
2017-03-02T18:20:06.424227: step 36785, loss 0.162578, acc 0.90625
2017-03-02T18:20:06.501226: step 36786, loss 0.155072, acc 0.9375
2017-03-02T18:20:06.581150: step 36787, loss 0.125344, acc 0.953125
2017-03-02T18:20:06.652450: step 36788, loss 0.0613235, acc 0.984375
2017-03-02T18:20:06.723592: step 36789, loss 0.164836, acc 0.9375
2017-03-02T18:20:06.802036: step 36790, loss 0.157867, acc 0.921875
2017-03-02T18:20:06.869682: step 36791, loss 0.0882712, acc 0.9375
2017-03-02T18:20:06.938126: step 36792, loss 0.185543, acc 0.9375
2017-03-02T18:20:07.011654: step 36793, loss 0.0856336, acc 0.953125
2017-03-02T18:20:07.086432: step 36794, loss 0.0822284, acc 0.953125
2017-03-02T18:20:07.156231: step 36795, loss 0.131437, acc 0.9375
2017-03-02T18:20:07.229953: step 36796, loss 0.0911248, acc 0.953125
2017-03-02T18:20:07.303089: step 36797, loss 0.229776, acc 0.90625
2017-03-02T18:20:07.373369: step 36798, loss 0.163628, acc 0.9375
2017-03-02T18:20:07.439037: step 36799, loss 0.211751, acc 0.90625
2017-03-02T18:20:07.506109: step 36800, loss 0.157104, acc 0.9375

Evaluation:
2017-03-02T18:20:07.537040: step 36800, loss 5.42193, acc 0.644557

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36800

2017-03-02T18:20:07.994764: step 36801, loss 0.0994611, acc 0.953125
2017-03-02T18:20:08.076534: step 36802, loss 0.116679, acc 0.96875
2017-03-02T18:20:08.151359: step 36803, loss 0.182504, acc 0.890625
2017-03-02T18:20:08.214179: step 36804, loss 0.104795, acc 0.953125
2017-03-02T18:20:08.285026: step 36805, loss 0.2179, acc 0.859375
2017-03-02T18:20:08.357549: step 36806, loss 0.227267, acc 0.890625
2017-03-02T18:20:08.429278: step 36807, loss 0.189206, acc 0.90625
2017-03-02T18:20:08.501456: step 36808, loss 0.092461, acc 0.96875
2017-03-02T18:20:08.576763: step 36809, loss 0.15022, acc 0.953125
2017-03-02T18:20:08.647082: step 36810, loss 0.201957, acc 0.921875
2017-03-02T18:20:08.710327: step 36811, loss 0.173263, acc 0.9375
2017-03-02T18:20:08.788348: step 36812, loss 0.147284, acc 0.9375
2017-03-02T18:20:08.861873: step 36813, loss 0.231434, acc 0.921875
2017-03-02T18:20:08.931714: step 36814, loss 0.112929, acc 0.921875
2017-03-02T18:20:09.020067: step 36815, loss 0.114583, acc 0.9375
2017-03-02T18:20:09.092742: step 36816, loss 0.23269, acc 0.921875
2017-03-02T18:20:09.171565: step 36817, loss 0.136557, acc 0.921875
2017-03-02T18:20:09.250659: step 36818, loss 0.114448, acc 0.9375
2017-03-02T18:20:09.319032: step 36819, loss 0.175023, acc 0.953125
2017-03-02T18:20:09.388128: step 36820, loss 0.179213, acc 0.921875
2017-03-02T18:20:09.458968: step 36821, loss 0.130918, acc 0.9375
2017-03-02T18:20:09.530578: step 36822, loss 0.190432, acc 0.921875
2017-03-02T18:20:09.601477: step 36823, loss 0.182732, acc 0.9375
2017-03-02T18:20:09.679372: step 36824, loss 0.131198, acc 0.953125
2017-03-02T18:20:09.770822: step 36825, loss 0.126425, acc 0.9375
2017-03-02T18:20:09.853753: step 36826, loss 0.0987399, acc 0.953125
2017-03-02T18:20:09.932664: step 36827, loss 0.139613, acc 0.9375
2017-03-02T18:20:10.010437: step 36828, loss 0.103584, acc 0.953125
2017-03-02T18:20:10.080802: step 36829, loss 0.239424, acc 0.90625
2017-03-02T18:20:10.155584: step 36830, loss 0.16145, acc 0.953125
2017-03-02T18:20:10.228429: step 36831, loss 0.201189, acc 0.921875
2017-03-02T18:20:10.298178: step 36832, loss 0.163471, acc 0.9375
2017-03-02T18:20:10.370878: step 36833, loss 0.219583, acc 0.890625
2017-03-02T18:20:10.446866: step 36834, loss 0.103051, acc 0.9375
2017-03-02T18:20:10.522462: step 36835, loss 0.212055, acc 0.90625
2017-03-02T18:20:10.592229: step 36836, loss 0.119052, acc 0.96875
2017-03-02T18:20:10.662218: step 36837, loss 0.113709, acc 0.9375
2017-03-02T18:20:10.747916: step 36838, loss 0.103529, acc 0.953125
2017-03-02T18:20:10.814989: step 36839, loss 0.138803, acc 0.890625
2017-03-02T18:20:10.886239: step 36840, loss 0.156162, acc 0.921875
2017-03-02T18:20:10.952026: step 36841, loss 0.112463, acc 0.96875
2017-03-02T18:20:11.029451: step 36842, loss 0.205935, acc 0.890625
2017-03-02T18:20:11.107304: step 36843, loss 0.0935588, acc 0.953125
2017-03-02T18:20:11.177646: step 36844, loss 0.169621, acc 0.890625
2017-03-02T18:20:11.248882: step 36845, loss 0.0602907, acc 0.96875
2017-03-02T18:20:11.319367: step 36846, loss 0.166958, acc 0.9375
2017-03-02T18:20:11.394587: step 36847, loss 0.161436, acc 0.9375
2017-03-02T18:20:11.474680: step 36848, loss 2.38419e-07, acc 1
2017-03-02T18:20:11.549086: step 36849, loss 0.121014, acc 0.96875
2017-03-02T18:20:11.616306: step 36850, loss 0.134301, acc 0.953125
2017-03-02T18:20:11.684439: step 36851, loss 0.120699, acc 0.9375
2017-03-02T18:20:11.760973: step 36852, loss 0.172181, acc 0.953125
2017-03-02T18:20:11.834022: step 36853, loss 0.147665, acc 0.96875
2017-03-02T18:20:11.905928: step 36854, loss 0.0833716, acc 0.96875
2017-03-02T18:20:11.981701: step 36855, loss 0.0934726, acc 0.9375
2017-03-02T18:20:12.059620: step 36856, loss 0.113997, acc 0.953125
2017-03-02T18:20:12.130733: step 36857, loss 0.0624966, acc 0.96875
2017-03-02T18:20:12.204454: step 36858, loss 0.0904115, acc 0.953125
2017-03-02T18:20:12.279689: step 36859, loss 0.117096, acc 0.9375
2017-03-02T18:20:12.350565: step 36860, loss 0.128969, acc 0.953125
2017-03-02T18:20:12.414957: step 36861, loss 0.210112, acc 0.875
2017-03-02T18:20:12.483986: step 36862, loss 0.114587, acc 0.96875
2017-03-02T18:20:12.555884: step 36863, loss 0.140652, acc 0.953125
2017-03-02T18:20:12.627823: step 36864, loss 0.0878129, acc 0.96875
2017-03-02T18:20:12.699633: step 36865, loss 0.161324, acc 0.9375
2017-03-02T18:20:12.773656: step 36866, loss 0.0930535, acc 0.953125
2017-03-02T18:20:12.842560: step 36867, loss 0.0914351, acc 0.96875
2017-03-02T18:20:12.914860: step 36868, loss 0.120598, acc 0.953125
2017-03-02T18:20:12.988976: step 36869, loss 0.0866614, acc 0.96875
2017-03-02T18:20:13.057912: step 36870, loss 0.10654, acc 0.953125
2017-03-02T18:20:13.124470: step 36871, loss 0.0940599, acc 0.96875
2017-03-02T18:20:13.191500: step 36872, loss 0.0923402, acc 0.953125
2017-03-02T18:20:13.264380: step 36873, loss 0.128785, acc 0.96875
2017-03-02T18:20:13.335231: step 36874, loss 0.164328, acc 0.953125
2017-03-02T18:20:13.407842: step 36875, loss 0.101687, acc 0.9375
2017-03-02T18:20:13.477523: step 36876, loss 0.0864716, acc 0.96875
2017-03-02T18:20:13.550849: step 36877, loss 0.105446, acc 0.9375
2017-03-02T18:20:13.622872: step 36878, loss 0.111535, acc 0.96875
2017-03-02T18:20:13.697413: step 36879, loss 0.148938, acc 0.9375
2017-03-02T18:20:13.770660: step 36880, loss 0.173689, acc 0.921875
2017-03-02T18:20:13.842931: step 36881, loss 0.135382, acc 0.96875
2017-03-02T18:20:13.914857: step 36882, loss 0.13176, acc 0.953125
2017-03-02T18:20:13.984814: step 36883, loss 0.333726, acc 0.90625
2017-03-02T18:20:14.056629: step 36884, loss 0.0664503, acc 0.984375
2017-03-02T18:20:14.130643: step 36885, loss 0.142094, acc 0.9375
2017-03-02T18:20:14.200695: step 36886, loss 0.0965242, acc 0.96875
2017-03-02T18:20:14.275217: step 36887, loss 0.153378, acc 0.90625
2017-03-02T18:20:14.348752: step 36888, loss 0.138009, acc 0.9375
2017-03-02T18:20:14.417114: step 36889, loss 0.102721, acc 0.953125
2017-03-02T18:20:14.488723: step 36890, loss 0.123906, acc 0.9375
2017-03-02T18:20:14.560523: step 36891, loss 0.195225, acc 0.921875
2017-03-02T18:20:14.635638: step 36892, loss 0.148604, acc 0.921875
2017-03-02T18:20:14.721652: step 36893, loss 0.171215, acc 0.921875
2017-03-02T18:20:14.791958: step 36894, loss 0.085608, acc 0.96875
2017-03-02T18:20:14.868202: step 36895, loss 0.107943, acc 0.9375
2017-03-02T18:20:14.938327: step 36896, loss 0.110154, acc 0.953125
2017-03-02T18:20:15.015122: step 36897, loss 0.0992993, acc 0.9375
2017-03-02T18:20:15.101022: step 36898, loss 0.162664, acc 0.921875
2017-03-02T18:20:15.186220: step 36899, loss 0.106245, acc 0.921875
2017-03-02T18:20:15.256517: step 36900, loss 0.10209, acc 0.9375

Evaluation:
2017-03-02T18:20:15.296180: step 36900, loss 5.51746, acc 0.643115

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-36900

2017-03-02T18:20:15.753051: step 36901, loss 0.178289, acc 0.9375
2017-03-02T18:20:15.824343: step 36902, loss 0.100978, acc 0.96875
2017-03-02T18:20:15.892063: step 36903, loss 0.0735918, acc 0.96875
2017-03-02T18:20:15.967475: step 36904, loss 0.112002, acc 0.9375
2017-03-02T18:20:16.042779: step 36905, loss 0.116128, acc 0.921875
2017-03-02T18:20:16.129310: step 36906, loss 0.143743, acc 0.921875
2017-03-02T18:20:16.216440: step 36907, loss 0.0731391, acc 0.96875
2017-03-02T18:20:16.294618: step 36908, loss 0.087238, acc 0.9375
2017-03-02T18:20:16.367380: step 36909, loss 0.186791, acc 0.90625
2017-03-02T18:20:16.444706: step 36910, loss 0.11643, acc 0.96875
2017-03-02T18:20:16.516506: step 36911, loss 0.171122, acc 0.921875
2017-03-02T18:20:16.584401: step 36912, loss 0.149709, acc 0.890625
2017-03-02T18:20:16.658381: step 36913, loss 0.222371, acc 0.9375
2017-03-02T18:20:16.734741: step 36914, loss 0.155951, acc 0.9375
2017-03-02T18:20:16.806644: step 36915, loss 0.142479, acc 0.921875
2017-03-02T18:20:16.876875: step 36916, loss 0.142233, acc 0.9375
2017-03-02T18:20:16.950290: step 36917, loss 0.124959, acc 0.9375
2017-03-02T18:20:17.022125: step 36918, loss 0.204999, acc 0.890625
2017-03-02T18:20:17.099708: step 36919, loss 0.107227, acc 0.96875
2017-03-02T18:20:17.165358: step 36920, loss 0.157692, acc 0.90625
2017-03-02T18:20:17.234127: step 36921, loss 0.0895338, acc 0.953125
2017-03-02T18:20:17.312541: step 36922, loss 0.100663, acc 0.9375
2017-03-02T18:20:17.388792: step 36923, loss 0.242772, acc 0.921875
2017-03-02T18:20:17.475829: step 36924, loss 0.162994, acc 0.9375
2017-03-02T18:20:17.554744: step 36925, loss 0.183461, acc 0.921875
2017-03-02T18:20:17.626032: step 36926, loss 0.133086, acc 0.9375
2017-03-02T18:20:17.700150: step 36927, loss 0.0342789, acc 0.984375
2017-03-02T18:20:17.775192: step 36928, loss 0.153808, acc 0.953125
2017-03-02T18:20:17.854396: step 36929, loss 0.173389, acc 0.890625
2017-03-02T18:20:17.927870: step 36930, loss 0.102977, acc 0.96875
2017-03-02T18:20:17.998410: step 36931, loss 0.261268, acc 0.875
2017-03-02T18:20:18.067118: step 36932, loss 0.194304, acc 0.875
2017-03-02T18:20:18.146976: step 36933, loss 0.123431, acc 0.96875
2017-03-02T18:20:18.223144: step 36934, loss 0.253058, acc 0.859375
2017-03-02T18:20:18.294931: step 36935, loss 0.130056, acc 0.953125
2017-03-02T18:20:18.369178: step 36936, loss 0.0771316, acc 0.953125
2017-03-02T18:20:18.441084: step 36937, loss 0.173699, acc 0.90625
2017-03-02T18:20:18.514629: step 36938, loss 0.102742, acc 0.9375
2017-03-02T18:20:18.588719: step 36939, loss 0.102459, acc 0.953125
2017-03-02T18:20:18.654335: step 36940, loss 0.152516, acc 0.921875
2017-03-02T18:20:18.728496: step 36941, loss 0.08512, acc 0.953125
2017-03-02T18:20:18.797130: step 36942, loss 0.0946326, acc 0.953125
2017-03-02T18:20:18.866034: step 36943, loss 0.119431, acc 0.9375
2017-03-02T18:20:18.941263: step 36944, loss 0.221825, acc 0.890625
2017-03-02T18:20:19.011573: step 36945, loss 0.277251, acc 0.921875
2017-03-02T18:20:19.087272: step 36946, loss 0.191256, acc 0.921875
2017-03-02T18:20:19.159151: step 36947, loss 0.102665, acc 0.9375
2017-03-02T18:20:19.230890: step 36948, loss 0.141727, acc 0.9375
2017-03-02T18:20:19.306991: step 36949, loss 0.151681, acc 0.921875
2017-03-02T18:20:19.374564: step 36950, loss 0.15408, acc 0.9375
2017-03-02T18:20:19.442179: step 36951, loss 0.261965, acc 0.859375
2017-03-02T18:20:19.511383: step 36952, loss 0.167654, acc 0.921875
2017-03-02T18:20:19.582746: step 36953, loss 0.169596, acc 0.921875
2017-03-02T18:20:19.657047: step 36954, loss 0.147908, acc 0.921875
2017-03-02T18:20:19.739292: step 36955, loss 0.276519, acc 0.875
2017-03-02T18:20:19.816647: step 36956, loss 0.162154, acc 0.953125
2017-03-02T18:20:19.895776: step 36957, loss 0.0882477, acc 0.953125
2017-03-02T18:20:19.960706: step 36958, loss 0.0527671, acc 0.984375
2017-03-02T18:20:20.026581: step 36959, loss 0.132023, acc 0.953125
2017-03-02T18:20:20.102182: step 36960, loss 0.170519, acc 0.9375
2017-03-02T18:20:20.185200: step 36961, loss 0.106728, acc 0.953125
2017-03-02T18:20:20.262480: step 36962, loss 0.104905, acc 0.953125
2017-03-02T18:20:20.333548: step 36963, loss 0.230866, acc 0.921875
2017-03-02T18:20:20.405795: step 36964, loss 0.150395, acc 0.921875
2017-03-02T18:20:20.475179: step 36965, loss 0.0613028, acc 0.96875
2017-03-02T18:20:20.537463: step 36966, loss 0.163404, acc 0.9375
2017-03-02T18:20:20.605966: step 36967, loss 0.247169, acc 0.890625
2017-03-02T18:20:20.674913: step 36968, loss 0.107351, acc 0.921875
2017-03-02T18:20:20.738254: step 36969, loss 0.0872296, acc 0.953125
2017-03-02T18:20:20.818801: step 36970, loss 0.148142, acc 0.921875
2017-03-02T18:20:20.888693: step 36971, loss 0.149306, acc 0.9375
2017-03-02T18:20:20.959660: step 36972, loss 0.205908, acc 0.921875
2017-03-02T18:20:21.029738: step 36973, loss 0.0914033, acc 0.96875
2017-03-02T18:20:21.107858: step 36974, loss 0.161996, acc 0.953125
2017-03-02T18:20:21.192614: step 36975, loss 0.10861, acc 0.9375
2017-03-02T18:20:21.266556: step 36976, loss 0.214738, acc 0.84375
2017-03-02T18:20:21.340520: step 36977, loss 0.122283, acc 0.953125
2017-03-02T18:20:21.415862: step 36978, loss 0.101833, acc 0.96875
2017-03-02T18:20:21.487109: step 36979, loss 0.158522, acc 0.9375
2017-03-02T18:20:21.563077: step 36980, loss 0.0716331, acc 0.96875
2017-03-02T18:20:21.639743: step 36981, loss 0.176149, acc 0.921875
2017-03-02T18:20:21.715319: step 36982, loss 0.16418, acc 0.90625
2017-03-02T18:20:21.784078: step 36983, loss 0.163842, acc 0.890625
2017-03-02T18:20:21.856887: step 36984, loss 0.0392504, acc 0.984375
2017-03-02T18:20:21.927512: step 36985, loss 0.0987128, acc 0.9375
2017-03-02T18:20:21.992370: step 36986, loss 0.0925275, acc 0.96875
2017-03-02T18:20:22.068682: step 36987, loss 0.208939, acc 0.890625
2017-03-02T18:20:22.133584: step 36988, loss 0.186665, acc 0.875
2017-03-02T18:20:22.205546: step 36989, loss 0.188317, acc 0.9375
2017-03-02T18:20:22.276899: step 36990, loss 0.117323, acc 0.921875
2017-03-02T18:20:22.346254: step 36991, loss 0.215111, acc 0.875
2017-03-02T18:20:22.426008: step 36992, loss 0.233919, acc 0.921875
2017-03-02T18:20:22.500123: step 36993, loss 0.0784561, acc 0.953125
2017-03-02T18:20:22.578956: step 36994, loss 0.145125, acc 0.96875
2017-03-02T18:20:22.649345: step 36995, loss 0.215747, acc 0.875
2017-03-02T18:20:22.714089: step 36996, loss 0.0896419, acc 0.9375
2017-03-02T18:20:22.781421: step 36997, loss 0.151157, acc 0.9375
2017-03-02T18:20:22.849584: step 36998, loss 0.160508, acc 0.890625
2017-03-02T18:20:22.921257: step 36999, loss 0.134918, acc 0.9375
2017-03-02T18:20:23.000458: step 37000, loss 0.16632, acc 0.921875

Evaluation:
2017-03-02T18:20:23.034151: step 37000, loss 5.41647, acc 0.645278

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37000

2017-03-02T18:20:23.550613: step 37001, loss 0.0938318, acc 0.9375
2017-03-02T18:20:23.622935: step 37002, loss 0.194367, acc 0.890625
2017-03-02T18:20:23.696279: step 37003, loss 0.120316, acc 0.96875
2017-03-02T18:20:23.757103: step 37004, loss 0.246638, acc 0.890625
2017-03-02T18:20:23.832513: step 37005, loss 0.170058, acc 0.90625
2017-03-02T18:20:23.906873: step 37006, loss 0.13251, acc 0.9375
2017-03-02T18:20:23.978028: step 37007, loss 0.082885, acc 0.953125
2017-03-02T18:20:24.051855: step 37008, loss 0.135272, acc 0.953125
2017-03-02T18:20:24.119807: step 37009, loss 0.1245, acc 0.921875
2017-03-02T18:20:24.187242: step 37010, loss 0.20706, acc 0.921875
2017-03-02T18:20:24.258964: step 37011, loss 0.0924158, acc 0.953125
2017-03-02T18:20:24.342286: step 37012, loss 0.316138, acc 0.890625
2017-03-02T18:20:24.415958: step 37013, loss 0.164972, acc 0.9375
2017-03-02T18:20:24.508682: step 37014, loss 0.137007, acc 0.953125
2017-03-02T18:20:24.582904: step 37015, loss 0.20901, acc 0.90625
2017-03-02T18:20:24.655737: step 37016, loss 0.137126, acc 0.921875
2017-03-02T18:20:24.726799: step 37017, loss 0.174196, acc 0.921875
2017-03-02T18:20:24.798046: step 37018, loss 0.189432, acc 0.9375
2017-03-02T18:20:24.894945: step 37019, loss 0.0853715, acc 0.984375
2017-03-02T18:20:24.971587: step 37020, loss 0.171917, acc 0.90625
2017-03-02T18:20:25.053284: step 37021, loss 0.15842, acc 0.921875
2017-03-02T18:20:25.121336: step 37022, loss 0.142074, acc 0.921875
2017-03-02T18:20:25.194293: step 37023, loss 0.160825, acc 0.90625
2017-03-02T18:20:25.263211: step 37024, loss 0.0823235, acc 0.953125
2017-03-02T18:20:25.333340: step 37025, loss 0.216489, acc 0.84375
2017-03-02T18:20:25.411301: step 37026, loss 0.114274, acc 0.96875
2017-03-02T18:20:25.484127: step 37027, loss 0.123623, acc 0.9375
2017-03-02T18:20:25.552582: step 37028, loss 0.232774, acc 0.890625
2017-03-02T18:20:25.632994: step 37029, loss 0.217673, acc 0.875
2017-03-02T18:20:25.701352: step 37030, loss 0.171715, acc 0.890625
2017-03-02T18:20:25.771501: step 37031, loss 0.118284, acc 0.9375
2017-03-02T18:20:25.837638: step 37032, loss 0.193904, acc 0.890625
2017-03-02T18:20:25.919819: step 37033, loss 0.119706, acc 0.9375
2017-03-02T18:20:26.002386: step 37034, loss 0.0920956, acc 0.953125
2017-03-02T18:20:26.075936: step 37035, loss 0.239768, acc 0.90625
2017-03-02T18:20:26.137417: step 37036, loss 0.254761, acc 0.890625
2017-03-02T18:20:26.210593: step 37037, loss 0.0925055, acc 0.9375
2017-03-02T18:20:26.276954: step 37038, loss 0.188405, acc 0.890625
2017-03-02T18:20:26.355184: step 37039, loss 0.074638, acc 0.984375
2017-03-02T18:20:26.452140: step 37040, loss 0.0793972, acc 0.9375
2017-03-02T18:20:26.526564: step 37041, loss 0.156095, acc 0.9375
2017-03-02T18:20:26.604835: step 37042, loss 0.0622317, acc 1
2017-03-02T18:20:26.675593: step 37043, loss 0.154672, acc 0.9375
2017-03-02T18:20:26.761809: step 37044, loss 0.368406, acc 0.75
2017-03-02T18:20:26.833592: step 37045, loss 0.157586, acc 0.953125
2017-03-02T18:20:26.899623: step 37046, loss 0.184546, acc 0.9375
2017-03-02T18:20:26.960761: step 37047, loss 0.147248, acc 0.953125
2017-03-02T18:20:27.029548: step 37048, loss 0.156251, acc 0.921875
2017-03-02T18:20:27.101416: step 37049, loss 0.153657, acc 0.96875
2017-03-02T18:20:27.177897: step 37050, loss 0.0690563, acc 0.96875
2017-03-02T18:20:27.255918: step 37051, loss 0.174631, acc 0.9375
2017-03-02T18:20:27.328235: step 37052, loss 0.198213, acc 0.90625
2017-03-02T18:20:27.399270: step 37053, loss 0.162402, acc 0.953125
2017-03-02T18:20:27.473965: step 37054, loss 0.146081, acc 0.921875
2017-03-02T18:20:27.547116: step 37055, loss 0.119808, acc 0.9375
2017-03-02T18:20:27.621007: step 37056, loss 0.182793, acc 0.90625
2017-03-02T18:20:27.690974: step 37057, loss 0.134294, acc 0.921875
2017-03-02T18:20:27.761677: step 37058, loss 0.258625, acc 0.890625
2017-03-02T18:20:27.857509: step 37059, loss 0.238909, acc 0.84375
2017-03-02T18:20:27.929250: step 37060, loss 0.0905861, acc 0.984375
2017-03-02T18:20:28.000744: step 37061, loss 0.123467, acc 0.96875
2017-03-02T18:20:28.072107: step 37062, loss 0.0755287, acc 0.96875
2017-03-02T18:20:28.146178: step 37063, loss 0.162432, acc 0.90625
2017-03-02T18:20:28.215698: step 37064, loss 0.186014, acc 0.953125
2017-03-02T18:20:28.285748: step 37065, loss 0.0795099, acc 0.96875
2017-03-02T18:20:28.361684: step 37066, loss 0.100039, acc 0.984375
2017-03-02T18:20:28.432349: step 37067, loss 0.266875, acc 0.875
2017-03-02T18:20:28.507574: step 37068, loss 0.153284, acc 0.921875
2017-03-02T18:20:28.581787: step 37069, loss 0.101894, acc 0.953125
2017-03-02T18:20:28.653691: step 37070, loss 0.0847681, acc 0.953125
2017-03-02T18:20:28.724533: step 37071, loss 0.156251, acc 0.90625
2017-03-02T18:20:28.800663: step 37072, loss 0.087531, acc 0.96875
2017-03-02T18:20:28.869590: step 37073, loss 0.0869367, acc 0.953125
2017-03-02T18:20:28.937932: step 37074, loss 0.24965, acc 0.875
2017-03-02T18:20:29.006590: step 37075, loss 0.126503, acc 0.9375
2017-03-02T18:20:29.099255: step 37076, loss 0.143978, acc 0.9375
2017-03-02T18:20:29.182527: step 37077, loss 0.128345, acc 0.984375
2017-03-02T18:20:29.253820: step 37078, loss 0.112645, acc 0.96875
2017-03-02T18:20:29.324660: step 37079, loss 0.0717055, acc 0.984375
2017-03-02T18:20:29.393433: step 37080, loss 0.166125, acc 0.890625
2017-03-02T18:20:29.463357: step 37081, loss 0.0943118, acc 0.96875
2017-03-02T18:20:29.534882: step 37082, loss 0.112181, acc 0.96875
2017-03-02T18:20:29.599254: step 37083, loss 0.146389, acc 0.9375
2017-03-02T18:20:29.670538: step 37084, loss 0.117605, acc 0.9375
2017-03-02T18:20:29.750169: step 37085, loss 0.138096, acc 0.953125
2017-03-02T18:20:29.824807: step 37086, loss 0.0678793, acc 0.96875
2017-03-02T18:20:29.903487: step 37087, loss 0.114422, acc 0.9375
2017-03-02T18:20:29.982363: step 37088, loss 0.0945487, acc 0.953125
2017-03-02T18:20:30.047906: step 37089, loss 0.0979114, acc 0.953125
2017-03-02T18:20:30.112231: step 37090, loss 0.0863264, acc 0.953125
2017-03-02T18:20:30.190482: step 37091, loss 0.225528, acc 0.890625
2017-03-02T18:20:30.247847: step 37092, loss 0.117881, acc 0.953125
2017-03-02T18:20:30.342266: step 37093, loss 0.142639, acc 0.9375
2017-03-02T18:20:30.412949: step 37094, loss 0.148549, acc 0.90625
2017-03-02T18:20:30.479591: step 37095, loss 0.146917, acc 0.890625
2017-03-02T18:20:30.554885: step 37096, loss 0.115976, acc 0.9375
2017-03-02T18:20:30.640184: step 37097, loss 0.153151, acc 0.953125
2017-03-02T18:20:30.712052: step 37098, loss 0.1065, acc 0.953125
2017-03-02T18:20:30.787492: step 37099, loss 0.0889549, acc 0.953125
2017-03-02T18:20:30.858965: step 37100, loss 0.140222, acc 0.921875

Evaluation:
2017-03-02T18:20:30.893401: step 37100, loss 5.4312, acc 0.641673

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37100

2017-03-02T18:20:31.349206: step 37101, loss 0.1377, acc 0.90625
2017-03-02T18:20:31.423764: step 37102, loss 0.118552, acc 0.96875
2017-03-02T18:20:31.493892: step 37103, loss 0.132983, acc 0.9375
2017-03-02T18:20:31.568119: step 37104, loss 0.0582728, acc 0.984375
2017-03-02T18:20:31.634284: step 37105, loss 0.140074, acc 0.9375
2017-03-02T18:20:31.723095: step 37106, loss 0.126569, acc 0.921875
2017-03-02T18:20:31.796742: step 37107, loss 0.136277, acc 0.921875
2017-03-02T18:20:31.867221: step 37108, loss 0.102122, acc 0.9375
2017-03-02T18:20:31.938371: step 37109, loss 0.113328, acc 0.9375
2017-03-02T18:20:32.011752: step 37110, loss 0.247395, acc 0.921875
2017-03-02T18:20:32.081473: step 37111, loss 0.182205, acc 0.921875
2017-03-02T18:20:32.157140: step 37112, loss 0.157625, acc 0.96875
2017-03-02T18:20:32.228286: step 37113, loss 0.083212, acc 0.953125
2017-03-02T18:20:32.299659: step 37114, loss 0.0829777, acc 0.96875
2017-03-02T18:20:32.369382: step 37115, loss 0.17118, acc 0.9375
2017-03-02T18:20:32.439271: step 37116, loss 0.0998731, acc 0.953125
2017-03-02T18:20:32.524899: step 37117, loss 0.184879, acc 0.890625
2017-03-02T18:20:32.595812: step 37118, loss 0.241958, acc 0.890625
2017-03-02T18:20:32.755624: step 37119, loss 0.0775642, acc 0.984375
2017-03-02T18:20:32.824546: step 37120, loss 0.074365, acc 1
2017-03-02T18:20:32.896274: step 37121, loss 0.127167, acc 0.921875
2017-03-02T18:20:32.965499: step 37122, loss 0.202638, acc 0.90625
2017-03-02T18:20:33.035272: step 37123, loss 0.137471, acc 0.9375
2017-03-02T18:20:33.101032: step 37124, loss 0.150781, acc 0.953125
2017-03-02T18:20:33.187240: step 37125, loss 0.286885, acc 0.90625
2017-03-02T18:20:33.267809: step 37126, loss 0.131164, acc 0.953125
2017-03-02T18:20:33.337611: step 37127, loss 0.136438, acc 0.921875
2017-03-02T18:20:33.414897: step 37128, loss 0.157526, acc 0.90625
2017-03-02T18:20:33.485724: step 37129, loss 0.156984, acc 0.90625
2017-03-02T18:20:33.559235: step 37130, loss 0.0384686, acc 0.984375
2017-03-02T18:20:33.630947: step 37131, loss 0.119722, acc 0.9375
2017-03-02T18:20:33.702146: step 37132, loss 0.184171, acc 0.921875
2017-03-02T18:20:33.788614: step 37133, loss 0.104207, acc 0.96875
2017-03-02T18:20:33.857968: step 37134, loss 0.149862, acc 0.96875
2017-03-02T18:20:33.929780: step 37135, loss 0.088256, acc 0.984375
2017-03-02T18:20:34.000230: step 37136, loss 0.157796, acc 0.953125
2017-03-02T18:20:34.073080: step 37137, loss 0.0717895, acc 0.9375
2017-03-02T18:20:34.143728: step 37138, loss 0.169515, acc 0.921875
2017-03-02T18:20:34.213181: step 37139, loss 0.07657, acc 0.953125
2017-03-02T18:20:34.287689: step 37140, loss 0.131484, acc 0.921875
2017-03-02T18:20:34.354434: step 37141, loss 0.173667, acc 0.921875
2017-03-02T18:20:34.421288: step 37142, loss 0.147995, acc 0.890625
2017-03-02T18:20:34.490323: step 37143, loss 0.101886, acc 0.921875
2017-03-02T18:20:34.563773: step 37144, loss 0.182147, acc 0.90625
2017-03-02T18:20:34.632481: step 37145, loss 0.134038, acc 0.953125
2017-03-02T18:20:34.709051: step 37146, loss 0.179223, acc 0.90625
2017-03-02T18:20:34.782132: step 37147, loss 0.302945, acc 0.78125
2017-03-02T18:20:34.855090: step 37148, loss 0.0671506, acc 0.96875
2017-03-02T18:20:34.931977: step 37149, loss 0.129222, acc 0.921875
2017-03-02T18:20:35.005549: step 37150, loss 0.101189, acc 0.953125
2017-03-02T18:20:35.075156: step 37151, loss 0.111397, acc 0.953125
2017-03-02T18:20:35.144163: step 37152, loss 0.130585, acc 0.9375
2017-03-02T18:20:35.214952: step 37153, loss 0.167685, acc 0.921875
2017-03-02T18:20:35.289058: step 37154, loss 0.191879, acc 0.890625
2017-03-02T18:20:35.358571: step 37155, loss 0.0703391, acc 0.953125
2017-03-02T18:20:35.429018: step 37156, loss 0.11852, acc 0.96875
2017-03-02T18:20:35.511192: step 37157, loss 0.144393, acc 0.953125
2017-03-02T18:20:35.601177: step 37158, loss 0.0487975, acc 0.984375
2017-03-02T18:20:35.678312: step 37159, loss 0.162874, acc 0.9375
2017-03-02T18:20:35.750527: step 37160, loss 0.138342, acc 0.90625
2017-03-02T18:20:35.814506: step 37161, loss 0.0494906, acc 0.984375
2017-03-02T18:20:35.883777: step 37162, loss 0.144932, acc 0.921875
2017-03-02T18:20:35.955467: step 37163, loss 0.115526, acc 0.953125
2017-03-02T18:20:36.027860: step 37164, loss 0.115865, acc 0.953125
2017-03-02T18:20:36.100134: step 37165, loss 0.125226, acc 0.953125
2017-03-02T18:20:36.176349: step 37166, loss 0.302206, acc 0.828125
2017-03-02T18:20:36.272550: step 37167, loss 0.222131, acc 0.890625
2017-03-02T18:20:36.344214: step 37168, loss 0.273974, acc 0.859375
2017-03-02T18:20:36.428435: step 37169, loss 0.197515, acc 0.921875
2017-03-02T18:20:36.496415: step 37170, loss 0.12812, acc 0.953125
2017-03-02T18:20:36.562541: step 37171, loss 0.190472, acc 0.90625
2017-03-02T18:20:36.635226: step 37172, loss 0.0925758, acc 0.9375
2017-03-02T18:20:36.705365: step 37173, loss 0.0932841, acc 0.96875
2017-03-02T18:20:36.774128: step 37174, loss 0.183499, acc 0.890625
2017-03-02T18:20:36.860453: step 37175, loss 0.214902, acc 0.90625
2017-03-02T18:20:36.929696: step 37176, loss 0.0818791, acc 0.984375
2017-03-02T18:20:36.999467: step 37177, loss 0.116003, acc 0.9375
2017-03-02T18:20:37.071932: step 37178, loss 0.251983, acc 0.90625
2017-03-02T18:20:37.143005: step 37179, loss 0.106948, acc 0.953125
2017-03-02T18:20:37.214247: step 37180, loss 0.0558475, acc 0.984375
2017-03-02T18:20:37.277456: step 37181, loss 0.0988866, acc 0.921875
2017-03-02T18:20:37.346753: step 37182, loss 0.18196, acc 0.875
2017-03-02T18:20:37.418032: step 37183, loss 0.212522, acc 0.890625
2017-03-02T18:20:37.489225: step 37184, loss 0.260787, acc 0.9375
2017-03-02T18:20:37.562635: step 37185, loss 0.101682, acc 0.984375
2017-03-02T18:20:37.635440: step 37186, loss 0.0862479, acc 0.96875
2017-03-02T18:20:37.706461: step 37187, loss 0.105367, acc 0.96875
2017-03-02T18:20:37.781162: step 37188, loss 0.107955, acc 0.9375
2017-03-02T18:20:37.859460: step 37189, loss 0.112491, acc 0.9375
2017-03-02T18:20:37.926985: step 37190, loss 0.220441, acc 0.921875
2017-03-02T18:20:38.012021: step 37191, loss 0.042832, acc 1
2017-03-02T18:20:38.081593: step 37192, loss 0.105205, acc 0.96875
2017-03-02T18:20:38.155477: step 37193, loss 0.0533593, acc 0.984375
2017-03-02T18:20:38.228030: step 37194, loss 0.202154, acc 0.921875
2017-03-02T18:20:38.305920: step 37195, loss 0.168864, acc 0.90625
2017-03-02T18:20:38.380694: step 37196, loss 0.137528, acc 0.90625
2017-03-02T18:20:38.453641: step 37197, loss 0.162511, acc 0.953125
2017-03-02T18:20:38.529418: step 37198, loss 0.157431, acc 0.921875
2017-03-02T18:20:38.600379: step 37199, loss 0.121059, acc 0.9375
2017-03-02T18:20:38.662568: step 37200, loss 0.165227, acc 0.921875

Evaluation:
2017-03-02T18:20:38.695845: step 37200, loss 5.40088, acc 0.637347

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37200

2017-03-02T18:20:39.180098: step 37201, loss 0.300584, acc 0.890625
2017-03-02T18:20:39.243541: step 37202, loss 0.220968, acc 0.890625
2017-03-02T18:20:39.319451: step 37203, loss 0.060045, acc 0.984375
2017-03-02T18:20:39.384644: step 37204, loss 0.176125, acc 0.953125
2017-03-02T18:20:39.453071: step 37205, loss 0.0899219, acc 0.953125
2017-03-02T18:20:39.525837: step 37206, loss 0.126917, acc 0.9375
2017-03-02T18:20:39.595159: step 37207, loss 0.154755, acc 0.90625
2017-03-02T18:20:39.668068: step 37208, loss 0.151267, acc 0.9375
2017-03-02T18:20:39.740881: step 37209, loss 0.243157, acc 0.921875
2017-03-02T18:20:39.816721: step 37210, loss 0.171802, acc 0.9375
2017-03-02T18:20:39.888482: step 37211, loss 0.182569, acc 0.9375
2017-03-02T18:20:39.954869: step 37212, loss 0.107259, acc 0.953125
2017-03-02T18:20:40.032363: step 37213, loss 0.11536, acc 0.921875
2017-03-02T18:20:40.101798: step 37214, loss 0.167336, acc 0.921875
2017-03-02T18:20:40.175946: step 37215, loss 0.177918, acc 0.890625
2017-03-02T18:20:40.251335: step 37216, loss 0.147926, acc 0.9375
2017-03-02T18:20:40.326248: step 37217, loss 0.22001, acc 0.890625
2017-03-02T18:20:40.397980: step 37218, loss 0.0744022, acc 0.984375
2017-03-02T18:20:40.467151: step 37219, loss 0.125896, acc 0.9375
2017-03-02T18:20:40.549678: step 37220, loss 0.113499, acc 0.96875
2017-03-02T18:20:40.613502: step 37221, loss 0.150117, acc 0.90625
2017-03-02T18:20:40.681632: step 37222, loss 0.138036, acc 0.921875
2017-03-02T18:20:40.754635: step 37223, loss 0.1624, acc 0.9375
2017-03-02T18:20:40.827073: step 37224, loss 0.162998, acc 0.921875
2017-03-02T18:20:40.895645: step 37225, loss 0.180151, acc 0.890625
2017-03-02T18:20:40.968077: step 37226, loss 0.0486428, acc 0.96875
2017-03-02T18:20:41.044898: step 37227, loss 0.204158, acc 0.890625
2017-03-02T18:20:41.117417: step 37228, loss 0.189242, acc 0.9375
2017-03-02T18:20:41.188991: step 37229, loss 0.263134, acc 0.90625
2017-03-02T18:20:41.265165: step 37230, loss 0.132846, acc 0.921875
2017-03-02T18:20:41.337264: step 37231, loss 0.277448, acc 0.890625
2017-03-02T18:20:41.415338: step 37232, loss 0.188951, acc 0.921875
2017-03-02T18:20:41.490700: step 37233, loss 0.119411, acc 0.953125
2017-03-02T18:20:41.562809: step 37234, loss 0.145017, acc 0.90625
2017-03-02T18:20:41.635872: step 37235, loss 0.181015, acc 0.90625
2017-03-02T18:20:41.719559: step 37236, loss 0.137522, acc 0.9375
2017-03-02T18:20:41.794613: step 37237, loss 0.109941, acc 0.96875
2017-03-02T18:20:41.867208: step 37238, loss 0.170579, acc 0.90625
2017-03-02T18:20:41.941110: step 37239, loss 0.147191, acc 0.921875
2017-03-02T18:20:42.013139: step 37240, loss 0, acc 1
2017-03-02T18:20:42.088633: step 37241, loss 0.158413, acc 0.9375
2017-03-02T18:20:42.157762: step 37242, loss 0.129583, acc 0.96875
2017-03-02T18:20:42.243941: step 37243, loss 0.0928716, acc 0.96875
2017-03-02T18:20:42.315815: step 37244, loss 0.152353, acc 0.9375
2017-03-02T18:20:42.390307: step 37245, loss 0.140165, acc 0.9375
2017-03-02T18:20:42.463403: step 37246, loss 0.161702, acc 0.921875
2017-03-02T18:20:42.533157: step 37247, loss 0.15068, acc 0.921875
2017-03-02T18:20:42.599887: step 37248, loss 0.105459, acc 0.953125
2017-03-02T18:20:42.678645: step 37249, loss 0.0689304, acc 0.96875
2017-03-02T18:20:42.744802: step 37250, loss 0.185625, acc 0.953125
2017-03-02T18:20:42.815586: step 37251, loss 0.0558586, acc 0.984375
2017-03-02T18:20:42.894072: step 37252, loss 0.137936, acc 0.90625
2017-03-02T18:20:42.973344: step 37253, loss 0.103764, acc 0.96875
2017-03-02T18:20:43.050555: step 37254, loss 0.100308, acc 0.96875
2017-03-02T18:20:43.128326: step 37255, loss 0.119009, acc 0.9375
2017-03-02T18:20:43.203876: step 37256, loss 0.132968, acc 0.9375
2017-03-02T18:20:43.274798: step 37257, loss 0.118146, acc 0.953125
2017-03-02T18:20:43.351213: step 37258, loss 0.0962901, acc 0.96875
2017-03-02T18:20:43.415423: step 37259, loss 0.105896, acc 0.9375
2017-03-02T18:20:43.490465: step 37260, loss 0.067817, acc 0.984375
2017-03-02T18:20:43.557887: step 37261, loss 0.194888, acc 0.921875
2017-03-02T18:20:43.633422: step 37262, loss 0.183542, acc 0.921875
2017-03-02T18:20:43.704858: step 37263, loss 0.0725028, acc 0.953125
2017-03-02T18:20:43.787045: step 37264, loss 0.121777, acc 0.953125
2017-03-02T18:20:43.855154: step 37265, loss 0.110197, acc 0.9375
2017-03-02T18:20:43.927857: step 37266, loss 0.214288, acc 0.859375
2017-03-02T18:20:43.992678: step 37267, loss 0.20465, acc 0.90625
2017-03-02T18:20:44.066749: step 37268, loss 0.0864189, acc 0.96875
2017-03-02T18:20:44.134896: step 37269, loss 0.322653, acc 0.84375
2017-03-02T18:20:44.201638: step 37270, loss 0.137971, acc 0.890625
2017-03-02T18:20:44.277961: step 37271, loss 0.246744, acc 0.90625
2017-03-02T18:20:44.352619: step 37272, loss 0.137368, acc 0.921875
2017-03-02T18:20:44.429089: step 37273, loss 0.137683, acc 0.953125
2017-03-02T18:20:44.501244: step 37274, loss 0.18558, acc 0.921875
2017-03-02T18:20:44.573420: step 37275, loss 0.118652, acc 0.953125
2017-03-02T18:20:44.649160: step 37276, loss 0.168621, acc 0.921875
2017-03-02T18:20:44.721948: step 37277, loss 0.129821, acc 0.9375
2017-03-02T18:20:44.791084: step 37278, loss 0.155941, acc 0.921875
2017-03-02T18:20:44.859519: step 37279, loss 0.0723457, acc 0.984375
2017-03-02T18:20:44.928004: step 37280, loss 0.16931, acc 0.953125
2017-03-02T18:20:45.002755: step 37281, loss 0.262576, acc 0.890625
2017-03-02T18:20:45.073656: step 37282, loss 0.170258, acc 0.921875
2017-03-02T18:20:45.149499: step 37283, loss 0.243356, acc 0.90625
2017-03-02T18:20:45.227733: step 37284, loss 0.12201, acc 0.9375
2017-03-02T18:20:45.302190: step 37285, loss 0.103699, acc 0.9375
2017-03-02T18:20:45.373513: step 37286, loss 0.237522, acc 0.90625
2017-03-02T18:20:45.449134: step 37287, loss 0.153322, acc 0.9375
2017-03-02T18:20:45.518692: step 37288, loss 0.109898, acc 0.953125
2017-03-02T18:20:45.582045: step 37289, loss 0.160595, acc 0.90625
2017-03-02T18:20:45.652106: step 37290, loss 0.141464, acc 0.890625
2017-03-02T18:20:45.734747: step 37291, loss 0.233362, acc 0.921875
2017-03-02T18:20:45.809578: step 37292, loss 0.166925, acc 0.90625
2017-03-02T18:20:45.895859: step 37293, loss 0.242746, acc 0.921875
2017-03-02T18:20:45.965071: step 37294, loss 0.0512275, acc 0.96875
2017-03-02T18:20:46.028559: step 37295, loss 0.111431, acc 0.9375
2017-03-02T18:20:46.103391: step 37296, loss 0.112399, acc 0.953125
2017-03-02T18:20:46.176681: step 37297, loss 0.100704, acc 0.96875
2017-03-02T18:20:46.242090: step 37298, loss 0.118735, acc 0.953125
2017-03-02T18:20:46.307353: step 37299, loss 0.095293, acc 0.9375
2017-03-02T18:20:46.379377: step 37300, loss 0.0927956, acc 0.953125

Evaluation:
2017-03-02T18:20:46.415299: step 37300, loss 5.7204, acc 0.647441

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37300

2017-03-02T18:20:46.908688: step 37301, loss 0.0957826, acc 0.9375
2017-03-02T18:20:46.972021: step 37302, loss 0.180835, acc 0.90625
2017-03-02T18:20:47.044733: step 37303, loss 0.0791987, acc 0.96875
2017-03-02T18:20:47.119589: step 37304, loss 0.150195, acc 0.921875
2017-03-02T18:20:47.194013: step 37305, loss 0.0759946, acc 0.96875
2017-03-02T18:20:47.266562: step 37306, loss 0.0739687, acc 0.96875
2017-03-02T18:20:47.341847: step 37307, loss 0.116369, acc 0.96875
2017-03-02T18:20:47.414786: step 37308, loss 0.176416, acc 0.9375
2017-03-02T18:20:47.489904: step 37309, loss 0.244187, acc 0.890625
2017-03-02T18:20:47.572226: step 37310, loss 0.159717, acc 0.9375
2017-03-02T18:20:47.644940: step 37311, loss 0.123935, acc 0.953125
2017-03-02T18:20:47.717613: step 37312, loss 0.170179, acc 0.921875
2017-03-02T18:20:47.790586: step 37313, loss 0.190556, acc 0.9375
2017-03-02T18:20:47.864929: step 37314, loss 0.108751, acc 0.96875
2017-03-02T18:20:47.935879: step 37315, loss 0.173127, acc 0.90625
2017-03-02T18:20:48.022266: step 37316, loss 0.202732, acc 0.921875
2017-03-02T18:20:48.102886: step 37317, loss 0.0853407, acc 0.953125
2017-03-02T18:20:48.177733: step 37318, loss 0.144963, acc 0.953125
2017-03-02T18:20:48.244446: step 37319, loss 0.0931308, acc 0.9375
2017-03-02T18:20:48.314409: step 37320, loss 0.259457, acc 0.921875
2017-03-02T18:20:48.385340: step 37321, loss 0.198795, acc 0.890625
2017-03-02T18:20:48.457726: step 37322, loss 0.0249857, acc 1
2017-03-02T18:20:48.524895: step 37323, loss 0.0980171, acc 0.96875
2017-03-02T18:20:48.599418: step 37324, loss 0.145281, acc 0.921875
2017-03-02T18:20:48.673259: step 37325, loss 0.152287, acc 0.9375
2017-03-02T18:20:48.747200: step 37326, loss 0.121788, acc 0.953125
2017-03-02T18:20:48.818842: step 37327, loss 0.122101, acc 0.96875
2017-03-02T18:20:48.891821: step 37328, loss 0.109241, acc 0.953125
2017-03-02T18:20:48.960863: step 37329, loss 0.220941, acc 0.921875
2017-03-02T18:20:49.027183: step 37330, loss 0.156569, acc 0.90625
2017-03-02T18:20:49.091535: step 37331, loss 0.180497, acc 0.90625
2017-03-02T18:20:49.168126: step 37332, loss 0.153118, acc 0.921875
2017-03-02T18:20:49.236687: step 37333, loss 0.0928072, acc 0.9375
2017-03-02T18:20:49.309638: step 37334, loss 0.104192, acc 0.953125
2017-03-02T18:20:49.383421: step 37335, loss 0.135375, acc 0.953125
2017-03-02T18:20:49.463182: step 37336, loss 0.131957, acc 0.953125
2017-03-02T18:20:49.542172: step 37337, loss 0.123542, acc 0.9375
2017-03-02T18:20:49.615789: step 37338, loss 0.0647389, acc 0.953125
2017-03-02T18:20:49.684719: step 37339, loss 0.158993, acc 0.921875
2017-03-02T18:20:49.745286: step 37340, loss 0.10004, acc 0.9375
2017-03-02T18:20:49.812425: step 37341, loss 0.0784419, acc 0.953125
2017-03-02T18:20:49.884197: step 37342, loss 0.0686245, acc 0.984375
2017-03-02T18:20:49.962971: step 37343, loss 0.206059, acc 0.9375
2017-03-02T18:20:50.034535: step 37344, loss 0.126438, acc 0.9375
2017-03-02T18:20:50.104897: step 37345, loss 0.143096, acc 0.9375
2017-03-02T18:20:50.175915: step 37346, loss 0.193145, acc 0.921875
2017-03-02T18:20:50.246963: step 37347, loss 0.172604, acc 0.921875
2017-03-02T18:20:50.319748: step 37348, loss 0.200329, acc 0.921875
2017-03-02T18:20:50.387270: step 37349, loss 0.135661, acc 0.9375
2017-03-02T18:20:50.452421: step 37350, loss 0.185389, acc 0.921875
2017-03-02T18:20:50.523988: step 37351, loss 0.0922656, acc 0.953125
2017-03-02T18:20:50.608110: step 37352, loss 0.183037, acc 0.875
2017-03-02T18:20:50.682962: step 37353, loss 0.284309, acc 0.890625
2017-03-02T18:20:50.752735: step 37354, loss 0.116638, acc 0.9375
2017-03-02T18:20:50.822921: step 37355, loss 0.158221, acc 0.921875
2017-03-02T18:20:50.893206: step 37356, loss 0.088674, acc 0.96875
2017-03-02T18:20:50.965268: step 37357, loss 0.167502, acc 0.875
2017-03-02T18:20:51.042549: step 37358, loss 0.196603, acc 0.890625
2017-03-02T18:20:51.111811: step 37359, loss 0.115617, acc 0.953125
2017-03-02T18:20:51.181816: step 37360, loss 0.216703, acc 0.90625
2017-03-02T18:20:51.257638: step 37361, loss 0.181476, acc 0.921875
2017-03-02T18:20:51.334937: step 37362, loss 0.248618, acc 0.875
2017-03-02T18:20:51.411916: step 37363, loss 0.132313, acc 0.9375
2017-03-02T18:20:51.488456: step 37364, loss 0.162109, acc 0.90625
2017-03-02T18:20:51.558394: step 37365, loss 0.104058, acc 0.953125
2017-03-02T18:20:51.624501: step 37366, loss 0.109282, acc 0.953125
2017-03-02T18:20:51.703254: step 37367, loss 0.146447, acc 0.90625
2017-03-02T18:20:51.768934: step 37368, loss 0.0874983, acc 0.96875
2017-03-02T18:20:51.834119: step 37369, loss 0.134591, acc 0.9375
2017-03-02T18:20:51.897640: step 37370, loss 0.175972, acc 0.90625
2017-03-02T18:20:51.960779: step 37371, loss 0.0856241, acc 0.96875
2017-03-02T18:20:52.035404: step 37372, loss 0.173607, acc 0.90625
2017-03-02T18:20:52.111411: step 37373, loss 0.170856, acc 0.921875
2017-03-02T18:20:52.183499: step 37374, loss 0.200301, acc 0.90625
2017-03-02T18:20:52.263659: step 37375, loss 0.146703, acc 0.9375
2017-03-02T18:20:52.335442: step 37376, loss 0.156019, acc 0.953125
2017-03-02T18:20:52.407513: step 37377, loss 0.0880813, acc 0.953125
2017-03-02T18:20:52.476501: step 37378, loss 0.142031, acc 0.9375
2017-03-02T18:20:52.572599: step 37379, loss 0.0421587, acc 1
2017-03-02T18:20:52.647725: step 37380, loss 0.179204, acc 0.90625
2017-03-02T18:20:52.718306: step 37381, loss 0.120077, acc 0.9375
2017-03-02T18:20:52.793112: step 37382, loss 0.0592509, acc 0.984375
2017-03-02T18:20:52.864909: step 37383, loss 0.0746052, acc 0.953125
2017-03-02T18:20:52.933613: step 37384, loss 0.183412, acc 0.9375
2017-03-02T18:20:53.004209: step 37385, loss 0.277336, acc 0.875
2017-03-02T18:20:53.078211: step 37386, loss 0.248963, acc 0.859375
2017-03-02T18:20:53.153967: step 37387, loss 0.13931, acc 0.90625
2017-03-02T18:20:53.223996: step 37388, loss 0.144452, acc 0.921875
2017-03-02T18:20:53.296686: step 37389, loss 0.0770125, acc 0.96875
2017-03-02T18:20:53.363502: step 37390, loss 0.0760533, acc 0.96875
2017-03-02T18:20:53.436491: step 37391, loss 0.150516, acc 0.953125
2017-03-02T18:20:53.507866: step 37392, loss 0.161299, acc 0.90625
2017-03-02T18:20:53.586693: step 37393, loss 0.09709, acc 0.96875
2017-03-02T18:20:53.661048: step 37394, loss 0.0764589, acc 0.96875
2017-03-02T18:20:53.736793: step 37395, loss 0.104796, acc 0.9375
2017-03-02T18:20:53.814076: step 37396, loss 0.100932, acc 0.953125
2017-03-02T18:20:53.884783: step 37397, loss 0.242038, acc 0.84375
2017-03-02T18:20:53.958335: step 37398, loss 0.128819, acc 0.953125
2017-03-02T18:20:54.031076: step 37399, loss 0.299791, acc 0.859375
2017-03-02T18:20:54.129469: step 37400, loss 0.189171, acc 0.921875

Evaluation:
2017-03-02T18:20:54.165519: step 37400, loss 5.56218, acc 0.634463

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37400

2017-03-02T18:20:54.601417: step 37401, loss 0.0896575, acc 0.984375
2017-03-02T18:20:54.670445: step 37402, loss 0.0785274, acc 0.984375
2017-03-02T18:20:54.738588: step 37403, loss 0.128941, acc 0.9375
2017-03-02T18:20:54.803029: step 37404, loss 0.12057, acc 0.921875
2017-03-02T18:20:54.876357: step 37405, loss 0.167624, acc 0.9375
2017-03-02T18:20:54.950034: step 37406, loss 0.138418, acc 0.9375
2017-03-02T18:20:55.022092: step 37407, loss 0.084482, acc 0.953125
2017-03-02T18:20:55.090132: step 37408, loss 0.27039, acc 0.859375
2017-03-02T18:20:55.175251: step 37409, loss 0.0621218, acc 0.96875
2017-03-02T18:20:55.242999: step 37410, loss 0.167299, acc 0.921875
2017-03-02T18:20:55.312494: step 37411, loss 0.192219, acc 0.890625
2017-03-02T18:20:55.386143: step 37412, loss 0.205065, acc 0.90625
2017-03-02T18:20:55.461312: step 37413, loss 0.184404, acc 0.921875
2017-03-02T18:20:55.534616: step 37414, loss 0.095353, acc 0.9375
2017-03-02T18:20:55.621186: step 37415, loss 0.0791802, acc 0.96875
2017-03-02T18:20:55.694417: step 37416, loss 0.225542, acc 0.921875
2017-03-02T18:20:55.766046: step 37417, loss 0.150623, acc 0.921875
2017-03-02T18:20:55.839133: step 37418, loss 0.14896, acc 0.921875
2017-03-02T18:20:55.905372: step 37419, loss 0.0937982, acc 0.953125
2017-03-02T18:20:55.971538: step 37420, loss 0.141976, acc 0.953125
2017-03-02T18:20:56.042900: step 37421, loss 0.250941, acc 0.890625
2017-03-02T18:20:56.109640: step 37422, loss 0.0764097, acc 0.953125
2017-03-02T18:20:56.179005: step 37423, loss 0.0769707, acc 1
2017-03-02T18:20:56.249107: step 37424, loss 0.186296, acc 0.90625
2017-03-02T18:20:56.318527: step 37425, loss 0.136678, acc 0.9375
2017-03-02T18:20:56.389162: step 37426, loss 0.21582, acc 0.9375
2017-03-02T18:20:56.455968: step 37427, loss 0.102483, acc 0.90625
2017-03-02T18:20:56.536952: step 37428, loss 0.146089, acc 0.921875
2017-03-02T18:20:56.611213: step 37429, loss 0.140044, acc 0.9375
2017-03-02T18:20:56.680248: step 37430, loss 0.0461773, acc 0.984375
2017-03-02T18:20:56.749203: step 37431, loss 0.140432, acc 0.9375
2017-03-02T18:20:56.822868: step 37432, loss 0.177613, acc 0.9375
2017-03-02T18:20:56.896532: step 37433, loss 0.10488, acc 0.953125
2017-03-02T18:20:56.974335: step 37434, loss 0.147664, acc 0.9375
2017-03-02T18:20:57.041862: step 37435, loss 0.362944, acc 0.84375
2017-03-02T18:20:57.100340: step 37436, loss 2.98023e-08, acc 1
2017-03-02T18:20:57.175620: step 37437, loss 0.127746, acc 0.96875
2017-03-02T18:20:57.245417: step 37438, loss 0.157103, acc 0.90625
2017-03-02T18:20:57.308974: step 37439, loss 0.105696, acc 0.96875
2017-03-02T18:20:57.367795: step 37440, loss 0.175516, acc 0.921875
2017-03-02T18:20:57.439246: step 37441, loss 0.074985, acc 0.96875
2017-03-02T18:20:57.509410: step 37442, loss 0.157846, acc 0.921875
2017-03-02T18:20:57.579942: step 37443, loss 0.211269, acc 0.921875
2017-03-02T18:20:57.651039: step 37444, loss 0.10325, acc 0.953125
2017-03-02T18:20:57.725082: step 37445, loss 0.139118, acc 0.921875
2017-03-02T18:20:57.794096: step 37446, loss 0.248396, acc 0.890625
2017-03-02T18:20:57.870276: step 37447, loss 0.207517, acc 0.921875
2017-03-02T18:20:57.935639: step 37448, loss 0.193515, acc 0.90625
2017-03-02T18:20:57.998350: step 37449, loss 0.122793, acc 0.921875
2017-03-02T18:20:58.067015: step 37450, loss 0.0612704, acc 0.984375
2017-03-02T18:20:58.154154: step 37451, loss 0.182152, acc 0.890625
2017-03-02T18:20:58.227324: step 37452, loss 0.181225, acc 0.9375
2017-03-02T18:20:58.302143: step 37453, loss 0.196493, acc 0.9375
2017-03-02T18:20:58.375696: step 37454, loss 0.154278, acc 0.921875
2017-03-02T18:20:58.447669: step 37455, loss 0.114204, acc 0.96875
2017-03-02T18:20:58.519962: step 37456, loss 0.0856513, acc 0.953125
2017-03-02T18:20:58.595073: step 37457, loss 0.0809804, acc 0.96875
2017-03-02T18:20:58.664962: step 37458, loss 0.0511815, acc 0.984375
2017-03-02T18:20:58.735990: step 37459, loss 0.147315, acc 0.9375
2017-03-02T18:20:58.805172: step 37460, loss 0.101465, acc 0.953125
2017-03-02T18:20:58.882081: step 37461, loss 0.173452, acc 0.921875
2017-03-02T18:20:58.959647: step 37462, loss 0.214983, acc 0.890625
2017-03-02T18:20:59.035427: step 37463, loss 0.0931397, acc 0.9375
2017-03-02T18:20:59.108378: step 37464, loss 0.117026, acc 0.953125
2017-03-02T18:20:59.174766: step 37465, loss 0.0831252, acc 0.953125
2017-03-02T18:20:59.248622: step 37466, loss 0.12362, acc 0.9375
2017-03-02T18:20:59.315217: step 37467, loss 0.0929157, acc 0.96875
2017-03-02T18:20:59.380923: step 37468, loss 0.186088, acc 0.9375
2017-03-02T18:20:59.448541: step 37469, loss 0.0366514, acc 1
2017-03-02T18:20:59.529138: step 37470, loss 0.186306, acc 0.9375
2017-03-02T18:20:59.599914: step 37471, loss 0.295619, acc 0.90625
2017-03-02T18:20:59.669434: step 37472, loss 0.126976, acc 0.953125
2017-03-02T18:20:59.741082: step 37473, loss 0.102067, acc 0.9375
2017-03-02T18:20:59.814657: step 37474, loss 0.151004, acc 0.953125
2017-03-02T18:20:59.878417: step 37475, loss 0.114831, acc 0.953125
2017-03-02T18:20:59.954096: step 37476, loss 0.164649, acc 0.90625
2017-03-02T18:21:00.022552: step 37477, loss 0.174481, acc 0.90625
2017-03-02T18:21:00.091090: step 37478, loss 0.175047, acc 0.921875
2017-03-02T18:21:00.168742: step 37479, loss 0.139168, acc 0.96875
2017-03-02T18:21:00.244008: step 37480, loss 0.127919, acc 0.96875
2017-03-02T18:21:00.313349: step 37481, loss 0.239337, acc 0.875
2017-03-02T18:21:00.384202: step 37482, loss 0.169451, acc 0.921875
2017-03-02T18:21:00.447873: step 37483, loss 0.0679663, acc 0.984375
2017-03-02T18:21:00.520383: step 37484, loss 0.175351, acc 0.96875
2017-03-02T18:21:00.593205: step 37485, loss 0.111912, acc 0.96875
2017-03-02T18:21:00.668446: step 37486, loss 0.108891, acc 0.9375
2017-03-02T18:21:00.745482: step 37487, loss 0.102575, acc 0.96875
2017-03-02T18:21:00.823718: step 37488, loss 0.087892, acc 0.96875
2017-03-02T18:21:00.900028: step 37489, loss 0.115516, acc 0.953125
2017-03-02T18:21:00.976797: step 37490, loss 0.110184, acc 0.921875
2017-03-02T18:21:01.054587: step 37491, loss 0.108828, acc 0.953125
2017-03-02T18:21:01.131310: step 37492, loss 0.147188, acc 0.921875
2017-03-02T18:21:01.202585: step 37493, loss 0.21276, acc 0.890625
2017-03-02T18:21:01.272886: step 37494, loss 0.122087, acc 0.953125
2017-03-02T18:21:01.347255: step 37495, loss 0.131144, acc 0.953125
2017-03-02T18:21:01.419690: step 37496, loss 0.0868493, acc 0.953125
2017-03-02T18:21:01.488979: step 37497, loss 0.0552176, acc 0.96875
2017-03-02T18:21:01.562342: step 37498, loss 0.107595, acc 0.96875
2017-03-02T18:21:01.641676: step 37499, loss 0.138329, acc 0.953125
2017-03-02T18:21:01.723414: step 37500, loss 0.151494, acc 0.921875

Evaluation:
2017-03-02T18:21:01.760660: step 37500, loss 5.52901, acc 0.635905

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37500

2017-03-02T18:21:02.216891: step 37501, loss 0.128861, acc 0.921875
2017-03-02T18:21:02.285592: step 37502, loss 0.0875733, acc 0.96875
2017-03-02T18:21:02.355769: step 37503, loss 0.104484, acc 0.953125
2017-03-02T18:21:02.426736: step 37504, loss 0.156364, acc 0.921875
2017-03-02T18:21:02.515224: step 37505, loss 0.150836, acc 0.9375
2017-03-02T18:21:02.592741: step 37506, loss 0.166889, acc 0.9375
2017-03-02T18:21:02.667765: step 37507, loss 0.179965, acc 0.921875
2017-03-02T18:21:02.743866: step 37508, loss 0.0744036, acc 0.953125
2017-03-02T18:21:02.812212: step 37509, loss 0.188231, acc 0.90625
2017-03-02T18:21:02.886139: step 37510, loss 0.0368094, acc 0.984375
2017-03-02T18:21:02.956086: step 37511, loss 0.0842012, acc 0.953125
2017-03-02T18:21:03.025097: step 37512, loss 0.340954, acc 0.890625
2017-03-02T18:21:03.098827: step 37513, loss 0.138103, acc 0.9375
2017-03-02T18:21:03.171693: step 37514, loss 0.0896023, acc 0.96875
2017-03-02T18:21:03.242130: step 37515, loss 0.162452, acc 0.921875
2017-03-02T18:21:03.313599: step 37516, loss 0.0181652, acc 1
2017-03-02T18:21:03.385435: step 37517, loss 0.186913, acc 0.890625
2017-03-02T18:21:03.463965: step 37518, loss 0.168117, acc 0.921875
2017-03-02T18:21:03.533317: step 37519, loss 0.0862436, acc 0.953125
2017-03-02T18:21:03.601257: step 37520, loss 0.135704, acc 0.921875
2017-03-02T18:21:03.675602: step 37521, loss 0.196809, acc 0.890625
2017-03-02T18:21:03.751198: step 37522, loss 0.173211, acc 0.890625
2017-03-02T18:21:03.826448: step 37523, loss 0.148811, acc 0.953125
2017-03-02T18:21:03.901809: step 37524, loss 0.136718, acc 0.96875
2017-03-02T18:21:03.974706: step 37525, loss 0.151084, acc 0.9375
2017-03-02T18:21:04.048660: step 37526, loss 0.256649, acc 0.875
2017-03-02T18:21:04.121831: step 37527, loss 0.112693, acc 0.953125
2017-03-02T18:21:04.190664: step 37528, loss 0.130653, acc 0.953125
2017-03-02T18:21:04.255717: step 37529, loss 0.108537, acc 0.953125
2017-03-02T18:21:04.330289: step 37530, loss 0.153929, acc 0.953125
2017-03-02T18:21:04.398615: step 37531, loss 0.0906323, acc 0.953125
2017-03-02T18:21:04.468451: step 37532, loss 0.246809, acc 0.921875
2017-03-02T18:21:04.537563: step 37533, loss 0.115414, acc 0.96875
2017-03-02T18:21:04.616436: step 37534, loss 0.209645, acc 0.875
2017-03-02T18:21:04.687026: step 37535, loss 0.104398, acc 0.953125
2017-03-02T18:21:04.761115: step 37536, loss 0.142991, acc 0.921875
2017-03-02T18:21:04.839579: step 37537, loss 0.127265, acc 0.90625
2017-03-02T18:21:04.914789: step 37538, loss 0.0665854, acc 0.96875
2017-03-02T18:21:04.989052: step 37539, loss 0.18047, acc 0.921875
2017-03-02T18:21:05.081957: step 37540, loss 0.182654, acc 0.90625
2017-03-02T18:21:05.170106: step 37541, loss 0.244014, acc 0.921875
2017-03-02T18:21:05.239335: step 37542, loss 0.141214, acc 0.953125
2017-03-02T18:21:05.311081: step 37543, loss 0.105056, acc 0.96875
2017-03-02T18:21:05.379517: step 37544, loss 0.142948, acc 0.953125
2017-03-02T18:21:05.458467: step 37545, loss 0.183334, acc 0.921875
2017-03-02T18:21:05.542364: step 37546, loss 0.063552, acc 0.96875
2017-03-02T18:21:05.617755: step 37547, loss 0.115523, acc 0.9375
2017-03-02T18:21:05.681858: step 37548, loss 0.246689, acc 0.84375
2017-03-02T18:21:05.754545: step 37549, loss 0.124738, acc 0.9375
2017-03-02T18:21:05.824336: step 37550, loss 0.117323, acc 0.9375
2017-03-02T18:21:05.903377: step 37551, loss 0.157126, acc 0.921875
2017-03-02T18:21:05.980359: step 37552, loss 0.157572, acc 0.9375
2017-03-02T18:21:06.057759: step 37553, loss 0.119283, acc 0.96875
2017-03-02T18:21:06.131120: step 37554, loss 0.219681, acc 0.890625
2017-03-02T18:21:06.207300: step 37555, loss 0.147674, acc 0.9375
2017-03-02T18:21:06.288735: step 37556, loss 0.171206, acc 0.921875
2017-03-02T18:21:06.373207: step 37557, loss 0.173954, acc 0.921875
2017-03-02T18:21:06.444130: step 37558, loss 0.197217, acc 0.875
2017-03-02T18:21:06.522921: step 37559, loss 0.127586, acc 0.9375
2017-03-02T18:21:06.593271: step 37560, loss 0.0611469, acc 0.984375
2017-03-02T18:21:06.665869: step 37561, loss 0.155179, acc 0.9375
2017-03-02T18:21:06.734531: step 37562, loss 0.20314, acc 0.921875
2017-03-02T18:21:06.808360: step 37563, loss 0.127236, acc 0.921875
2017-03-02T18:21:06.879054: step 37564, loss 0.124543, acc 0.96875
2017-03-02T18:21:06.967811: step 37565, loss 0.0857278, acc 0.984375
2017-03-02T18:21:07.037233: step 37566, loss 0.201328, acc 0.90625
2017-03-02T18:21:07.098005: step 37567, loss 0.145794, acc 0.921875
2017-03-02T18:21:07.171183: step 37568, loss 0.101516, acc 0.921875
2017-03-02T18:21:07.239204: step 37569, loss 0.135759, acc 0.953125
2017-03-02T18:21:07.308999: step 37570, loss 0.0835151, acc 0.96875
2017-03-02T18:21:07.387682: step 37571, loss 0.169044, acc 0.921875
2017-03-02T18:21:07.457894: step 37572, loss 0.132849, acc 0.921875
2017-03-02T18:21:07.529257: step 37573, loss 0.116523, acc 0.96875
2017-03-02T18:21:07.603097: step 37574, loss 0.196288, acc 0.9375
2017-03-02T18:21:07.677185: step 37575, loss 0.081013, acc 0.984375
2017-03-02T18:21:07.749599: step 37576, loss 0.136544, acc 0.953125
2017-03-02T18:21:07.823650: step 37577, loss 0.0865988, acc 0.9375
2017-03-02T18:21:07.893900: step 37578, loss 0.173236, acc 0.890625
2017-03-02T18:21:07.969919: step 37579, loss 0.145995, acc 0.921875
2017-03-02T18:21:08.042013: step 37580, loss 0.143312, acc 0.9375
2017-03-02T18:21:08.113160: step 37581, loss 0.212173, acc 0.90625
2017-03-02T18:21:08.186517: step 37582, loss 0.173972, acc 0.9375
2017-03-02T18:21:08.258989: step 37583, loss 0.103499, acc 0.953125
2017-03-02T18:21:08.329135: step 37584, loss 0.136546, acc 0.9375
2017-03-02T18:21:08.400095: step 37585, loss 0.231826, acc 0.84375
2017-03-02T18:21:08.470829: step 37586, loss 0.120836, acc 0.96875
2017-03-02T18:21:08.539953: step 37587, loss 0.260254, acc 0.890625
2017-03-02T18:21:08.610150: step 37588, loss 0.116895, acc 0.9375
2017-03-02T18:21:08.683279: step 37589, loss 0.163435, acc 0.9375
2017-03-02T18:21:08.751865: step 37590, loss 0.265068, acc 0.875
2017-03-02T18:21:08.822568: step 37591, loss 0.187322, acc 0.9375
2017-03-02T18:21:08.891956: step 37592, loss 0.0807133, acc 0.953125
2017-03-02T18:21:08.965352: step 37593, loss 0.114864, acc 0.953125
2017-03-02T18:21:09.045367: step 37594, loss 0.172647, acc 0.921875
2017-03-02T18:21:09.112728: step 37595, loss 0.0873967, acc 0.953125
2017-03-02T18:21:09.179437: step 37596, loss 0.0960949, acc 0.9375
2017-03-02T18:21:09.253788: step 37597, loss 0.151335, acc 0.9375
2017-03-02T18:21:09.341387: step 37598, loss 0.143428, acc 0.921875
2017-03-02T18:21:09.424477: step 37599, loss 0.080106, acc 0.953125
2017-03-02T18:21:09.496637: step 37600, loss 0.158815, acc 0.9375

Evaluation:
2017-03-02T18:21:09.534007: step 37600, loss 5.49329, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37600

2017-03-02T18:21:09.987784: step 37601, loss 0.110485, acc 0.921875
2017-03-02T18:21:10.061273: step 37602, loss 0.159995, acc 0.90625
2017-03-02T18:21:10.138498: step 37603, loss 0.160653, acc 0.890625
2017-03-02T18:21:10.209946: step 37604, loss 0.168149, acc 0.921875
2017-03-02T18:21:10.279563: step 37605, loss 0.149289, acc 0.9375
2017-03-02T18:21:10.355488: step 37606, loss 0.0969033, acc 0.953125
2017-03-02T18:21:10.425740: step 37607, loss 0.171284, acc 0.90625
2017-03-02T18:21:10.492920: step 37608, loss 0.0775912, acc 0.984375
2017-03-02T18:21:10.564580: step 37609, loss 0.0612871, acc 0.984375
2017-03-02T18:21:10.636086: step 37610, loss 0.0740776, acc 0.953125
2017-03-02T18:21:10.716537: step 37611, loss 0.187683, acc 0.890625
2017-03-02T18:21:10.791638: step 37612, loss 0.162511, acc 0.9375
2017-03-02T18:21:10.863239: step 37613, loss 0.125595, acc 0.96875
2017-03-02T18:21:10.936349: step 37614, loss 0.177809, acc 0.890625
2017-03-02T18:21:11.007851: step 37615, loss 0.136798, acc 0.9375
2017-03-02T18:21:11.086993: step 37616, loss 0.185656, acc 0.921875
2017-03-02T18:21:11.161115: step 37617, loss 0.0913457, acc 0.984375
2017-03-02T18:21:11.229057: step 37618, loss 0.177119, acc 0.90625
2017-03-02T18:21:11.296614: step 37619, loss 0.122132, acc 0.953125
2017-03-02T18:21:11.376643: step 37620, loss 0.0988858, acc 0.96875
2017-03-02T18:21:11.451954: step 37621, loss 0.173548, acc 0.921875
2017-03-02T18:21:11.531078: step 37622, loss 0.077336, acc 0.9375
2017-03-02T18:21:11.598582: step 37623, loss 0.0862645, acc 0.984375
2017-03-02T18:21:11.668800: step 37624, loss 0.216105, acc 0.90625
2017-03-02T18:21:11.740498: step 37625, loss 0.113164, acc 0.953125
2017-03-02T18:21:11.810506: step 37626, loss 0.161696, acc 0.9375
2017-03-02T18:21:11.875562: step 37627, loss 0.251424, acc 0.859375
2017-03-02T18:21:11.953162: step 37628, loss 0.19331, acc 0.921875
2017-03-02T18:21:12.033575: step 37629, loss 0.13124, acc 0.921875
2017-03-02T18:21:12.113333: step 37630, loss 0.132279, acc 0.9375
2017-03-02T18:21:12.186498: step 37631, loss 0.18186, acc 0.921875
2017-03-02T18:21:12.256176: step 37632, loss 0.00609958, acc 1
2017-03-02T18:21:12.328874: step 37633, loss 0.0931817, acc 0.9375
2017-03-02T18:21:12.398975: step 37634, loss 0.130212, acc 0.9375
2017-03-02T18:21:12.461127: step 37635, loss 0.16109, acc 0.9375
2017-03-02T18:21:12.532516: step 37636, loss 0.0894505, acc 0.953125
2017-03-02T18:21:12.597493: step 37637, loss 0.231874, acc 0.890625
2017-03-02T18:21:12.671304: step 37638, loss 0.163237, acc 0.921875
2017-03-02T18:21:12.747865: step 37639, loss 0.144002, acc 0.90625
2017-03-02T18:21:12.825570: step 37640, loss 0.162938, acc 0.921875
2017-03-02T18:21:12.895494: step 37641, loss 0.127515, acc 0.953125
2017-03-02T18:21:12.960298: step 37642, loss 0.132083, acc 0.9375
2017-03-02T18:21:13.040907: step 37643, loss 0.0772751, acc 0.984375
2017-03-02T18:21:13.115840: step 37644, loss 0.175433, acc 0.90625
2017-03-02T18:21:13.189362: step 37645, loss 0.0452253, acc 0.96875
2017-03-02T18:21:13.251308: step 37646, loss 0.140858, acc 0.921875
2017-03-02T18:21:13.322818: step 37647, loss 0.0846146, acc 0.953125
2017-03-02T18:21:13.394289: step 37648, loss 0.0936864, acc 0.96875
2017-03-02T18:21:13.467183: step 37649, loss 0.155202, acc 0.921875
2017-03-02T18:21:13.531838: step 37650, loss 0.119587, acc 0.953125
2017-03-02T18:21:13.603991: step 37651, loss 0.207024, acc 0.890625
2017-03-02T18:21:13.673676: step 37652, loss 0.192161, acc 0.921875
2017-03-02T18:21:13.747247: step 37653, loss 0.177168, acc 0.890625
2017-03-02T18:21:13.819292: step 37654, loss 0.0878392, acc 0.9375
2017-03-02T18:21:13.890321: step 37655, loss 0.171982, acc 0.90625
2017-03-02T18:21:13.957190: step 37656, loss 0.176764, acc 0.9375
2017-03-02T18:21:14.025488: step 37657, loss 0.0480167, acc 0.984375
2017-03-02T18:21:14.098460: step 37658, loss 0.182094, acc 0.921875
2017-03-02T18:21:14.170421: step 37659, loss 0.10628, acc 0.953125
2017-03-02T18:21:14.246580: step 37660, loss 0.0920218, acc 0.953125
2017-03-02T18:21:14.319725: step 37661, loss 0.168569, acc 0.9375
2017-03-02T18:21:14.395578: step 37662, loss 0.175011, acc 0.9375
2017-03-02T18:21:14.468231: step 37663, loss 0.0934945, acc 0.9375
2017-03-02T18:21:14.537791: step 37664, loss 0.234114, acc 0.859375
2017-03-02T18:21:14.603932: step 37665, loss 0.135933, acc 0.96875
2017-03-02T18:21:14.673146: step 37666, loss 0.176754, acc 0.953125
2017-03-02T18:21:14.746083: step 37667, loss 0.120203, acc 0.953125
2017-03-02T18:21:14.820927: step 37668, loss 0.144315, acc 0.9375
2017-03-02T18:21:14.892951: step 37669, loss 0.126971, acc 0.953125
2017-03-02T18:21:14.963472: step 37670, loss 0.224079, acc 0.875
2017-03-02T18:21:15.038870: step 37671, loss 0.185942, acc 0.9375
2017-03-02T18:21:15.111547: step 37672, loss 0.207326, acc 0.875
2017-03-02T18:21:15.182023: step 37673, loss 0.203815, acc 0.90625
2017-03-02T18:21:15.252425: step 37674, loss 0.102505, acc 0.953125
2017-03-02T18:21:15.319679: step 37675, loss 0.0193207, acc 1
2017-03-02T18:21:15.388663: step 37676, loss 0.0987339, acc 0.953125
2017-03-02T18:21:15.461935: step 37677, loss 0.0958859, acc 0.96875
2017-03-02T18:21:15.540523: step 37678, loss 0.0855122, acc 0.953125
2017-03-02T18:21:15.614918: step 37679, loss 0.191019, acc 0.921875
2017-03-02T18:21:15.686253: step 37680, loss 0.0769668, acc 0.953125
2017-03-02T18:21:15.748865: step 37681, loss 0.133294, acc 0.90625
2017-03-02T18:21:15.819910: step 37682, loss 0.0936218, acc 0.984375
2017-03-02T18:21:15.891020: step 37683, loss 0.174551, acc 0.90625
2017-03-02T18:21:15.958425: step 37684, loss 0.192568, acc 0.953125
2017-03-02T18:21:16.037898: step 37685, loss 0.0977899, acc 0.96875
2017-03-02T18:21:16.108921: step 37686, loss 0.166096, acc 0.9375
2017-03-02T18:21:16.182238: step 37687, loss 0.185005, acc 0.90625
2017-03-02T18:21:16.249147: step 37688, loss 0.111343, acc 0.953125
2017-03-02T18:21:16.322701: step 37689, loss 0.168512, acc 0.921875
2017-03-02T18:21:16.400666: step 37690, loss 0.166599, acc 0.90625
2017-03-02T18:21:16.477050: step 37691, loss 0.110078, acc 0.90625
2017-03-02T18:21:16.554572: step 37692, loss 0.142886, acc 0.921875
2017-03-02T18:21:16.638795: step 37693, loss 0.112974, acc 0.96875
2017-03-02T18:21:16.705606: step 37694, loss 0.171188, acc 0.890625
2017-03-02T18:21:16.775926: step 37695, loss 0.256468, acc 0.90625
2017-03-02T18:21:16.850811: step 37696, loss 0.192405, acc 0.890625
2017-03-02T18:21:16.919496: step 37697, loss 0.0646933, acc 0.984375
2017-03-02T18:21:17.001922: step 37698, loss 0.0969565, acc 0.953125
2017-03-02T18:21:17.074032: step 37699, loss 0.249973, acc 0.890625
2017-03-02T18:21:17.151239: step 37700, loss 0.112027, acc 0.953125

Evaluation:
2017-03-02T18:21:17.190353: step 37700, loss 5.47665, acc 0.635184

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37700

2017-03-02T18:21:17.642926: step 37701, loss 0.119345, acc 0.9375
2017-03-02T18:21:17.718036: step 37702, loss 0.129063, acc 0.953125
2017-03-02T18:21:17.796479: step 37703, loss 0.245196, acc 0.921875
2017-03-02T18:21:17.869994: step 37704, loss 0.210103, acc 0.875
2017-03-02T18:21:17.952748: step 37705, loss 0.0839449, acc 0.96875
2017-03-02T18:21:18.031010: step 37706, loss 0.177467, acc 0.90625
2017-03-02T18:21:18.097998: step 37707, loss 0.0590705, acc 1
2017-03-02T18:21:18.170194: step 37708, loss 0.0878124, acc 0.953125
2017-03-02T18:21:18.235370: step 37709, loss 0.0799988, acc 0.984375
2017-03-02T18:21:18.300097: step 37710, loss 0.121282, acc 0.921875
2017-03-02T18:21:18.377227: step 37711, loss 0.117721, acc 0.9375
2017-03-02T18:21:18.448137: step 37712, loss 0.15651, acc 0.9375
2017-03-02T18:21:18.515650: step 37713, loss 0.217094, acc 0.90625
2017-03-02T18:21:18.582761: step 37714, loss 0.114946, acc 0.953125
2017-03-02T18:21:18.653884: step 37715, loss 0.12005, acc 0.96875
2017-03-02T18:21:18.720947: step 37716, loss 0.167901, acc 0.921875
2017-03-02T18:21:18.791206: step 37717, loss 0.121797, acc 0.9375
2017-03-02T18:21:18.855600: step 37718, loss 0.0791741, acc 0.96875
2017-03-02T18:21:18.927868: step 37719, loss 0.12324, acc 0.9375
2017-03-02T18:21:18.999471: step 37720, loss 0.0872601, acc 0.953125
2017-03-02T18:21:19.080384: step 37721, loss 0.0971739, acc 0.9375
2017-03-02T18:21:19.152835: step 37722, loss 0.0995479, acc 0.953125
2017-03-02T18:21:19.231122: step 37723, loss 0.209757, acc 0.875
2017-03-02T18:21:19.301759: step 37724, loss 0.105807, acc 0.96875
2017-03-02T18:21:19.381033: step 37725, loss 0.101838, acc 0.953125
2017-03-02T18:21:19.446076: step 37726, loss 0.0939948, acc 0.9375
2017-03-02T18:21:19.517305: step 37727, loss 0.173353, acc 0.921875
2017-03-02T18:21:19.605075: step 37728, loss 0.177257, acc 0.9375
2017-03-02T18:21:19.673407: step 37729, loss 0.112848, acc 0.953125
2017-03-02T18:21:19.743842: step 37730, loss 0.155039, acc 0.90625
2017-03-02T18:21:19.815797: step 37731, loss 0.236041, acc 0.890625
2017-03-02T18:21:19.887729: step 37732, loss 0.2407, acc 0.9375
2017-03-02T18:21:19.957696: step 37733, loss 0.0893139, acc 0.96875
2017-03-02T18:21:20.032596: step 37734, loss 0.205795, acc 0.953125
2017-03-02T18:21:20.104028: step 37735, loss 0.0190628, acc 0.984375
2017-03-02T18:21:20.170729: step 37736, loss 0.0756046, acc 0.96875
2017-03-02T18:21:20.236234: step 37737, loss 0.166223, acc 0.953125
2017-03-02T18:21:20.307650: step 37738, loss 0.0941585, acc 0.953125
2017-03-02T18:21:20.383239: step 37739, loss 0.144344, acc 0.921875
2017-03-02T18:21:20.455987: step 37740, loss 0.07736, acc 0.96875
2017-03-02T18:21:20.528936: step 37741, loss 0.0984089, acc 0.953125
2017-03-02T18:21:20.600681: step 37742, loss 0.0857045, acc 0.9375
2017-03-02T18:21:20.679849: step 37743, loss 0.0825961, acc 0.9375
2017-03-02T18:21:20.749235: step 37744, loss 0.254058, acc 0.875
2017-03-02T18:21:20.817734: step 37745, loss 0.0744825, acc 0.953125
2017-03-02T18:21:20.883960: step 37746, loss 0.217785, acc 0.921875
2017-03-02T18:21:20.953375: step 37747, loss 0.150181, acc 0.9375
2017-03-02T18:21:21.027178: step 37748, loss 0.12542, acc 0.953125
2017-03-02T18:21:21.099730: step 37749, loss 0.197159, acc 0.921875
2017-03-02T18:21:21.173057: step 37750, loss 0.125243, acc 0.953125
2017-03-02T18:21:21.246295: step 37751, loss 0.119442, acc 0.9375
2017-03-02T18:21:21.319631: step 37752, loss 0.0949399, acc 0.984375
2017-03-02T18:21:21.389024: step 37753, loss 0.142456, acc 0.953125
2017-03-02T18:21:21.459671: step 37754, loss 0.23442, acc 0.921875
2017-03-02T18:21:21.526209: step 37755, loss 0.147869, acc 0.953125
2017-03-02T18:21:21.600855: step 37756, loss 0.119242, acc 0.984375
2017-03-02T18:21:21.676628: step 37757, loss 0.134282, acc 0.953125
2017-03-02T18:21:21.749286: step 37758, loss 0.211785, acc 0.890625
2017-03-02T18:21:21.820032: step 37759, loss 0.0479507, acc 1
2017-03-02T18:21:21.893604: step 37760, loss 0.162022, acc 0.953125
2017-03-02T18:21:21.974858: step 37761, loss 0.149135, acc 0.90625
2017-03-02T18:21:22.049057: step 37762, loss 0.0695756, acc 0.984375
2017-03-02T18:21:22.123640: step 37763, loss 0.150632, acc 0.90625
2017-03-02T18:21:22.197332: step 37764, loss 0.232309, acc 0.875
2017-03-02T18:21:22.265309: step 37765, loss 0.140563, acc 0.921875
2017-03-02T18:21:22.344125: step 37766, loss 0.233438, acc 0.9375
2017-03-02T18:21:22.420878: step 37767, loss 0.137348, acc 0.921875
2017-03-02T18:21:22.492127: step 37768, loss 0.165673, acc 0.90625
2017-03-02T18:21:22.565685: step 37769, loss 0.133019, acc 0.921875
2017-03-02T18:21:22.641758: step 37770, loss 0.101709, acc 0.984375
2017-03-02T18:21:22.715808: step 37771, loss 0.0671225, acc 0.953125
2017-03-02T18:21:22.782832: step 37772, loss 0.0778494, acc 0.953125
2017-03-02T18:21:22.856102: step 37773, loss 0.289299, acc 0.921875
2017-03-02T18:21:22.922525: step 37774, loss 0.245096, acc 0.875
2017-03-02T18:21:22.986393: step 37775, loss 0.249787, acc 0.90625
2017-03-02T18:21:23.052466: step 37776, loss 0.122766, acc 0.953125
2017-03-02T18:21:23.121370: step 37777, loss 0.128982, acc 0.96875
2017-03-02T18:21:23.209114: step 37778, loss 0.150134, acc 0.9375
2017-03-02T18:21:23.284503: step 37779, loss 0.128661, acc 0.96875
2017-03-02T18:21:23.358550: step 37780, loss 0.0485106, acc 0.96875
2017-03-02T18:21:23.428366: step 37781, loss 0.125902, acc 0.9375
2017-03-02T18:21:23.498197: step 37782, loss 0.0978053, acc 0.984375
2017-03-02T18:21:23.575048: step 37783, loss 0.185812, acc 0.90625
2017-03-02T18:21:23.642377: step 37784, loss 0.149177, acc 0.921875
2017-03-02T18:21:23.714721: step 37785, loss 0.108901, acc 0.953125
2017-03-02T18:21:23.789767: step 37786, loss 0.177419, acc 0.890625
2017-03-02T18:21:23.861070: step 37787, loss 0.20489, acc 0.890625
2017-03-02T18:21:23.940930: step 37788, loss 0.175087, acc 0.9375
2017-03-02T18:21:24.013215: step 37789, loss 0.12044, acc 0.9375
2017-03-02T18:21:24.084253: step 37790, loss 0.129244, acc 0.921875
2017-03-02T18:21:24.152492: step 37791, loss 0.221296, acc 0.859375
2017-03-02T18:21:24.227938: step 37792, loss 0.164487, acc 0.9375
2017-03-02T18:21:24.291672: step 37793, loss 0.260025, acc 0.90625
2017-03-02T18:21:24.356206: step 37794, loss 0.136309, acc 0.921875
2017-03-02T18:21:24.427297: step 37795, loss 0.0762112, acc 0.96875
2017-03-02T18:21:24.505418: step 37796, loss 0.21721, acc 0.890625
2017-03-02T18:21:24.581537: step 37797, loss 0.0949436, acc 0.953125
2017-03-02T18:21:24.660916: step 37798, loss 0.184183, acc 0.90625
2017-03-02T18:21:24.734059: step 37799, loss 0.139116, acc 0.953125
2017-03-02T18:21:24.809571: step 37800, loss 0.142539, acc 0.90625

Evaluation:
2017-03-02T18:21:24.845856: step 37800, loss 5.48655, acc 0.640231

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37800

2017-03-02T18:21:25.302641: step 37801, loss 0.166618, acc 0.921875
2017-03-02T18:21:25.384926: step 37802, loss 0.0439909, acc 1
2017-03-02T18:21:25.451465: step 37803, loss 0.158967, acc 0.9375
2017-03-02T18:21:25.525919: step 37804, loss 0.174503, acc 0.953125
2017-03-02T18:21:25.605279: step 37805, loss 0.182433, acc 0.9375
2017-03-02T18:21:25.682286: step 37806, loss 0.129067, acc 0.96875
2017-03-02T18:21:25.753939: step 37807, loss 0.213268, acc 0.875
2017-03-02T18:21:25.847232: step 37808, loss 0.127453, acc 0.921875
2017-03-02T18:21:25.917648: step 37809, loss 0.113967, acc 0.921875
2017-03-02T18:21:25.992151: step 37810, loss 0.10638, acc 0.921875
2017-03-02T18:21:26.062573: step 37811, loss 0.196256, acc 0.90625
2017-03-02T18:21:26.137809: step 37812, loss 0.200349, acc 0.9375
2017-03-02T18:21:26.212641: step 37813, loss 0.0847567, acc 0.96875
2017-03-02T18:21:26.286064: step 37814, loss 0.149263, acc 0.953125
2017-03-02T18:21:26.355026: step 37815, loss 0.141924, acc 0.9375
2017-03-02T18:21:26.432582: step 37816, loss 0.120155, acc 0.953125
2017-03-02T18:21:26.502831: step 37817, loss 0.241053, acc 0.921875
2017-03-02T18:21:26.565623: step 37818, loss 0.127473, acc 0.9375
2017-03-02T18:21:26.635659: step 37819, loss 0.217144, acc 0.890625
2017-03-02T18:21:26.705175: step 37820, loss 0.190627, acc 0.921875
2017-03-02T18:21:26.774200: step 37821, loss 0.140716, acc 0.9375
2017-03-02T18:21:26.853504: step 37822, loss 0.157997, acc 0.90625
2017-03-02T18:21:26.927363: step 37823, loss 0.18421, acc 0.921875
2017-03-02T18:21:27.010016: step 37824, loss 0.098039, acc 0.96875
2017-03-02T18:21:27.078655: step 37825, loss 0.183837, acc 0.90625
2017-03-02T18:21:27.143213: step 37826, loss 0.109307, acc 0.953125
2017-03-02T18:21:27.211006: step 37827, loss 0.118425, acc 0.953125
2017-03-02T18:21:27.278561: step 37828, loss 0, acc 1
2017-03-02T18:21:27.349694: step 37829, loss 0.132983, acc 0.96875
2017-03-02T18:21:27.416384: step 37830, loss 0.141284, acc 0.9375
2017-03-02T18:21:27.485890: step 37831, loss 0.104296, acc 0.953125
2017-03-02T18:21:27.558167: step 37832, loss 0.173892, acc 0.921875
2017-03-02T18:21:27.621820: step 37833, loss 0.0756959, acc 0.953125
2017-03-02T18:21:27.695305: step 37834, loss 0.165009, acc 0.9375
2017-03-02T18:21:27.762011: step 37835, loss 0.0701297, acc 0.96875
2017-03-02T18:21:27.827645: step 37836, loss 0.145276, acc 0.9375
2017-03-02T18:21:27.897408: step 37837, loss 0.132909, acc 0.9375
2017-03-02T18:21:27.977605: step 37838, loss 0.140122, acc 0.921875
2017-03-02T18:21:28.049275: step 37839, loss 0.227612, acc 0.859375
2017-03-02T18:21:28.121943: step 37840, loss 0.132403, acc 0.9375
2017-03-02T18:21:28.191997: step 37841, loss 0.213334, acc 0.921875
2017-03-02T18:21:28.262533: step 37842, loss 0.0862765, acc 0.96875
2017-03-02T18:21:28.338196: step 37843, loss 0.289497, acc 0.875
2017-03-02T18:21:28.401575: step 37844, loss 0.13858, acc 0.921875
2017-03-02T18:21:28.472914: step 37845, loss 0.117269, acc 0.9375
2017-03-02T18:21:28.550538: step 37846, loss 0.127318, acc 0.921875
2017-03-02T18:21:28.626885: step 37847, loss 0.158019, acc 0.890625
2017-03-02T18:21:28.698607: step 37848, loss 0.208815, acc 0.90625
2017-03-02T18:21:28.775180: step 37849, loss 0.282639, acc 0.890625
2017-03-02T18:21:28.850639: step 37850, loss 0.0585023, acc 0.96875
2017-03-02T18:21:28.924814: step 37851, loss 0.131604, acc 0.953125
2017-03-02T18:21:28.993040: step 37852, loss 0.0797986, acc 0.96875
2017-03-02T18:21:29.065461: step 37853, loss 0.209444, acc 0.875
2017-03-02T18:21:29.131787: step 37854, loss 0.166287, acc 0.921875
2017-03-02T18:21:29.208132: step 37855, loss 0.113086, acc 0.953125
2017-03-02T18:21:29.283613: step 37856, loss 0.0915044, acc 0.984375
2017-03-02T18:21:29.355971: step 37857, loss 0.220713, acc 0.90625
2017-03-02T18:21:29.432908: step 37858, loss 0.0661397, acc 0.984375
2017-03-02T18:21:29.501086: step 37859, loss 0.151092, acc 0.921875
2017-03-02T18:21:29.578085: step 37860, loss 0.101117, acc 0.9375
2017-03-02T18:21:29.659264: step 37861, loss 0.0835484, acc 0.984375
2017-03-02T18:21:29.732826: step 37862, loss 0.130839, acc 0.953125
2017-03-02T18:21:29.803036: step 37863, loss 0.109365, acc 0.953125
2017-03-02T18:21:29.880555: step 37864, loss 0.163274, acc 0.9375
2017-03-02T18:21:29.953749: step 37865, loss 0.178686, acc 0.953125
2017-03-02T18:21:30.025636: step 37866, loss 0.187522, acc 0.9375
2017-03-02T18:21:30.100905: step 37867, loss 0.176537, acc 0.90625
2017-03-02T18:21:30.169255: step 37868, loss 0.0952871, acc 0.96875
2017-03-02T18:21:30.242093: step 37869, loss 0.137583, acc 0.9375
2017-03-02T18:21:30.324815: step 37870, loss 0.17657, acc 0.921875
2017-03-02T18:21:30.399741: step 37871, loss 0.0693599, acc 0.984375
2017-03-02T18:21:30.470489: step 37872, loss 0.125925, acc 0.921875
2017-03-02T18:21:30.540230: step 37873, loss 0.191654, acc 0.90625
2017-03-02T18:21:30.612641: step 37874, loss 0.0476073, acc 1
2017-03-02T18:21:30.707199: step 37875, loss 0.117766, acc 0.921875
2017-03-02T18:21:30.780036: step 37876, loss 0.0635188, acc 0.984375
2017-03-02T18:21:30.857861: step 37877, loss 0.107049, acc 0.9375
2017-03-02T18:21:30.928746: step 37878, loss 0.162106, acc 0.9375
2017-03-02T18:21:31.002441: step 37879, loss 0.0902255, acc 0.984375
2017-03-02T18:21:31.076173: step 37880, loss 0.145238, acc 0.953125
2017-03-02T18:21:31.142526: step 37881, loss 0.0843083, acc 0.953125
2017-03-02T18:21:31.231424: step 37882, loss 0.0674721, acc 0.96875
2017-03-02T18:21:31.300600: step 37883, loss 0.101408, acc 0.953125
2017-03-02T18:21:31.372196: step 37884, loss 0.0989569, acc 0.953125
2017-03-02T18:21:31.449078: step 37885, loss 0.0920108, acc 0.96875
2017-03-02T18:21:31.524081: step 37886, loss 0.0897835, acc 0.953125
2017-03-02T18:21:31.598159: step 37887, loss 0.0276005, acc 1
2017-03-02T18:21:31.670810: step 37888, loss 0.126462, acc 0.9375
2017-03-02T18:21:31.743498: step 37889, loss 0.171698, acc 0.921875
2017-03-02T18:21:31.832739: step 37890, loss 0.244537, acc 0.890625
2017-03-02T18:21:31.905895: step 37891, loss 0.0898918, acc 0.96875
2017-03-02T18:21:31.971397: step 37892, loss 0.182183, acc 0.921875
2017-03-02T18:21:32.043668: step 37893, loss 0.0604937, acc 0.96875
2017-03-02T18:21:32.118885: step 37894, loss 0.0578255, acc 0.96875
2017-03-02T18:21:32.190315: step 37895, loss 0.114439, acc 0.953125
2017-03-02T18:21:32.262158: step 37896, loss 0.054033, acc 0.984375
2017-03-02T18:21:32.332773: step 37897, loss 0.188005, acc 0.953125
2017-03-02T18:21:32.407727: step 37898, loss 0.160987, acc 0.9375
2017-03-02T18:21:32.477503: step 37899, loss 0.177737, acc 0.90625
2017-03-02T18:21:32.547590: step 37900, loss 0.0776275, acc 0.953125

Evaluation:
2017-03-02T18:21:32.577870: step 37900, loss 5.61883, acc 0.64672

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-37900

2017-03-02T18:21:33.228123: step 37901, loss 0.0666967, acc 0.96875
2017-03-02T18:21:33.289296: step 37902, loss 0.163935, acc 0.90625
2017-03-02T18:21:33.363610: step 37903, loss 0.125445, acc 0.9375
2017-03-02T18:21:33.436280: step 37904, loss 0.181816, acc 0.921875
2017-03-02T18:21:33.509091: step 37905, loss 0.138691, acc 0.9375
2017-03-02T18:21:33.578750: step 37906, loss 0.205725, acc 0.921875
2017-03-02T18:21:33.655597: step 37907, loss 0.206892, acc 0.875
2017-03-02T18:21:33.727433: step 37908, loss 0.203654, acc 0.859375
2017-03-02T18:21:33.793152: step 37909, loss 0.0506792, acc 1
2017-03-02T18:21:33.866007: step 37910, loss 0.17317, acc 0.90625
2017-03-02T18:21:33.930341: step 37911, loss 0.174812, acc 0.953125
2017-03-02T18:21:33.998682: step 37912, loss 0.0822863, acc 0.953125
2017-03-02T18:21:34.073354: step 37913, loss 0.0792465, acc 1
2017-03-02T18:21:34.144118: step 37914, loss 0.171614, acc 0.90625
2017-03-02T18:21:34.218725: step 37915, loss 0.26869, acc 0.875
2017-03-02T18:21:34.291257: step 37916, loss 0.0571142, acc 0.984375
2017-03-02T18:21:34.366112: step 37917, loss 0.110413, acc 0.96875
2017-03-02T18:21:34.436525: step 37918, loss 0.105554, acc 0.9375
2017-03-02T18:21:34.508121: step 37919, loss 0.113319, acc 0.9375
2017-03-02T18:21:34.582215: step 37920, loss 0.137387, acc 0.921875
2017-03-02T18:21:34.647984: step 37921, loss 0.184898, acc 0.921875
2017-03-02T18:21:34.718615: step 37922, loss 0.198568, acc 0.90625
2017-03-02T18:21:34.792514: step 37923, loss 0.108481, acc 0.953125
2017-03-02T18:21:34.862564: step 37924, loss 0.20554, acc 0.859375
2017-03-02T18:21:34.934533: step 37925, loss 0.0833824, acc 0.9375
2017-03-02T18:21:34.994489: step 37926, loss 0.276734, acc 0.921875
2017-03-02T18:21:35.058358: step 37927, loss 0.157929, acc 0.9375
2017-03-02T18:21:35.133607: step 37928, loss 0.147061, acc 0.9375
2017-03-02T18:21:35.200861: step 37929, loss 0.105739, acc 0.953125
2017-03-02T18:21:35.264495: step 37930, loss 0.130253, acc 0.9375
2017-03-02T18:21:35.330509: step 37931, loss 0.262012, acc 0.921875
2017-03-02T18:21:35.402850: step 37932, loss 0.0926435, acc 0.9375
2017-03-02T18:21:35.482670: step 37933, loss 0.160392, acc 0.9375
2017-03-02T18:21:35.559574: step 37934, loss 0.12561, acc 0.9375
2017-03-02T18:21:35.668161: step 37935, loss 0.108157, acc 0.96875
2017-03-02T18:21:35.738563: step 37936, loss 0.0650304, acc 0.984375
2017-03-02T18:21:35.813489: step 37937, loss 0.0713006, acc 0.96875
2017-03-02T18:21:35.884075: step 37938, loss 0.137218, acc 0.921875
2017-03-02T18:21:35.955984: step 37939, loss 0.113958, acc 0.9375
2017-03-02T18:21:36.023364: step 37940, loss 0.0811514, acc 0.953125
2017-03-02T18:21:36.087751: step 37941, loss 0.130245, acc 0.953125
2017-03-02T18:21:36.160695: step 37942, loss 0.0440705, acc 0.984375
2017-03-02T18:21:36.239087: step 37943, loss 0.178995, acc 0.9375
2017-03-02T18:21:36.313658: step 37944, loss 0.153872, acc 0.9375
2017-03-02T18:21:36.386267: step 37945, loss 0.076542, acc 0.953125
2017-03-02T18:21:36.458585: step 37946, loss 0.156137, acc 0.921875
2017-03-02T18:21:36.529791: step 37947, loss 0.126229, acc 0.953125
2017-03-02T18:21:36.602131: step 37948, loss 0.116922, acc 0.9375
2017-03-02T18:21:36.669240: step 37949, loss 0.116948, acc 0.9375
2017-03-02T18:21:36.743520: step 37950, loss 0.206818, acc 0.890625
2017-03-02T18:21:36.800230: step 37951, loss 0.133005, acc 0.9375
2017-03-02T18:21:36.886065: step 37952, loss 0.322347, acc 0.84375
2017-03-02T18:21:36.957353: step 37953, loss 0.173688, acc 0.90625
2017-03-02T18:21:37.025658: step 37954, loss 0.0823824, acc 0.96875
2017-03-02T18:21:37.107138: step 37955, loss 0.0623004, acc 0.96875
2017-03-02T18:21:37.180027: step 37956, loss 0.185809, acc 0.890625
2017-03-02T18:21:37.252022: step 37957, loss 0.107083, acc 0.921875
2017-03-02T18:21:37.322895: step 37958, loss 0.191939, acc 0.875
2017-03-02T18:21:37.390673: step 37959, loss 0.214347, acc 0.90625
2017-03-02T18:21:37.454459: step 37960, loss 0.250563, acc 0.90625
2017-03-02T18:21:37.539277: step 37961, loss 0.0659908, acc 0.96875
2017-03-02T18:21:37.611488: step 37962, loss 0.0819357, acc 0.96875
2017-03-02T18:21:37.683321: step 37963, loss 0.139578, acc 0.921875
2017-03-02T18:21:37.755992: step 37964, loss 0.0886571, acc 0.96875
2017-03-02T18:21:37.833084: step 37965, loss 0.24666, acc 0.875
2017-03-02T18:21:37.901596: step 37966, loss 0.132873, acc 0.96875
2017-03-02T18:21:37.986476: step 37967, loss 0.232396, acc 0.875
2017-03-02T18:21:38.058828: step 37968, loss 0.197189, acc 0.890625
2017-03-02T18:21:38.126048: step 37969, loss 0.0898212, acc 0.953125
2017-03-02T18:21:38.199239: step 37970, loss 0.161748, acc 0.953125
2017-03-02T18:21:38.271452: step 37971, loss 0.113738, acc 0.953125
2017-03-02T18:21:38.345045: step 37972, loss 0.0932286, acc 0.96875
2017-03-02T18:21:38.416546: step 37973, loss 0.103503, acc 0.9375
2017-03-02T18:21:38.488032: step 37974, loss 0.135045, acc 0.96875
2017-03-02T18:21:38.564720: step 37975, loss 0.119242, acc 0.953125
2017-03-02T18:21:38.641365: step 37976, loss 0.119262, acc 0.9375
2017-03-02T18:21:38.709845: step 37977, loss 0.145471, acc 0.921875
2017-03-02T18:21:38.781814: step 37978, loss 0.0877619, acc 0.984375
2017-03-02T18:21:38.846012: step 37979, loss 0.124787, acc 0.921875
2017-03-02T18:21:38.920749: step 37980, loss 0.127438, acc 0.96875
2017-03-02T18:21:38.997778: step 37981, loss 0.172377, acc 0.90625
2017-03-02T18:21:39.059249: step 37982, loss 0.308597, acc 0.875
2017-03-02T18:21:39.143352: step 37983, loss 0.192159, acc 0.921875
2017-03-02T18:21:39.216140: step 37984, loss 0.15057, acc 0.9375
2017-03-02T18:21:39.289727: step 37985, loss 0.199399, acc 0.90625
2017-03-02T18:21:39.374842: step 37986, loss 0.122251, acc 0.921875
2017-03-02T18:21:39.443627: step 37987, loss 0.15401, acc 0.90625
2017-03-02T18:21:39.510953: step 37988, loss 0.0775867, acc 0.96875
2017-03-02T18:21:39.588127: step 37989, loss 0.127411, acc 0.9375
2017-03-02T18:21:39.668506: step 37990, loss 0.160605, acc 0.90625
2017-03-02T18:21:39.745703: step 37991, loss 0.303957, acc 0.859375
2017-03-02T18:21:39.821345: step 37992, loss 0.203032, acc 0.875
2017-03-02T18:21:39.888610: step 37993, loss 0.16491, acc 0.953125
2017-03-02T18:21:39.956666: step 37994, loss 0.122089, acc 0.953125
2017-03-02T18:21:40.025692: step 37995, loss 0.180284, acc 0.9375
2017-03-02T18:21:40.096223: step 37996, loss 0.130648, acc 0.96875
2017-03-02T18:21:40.190490: step 37997, loss 0.0490583, acc 0.96875
2017-03-02T18:21:40.254679: step 37998, loss 0.101824, acc 0.953125
2017-03-02T18:21:40.331339: step 37999, loss 0.179893, acc 0.921875
2017-03-02T18:21:40.403173: step 38000, loss 0.125534, acc 0.90625

Evaluation:
2017-03-02T18:21:40.436515: step 38000, loss 5.56059, acc 0.627253

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38000

2017-03-02T18:21:40.881335: step 38001, loss 0.08887, acc 0.953125
2017-03-02T18:21:40.953908: step 38002, loss 0.145887, acc 0.90625
2017-03-02T18:21:41.029082: step 38003, loss 0.227545, acc 0.90625
2017-03-02T18:21:41.101540: step 38004, loss 0.181464, acc 0.90625
2017-03-02T18:21:41.175736: step 38005, loss 0.144244, acc 0.9375
2017-03-02T18:21:41.250803: step 38006, loss 0.129347, acc 0.96875
2017-03-02T18:21:41.317183: step 38007, loss 0.113703, acc 0.9375
2017-03-02T18:21:41.387840: step 38008, loss 0.150013, acc 0.9375
2017-03-02T18:21:41.463843: step 38009, loss 0.321223, acc 0.875
2017-03-02T18:21:41.541929: step 38010, loss 0.155034, acc 0.90625
2017-03-02T18:21:41.604942: step 38011, loss 0.162948, acc 0.921875
2017-03-02T18:21:41.671771: step 38012, loss 0.114287, acc 0.953125
2017-03-02T18:21:41.753893: step 38013, loss 0.0968846, acc 0.984375
2017-03-02T18:21:41.825244: step 38014, loss 0.223128, acc 0.84375
2017-03-02T18:21:41.899457: step 38015, loss 0.119529, acc 0.9375
2017-03-02T18:21:41.967160: step 38016, loss 0.240934, acc 0.890625
2017-03-02T18:21:42.043718: step 38017, loss 0.13939, acc 0.9375
2017-03-02T18:21:42.122592: step 38018, loss 0.269322, acc 0.90625
2017-03-02T18:21:42.191388: step 38019, loss 0.13182, acc 0.9375
2017-03-02T18:21:42.257226: step 38020, loss 0.0948576, acc 0.96875
2017-03-02T18:21:42.328162: step 38021, loss 0.117486, acc 0.953125
2017-03-02T18:21:42.411815: step 38022, loss 0.230167, acc 0.890625
2017-03-02T18:21:42.483878: step 38023, loss 0.140531, acc 0.90625
2017-03-02T18:21:42.554254: step 38024, loss 0.109581, acc 1
2017-03-02T18:21:42.628223: step 38025, loss 0.0949595, acc 0.953125
2017-03-02T18:21:42.703597: step 38026, loss 0.104373, acc 0.9375
2017-03-02T18:21:42.763840: step 38027, loss 0.147461, acc 0.921875
2017-03-02T18:21:42.835178: step 38028, loss 0.0527172, acc 0.96875
2017-03-02T18:21:42.908227: step 38029, loss 0.22416, acc 0.84375
2017-03-02T18:21:42.975259: step 38030, loss 0.26893, acc 0.890625
2017-03-02T18:21:43.047886: step 38031, loss 0.14434, acc 0.9375
2017-03-02T18:21:43.118994: step 38032, loss 0.131492, acc 0.921875
2017-03-02T18:21:43.192762: step 38033, loss 0.161172, acc 0.90625
2017-03-02T18:21:43.265929: step 38034, loss 0.174337, acc 0.90625
2017-03-02T18:21:43.338386: step 38035, loss 0.150669, acc 0.921875
2017-03-02T18:21:43.398622: step 38036, loss 0.0514782, acc 0.984375
2017-03-02T18:21:43.472358: step 38037, loss 0.0696836, acc 0.96875
2017-03-02T18:21:43.559764: step 38038, loss 0.110657, acc 0.953125
2017-03-02T18:21:43.624708: step 38039, loss 0.24111, acc 0.90625
2017-03-02T18:21:43.692505: step 38040, loss 0.17926, acc 0.890625
2017-03-02T18:21:43.767252: step 38041, loss 0.0975901, acc 0.984375
2017-03-02T18:21:43.837142: step 38042, loss 0.0627051, acc 0.96875
2017-03-02T18:21:43.931523: step 38043, loss 0.0982459, acc 0.984375
2017-03-02T18:21:44.004438: step 38044, loss 0.0973664, acc 0.96875
2017-03-02T18:21:44.081105: step 38045, loss 0.131277, acc 0.9375
2017-03-02T18:21:44.163137: step 38046, loss 0.112142, acc 0.953125
2017-03-02T18:21:44.233299: step 38047, loss 0.144014, acc 0.9375
2017-03-02T18:21:44.301334: step 38048, loss 0.102378, acc 0.9375
2017-03-02T18:21:44.370269: step 38049, loss 0.209155, acc 0.921875
2017-03-02T18:21:44.443540: step 38050, loss 0.109135, acc 0.9375
2017-03-02T18:21:44.517872: step 38051, loss 0.0908816, acc 0.984375
2017-03-02T18:21:44.588961: step 38052, loss 0.101261, acc 0.984375
2017-03-02T18:21:44.662855: step 38053, loss 0.176201, acc 0.9375
2017-03-02T18:21:44.737030: step 38054, loss 0.187283, acc 0.921875
2017-03-02T18:21:44.818812: step 38055, loss 0.225179, acc 0.90625
2017-03-02T18:21:44.887375: step 38056, loss 0.182993, acc 0.90625
2017-03-02T18:21:44.960092: step 38057, loss 0.123401, acc 0.9375
2017-03-02T18:21:45.033488: step 38058, loss 0.115021, acc 0.921875
2017-03-02T18:21:45.107635: step 38059, loss 0.0958894, acc 0.953125
2017-03-02T18:21:45.178466: step 38060, loss 0.209894, acc 0.921875
2017-03-02T18:21:45.250663: step 38061, loss 0.216757, acc 0.921875
2017-03-02T18:21:45.319374: step 38062, loss 0.142086, acc 0.921875
2017-03-02T18:21:45.390310: step 38063, loss 0.288808, acc 0.890625
2017-03-02T18:21:45.460614: step 38064, loss 0.13136, acc 0.9375
2017-03-02T18:21:45.532766: step 38065, loss 0.12283, acc 0.96875
2017-03-02T18:21:45.607842: step 38066, loss 0.124982, acc 0.9375
2017-03-02T18:21:45.675230: step 38067, loss 0.126189, acc 0.921875
2017-03-02T18:21:45.746413: step 38068, loss 0.0646586, acc 0.984375
2017-03-02T18:21:45.817960: step 38069, loss 0.144608, acc 0.921875
2017-03-02T18:21:45.894726: step 38070, loss 0.0583955, acc 0.984375
2017-03-02T18:21:45.966372: step 38071, loss 0.0886132, acc 0.953125
2017-03-02T18:21:46.036854: step 38072, loss 0.118439, acc 0.953125
2017-03-02T18:21:46.108658: step 38073, loss 0.0587507, acc 0.96875
2017-03-02T18:21:46.181477: step 38074, loss 0.204534, acc 0.90625
2017-03-02T18:21:46.252852: step 38075, loss 0.207923, acc 0.921875
2017-03-02T18:21:46.323921: step 38076, loss 0.0920145, acc 0.953125
2017-03-02T18:21:46.399702: step 38077, loss 0.184599, acc 0.921875
2017-03-02T18:21:46.468753: step 38078, loss 0.139294, acc 0.9375
2017-03-02T18:21:46.552730: step 38079, loss 0.0823489, acc 0.96875
2017-03-02T18:21:46.631010: step 38080, loss 0.21123, acc 0.890625
2017-03-02T18:21:46.706278: step 38081, loss 0.166714, acc 0.9375
2017-03-02T18:21:46.783160: step 38082, loss 0.0913635, acc 0.953125
2017-03-02T18:21:46.852636: step 38083, loss 0.106237, acc 0.9375
2017-03-02T18:21:46.932436: step 38084, loss 0.182303, acc 0.9375
2017-03-02T18:21:47.002392: step 38085, loss 0.162529, acc 0.9375
2017-03-02T18:21:47.067631: step 38086, loss 0.251717, acc 0.890625
2017-03-02T18:21:47.146667: step 38087, loss 0.140014, acc 0.96875
2017-03-02T18:21:47.229601: step 38088, loss 0.201306, acc 0.921875
2017-03-02T18:21:47.301618: step 38089, loss 0.0902915, acc 0.953125
2017-03-02T18:21:47.374966: step 38090, loss 0.101021, acc 0.9375
2017-03-02T18:21:47.445633: step 38091, loss 0.155461, acc 0.9375
2017-03-02T18:21:47.517172: step 38092, loss 0.178267, acc 0.921875
2017-03-02T18:21:47.592307: step 38093, loss 0.0708532, acc 0.984375
2017-03-02T18:21:47.666972: step 38094, loss 0.22316, acc 0.90625
2017-03-02T18:21:47.744192: step 38095, loss 0.09973, acc 0.953125
2017-03-02T18:21:47.816876: step 38096, loss 0.140432, acc 0.9375
2017-03-02T18:21:47.881881: step 38097, loss 0.109215, acc 0.953125
2017-03-02T18:21:47.954912: step 38098, loss 0.0964573, acc 0.96875
2017-03-02T18:21:48.030487: step 38099, loss 0.0808398, acc 0.96875
2017-03-02T18:21:48.103886: step 38100, loss 0.136426, acc 0.921875

Evaluation:
2017-03-02T18:21:48.144826: step 38100, loss 5.54186, acc 0.630858

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38100

2017-03-02T18:21:48.616228: step 38101, loss 0.16111, acc 0.921875
2017-03-02T18:21:48.695111: step 38102, loss 0.169266, acc 0.890625
2017-03-02T18:21:48.764633: step 38103, loss 0.0853383, acc 0.953125
2017-03-02T18:21:48.845455: step 38104, loss 0.0474506, acc 0.984375
2017-03-02T18:21:48.924157: step 38105, loss 0.100613, acc 0.9375
2017-03-02T18:21:48.992794: step 38106, loss 0.209046, acc 0.875
2017-03-02T18:21:49.073164: step 38107, loss 0.31006, acc 0.859375
2017-03-02T18:21:49.145734: step 38108, loss 0.149516, acc 0.90625
2017-03-02T18:21:49.215151: step 38109, loss 0.133714, acc 0.921875
2017-03-02T18:21:49.288563: step 38110, loss 0.0856054, acc 0.96875
2017-03-02T18:21:49.360518: step 38111, loss 0.160562, acc 0.9375
2017-03-02T18:21:49.431079: step 38112, loss 0.130462, acc 0.90625
2017-03-02T18:21:49.503505: step 38113, loss 0.161815, acc 0.90625
2017-03-02T18:21:49.575981: step 38114, loss 0.115958, acc 0.9375
2017-03-02T18:21:49.648861: step 38115, loss 0.14642, acc 0.921875
2017-03-02T18:21:49.722414: step 38116, loss 0.145562, acc 0.9375
2017-03-02T18:21:49.789552: step 38117, loss 0.143519, acc 0.9375
2017-03-02T18:21:49.855729: step 38118, loss 0.224961, acc 0.90625
2017-03-02T18:21:49.917788: step 38119, loss 0.134841, acc 0.9375
2017-03-02T18:21:49.986158: step 38120, loss 0.155403, acc 0.953125
2017-03-02T18:21:50.056421: step 38121, loss 0.161964, acc 0.90625
2017-03-02T18:21:50.137233: step 38122, loss 0.132688, acc 0.9375
2017-03-02T18:21:50.213314: step 38123, loss 0.0361658, acc 1
2017-03-02T18:21:50.284479: step 38124, loss 0.0955308, acc 0.96875
2017-03-02T18:21:50.356132: step 38125, loss 0.126079, acc 0.921875
2017-03-02T18:21:50.438705: step 38126, loss 0.178276, acc 0.921875
2017-03-02T18:21:50.524042: step 38127, loss 0.202541, acc 0.921875
2017-03-02T18:21:50.593192: step 38128, loss 0.0361803, acc 0.984375
2017-03-02T18:21:50.673791: step 38129, loss 0.147724, acc 0.9375
2017-03-02T18:21:50.753854: step 38130, loss 0.178706, acc 0.9375
2017-03-02T18:21:50.825179: step 38131, loss 0.0614311, acc 0.96875
2017-03-02T18:21:50.896322: step 38132, loss 0.137803, acc 0.953125
2017-03-02T18:21:50.970271: step 38133, loss 0.326192, acc 0.890625
2017-03-02T18:21:51.044809: step 38134, loss 0.221611, acc 0.90625
2017-03-02T18:21:51.120436: step 38135, loss 0.239067, acc 0.890625
2017-03-02T18:21:51.188865: step 38136, loss 0.154455, acc 0.890625
2017-03-02T18:21:51.255296: step 38137, loss 0.124357, acc 0.9375
2017-03-02T18:21:51.331468: step 38138, loss 0.115354, acc 0.9375
2017-03-02T18:21:51.404758: step 38139, loss 0.0977869, acc 0.96875
2017-03-02T18:21:51.477023: step 38140, loss 0.0759633, acc 0.96875
2017-03-02T18:21:51.553881: step 38141, loss 0.108401, acc 0.96875
2017-03-02T18:21:51.625793: step 38142, loss 0.146278, acc 0.921875
2017-03-02T18:21:51.699492: step 38143, loss 0.183167, acc 0.921875
2017-03-02T18:21:51.777496: step 38144, loss 0.119504, acc 0.921875
2017-03-02T18:21:51.848726: step 38145, loss 0.096399, acc 0.9375
2017-03-02T18:21:51.916375: step 38146, loss 0.092556, acc 0.96875
2017-03-02T18:21:51.985651: step 38147, loss 0.150896, acc 0.90625
2017-03-02T18:21:52.059064: step 38148, loss 0.131425, acc 0.953125
2017-03-02T18:21:52.129835: step 38149, loss 0.156534, acc 0.921875
2017-03-02T18:21:52.218373: step 38150, loss 0.204685, acc 0.875
2017-03-02T18:21:52.293105: step 38151, loss 0.140676, acc 0.921875
2017-03-02T18:21:52.367772: step 38152, loss 0.136043, acc 0.90625
2017-03-02T18:21:52.438311: step 38153, loss 0.134398, acc 0.96875
2017-03-02T18:21:52.506189: step 38154, loss 0.0539165, acc 0.953125
2017-03-02T18:21:52.576106: step 38155, loss 0.125922, acc 0.921875
2017-03-02T18:21:52.645549: step 38156, loss 0.0520733, acc 0.984375
2017-03-02T18:21:52.717086: step 38157, loss 0.0375886, acc 1
2017-03-02T18:21:52.790015: step 38158, loss 0.190065, acc 0.90625
2017-03-02T18:21:52.866807: step 38159, loss 0.128364, acc 0.953125
2017-03-02T18:21:52.945731: step 38160, loss 0.145275, acc 0.90625
2017-03-02T18:21:53.013347: step 38161, loss 0.229999, acc 0.90625
2017-03-02T18:21:53.079174: step 38162, loss 0.250052, acc 0.890625
2017-03-02T18:21:53.149478: step 38163, loss 0.0700245, acc 0.984375
2017-03-02T18:21:53.220613: step 38164, loss 0.0813283, acc 0.96875
2017-03-02T18:21:53.302046: step 38165, loss 0.14955, acc 0.9375
2017-03-02T18:21:53.396918: step 38166, loss 0.0960766, acc 0.96875
2017-03-02T18:21:53.466943: step 38167, loss 0.134895, acc 0.90625
2017-03-02T18:21:53.541979: step 38168, loss 0.228162, acc 0.890625
2017-03-02T18:21:53.617329: step 38169, loss 0.111973, acc 0.953125
2017-03-02T18:21:53.689607: step 38170, loss 0.0454719, acc 0.984375
2017-03-02T18:21:53.765048: step 38171, loss 0.0871519, acc 0.953125
2017-03-02T18:21:53.837354: step 38172, loss 0.11719, acc 0.921875
2017-03-02T18:21:53.911508: step 38173, loss 0.142238, acc 0.921875
2017-03-02T18:21:53.978138: step 38174, loss 0.101357, acc 0.96875
2017-03-02T18:21:54.048015: step 38175, loss 0.108362, acc 0.96875
2017-03-02T18:21:54.127120: step 38176, loss 0.147914, acc 0.90625
2017-03-02T18:21:54.198860: step 38177, loss 0.181304, acc 0.921875
2017-03-02T18:21:54.270117: step 38178, loss 0.187837, acc 0.90625
2017-03-02T18:21:54.345933: step 38179, loss 0.138327, acc 0.9375
2017-03-02T18:21:54.415027: step 38180, loss 0.128939, acc 0.96875
2017-03-02T18:21:54.484734: step 38181, loss 0.235971, acc 0.859375
2017-03-02T18:21:54.557001: step 38182, loss 0.0902942, acc 0.984375
2017-03-02T18:21:54.631836: step 38183, loss 0.206886, acc 0.890625
2017-03-02T18:21:54.700852: step 38184, loss 0.103973, acc 0.921875
2017-03-02T18:21:54.769901: step 38185, loss 0.0633741, acc 0.984375
2017-03-02T18:21:54.839783: step 38186, loss 0.214718, acc 0.875
2017-03-02T18:21:54.920784: step 38187, loss 0.0771441, acc 0.96875
2017-03-02T18:21:54.992904: step 38188, loss 0.178665, acc 0.921875
2017-03-02T18:21:55.076877: step 38189, loss 0.112575, acc 0.953125
2017-03-02T18:21:55.176349: step 38190, loss 0.206875, acc 0.9375
2017-03-02T18:21:55.242386: step 38191, loss 0.0565125, acc 0.984375
2017-03-02T18:21:55.316226: step 38192, loss 0.140517, acc 0.96875
2017-03-02T18:21:55.385291: step 38193, loss 0.0770346, acc 0.984375
2017-03-02T18:21:55.461029: step 38194, loss 0.218725, acc 0.875
2017-03-02T18:21:55.534767: step 38195, loss 0.115026, acc 0.96875
2017-03-02T18:21:55.606832: step 38196, loss 0.138975, acc 0.921875
2017-03-02T18:21:55.689636: step 38197, loss 0.175937, acc 0.96875
2017-03-02T18:21:55.767278: step 38198, loss 0.0434311, acc 0.984375
2017-03-02T18:21:55.838573: step 38199, loss 0.151101, acc 0.9375
2017-03-02T18:21:55.912410: step 38200, loss 0.130985, acc 0.96875

Evaluation:
2017-03-02T18:21:55.950618: step 38200, loss 5.66429, acc 0.629416

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38200

2017-03-02T18:21:56.385281: step 38201, loss 0.0530888, acc 0.96875
2017-03-02T18:21:56.453458: step 38202, loss 0.0922981, acc 0.96875
2017-03-02T18:21:56.528844: step 38203, loss 0.205236, acc 0.921875
2017-03-02T18:21:56.597425: step 38204, loss 0.172688, acc 0.921875
2017-03-02T18:21:56.667853: step 38205, loss 0.228714, acc 0.890625
2017-03-02T18:21:56.740084: step 38206, loss 0.108399, acc 0.953125
2017-03-02T18:21:56.818768: step 38207, loss 0.168579, acc 0.890625
2017-03-02T18:21:56.894209: step 38208, loss 0.219834, acc 0.875
2017-03-02T18:21:56.975119: step 38209, loss 0.157277, acc 0.921875
2017-03-02T18:21:57.051217: step 38210, loss 0.21208, acc 0.9375
2017-03-02T18:21:57.126298: step 38211, loss 0.11219, acc 0.953125
2017-03-02T18:21:57.197872: step 38212, loss 0.201452, acc 0.921875
2017-03-02T18:21:57.268341: step 38213, loss 0.150424, acc 0.921875
2017-03-02T18:21:57.341237: step 38214, loss 0.160089, acc 0.90625
2017-03-02T18:21:57.417282: step 38215, loss 0.175274, acc 0.921875
2017-03-02T18:21:57.496119: step 38216, loss 0.0955079, acc 0.9375
2017-03-02T18:21:57.570626: step 38217, loss 0.177509, acc 0.90625
2017-03-02T18:21:57.642806: step 38218, loss 0.332931, acc 0.890625
2017-03-02T18:21:57.716065: step 38219, loss 0.211293, acc 0.921875
2017-03-02T18:21:57.790969: step 38220, loss 0, acc 1
2017-03-02T18:21:57.883884: step 38221, loss 0.0579329, acc 0.984375
2017-03-02T18:21:57.968762: step 38222, loss 0.0853803, acc 0.96875
2017-03-02T18:21:58.038576: step 38223, loss 0.0924443, acc 0.96875
2017-03-02T18:21:58.112077: step 38224, loss 0.0773175, acc 0.96875
2017-03-02T18:21:58.179517: step 38225, loss 0.116232, acc 0.96875
2017-03-02T18:21:58.249564: step 38226, loss 0.127304, acc 0.9375
2017-03-02T18:21:58.327650: step 38227, loss 0.0669968, acc 0.953125
2017-03-02T18:21:58.404158: step 38228, loss 0.150576, acc 0.90625
2017-03-02T18:21:58.478899: step 38229, loss 0.138904, acc 0.9375
2017-03-02T18:21:58.551375: step 38230, loss 0.203051, acc 0.890625
2017-03-02T18:21:58.622767: step 38231, loss 0.162734, acc 0.9375
2017-03-02T18:21:58.690759: step 38232, loss 0.195659, acc 0.921875
2017-03-02T18:21:58.767246: step 38233, loss 0.197904, acc 0.890625
2017-03-02T18:21:58.838763: step 38234, loss 0.153854, acc 0.921875
2017-03-02T18:21:58.908487: step 38235, loss 0.20945, acc 0.90625
2017-03-02T18:21:58.972901: step 38236, loss 0.129382, acc 0.921875
2017-03-02T18:21:59.047715: step 38237, loss 0.201604, acc 0.90625
2017-03-02T18:21:59.119842: step 38238, loss 0.139588, acc 0.96875
2017-03-02T18:21:59.188555: step 38239, loss 0.324619, acc 0.921875
2017-03-02T18:21:59.258765: step 38240, loss 0.0968079, acc 0.96875
2017-03-02T18:21:59.334925: step 38241, loss 0.164145, acc 0.9375
2017-03-02T18:21:59.409275: step 38242, loss 0.207528, acc 0.90625
2017-03-02T18:21:59.477919: step 38243, loss 0.22589, acc 0.921875
2017-03-02T18:21:59.554348: step 38244, loss 0.0745461, acc 0.96875
2017-03-02T18:21:59.622244: step 38245, loss 0.0499757, acc 0.984375
2017-03-02T18:21:59.694206: step 38246, loss 0.182497, acc 0.875
2017-03-02T18:21:59.765518: step 38247, loss 0.0830731, acc 0.9375
2017-03-02T18:21:59.835589: step 38248, loss 0.180715, acc 0.90625
2017-03-02T18:21:59.917541: step 38249, loss 0.129736, acc 0.9375
2017-03-02T18:21:59.991533: step 38250, loss 0.140299, acc 0.9375
2017-03-02T18:22:00.058566: step 38251, loss 0.154081, acc 0.90625
2017-03-02T18:22:00.132441: step 38252, loss 0.117379, acc 0.953125
2017-03-02T18:22:00.208045: step 38253, loss 0.0855157, acc 0.96875
2017-03-02T18:22:00.277195: step 38254, loss 0.128453, acc 0.90625
2017-03-02T18:22:00.351833: step 38255, loss 0.194153, acc 0.921875
2017-03-02T18:22:00.427436: step 38256, loss 0.140278, acc 0.921875
2017-03-02T18:22:00.500077: step 38257, loss 0.146606, acc 0.9375
2017-03-02T18:22:00.583106: step 38258, loss 0.223224, acc 0.9375
2017-03-02T18:22:00.656551: step 38259, loss 0.119158, acc 0.921875
2017-03-02T18:22:00.732743: step 38260, loss 0.182683, acc 0.90625
2017-03-02T18:22:00.808783: step 38261, loss 0.319966, acc 0.859375
2017-03-02T18:22:00.883348: step 38262, loss 0.129917, acc 0.953125
2017-03-02T18:22:00.951047: step 38263, loss 0.133166, acc 0.90625
2017-03-02T18:22:01.043121: step 38264, loss 0.178276, acc 0.921875
2017-03-02T18:22:01.118256: step 38265, loss 0.138701, acc 0.921875
2017-03-02T18:22:01.194587: step 38266, loss 0.126747, acc 0.96875
2017-03-02T18:22:01.265569: step 38267, loss 0.123866, acc 0.953125
2017-03-02T18:22:01.338479: step 38268, loss 0.0839328, acc 0.984375
2017-03-02T18:22:01.409448: step 38269, loss 0.157951, acc 0.90625
2017-03-02T18:22:01.481669: step 38270, loss 0.130108, acc 0.96875
2017-03-02T18:22:01.554023: step 38271, loss 0.119473, acc 0.96875
2017-03-02T18:22:01.619667: step 38272, loss 0.168543, acc 0.921875
2017-03-02T18:22:01.682737: step 38273, loss 0.126417, acc 0.953125
2017-03-02T18:22:01.754091: step 38274, loss 0.101718, acc 0.953125
2017-03-02T18:22:01.827333: step 38275, loss 0.15223, acc 0.921875
2017-03-02T18:22:01.901748: step 38276, loss 0.130491, acc 0.921875
2017-03-02T18:22:01.974983: step 38277, loss 0.105007, acc 0.9375
2017-03-02T18:22:02.059253: step 38278, loss 0.110089, acc 0.953125
2017-03-02T18:22:02.131082: step 38279, loss 0.165304, acc 0.890625
2017-03-02T18:22:02.201430: step 38280, loss 0.164939, acc 0.953125
2017-03-02T18:22:02.273335: step 38281, loss 0.125587, acc 0.96875
2017-03-02T18:22:02.350599: step 38282, loss 0.151107, acc 0.9375
2017-03-02T18:22:02.430139: step 38283, loss 0.0752752, acc 0.96875
2017-03-02T18:22:02.503003: step 38284, loss 0.194873, acc 0.921875
2017-03-02T18:22:02.573939: step 38285, loss 0.163281, acc 0.953125
2017-03-02T18:22:02.652114: step 38286, loss 0.135574, acc 0.9375
2017-03-02T18:22:02.735074: step 38287, loss 0.171004, acc 0.921875
2017-03-02T18:22:02.810272: step 38288, loss 0.10724, acc 0.90625
2017-03-02T18:22:02.881535: step 38289, loss 0.186018, acc 0.921875
2017-03-02T18:22:02.953423: step 38290, loss 0.200559, acc 0.875
2017-03-02T18:22:03.018932: step 38291, loss 0.20213, acc 0.921875
2017-03-02T18:22:03.087854: step 38292, loss 0.0535427, acc 0.984375
2017-03-02T18:22:03.160622: step 38293, loss 0.216383, acc 0.921875
2017-03-02T18:22:03.232530: step 38294, loss 0.159904, acc 0.90625
2017-03-02T18:22:03.302851: step 38295, loss 0.149563, acc 0.9375
2017-03-02T18:22:03.371305: step 38296, loss 0.118294, acc 0.96875
2017-03-02T18:22:03.444073: step 38297, loss 0.123066, acc 0.9375
2017-03-02T18:22:03.518385: step 38298, loss 0.159569, acc 0.890625
2017-03-02T18:22:03.590230: step 38299, loss 0.105769, acc 0.96875
2017-03-02T18:22:03.676249: step 38300, loss 0.129457, acc 0.953125

Evaluation:
2017-03-02T18:22:03.710854: step 38300, loss 5.82212, acc 0.629416

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38300

2017-03-02T18:22:04.162981: step 38301, loss 0.105696, acc 0.953125
2017-03-02T18:22:04.236067: step 38302, loss 0.0546487, acc 0.984375
2017-03-02T18:22:04.311742: step 38303, loss 0.0983429, acc 0.953125
2017-03-02T18:22:04.381271: step 38304, loss 0.126384, acc 0.9375
2017-03-02T18:22:04.452661: step 38305, loss 0.247334, acc 0.921875
2017-03-02T18:22:04.539473: step 38306, loss 0.138027, acc 0.9375
2017-03-02T18:22:04.609099: step 38307, loss 0.106419, acc 0.9375
2017-03-02T18:22:04.686660: step 38308, loss 0.265652, acc 0.90625
2017-03-02T18:22:04.771960: step 38309, loss 0.228302, acc 0.875
2017-03-02T18:22:04.839978: step 38310, loss 0.218994, acc 0.875
2017-03-02T18:22:04.913447: step 38311, loss 0.215273, acc 0.921875
2017-03-02T18:22:04.985601: step 38312, loss 0.0739831, acc 0.96875
2017-03-02T18:22:05.064390: step 38313, loss 0.0540417, acc 0.96875
2017-03-02T18:22:05.130314: step 38314, loss 0.147976, acc 0.953125
2017-03-02T18:22:05.200624: step 38315, loss 0.193301, acc 0.90625
2017-03-02T18:22:05.279855: step 38316, loss 0.128634, acc 0.921875
2017-03-02T18:22:05.362754: step 38317, loss 0.197631, acc 0.90625
2017-03-02T18:22:05.439376: step 38318, loss 0.124365, acc 0.921875
2017-03-02T18:22:05.537945: step 38319, loss 0.111284, acc 0.9375
2017-03-02T18:22:05.613707: step 38320, loss 0.0502178, acc 0.96875
2017-03-02T18:22:05.680136: step 38321, loss 0.108844, acc 0.9375
2017-03-02T18:22:05.741280: step 38322, loss 0.338777, acc 0.90625
2017-03-02T18:22:05.813717: step 38323, loss 0.228897, acc 0.90625
2017-03-02T18:22:05.886887: step 38324, loss 0.0669214, acc 0.96875
2017-03-02T18:22:05.956853: step 38325, loss 0.116925, acc 0.9375
2017-03-02T18:22:06.024049: step 38326, loss 0.134986, acc 0.90625
2017-03-02T18:22:06.096674: step 38327, loss 0.0952887, acc 0.984375
2017-03-02T18:22:06.169644: step 38328, loss 0.0714325, acc 0.96875
2017-03-02T18:22:06.241760: step 38329, loss 0.126369, acc 0.9375
2017-03-02T18:22:06.314752: step 38330, loss 0.189889, acc 0.90625
2017-03-02T18:22:06.396655: step 38331, loss 0.210013, acc 0.859375
2017-03-02T18:22:06.460304: step 38332, loss 0.0559048, acc 0.984375
2017-03-02T18:22:06.541362: step 38333, loss 0.0989599, acc 0.953125
2017-03-02T18:22:06.616814: step 38334, loss 0.249953, acc 0.90625
2017-03-02T18:22:06.688106: step 38335, loss 0.181508, acc 0.9375
2017-03-02T18:22:06.762327: step 38336, loss 0.219409, acc 0.875
2017-03-02T18:22:06.834506: step 38337, loss 0.170631, acc 0.90625
2017-03-02T18:22:06.912530: step 38338, loss 0.0698452, acc 0.984375
2017-03-02T18:22:06.979756: step 38339, loss 0.125043, acc 0.9375
2017-03-02T18:22:07.055180: step 38340, loss 0.122256, acc 0.921875
2017-03-02T18:22:07.125770: step 38341, loss 0.113558, acc 0.953125
2017-03-02T18:22:07.197797: step 38342, loss 0.129262, acc 0.96875
2017-03-02T18:22:07.267779: step 38343, loss 0.0734768, acc 0.953125
2017-03-02T18:22:07.339362: step 38344, loss 0.171017, acc 0.921875
2017-03-02T18:22:07.414255: step 38345, loss 0.200922, acc 0.921875
2017-03-02T18:22:07.487262: step 38346, loss 0.07045, acc 0.96875
2017-03-02T18:22:07.561082: step 38347, loss 0.14607, acc 0.921875
2017-03-02T18:22:07.635406: step 38348, loss 0.0824005, acc 0.96875
2017-03-02T18:22:07.704946: step 38349, loss 0.125698, acc 0.953125
2017-03-02T18:22:07.790033: step 38350, loss 0.139253, acc 0.953125
2017-03-02T18:22:07.857184: step 38351, loss 0.0651699, acc 0.984375
2017-03-02T18:22:07.922864: step 38352, loss 0.0736151, acc 0.984375
2017-03-02T18:22:07.998489: step 38353, loss 0.108954, acc 0.9375
2017-03-02T18:22:08.068613: step 38354, loss 0.181261, acc 0.90625
2017-03-02T18:22:08.141481: step 38355, loss 0.128989, acc 0.9375
2017-03-02T18:22:08.211389: step 38356, loss 0.0925336, acc 0.953125
2017-03-02T18:22:08.282079: step 38357, loss 0.16725, acc 0.890625
2017-03-02T18:22:08.358664: step 38358, loss 0.104151, acc 0.953125
2017-03-02T18:22:08.439069: step 38359, loss 0.130794, acc 0.953125
2017-03-02T18:22:08.520608: step 38360, loss 0.235372, acc 0.84375
2017-03-02T18:22:08.592291: step 38361, loss 0.0821477, acc 0.953125
2017-03-02T18:22:08.660580: step 38362, loss 0.12015, acc 0.9375
2017-03-02T18:22:08.728595: step 38363, loss 0.132377, acc 0.90625
2017-03-02T18:22:08.799370: step 38364, loss 0.169799, acc 0.9375
2017-03-02T18:22:08.901684: step 38365, loss 0.0911195, acc 0.953125
2017-03-02T18:22:08.978140: step 38366, loss 0.114153, acc 0.953125
2017-03-02T18:22:09.050770: step 38367, loss 0.182533, acc 0.9375
2017-03-02T18:22:09.121408: step 38368, loss 0.210405, acc 0.921875
2017-03-02T18:22:09.190279: step 38369, loss 0.10809, acc 0.9375
2017-03-02T18:22:09.259125: step 38370, loss 0.103649, acc 0.921875
2017-03-02T18:22:09.323726: step 38371, loss 0.100723, acc 0.953125
2017-03-02T18:22:09.397535: step 38372, loss 0.220547, acc 0.921875
2017-03-02T18:22:09.470545: step 38373, loss 0.103474, acc 0.953125
2017-03-02T18:22:09.546240: step 38374, loss 0.131208, acc 0.9375
2017-03-02T18:22:09.618006: step 38375, loss 0.082848, acc 0.984375
2017-03-02T18:22:09.691029: step 38376, loss 0.161242, acc 0.921875
2017-03-02T18:22:09.765630: step 38377, loss 0.124553, acc 0.921875
2017-03-02T18:22:09.844928: step 38378, loss 0.143736, acc 0.9375
2017-03-02T18:22:09.922298: step 38379, loss 0.149328, acc 0.9375
2017-03-02T18:22:10.018084: step 38380, loss 0.187013, acc 0.90625
2017-03-02T18:22:10.093626: step 38381, loss 0.0726236, acc 0.96875
2017-03-02T18:22:10.170038: step 38382, loss 0.111513, acc 0.953125
2017-03-02T18:22:10.251256: step 38383, loss 0.139843, acc 0.9375
2017-03-02T18:22:10.325395: step 38384, loss 0.119412, acc 0.953125
2017-03-02T18:22:10.396434: step 38385, loss 0.0961088, acc 0.953125
2017-03-02T18:22:10.469917: step 38386, loss 0.120696, acc 0.90625
2017-03-02T18:22:10.543505: step 38387, loss 0.342453, acc 0.875
2017-03-02T18:22:10.611669: step 38388, loss 0.170122, acc 0.90625
2017-03-02T18:22:10.680941: step 38389, loss 0.183175, acc 0.890625
2017-03-02T18:22:10.755263: step 38390, loss 0.0989452, acc 0.9375
2017-03-02T18:22:10.829619: step 38391, loss 0.104911, acc 0.921875
2017-03-02T18:22:10.904107: step 38392, loss 0.148841, acc 0.921875
2017-03-02T18:22:10.986357: step 38393, loss 0.154811, acc 0.953125
2017-03-02T18:22:11.057684: step 38394, loss 0.097361, acc 0.9375
2017-03-02T18:22:11.131381: step 38395, loss 0.17615, acc 0.9375
2017-03-02T18:22:11.209369: step 38396, loss 0.0879012, acc 0.96875
2017-03-02T18:22:11.281505: step 38397, loss 0.174303, acc 0.921875
2017-03-02T18:22:11.360350: step 38398, loss 0.253083, acc 0.890625
2017-03-02T18:22:11.425260: step 38399, loss 0.154139, acc 0.953125
2017-03-02T18:22:11.506126: step 38400, loss 0.0571838, acc 1

Evaluation:
2017-03-02T18:22:11.545722: step 38400, loss 5.84869, acc 0.631579

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38400

2017-03-02T18:22:11.983624: step 38401, loss 0.204355, acc 0.953125
2017-03-02T18:22:12.050776: step 38402, loss 0.220371, acc 0.890625
2017-03-02T18:22:12.125034: step 38403, loss 0.123115, acc 0.90625
2017-03-02T18:22:12.191038: step 38404, loss 0.159295, acc 0.9375
2017-03-02T18:22:12.274158: step 38405, loss 0.113858, acc 0.9375
2017-03-02T18:22:12.348011: step 38406, loss 0.153909, acc 0.921875
2017-03-02T18:22:12.422133: step 38407, loss 0.177505, acc 0.9375
2017-03-02T18:22:12.494699: step 38408, loss 0.142025, acc 0.9375
2017-03-02T18:22:12.571604: step 38409, loss 0.14644, acc 0.921875
2017-03-02T18:22:12.639720: step 38410, loss 0.240234, acc 0.859375
2017-03-02T18:22:12.722915: step 38411, loss 0.147333, acc 0.921875
2017-03-02T18:22:12.793444: step 38412, loss 0.0452512, acc 0.96875
2017-03-02T18:22:12.868531: step 38413, loss 0.201646, acc 0.921875
2017-03-02T18:22:12.941821: step 38414, loss 0.154157, acc 0.953125
2017-03-02T18:22:13.016106: step 38415, loss 0.087627, acc 0.953125
2017-03-02T18:22:13.093517: step 38416, loss 0.18812, acc 1
2017-03-02T18:22:13.179972: step 38417, loss 0.0787591, acc 0.953125
2017-03-02T18:22:13.258381: step 38418, loss 0.110464, acc 0.953125
2017-03-02T18:22:13.334614: step 38419, loss 0.0897437, acc 0.96875
2017-03-02T18:22:13.403888: step 38420, loss 0.0999607, acc 0.953125
2017-03-02T18:22:13.475263: step 38421, loss 0.187953, acc 0.890625
2017-03-02T18:22:13.544201: step 38422, loss 0.149741, acc 0.921875
2017-03-02T18:22:13.623595: step 38423, loss 0.111938, acc 0.953125
2017-03-02T18:22:13.695596: step 38424, loss 0.136755, acc 0.9375
2017-03-02T18:22:13.769386: step 38425, loss 0.155166, acc 0.921875
2017-03-02T18:22:13.842235: step 38426, loss 0.167514, acc 0.890625
2017-03-02T18:22:13.916217: step 38427, loss 0.0728437, acc 0.953125
2017-03-02T18:22:13.997871: step 38428, loss 0.0672105, acc 1
2017-03-02T18:22:14.068094: step 38429, loss 0.132394, acc 0.921875
2017-03-02T18:22:14.135630: step 38430, loss 0.143079, acc 0.953125
2017-03-02T18:22:14.210943: step 38431, loss 0.0831364, acc 0.953125
2017-03-02T18:22:14.284974: step 38432, loss 0.163077, acc 0.90625
2017-03-02T18:22:14.366068: step 38433, loss 0.0965898, acc 0.953125
2017-03-02T18:22:14.455997: step 38434, loss 0.175985, acc 0.921875
2017-03-02T18:22:14.530196: step 38435, loss 0.188009, acc 0.90625
2017-03-02T18:22:14.598466: step 38436, loss 0.210263, acc 0.921875
2017-03-02T18:22:14.676310: step 38437, loss 0.178079, acc 0.875
2017-03-02T18:22:14.746784: step 38438, loss 0.148224, acc 0.9375
2017-03-02T18:22:14.827881: step 38439, loss 0.220246, acc 0.890625
2017-03-02T18:22:14.918443: step 38440, loss 0.167758, acc 0.921875
2017-03-02T18:22:14.992069: step 38441, loss 0.228512, acc 0.9375
2017-03-02T18:22:15.063869: step 38442, loss 0.131588, acc 0.921875
2017-03-02T18:22:15.129933: step 38443, loss 0.192902, acc 0.9375
2017-03-02T18:22:15.211339: step 38444, loss 0.0816067, acc 0.953125
2017-03-02T18:22:15.281742: step 38445, loss 0.115977, acc 0.96875
2017-03-02T18:22:15.356763: step 38446, loss 0.134001, acc 0.953125
2017-03-02T18:22:15.424328: step 38447, loss 0.0938022, acc 0.984375
2017-03-02T18:22:15.495400: step 38448, loss 0.260393, acc 0.890625
2017-03-02T18:22:15.575685: step 38449, loss 0.137781, acc 0.953125
2017-03-02T18:22:15.658725: step 38450, loss 0.0887005, acc 0.96875
2017-03-02T18:22:15.733784: step 38451, loss 0.148812, acc 0.90625
2017-03-02T18:22:15.807495: step 38452, loss 0.154843, acc 0.953125
2017-03-02T18:22:15.882028: step 38453, loss 0.123406, acc 0.953125
2017-03-02T18:22:15.955079: step 38454, loss 0.259789, acc 0.859375
2017-03-02T18:22:16.029320: step 38455, loss 0.0699015, acc 0.984375
2017-03-02T18:22:16.096712: step 38456, loss 0.0971903, acc 0.984375
2017-03-02T18:22:16.165101: step 38457, loss 0.226534, acc 0.9375
2017-03-02T18:22:16.238516: step 38458, loss 0.205096, acc 0.9375
2017-03-02T18:22:16.317438: step 38459, loss 0.212714, acc 0.90625
2017-03-02T18:22:16.386846: step 38460, loss 0.151566, acc 0.921875
2017-03-02T18:22:16.458234: step 38461, loss 0.0878264, acc 0.9375
2017-03-02T18:22:16.530503: step 38462, loss 0.230428, acc 0.859375
2017-03-02T18:22:16.599729: step 38463, loss 0.100303, acc 0.96875
2017-03-02T18:22:16.675947: step 38464, loss 0.148007, acc 0.921875
2017-03-02T18:22:16.746871: step 38465, loss 0.139829, acc 0.921875
2017-03-02T18:22:16.821737: step 38466, loss 0.075035, acc 0.984375
2017-03-02T18:22:16.885329: step 38467, loss 0.235584, acc 0.875
2017-03-02T18:22:16.957318: step 38468, loss 0.206071, acc 0.90625
2017-03-02T18:22:17.035075: step 38469, loss 0.0964183, acc 0.96875
2017-03-02T18:22:17.108842: step 38470, loss 0.134175, acc 0.921875
2017-03-02T18:22:17.173271: step 38471, loss 0.150322, acc 0.96875
2017-03-02T18:22:17.257988: step 38472, loss 0.131262, acc 0.9375
2017-03-02T18:22:17.320110: step 38473, loss 0.0947847, acc 0.953125
2017-03-02T18:22:17.392188: step 38474, loss 0.198756, acc 0.921875
2017-03-02T18:22:17.464374: step 38475, loss 0.138001, acc 0.953125
2017-03-02T18:22:17.535255: step 38476, loss 0.181873, acc 0.90625
2017-03-02T18:22:17.605260: step 38477, loss 0.157345, acc 0.953125
2017-03-02T18:22:17.681405: step 38478, loss 0.266832, acc 0.890625
2017-03-02T18:22:17.757021: step 38479, loss 0.0830633, acc 0.96875
2017-03-02T18:22:17.833032: step 38480, loss 0.208376, acc 0.90625
2017-03-02T18:22:17.901144: step 38481, loss 0.0956463, acc 0.953125
2017-03-02T18:22:17.970733: step 38482, loss 0.164761, acc 0.921875
2017-03-02T18:22:18.038429: step 38483, loss 0.183371, acc 0.90625
2017-03-02T18:22:18.113035: step 38484, loss 0.13462, acc 0.921875
2017-03-02T18:22:18.186220: step 38485, loss 0.156424, acc 0.90625
2017-03-02T18:22:18.258131: step 38486, loss 0.20785, acc 0.890625
2017-03-02T18:22:18.338354: step 38487, loss 0.21481, acc 0.90625
2017-03-02T18:22:18.412818: step 38488, loss 0.180723, acc 0.890625
2017-03-02T18:22:18.484876: step 38489, loss 0.202775, acc 0.875
2017-03-02T18:22:18.557904: step 38490, loss 0.102539, acc 0.953125
2017-03-02T18:22:18.645168: step 38491, loss 0.171613, acc 0.9375
2017-03-02T18:22:18.721810: step 38492, loss 0.105608, acc 0.953125
2017-03-02T18:22:18.795042: step 38493, loss 0.0971271, acc 0.96875
2017-03-02T18:22:18.871081: step 38494, loss 0.137377, acc 0.90625
2017-03-02T18:22:18.937258: step 38495, loss 0.158581, acc 0.953125
2017-03-02T18:22:19.001940: step 38496, loss 0.106776, acc 0.953125
2017-03-02T18:22:19.079544: step 38497, loss 0.2172, acc 0.890625
2017-03-02T18:22:19.153904: step 38498, loss 0.155577, acc 0.9375
2017-03-02T18:22:19.226854: step 38499, loss 0.0910356, acc 0.953125
2017-03-02T18:22:19.302012: step 38500, loss 0.153226, acc 0.9375

Evaluation:
2017-03-02T18:22:19.338119: step 38500, loss 5.79046, acc 0.626532

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38500

2017-03-02T18:22:19.829855: step 38501, loss 0.162124, acc 0.953125
2017-03-02T18:22:19.904157: step 38502, loss 0.0686048, acc 0.96875
2017-03-02T18:22:19.974549: step 38503, loss 0.178759, acc 0.9375
2017-03-02T18:22:20.046507: step 38504, loss 0.212792, acc 0.890625
2017-03-02T18:22:20.116717: step 38505, loss 0.089748, acc 0.984375
2017-03-02T18:22:20.194558: step 38506, loss 0.057511, acc 1
2017-03-02T18:22:20.266494: step 38507, loss 0.151822, acc 0.921875
2017-03-02T18:22:20.335005: step 38508, loss 0.245477, acc 0.875
2017-03-02T18:22:20.404308: step 38509, loss 0.165968, acc 0.921875
2017-03-02T18:22:20.475159: step 38510, loss 0.194991, acc 0.921875
2017-03-02T18:22:20.542627: step 38511, loss 0.0777202, acc 0.953125
2017-03-02T18:22:20.615029: step 38512, loss 0.125516, acc 0.953125
2017-03-02T18:22:20.682649: step 38513, loss 0.0891315, acc 0.96875
2017-03-02T18:22:20.756347: step 38514, loss 0.132177, acc 0.9375
2017-03-02T18:22:20.827219: step 38515, loss 0.107384, acc 0.953125
2017-03-02T18:22:20.898313: step 38516, loss 0.18278, acc 0.921875
2017-03-02T18:22:20.970087: step 38517, loss 0.16811, acc 0.953125
2017-03-02T18:22:21.040725: step 38518, loss 0.131358, acc 0.96875
2017-03-02T18:22:21.120543: step 38519, loss 0.105065, acc 0.953125
2017-03-02T18:22:21.195088: step 38520, loss 0.161118, acc 0.921875
2017-03-02T18:22:21.272678: step 38521, loss 0.111953, acc 0.953125
2017-03-02T18:22:21.346945: step 38522, loss 0.156924, acc 0.90625
2017-03-02T18:22:21.414674: step 38523, loss 0.0853504, acc 0.96875
2017-03-02T18:22:21.505378: step 38524, loss 0.228034, acc 0.90625
2017-03-02T18:22:21.579347: step 38525, loss 0.0923322, acc 0.953125
2017-03-02T18:22:21.661624: step 38526, loss 0.127898, acc 0.96875
2017-03-02T18:22:21.732004: step 38527, loss 0.220407, acc 0.859375
2017-03-02T18:22:21.798639: step 38528, loss 0.109066, acc 0.953125
2017-03-02T18:22:21.869704: step 38529, loss 0.151637, acc 0.9375
2017-03-02T18:22:21.940565: step 38530, loss 0.11597, acc 0.9375
2017-03-02T18:22:22.011165: step 38531, loss 0.140035, acc 0.90625
2017-03-02T18:22:22.081622: step 38532, loss 0.109172, acc 0.921875
2017-03-02T18:22:22.155486: step 38533, loss 0.0651022, acc 0.953125
2017-03-02T18:22:22.227559: step 38534, loss 0.113472, acc 0.953125
2017-03-02T18:22:22.303943: step 38535, loss 0.17375, acc 0.953125
2017-03-02T18:22:22.391009: step 38536, loss 0.143582, acc 0.921875
2017-03-02T18:22:22.477299: step 38537, loss 0.190758, acc 0.875
2017-03-02T18:22:22.560880: step 38538, loss 0.139917, acc 0.9375
2017-03-02T18:22:22.632351: step 38539, loss 0.132318, acc 0.90625
2017-03-02T18:22:22.700490: step 38540, loss 0.197539, acc 0.875
2017-03-02T18:22:22.774796: step 38541, loss 0.148026, acc 0.953125
2017-03-02T18:22:22.847623: step 38542, loss 0.0913062, acc 0.96875
2017-03-02T18:22:22.913673: step 38543, loss 0.201505, acc 0.921875
2017-03-02T18:22:22.984228: step 38544, loss 0.129818, acc 0.921875
2017-03-02T18:22:23.056660: step 38545, loss 0.069115, acc 0.953125
2017-03-02T18:22:23.126545: step 38546, loss 0.0683359, acc 0.984375
2017-03-02T18:22:23.197474: step 38547, loss 0.159521, acc 0.921875
2017-03-02T18:22:23.268950: step 38548, loss 0.139962, acc 0.96875
2017-03-02T18:22:23.340483: step 38549, loss 0.191379, acc 0.921875
2017-03-02T18:22:23.421207: step 38550, loss 0.102651, acc 0.9375
2017-03-02T18:22:23.495931: step 38551, loss 0.106095, acc 0.96875
2017-03-02T18:22:23.562362: step 38552, loss 0.0717351, acc 0.984375
2017-03-02T18:22:23.632747: step 38553, loss 0.112335, acc 0.9375
2017-03-02T18:22:23.709480: step 38554, loss 0.0643866, acc 0.96875
2017-03-02T18:22:23.774959: step 38555, loss 0.121706, acc 0.96875
2017-03-02T18:22:23.845585: step 38556, loss 0.0752333, acc 0.96875
2017-03-02T18:22:23.920563: step 38557, loss 0.0806277, acc 0.96875
2017-03-02T18:22:23.999539: step 38558, loss 0.173382, acc 0.953125
2017-03-02T18:22:24.086471: step 38559, loss 0.172869, acc 0.9375
2017-03-02T18:22:24.158273: step 38560, loss 0.102446, acc 0.921875
2017-03-02T18:22:24.232372: step 38561, loss 0.113009, acc 0.9375
2017-03-02T18:22:24.305824: step 38562, loss 0.110591, acc 0.9375
2017-03-02T18:22:24.378737: step 38563, loss 0.204555, acc 0.90625
2017-03-02T18:22:24.450523: step 38564, loss 0.190925, acc 0.953125
2017-03-02T18:22:24.519351: step 38565, loss 0.0205757, acc 1
2017-03-02T18:22:24.595517: step 38566, loss 0.0695034, acc 0.96875
2017-03-02T18:22:24.665589: step 38567, loss 0.127202, acc 0.9375
2017-03-02T18:22:24.735293: step 38568, loss 0.17265, acc 0.9375
2017-03-02T18:22:24.817026: step 38569, loss 0.0563277, acc 0.953125
2017-03-02T18:22:24.898732: step 38570, loss 0.189117, acc 0.921875
2017-03-02T18:22:24.968610: step 38571, loss 0.208898, acc 0.90625
2017-03-02T18:22:25.026647: step 38572, loss 0.098139, acc 0.9375
2017-03-02T18:22:25.097883: step 38573, loss 0.251355, acc 0.859375
2017-03-02T18:22:25.165068: step 38574, loss 0.11331, acc 0.953125
2017-03-02T18:22:25.235829: step 38575, loss 0.0860262, acc 0.96875
2017-03-02T18:22:25.308436: step 38576, loss 0.219584, acc 0.90625
2017-03-02T18:22:25.389011: step 38577, loss 0.172956, acc 0.9375
2017-03-02T18:22:25.458359: step 38578, loss 0.128882, acc 0.9375
2017-03-02T18:22:25.540533: step 38579, loss 0.0956274, acc 0.953125
2017-03-02T18:22:25.611027: step 38580, loss 0.14063, acc 0.921875
2017-03-02T18:22:25.688591: step 38581, loss 0.136688, acc 0.90625
2017-03-02T18:22:25.765451: step 38582, loss 0.152417, acc 0.921875
2017-03-02T18:22:25.834805: step 38583, loss 0.11577, acc 0.953125
2017-03-02T18:22:25.899076: step 38584, loss 0.134151, acc 0.953125
2017-03-02T18:22:25.970089: step 38585, loss 0.111598, acc 0.953125
2017-03-02T18:22:26.046717: step 38586, loss 0.215209, acc 0.90625
2017-03-02T18:22:26.114955: step 38587, loss 0.208034, acc 0.9375
2017-03-02T18:22:26.185204: step 38588, loss 0.174252, acc 0.9375
2017-03-02T18:22:26.256510: step 38589, loss 0.117182, acc 0.9375
2017-03-02T18:22:26.322385: step 38590, loss 0.288358, acc 0.875
2017-03-02T18:22:26.406111: step 38591, loss 0.0506176, acc 0.984375
2017-03-02T18:22:26.478357: step 38592, loss 0.0782868, acc 0.96875
2017-03-02T18:22:26.545857: step 38593, loss 0.0723818, acc 0.96875
2017-03-02T18:22:26.619715: step 38594, loss 0.150367, acc 0.9375
2017-03-02T18:22:26.697361: step 38595, loss 0.11607, acc 0.9375
2017-03-02T18:22:26.767095: step 38596, loss 0.0591318, acc 0.96875
2017-03-02T18:22:26.839146: step 38597, loss 0.138112, acc 0.921875
2017-03-02T18:22:26.913146: step 38598, loss 0.0813824, acc 0.96875
2017-03-02T18:22:26.987780: step 38599, loss 0.0247277, acc 0.984375
2017-03-02T18:22:27.062040: step 38600, loss 0.183976, acc 0.9375

Evaluation:
2017-03-02T18:22:27.101888: step 38600, loss 5.9198, acc 0.623648

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38600

2017-03-02T18:22:27.554481: step 38601, loss 0.224506, acc 0.90625
2017-03-02T18:22:27.629442: step 38602, loss 0.173931, acc 0.90625
2017-03-02T18:22:27.709077: step 38603, loss 0.195463, acc 0.890625
2017-03-02T18:22:27.792399: step 38604, loss 0.10298, acc 0.953125
2017-03-02T18:22:27.864627: step 38605, loss 0.281494, acc 0.890625
2017-03-02T18:22:27.939100: step 38606, loss 0.123317, acc 0.9375
2017-03-02T18:22:27.998265: step 38607, loss 0.161227, acc 0.921875
2017-03-02T18:22:28.072064: step 38608, loss 0.138686, acc 0.9375
2017-03-02T18:22:28.147253: step 38609, loss 0.163386, acc 0.90625
2017-03-02T18:22:28.223556: step 38610, loss 0.0600077, acc 0.984375
2017-03-02T18:22:28.295062: step 38611, loss 0.139975, acc 0.953125
2017-03-02T18:22:28.368102: step 38612, loss 0.0448719, acc 1
2017-03-02T18:22:28.438387: step 38613, loss 0.132412, acc 0.9375
2017-03-02T18:22:28.512353: step 38614, loss 0.172653, acc 0.921875
2017-03-02T18:22:28.578171: step 38615, loss 0.152233, acc 0.9375
2017-03-02T18:22:28.647946: step 38616, loss 0.17164, acc 0.921875
2017-03-02T18:22:28.716294: step 38617, loss 0.187832, acc 0.9375
2017-03-02T18:22:28.784073: step 38618, loss 0.167964, acc 0.921875
2017-03-02T18:22:28.852921: step 38619, loss 0.29378, acc 0.890625
2017-03-02T18:22:28.932188: step 38620, loss 0.164829, acc 0.890625
2017-03-02T18:22:28.999168: step 38621, loss 0.0941044, acc 0.953125
2017-03-02T18:22:29.066161: step 38622, loss 0.140568, acc 0.921875
2017-03-02T18:22:29.146574: step 38623, loss 0.121056, acc 0.96875
2017-03-02T18:22:29.216851: step 38624, loss 0.171502, acc 0.921875
2017-03-02T18:22:29.283200: step 38625, loss 0.213545, acc 0.875
2017-03-02T18:22:29.350461: step 38626, loss 0.137564, acc 0.921875
2017-03-02T18:22:29.418969: step 38627, loss 0.119578, acc 0.953125
2017-03-02T18:22:29.490272: step 38628, loss 0.14336, acc 0.921875
2017-03-02T18:22:29.569758: step 38629, loss 0.0858773, acc 0.96875
2017-03-02T18:22:29.643360: step 38630, loss 0.137252, acc 0.96875
2017-03-02T18:22:29.715999: step 38631, loss 0.141409, acc 0.921875
2017-03-02T18:22:29.788446: step 38632, loss 0.100889, acc 0.953125
2017-03-02T18:22:29.859393: step 38633, loss 0.140807, acc 0.9375
2017-03-02T18:22:29.946734: step 38634, loss 0.180467, acc 0.921875
2017-03-02T18:22:30.013879: step 38635, loss 0.185811, acc 0.890625
2017-03-02T18:22:30.080579: step 38636, loss 0.135282, acc 0.9375
2017-03-02T18:22:30.148312: step 38637, loss 0.146591, acc 0.9375
2017-03-02T18:22:30.219937: step 38638, loss 0.0522914, acc 0.984375
2017-03-02T18:22:30.291928: step 38639, loss 0.0708425, acc 0.984375
2017-03-02T18:22:30.365294: step 38640, loss 0.167025, acc 0.921875
2017-03-02T18:22:30.444564: step 38641, loss 0.132879, acc 0.9375
2017-03-02T18:22:30.516455: step 38642, loss 0.200162, acc 0.875
2017-03-02T18:22:30.594199: step 38643, loss 0.066875, acc 0.96875
2017-03-02T18:22:30.663129: step 38644, loss 0.169053, acc 0.890625
2017-03-02T18:22:30.724505: step 38645, loss 0.152521, acc 0.921875
2017-03-02T18:22:30.795925: step 38646, loss 0.229713, acc 0.890625
2017-03-02T18:22:30.866827: step 38647, loss 0.118738, acc 0.9375
2017-03-02T18:22:30.944942: step 38648, loss 0.0614393, acc 0.984375
2017-03-02T18:22:31.017473: step 38649, loss 0.216883, acc 0.921875
2017-03-02T18:22:31.089221: step 38650, loss 0.128866, acc 0.921875
2017-03-02T18:22:31.162305: step 38651, loss 0.25666, acc 0.9375
2017-03-02T18:22:31.234596: step 38652, loss 0.202837, acc 0.9375
2017-03-02T18:22:31.292046: step 38653, loss 0.133529, acc 0.921875
2017-03-02T18:22:31.357401: step 38654, loss 0.0509023, acc 0.984375
2017-03-02T18:22:31.422117: step 38655, loss 0.201729, acc 0.9375
2017-03-02T18:22:31.491113: step 38656, loss 0.223162, acc 0.90625
2017-03-02T18:22:31.563285: step 38657, loss 0.0540167, acc 0.984375
2017-03-02T18:22:31.651633: step 38658, loss 0.112986, acc 0.96875
2017-03-02T18:22:31.721114: step 38659, loss 0.158277, acc 0.9375
2017-03-02T18:22:31.788025: step 38660, loss 0.0934928, acc 0.953125
2017-03-02T18:22:31.859408: step 38661, loss 0.183811, acc 0.921875
2017-03-02T18:22:31.929232: step 38662, loss 0.0389882, acc 0.984375
2017-03-02T18:22:31.998225: step 38663, loss 0.194571, acc 0.90625
2017-03-02T18:22:32.063156: step 38664, loss 0.176415, acc 0.953125
2017-03-02T18:22:32.139662: step 38665, loss 0.156857, acc 0.921875
2017-03-02T18:22:32.211868: step 38666, loss 0.100366, acc 0.953125
2017-03-02T18:22:32.286573: step 38667, loss 0.122078, acc 0.921875
2017-03-02T18:22:32.355290: step 38668, loss 0.111252, acc 0.921875
2017-03-02T18:22:32.430498: step 38669, loss 0.093578, acc 0.96875
2017-03-02T18:22:32.513491: step 38670, loss 0.0431503, acc 0.96875
2017-03-02T18:22:32.584194: step 38671, loss 0.129832, acc 0.96875
2017-03-02T18:22:32.663050: step 38672, loss 0.07286, acc 0.953125
2017-03-02T18:22:32.728388: step 38673, loss 0.167008, acc 0.890625
2017-03-02T18:22:32.793945: step 38674, loss 0.160294, acc 0.921875
2017-03-02T18:22:32.862763: step 38675, loss 0.152011, acc 0.921875
2017-03-02T18:22:32.934979: step 38676, loss 0.172125, acc 0.90625
2017-03-02T18:22:33.007168: step 38677, loss 0.0462258, acc 0.96875
2017-03-02T18:22:33.078081: step 38678, loss 0.129502, acc 0.953125
2017-03-02T18:22:33.151056: step 38679, loss 0.119994, acc 0.9375
2017-03-02T18:22:33.223473: step 38680, loss 0.100126, acc 0.984375
2017-03-02T18:22:33.315246: step 38681, loss 0.0955049, acc 0.953125
2017-03-02T18:22:33.386221: step 38682, loss 0.143127, acc 0.921875
2017-03-02T18:22:33.455341: step 38683, loss 0.216414, acc 0.953125
2017-03-02T18:22:33.528026: step 38684, loss 0.106226, acc 0.953125
2017-03-02T18:22:33.605513: step 38685, loss 0.106375, acc 0.9375
2017-03-02T18:22:33.676675: step 38686, loss 0.0804319, acc 0.96875
2017-03-02T18:22:33.756762: step 38687, loss 0.214576, acc 0.90625
2017-03-02T18:22:33.834264: step 38688, loss 0.266915, acc 0.96875
2017-03-02T18:22:33.904784: step 38689, loss 0.109208, acc 0.9375
2017-03-02T18:22:33.976553: step 38690, loss 0.135918, acc 0.953125
2017-03-02T18:22:34.043031: step 38691, loss 0.332145, acc 0.890625
2017-03-02T18:22:34.111119: step 38692, loss 0.170756, acc 0.96875
2017-03-02T18:22:34.180163: step 38693, loss 0.182221, acc 0.9375
2017-03-02T18:22:34.251399: step 38694, loss 0.144456, acc 0.921875
2017-03-02T18:22:34.329784: step 38695, loss 0.125207, acc 0.9375
2017-03-02T18:22:34.399941: step 38696, loss 0.123639, acc 0.921875
2017-03-02T18:22:34.481278: step 38697, loss 0.152384, acc 0.90625
2017-03-02T18:22:34.552587: step 38698, loss 0.271983, acc 0.90625
2017-03-02T18:22:34.624068: step 38699, loss 0.231032, acc 0.90625
2017-03-02T18:22:34.704146: step 38700, loss 0.144416, acc 0.9375

Evaluation:
2017-03-02T18:22:34.732553: step 38700, loss 6.05286, acc 0.638068

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38700

2017-03-02T18:22:35.184176: step 38701, loss 0.228875, acc 0.90625
2017-03-02T18:22:35.254854: step 38702, loss 0.16203, acc 0.9375
2017-03-02T18:22:35.330127: step 38703, loss 0.094034, acc 0.953125
2017-03-02T18:22:35.404814: step 38704, loss 0.0252633, acc 1
2017-03-02T18:22:35.493398: step 38705, loss 0.0597542, acc 0.984375
2017-03-02T18:22:35.568957: step 38706, loss 0.106782, acc 0.9375
2017-03-02T18:22:35.637822: step 38707, loss 0.0662541, acc 0.984375
2017-03-02T18:22:35.707848: step 38708, loss 0.0969916, acc 0.953125
2017-03-02T18:22:35.785774: step 38709, loss 0.173706, acc 0.90625
2017-03-02T18:22:35.859323: step 38710, loss 0.222309, acc 0.90625
2017-03-02T18:22:35.930789: step 38711, loss 0.101089, acc 0.9375
2017-03-02T18:22:36.003940: step 38712, loss 0.147669, acc 0.953125
2017-03-02T18:22:36.079956: step 38713, loss 0.0973886, acc 0.9375
2017-03-02T18:22:36.155366: step 38714, loss 0.126933, acc 0.921875
2017-03-02T18:22:36.233554: step 38715, loss 0.134138, acc 0.953125
2017-03-02T18:22:36.304180: step 38716, loss 0.103656, acc 0.9375
2017-03-02T18:22:36.371478: step 38717, loss 0.191474, acc 0.875
2017-03-02T18:22:36.444064: step 38718, loss 0.11323, acc 0.9375
2017-03-02T18:22:36.518803: step 38719, loss 0.222823, acc 0.90625
2017-03-02T18:22:36.597990: step 38720, loss 0.237435, acc 0.921875
2017-03-02T18:22:36.673583: step 38721, loss 0.139618, acc 0.9375
2017-03-02T18:22:36.749635: step 38722, loss 0.131993, acc 0.953125
2017-03-02T18:22:36.819901: step 38723, loss 0.152038, acc 0.953125
2017-03-02T18:22:36.891775: step 38724, loss 0.17079, acc 0.921875
2017-03-02T18:22:36.963930: step 38725, loss 0.064813, acc 0.984375
2017-03-02T18:22:37.039846: step 38726, loss 0.0741541, acc 0.96875
2017-03-02T18:22:37.113332: step 38727, loss 0.192956, acc 0.875
2017-03-02T18:22:37.207837: step 38728, loss 0.100359, acc 0.953125
2017-03-02T18:22:37.286192: step 38729, loss 0.104735, acc 0.953125
2017-03-02T18:22:37.358500: step 38730, loss 0.175794, acc 0.9375
2017-03-02T18:22:37.433038: step 38731, loss 0.171649, acc 0.921875
2017-03-02T18:22:37.501311: step 38732, loss 0.198931, acc 0.875
2017-03-02T18:22:37.570263: step 38733, loss 0.118393, acc 0.9375
2017-03-02T18:22:37.633738: step 38734, loss 0.130511, acc 0.9375
2017-03-02T18:22:37.706404: step 38735, loss 0.0321048, acc 1
2017-03-02T18:22:37.792276: step 38736, loss 0.192598, acc 0.875
2017-03-02T18:22:37.865687: step 38737, loss 0.147772, acc 0.953125
2017-03-02T18:22:37.946496: step 38738, loss 0.0490548, acc 1
2017-03-02T18:22:38.020195: step 38739, loss 0.0812957, acc 0.96875
2017-03-02T18:22:38.107308: step 38740, loss 0.120604, acc 0.921875
2017-03-02T18:22:38.177195: step 38741, loss 0.186246, acc 0.9375
2017-03-02T18:22:38.248749: step 38742, loss 0.091681, acc 0.96875
2017-03-02T18:22:38.327638: step 38743, loss 0.204186, acc 0.875
2017-03-02T18:22:38.403992: step 38744, loss 0.177726, acc 0.921875
2017-03-02T18:22:38.473694: step 38745, loss 0.12669, acc 0.953125
2017-03-02T18:22:38.551387: step 38746, loss 0.0902647, acc 0.96875
2017-03-02T18:22:38.622883: step 38747, loss 0.171091, acc 0.921875
2017-03-02T18:22:38.692294: step 38748, loss 0.172888, acc 0.921875
2017-03-02T18:22:38.768397: step 38749, loss 0.147757, acc 0.9375
2017-03-02T18:22:38.833259: step 38750, loss 0.116036, acc 0.90625
2017-03-02T18:22:38.900661: step 38751, loss 0.124632, acc 0.9375
2017-03-02T18:22:38.980172: step 38752, loss 0.0760216, acc 0.96875
2017-03-02T18:22:39.059650: step 38753, loss 0.255736, acc 0.875
2017-03-02T18:22:39.140625: step 38754, loss 0.189413, acc 0.90625
2017-03-02T18:22:39.212028: step 38755, loss 0.140533, acc 0.9375
2017-03-02T18:22:39.293200: step 38756, loss 0.0815423, acc 0.9375
2017-03-02T18:22:39.371308: step 38757, loss 0.195541, acc 0.90625
2017-03-02T18:22:39.449008: step 38758, loss 0.267775, acc 0.890625
2017-03-02T18:22:39.519830: step 38759, loss 0.225135, acc 0.90625
2017-03-02T18:22:39.593109: step 38760, loss 0.192225, acc 0.9375
2017-03-02T18:22:39.663255: step 38761, loss 0.133892, acc 0.9375
2017-03-02T18:22:39.732612: step 38762, loss 0.152086, acc 0.9375
2017-03-02T18:22:39.805821: step 38763, loss 0.203089, acc 0.90625
2017-03-02T18:22:39.880867: step 38764, loss 0.100782, acc 0.953125
2017-03-02T18:22:39.955689: step 38765, loss 0.171243, acc 0.890625
2017-03-02T18:22:40.029768: step 38766, loss 0.171642, acc 0.953125
2017-03-02T18:22:40.097259: step 38767, loss 0.238042, acc 0.890625
2017-03-02T18:22:40.170003: step 38768, loss 0.105201, acc 0.953125
2017-03-02T18:22:40.240279: step 38769, loss 0.130598, acc 0.9375
2017-03-02T18:22:40.309810: step 38770, loss 0.190846, acc 0.890625
2017-03-02T18:22:40.381372: step 38771, loss 0.137207, acc 0.9375
2017-03-02T18:22:40.455135: step 38772, loss 0.156006, acc 0.953125
2017-03-02T18:22:40.529632: step 38773, loss 0.142532, acc 0.90625
2017-03-02T18:22:40.588399: step 38774, loss 0.204565, acc 0.875
2017-03-02T18:22:40.664016: step 38775, loss 0.0848522, acc 0.9375
2017-03-02T18:22:40.739077: step 38776, loss 0.138757, acc 0.921875
2017-03-02T18:22:40.830767: step 38777, loss 0.269795, acc 0.90625
2017-03-02T18:22:40.925724: step 38778, loss 0.133521, acc 0.96875
2017-03-02T18:22:40.999603: step 38779, loss 0.186087, acc 0.90625
2017-03-02T18:22:41.068627: step 38780, loss 0.141152, acc 0.953125
2017-03-02T18:22:41.139757: step 38781, loss 0.103121, acc 0.96875
2017-03-02T18:22:41.217405: step 38782, loss 0.0934872, acc 0.96875
2017-03-02T18:22:41.285742: step 38783, loss 0.151202, acc 0.90625
2017-03-02T18:22:41.357786: step 38784, loss 0.221859, acc 0.90625
2017-03-02T18:22:41.428435: step 38785, loss 0.162258, acc 0.90625
2017-03-02T18:22:41.500243: step 38786, loss 0.0860263, acc 0.96875
2017-03-02T18:22:41.578127: step 38787, loss 0.0820985, acc 0.96875
2017-03-02T18:22:41.652957: step 38788, loss 0.18183, acc 0.921875
2017-03-02T18:22:41.718747: step 38789, loss 0.263604, acc 0.90625
2017-03-02T18:22:41.791624: step 38790, loss 0.142395, acc 0.9375
2017-03-02T18:22:41.869086: step 38791, loss 0.198792, acc 0.890625
2017-03-02T18:22:41.940810: step 38792, loss 0.132598, acc 0.9375
2017-03-02T18:22:42.024886: step 38793, loss 0.101093, acc 0.96875
2017-03-02T18:22:42.113623: step 38794, loss 0.0500362, acc 1
2017-03-02T18:22:42.183847: step 38795, loss 0.14177, acc 0.921875
2017-03-02T18:22:42.258563: step 38796, loss 0.0180498, acc 1
2017-03-02T18:22:42.327317: step 38797, loss 0.308868, acc 0.859375
2017-03-02T18:22:42.394411: step 38798, loss 0.222354, acc 0.890625
2017-03-02T18:22:42.470454: step 38799, loss 0.10513, acc 0.953125
2017-03-02T18:22:42.550029: step 38800, loss 0.0459922, acc 0.984375

Evaluation:
2017-03-02T18:22:42.586558: step 38800, loss 6.04694, acc 0.633021

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38800

2017-03-02T18:22:43.049612: step 38801, loss 0.119132, acc 0.90625
2017-03-02T18:22:43.116746: step 38802, loss 0.12748, acc 0.953125
2017-03-02T18:22:43.192851: step 38803, loss 0.132197, acc 0.921875
2017-03-02T18:22:43.269502: step 38804, loss 0.114384, acc 0.9375
2017-03-02T18:22:43.346650: step 38805, loss 0.06361, acc 0.984375
2017-03-02T18:22:43.419160: step 38806, loss 0.145476, acc 0.921875
2017-03-02T18:22:43.489021: step 38807, loss 0.118391, acc 0.921875
2017-03-02T18:22:43.559855: step 38808, loss 3.90407e-06, acc 1
2017-03-02T18:22:43.638238: step 38809, loss 0.124356, acc 0.953125
2017-03-02T18:22:43.708968: step 38810, loss 0.241095, acc 0.875
2017-03-02T18:22:43.779889: step 38811, loss 0.0809828, acc 0.984375
2017-03-02T18:22:43.850390: step 38812, loss 0.0915283, acc 0.9375
2017-03-02T18:22:43.925150: step 38813, loss 0.153785, acc 0.9375
2017-03-02T18:22:44.002246: step 38814, loss 0.0630318, acc 0.96875
2017-03-02T18:22:44.070425: step 38815, loss 0.149665, acc 0.9375
2017-03-02T18:22:44.146926: step 38816, loss 0.128344, acc 0.9375
2017-03-02T18:22:44.223076: step 38817, loss 0.134133, acc 0.921875
2017-03-02T18:22:44.295305: step 38818, loss 0.193234, acc 0.921875
2017-03-02T18:22:44.370437: step 38819, loss 0.137614, acc 0.921875
2017-03-02T18:22:44.455486: step 38820, loss 0.17884, acc 0.90625
2017-03-02T18:22:44.521214: step 38821, loss 0.181967, acc 0.90625
2017-03-02T18:22:44.594486: step 38822, loss 0.0990954, acc 0.921875
2017-03-02T18:22:44.669724: step 38823, loss 0.135227, acc 0.90625
2017-03-02T18:22:44.743338: step 38824, loss 0.212258, acc 0.875
2017-03-02T18:22:44.821383: step 38825, loss 0.215208, acc 0.90625
2017-03-02T18:22:44.897065: step 38826, loss 0.114566, acc 0.9375
2017-03-02T18:22:44.975893: step 38827, loss 0.0583829, acc 0.96875
2017-03-02T18:22:45.052108: step 38828, loss 0.150097, acc 0.90625
2017-03-02T18:22:45.125257: step 38829, loss 0.1225, acc 0.953125
2017-03-02T18:22:45.193699: step 38830, loss 0.170756, acc 0.90625
2017-03-02T18:22:45.269872: step 38831, loss 0.127842, acc 0.9375
2017-03-02T18:22:45.351577: step 38832, loss 0.202607, acc 0.890625
2017-03-02T18:22:45.424351: step 38833, loss 0.065243, acc 0.984375
2017-03-02T18:22:45.496540: step 38834, loss 0.17281, acc 0.890625
2017-03-02T18:22:45.568126: step 38835, loss 0.172171, acc 0.9375
2017-03-02T18:22:45.640239: step 38836, loss 0.215457, acc 0.921875
2017-03-02T18:22:45.719287: step 38837, loss 0.16259, acc 0.890625
2017-03-02T18:22:45.787930: step 38838, loss 0.147864, acc 0.90625
2017-03-02T18:22:45.864736: step 38839, loss 0.0514811, acc 0.984375
2017-03-02T18:22:45.936064: step 38840, loss 0.186829, acc 0.921875
2017-03-02T18:22:46.011797: step 38841, loss 0.127787, acc 0.9375
2017-03-02T18:22:46.089463: step 38842, loss 0.117323, acc 0.921875
2017-03-02T18:22:46.158460: step 38843, loss 0.102227, acc 0.9375
2017-03-02T18:22:46.226150: step 38844, loss 0.149495, acc 0.921875
2017-03-02T18:22:46.299399: step 38845, loss 0.109745, acc 0.96875
2017-03-02T18:22:46.379373: step 38846, loss 0.170611, acc 0.921875
2017-03-02T18:22:46.458175: step 38847, loss 0.268024, acc 0.890625
2017-03-02T18:22:46.525546: step 38848, loss 0.158272, acc 0.921875
2017-03-02T18:22:46.597513: step 38849, loss 0.0860946, acc 0.984375
2017-03-02T18:22:46.670910: step 38850, loss 0.184435, acc 0.90625
2017-03-02T18:22:46.742962: step 38851, loss 0.117019, acc 0.921875
2017-03-02T18:22:46.814372: step 38852, loss 0.195527, acc 0.921875
2017-03-02T18:22:46.889330: step 38853, loss 0.10307, acc 0.953125
2017-03-02T18:22:46.958584: step 38854, loss 0.158678, acc 0.9375
2017-03-02T18:22:47.034486: step 38855, loss 0.129096, acc 0.9375
2017-03-02T18:22:47.107911: step 38856, loss 0.144347, acc 0.90625
2017-03-02T18:22:47.189546: step 38857, loss 0.0910054, acc 0.96875
2017-03-02T18:22:47.257970: step 38858, loss 0.0563116, acc 0.96875
2017-03-02T18:22:47.329633: step 38859, loss 0.159558, acc 0.890625
2017-03-02T18:22:47.404667: step 38860, loss 0.0820066, acc 0.96875
2017-03-02T18:22:47.475991: step 38861, loss 0.118693, acc 0.953125
2017-03-02T18:22:47.547874: step 38862, loss 0.227114, acc 0.890625
2017-03-02T18:22:47.618544: step 38863, loss 0.107436, acc 0.921875
2017-03-02T18:22:47.690373: step 38864, loss 0.135469, acc 0.953125
2017-03-02T18:22:47.773831: step 38865, loss 0.0822513, acc 0.96875
2017-03-02T18:22:47.838946: step 38866, loss 0.136166, acc 0.9375
2017-03-02T18:22:47.907519: step 38867, loss 0.165417, acc 0.953125
2017-03-02T18:22:47.981793: step 38868, loss 0.21306, acc 0.90625
2017-03-02T18:22:48.057498: step 38869, loss 0.214224, acc 0.859375
2017-03-02T18:22:48.134464: step 38870, loss 0.099698, acc 0.96875
2017-03-02T18:22:48.208466: step 38871, loss 0.286445, acc 0.84375
2017-03-02T18:22:48.289177: step 38872, loss 0.174191, acc 0.921875
2017-03-02T18:22:48.354445: step 38873, loss 0.0929252, acc 0.953125
2017-03-02T18:22:48.427021: step 38874, loss 0.172715, acc 0.921875
2017-03-02T18:22:48.498042: step 38875, loss 0.128722, acc 0.921875
2017-03-02T18:22:48.563249: step 38876, loss 0.0667585, acc 0.96875
2017-03-02T18:22:48.635131: step 38877, loss 0.320949, acc 0.859375
2017-03-02T18:22:48.708591: step 38878, loss 0.133685, acc 0.953125
2017-03-02T18:22:48.788254: step 38879, loss 0.138628, acc 0.953125
2017-03-02T18:22:48.859973: step 38880, loss 0.123824, acc 0.96875
2017-03-02T18:22:48.937636: step 38881, loss 0.0557545, acc 0.984375
2017-03-02T18:22:49.012805: step 38882, loss 0.291808, acc 0.890625
2017-03-02T18:22:49.082131: step 38883, loss 0.0847921, acc 0.96875
2017-03-02T18:22:49.155875: step 38884, loss 0.240985, acc 0.90625
2017-03-02T18:22:49.226608: step 38885, loss 0.0529806, acc 1
2017-03-02T18:22:49.286769: step 38886, loss 0.221197, acc 0.875
2017-03-02T18:22:49.352400: step 38887, loss 0.249444, acc 0.90625
2017-03-02T18:22:49.429077: step 38888, loss 0.243611, acc 0.875
2017-03-02T18:22:49.511192: step 38889, loss 0.112929, acc 0.9375
2017-03-02T18:22:49.583872: step 38890, loss 0.219981, acc 0.921875
2017-03-02T18:22:49.661488: step 38891, loss 0.0577517, acc 0.984375
2017-03-02T18:22:49.743199: step 38892, loss 0.10604, acc 0.9375
2017-03-02T18:22:49.826139: step 38893, loss 0.0995389, acc 0.953125
2017-03-02T18:22:49.906636: step 38894, loss 0.10067, acc 0.96875
2017-03-02T18:22:49.967765: step 38895, loss 0.17119, acc 0.90625
2017-03-02T18:22:50.042527: step 38896, loss 0.0675468, acc 0.96875
2017-03-02T18:22:50.116458: step 38897, loss 0.19756, acc 0.953125
2017-03-02T18:22:50.191846: step 38898, loss 0.124687, acc 0.9375
2017-03-02T18:22:50.262609: step 38899, loss 0.116833, acc 0.953125
2017-03-02T18:22:50.334595: step 38900, loss 0.148812, acc 0.921875

Evaluation:
2017-03-02T18:22:50.370868: step 38900, loss 5.98393, acc 0.629416

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-38900

2017-03-02T18:22:50.813831: step 38901, loss 0.100935, acc 0.953125
2017-03-02T18:22:50.886610: step 38902, loss 0.116146, acc 0.953125
2017-03-02T18:22:50.959776: step 38903, loss 0.128265, acc 0.953125
2017-03-02T18:22:51.035172: step 38904, loss 0.199767, acc 0.9375
2017-03-02T18:22:51.112665: step 38905, loss 0.0474562, acc 0.984375
2017-03-02T18:22:51.184131: step 38906, loss 0.0487268, acc 0.984375
2017-03-02T18:22:51.252124: step 38907, loss 0.128074, acc 0.921875
2017-03-02T18:22:51.327653: step 38908, loss 0.126801, acc 0.921875
2017-03-02T18:22:51.399022: step 38909, loss 0.232906, acc 0.90625
2017-03-02T18:22:51.474447: step 38910, loss 0.12108, acc 0.9375
2017-03-02T18:22:51.546539: step 38911, loss 0.165734, acc 0.90625
2017-03-02T18:22:51.618594: step 38912, loss 0.187518, acc 0.9375
2017-03-02T18:22:51.691186: step 38913, loss 0.18198, acc 0.953125
2017-03-02T18:22:51.761772: step 38914, loss 0.159634, acc 0.953125
2017-03-02T18:22:51.831518: step 38915, loss 0.144824, acc 0.953125
2017-03-02T18:22:51.906027: step 38916, loss 0.125982, acc 0.9375
2017-03-02T18:22:51.978328: step 38917, loss 0.147235, acc 0.921875
2017-03-02T18:22:52.062459: step 38918, loss 0.113984, acc 0.953125
2017-03-02T18:22:52.150439: step 38919, loss 0.201698, acc 0.9375
2017-03-02T18:22:52.225991: step 38920, loss 0.135737, acc 0.921875
2017-03-02T18:22:52.301426: step 38921, loss 0.148645, acc 0.921875
2017-03-02T18:22:52.374064: step 38922, loss 0.128945, acc 0.9375
2017-03-02T18:22:52.445331: step 38923, loss 0.181343, acc 0.890625
2017-03-02T18:22:52.515748: step 38924, loss 0.088797, acc 0.984375
2017-03-02T18:22:52.587422: step 38925, loss 0.138044, acc 0.9375
2017-03-02T18:22:52.662937: step 38926, loss 0.111492, acc 0.953125
2017-03-02T18:22:52.732201: step 38927, loss 0.271918, acc 0.890625
2017-03-02T18:22:52.802533: step 38928, loss 0.0972175, acc 0.9375
2017-03-02T18:22:52.872105: step 38929, loss 0.120114, acc 0.921875
2017-03-02T18:22:52.946212: step 38930, loss 0.142942, acc 0.921875
2017-03-02T18:22:53.021942: step 38931, loss 0.106225, acc 0.96875
2017-03-02T18:22:53.100722: step 38932, loss 0.0789998, acc 0.953125
2017-03-02T18:22:53.172115: step 38933, loss 0.0945741, acc 0.953125
2017-03-02T18:22:53.243534: step 38934, loss 0.102675, acc 0.9375
2017-03-02T18:22:53.315968: step 38935, loss 0.0171137, acc 1
2017-03-02T18:22:53.384113: step 38936, loss 0.169219, acc 0.9375
2017-03-02T18:22:53.452767: step 38937, loss 0.143395, acc 0.9375
2017-03-02T18:22:53.518190: step 38938, loss 0.192172, acc 0.953125
2017-03-02T18:22:53.589292: step 38939, loss 0.140255, acc 0.921875
2017-03-02T18:22:53.674906: step 38940, loss 0.180315, acc 0.890625
2017-03-02T18:22:53.748839: step 38941, loss 0.131842, acc 0.953125
2017-03-02T18:22:53.816054: step 38942, loss 0.125857, acc 0.9375
2017-03-02T18:22:53.879317: step 38943, loss 0.233632, acc 0.90625
2017-03-02T18:22:53.953440: step 38944, loss 0.0984591, acc 0.96875
2017-03-02T18:22:54.025814: step 38945, loss 0.118273, acc 0.921875
2017-03-02T18:22:54.102082: step 38946, loss 0.113003, acc 0.921875
2017-03-02T18:22:54.183846: step 38947, loss 0.156378, acc 0.9375
2017-03-02T18:22:54.253986: step 38948, loss 0.204538, acc 0.890625
2017-03-02T18:22:54.339968: step 38949, loss 0.151572, acc 0.9375
2017-03-02T18:22:54.408694: step 38950, loss 0.207202, acc 0.921875
2017-03-02T18:22:54.489010: step 38951, loss 0.167117, acc 0.921875
2017-03-02T18:22:54.560469: step 38952, loss 0.133491, acc 0.921875
2017-03-02T18:22:54.637173: step 38953, loss 0.156535, acc 0.921875
2017-03-02T18:22:54.712214: step 38954, loss 0.0802475, acc 0.984375
2017-03-02T18:22:54.788715: step 38955, loss 0.0815151, acc 0.953125
2017-03-02T18:22:54.858911: step 38956, loss 0.123356, acc 0.953125
2017-03-02T18:22:54.931433: step 38957, loss 0.142501, acc 0.921875
2017-03-02T18:22:55.003999: step 38958, loss 0.108492, acc 0.9375
2017-03-02T18:22:55.090439: step 38959, loss 0.136135, acc 0.9375
2017-03-02T18:22:55.162592: step 38960, loss 0.0859144, acc 0.953125
2017-03-02T18:22:55.238636: step 38961, loss 0.140401, acc 0.921875
2017-03-02T18:22:55.310818: step 38962, loss 0.122417, acc 0.96875
2017-03-02T18:22:55.389702: step 38963, loss 0.21228, acc 0.90625
2017-03-02T18:22:55.480216: step 38964, loss 0.227489, acc 0.890625
2017-03-02T18:22:55.551501: step 38965, loss 0.168246, acc 0.90625
2017-03-02T18:22:55.625366: step 38966, loss 0.168358, acc 0.921875
2017-03-02T18:22:55.701572: step 38967, loss 0.0975712, acc 0.953125
2017-03-02T18:22:55.778507: step 38968, loss 0.168343, acc 0.9375
2017-03-02T18:22:55.853391: step 38969, loss 0.14602, acc 0.953125
2017-03-02T18:22:55.925394: step 38970, loss 0.149191, acc 0.921875
2017-03-02T18:22:55.996236: step 38971, loss 0.102225, acc 0.9375
2017-03-02T18:22:56.060741: step 38972, loss 0.108078, acc 0.953125
2017-03-02T18:22:56.137833: step 38973, loss 0.108666, acc 0.953125
2017-03-02T18:22:56.209767: step 38974, loss 0.067483, acc 1
2017-03-02T18:22:56.279917: step 38975, loss 0.0617499, acc 0.96875
2017-03-02T18:22:56.352773: step 38976, loss 0.10307, acc 0.921875
2017-03-02T18:22:56.426914: step 38977, loss 0.125917, acc 0.9375
2017-03-02T18:22:56.504417: step 38978, loss 0.186191, acc 0.921875
2017-03-02T18:22:56.574630: step 38979, loss 0.266367, acc 0.859375
2017-03-02T18:22:56.645069: step 38980, loss 0.229862, acc 0.90625
2017-03-02T18:22:56.705728: step 38981, loss 0.134763, acc 0.953125
2017-03-02T18:22:56.778556: step 38982, loss 0.14009, acc 0.953125
2017-03-02T18:22:56.859515: step 38983, loss 0.0629314, acc 0.984375
2017-03-02T18:22:56.931991: step 38984, loss 0.128857, acc 0.9375
2017-03-02T18:22:57.002977: step 38985, loss 0.0992742, acc 0.96875
2017-03-02T18:22:57.073771: step 38986, loss 0.143386, acc 0.953125
2017-03-02T18:22:57.155654: step 38987, loss 0.144257, acc 0.9375
2017-03-02T18:22:57.228494: step 38988, loss 0.115817, acc 0.96875
2017-03-02T18:22:57.303426: step 38989, loss 0.107911, acc 0.953125
2017-03-02T18:22:57.385669: step 38990, loss 0.183394, acc 0.90625
2017-03-02T18:22:57.466203: step 38991, loss 0.165927, acc 0.9375
2017-03-02T18:22:57.533068: step 38992, loss 0.188823, acc 0.90625
2017-03-02T18:22:57.601862: step 38993, loss 0.151429, acc 0.953125
2017-03-02T18:22:57.679272: step 38994, loss 0.124426, acc 0.953125
2017-03-02T18:22:57.751034: step 38995, loss 0.178393, acc 0.921875
2017-03-02T18:22:57.821263: step 38996, loss 0.144998, acc 0.90625
2017-03-02T18:22:57.892452: step 38997, loss 0.194009, acc 0.9375
2017-03-02T18:22:57.970167: step 38998, loss 0.244278, acc 0.921875
2017-03-02T18:22:58.044970: step 38999, loss 0.184782, acc 0.9375
2017-03-02T18:22:58.117641: step 39000, loss 0.0572466, acc 0.96875

Evaluation:
2017-03-02T18:22:58.147264: step 39000, loss 5.97952, acc 0.631579

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-39000

2017-03-02T18:22:58.603968: step 39001, loss 0.0512162, acc 0.96875
2017-03-02T18:22:58.675533: step 39002, loss 0.0790579, acc 0.9375
2017-03-02T18:22:58.749108: step 39003, loss 0.16034, acc 0.921875
2017-03-02T18:22:58.825557: step 39004, loss 1.19209e-07, acc 1
2017-03-02T18:22:58.896331: step 39005, loss 0.203164, acc 0.90625
2017-03-02T18:22:58.969125: step 39006, loss 0.116875, acc 0.96875
2017-03-02T18:22:59.047132: step 39007, loss 0.165623, acc 0.921875
2017-03-02T18:22:59.118092: step 39008, loss 0.12914, acc 0.9375
2017-03-02T18:22:59.193012: step 39009, loss 0.171665, acc 0.9375
2017-03-02T18:22:59.257269: step 39010, loss 0.149082, acc 0.953125
2017-03-02T18:22:59.335211: step 39011, loss 0.0730429, acc 0.96875
2017-03-02T18:22:59.404180: step 39012, loss 0.137837, acc 0.921875
2017-03-02T18:22:59.478350: step 39013, loss 0.0788511, acc 0.96875
2017-03-02T18:22:59.549832: step 39014, loss 0.106697, acc 0.9375
2017-03-02T18:22:59.615674: step 39015, loss 0.166939, acc 0.921875
2017-03-02T18:22:59.681875: step 39016, loss 0.0922606, acc 0.9375
2017-03-02T18:22:59.749479: step 39017, loss 0.151847, acc 0.90625
2017-03-02T18:22:59.824201: step 39018, loss 0.123074, acc 0.953125
2017-03-02T18:22:59.897790: step 39019, loss 0.0916006, acc 0.953125
2017-03-02T18:22:59.971672: step 39020, loss 0.121634, acc 0.9375
2017-03-02T18:23:00.042221: step 39021, loss 0.169969, acc 0.921875
2017-03-02T18:23:00.119159: step 39022, loss 0.0785465, acc 0.96875
2017-03-02T18:23:00.188870: step 39023, loss 0.0880317, acc 0.9375
2017-03-02T18:23:00.257594: step 39024, loss 0.145057, acc 0.90625
2017-03-02T18:23:00.326372: step 39025, loss 0.166246, acc 0.953125
2017-03-02T18:23:00.390746: step 39026, loss 0.117433, acc 0.953125
2017-03-02T18:23:00.459107: step 39027, loss 0.201003, acc 0.90625
2017-03-02T18:23:00.533816: step 39028, loss 0.221466, acc 0.90625
2017-03-02T18:23:00.606712: step 39029, loss 0.161103, acc 0.9375
2017-03-02T18:23:00.699342: step 39030, loss 0.130555, acc 0.9375
2017-03-02T18:23:00.774286: step 39031, loss 0.133032, acc 0.953125
2017-03-02T18:23:00.844503: step 39032, loss 0.157836, acc 0.9375
2017-03-02T18:23:00.916152: step 39033, loss 0.0364318, acc 0.984375
2017-03-02T18:23:00.987798: step 39034, loss 0.153865, acc 0.921875
2017-03-02T18:23:01.052355: step 39035, loss 0.132897, acc 0.953125
2017-03-02T18:23:01.125687: step 39036, loss 0.281544, acc 0.890625
2017-03-02T18:23:01.202279: step 39037, loss 0.152996, acc 0.875
2017-03-02T18:23:01.276321: step 39038, loss 0.124752, acc 0.953125
2017-03-02T18:23:01.350212: step 39039, loss 0.12046, acc 0.953125
2017-03-02T18:23:01.420684: step 39040, loss 0.104455, acc 0.9375
2017-03-02T18:23:01.499200: step 39041, loss 0.118695, acc 0.953125
2017-03-02T18:23:01.572585: step 39042, loss 0.134505, acc 0.921875
2017-03-02T18:23:01.652905: step 39043, loss 0.0844363, acc 0.9375
2017-03-02T18:23:01.723229: step 39044, loss 0.0820417, acc 0.96875
2017-03-02T18:23:01.792839: step 39045, loss 0.398524, acc 0.828125
2017-03-02T18:23:01.868746: step 39046, loss 0.0855701, acc 0.96875
2017-03-02T18:23:01.943102: step 39047, loss 0.117104, acc 0.921875
2017-03-02T18:23:02.006795: step 39048, loss 0.0992672, acc 0.96875
2017-03-02T18:23:02.099804: step 39049, loss 0.104316, acc 0.9375
2017-03-02T18:23:02.171018: step 39050, loss 0.137917, acc 0.921875
2017-03-02T18:23:02.240242: step 39051, loss 0.147837, acc 0.921875
2017-03-02T18:23:02.311170: step 39052, loss 0.190365, acc 0.921875
2017-03-02T18:23:02.380576: step 39053, loss 0.08356, acc 0.96875
2017-03-02T18:23:02.450570: step 39054, loss 0.121675, acc 0.953125
2017-03-02T18:23:02.523658: step 39055, loss 0.0795476, acc 0.953125
2017-03-02T18:23:02.598416: step 39056, loss 0.250412, acc 0.90625
2017-03-02T18:23:02.669706: step 39057, loss 0.167668, acc 0.90625
2017-03-02T18:23:02.747682: step 39058, loss 0.184248, acc 0.9375
2017-03-02T18:23:02.827045: step 39059, loss 0.241407, acc 0.875
2017-03-02T18:23:02.895877: step 39060, loss 0.209561, acc 0.90625
2017-03-02T18:23:02.970537: step 39061, loss 0.145337, acc 0.953125
2017-03-02T18:23:03.051911: step 39062, loss 0.130349, acc 0.921875
2017-03-02T18:23:03.119113: step 39063, loss 0.125262, acc 0.953125
2017-03-02T18:23:03.186147: step 39064, loss 0.156889, acc 0.9375
2017-03-02T18:23:03.256945: step 39065, loss 0.144628, acc 0.90625
2017-03-02T18:23:03.330124: step 39066, loss 0.152916, acc 0.953125
2017-03-02T18:23:03.403464: step 39067, loss 0.151988, acc 0.90625
2017-03-02T18:23:03.482066: step 39068, loss 0.102418, acc 0.953125
2017-03-02T18:23:03.558742: step 39069, loss 0.11643, acc 0.953125
2017-03-02T18:23:03.624597: step 39070, loss 0.0716291, acc 0.984375
2017-03-02T18:23:03.705244: step 39071, loss 0.113791, acc 0.953125
2017-03-02T18:23:03.771823: step 39072, loss 0.135122, acc 0.9375
2017-03-02T18:23:03.839764: step 39073, loss 0.142305, acc 0.9375
2017-03-02T18:23:03.913404: step 39074, loss 0.141992, acc 0.890625
2017-03-02T18:23:03.989015: step 39075, loss 0.15151, acc 0.921875
2017-03-02T18:23:04.059901: step 39076, loss 0.171067, acc 0.9375
2017-03-02T18:23:04.123173: step 39077, loss 0.148385, acc 0.890625
2017-03-02T18:23:04.219418: step 39078, loss 0.207923, acc 0.875
2017-03-02T18:23:04.288290: step 39079, loss 0.0967302, acc 0.953125
2017-03-02T18:23:04.362788: step 39080, loss 0.17185, acc 0.921875
2017-03-02T18:23:04.430124: step 39081, loss 0.230629, acc 0.9375
2017-03-02T18:23:04.494216: step 39082, loss 0.103584, acc 0.953125
2017-03-02T18:23:04.565195: step 39083, loss 0.16429, acc 0.90625
2017-03-02T18:23:04.633540: step 39084, loss 0.217442, acc 0.90625
2017-03-02T18:23:04.701311: step 39085, loss 0.0809002, acc 0.96875
2017-03-02T18:23:04.777083: step 39086, loss 0.0938162, acc 0.96875
2017-03-02T18:23:04.845914: step 39087, loss 0.115703, acc 0.96875
2017-03-02T18:23:04.919317: step 39088, loss 0.160215, acc 0.921875
2017-03-02T18:23:04.992523: step 39089, loss 0.138188, acc 0.90625
2017-03-02T18:23:05.068738: step 39090, loss 0.156528, acc 0.890625
2017-03-02T18:23:05.133352: step 39091, loss 0.153832, acc 0.90625
2017-03-02T18:23:05.205182: step 39092, loss 0.067302, acc 0.953125
2017-03-02T18:23:05.276899: step 39093, loss 0.0967604, acc 0.96875
2017-03-02T18:23:05.350486: step 39094, loss 0.142386, acc 0.90625
2017-03-02T18:23:05.421407: step 39095, loss 0.137467, acc 0.9375
2017-03-02T18:23:05.489579: step 39096, loss 0.105851, acc 0.984375
2017-03-02T18:23:05.562376: step 39097, loss 0.124241, acc 0.9375
2017-03-02T18:23:05.637118: step 39098, loss 0.115818, acc 0.921875
2017-03-02T18:23:05.705476: step 39099, loss 0.209031, acc 0.859375
2017-03-02T18:23:05.775658: step 39100, loss 0.0998925, acc 0.953125

Evaluation:
2017-03-02T18:23:05.803331: step 39100, loss 5.99779, acc 0.630137

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-39100

2017-03-02T18:23:06.244275: step 39101, loss 0.0647184, acc 0.96875
2017-03-02T18:23:06.318041: step 39102, loss 0.0956324, acc 0.96875
2017-03-02T18:23:06.395070: step 39103, loss 0.110732, acc 0.96875
2017-03-02T18:23:06.472599: step 39104, loss 0.189427, acc 0.859375
2017-03-02T18:23:06.537349: step 39105, loss 0.176608, acc 0.90625
2017-03-02T18:23:06.603812: step 39106, loss 0.126738, acc 0.921875
2017-03-02T18:23:06.681467: step 39107, loss 0.124624, acc 0.9375
2017-03-02T18:23:06.757457: step 39108, loss 0.0799537, acc 0.953125
2017-03-02T18:23:06.846968: step 39109, loss 0.138766, acc 0.9375
2017-03-02T18:23:06.917720: step 39110, loss 0.112251, acc 0.953125
2017-03-02T18:23:07.001944: step 39111, loss 0.159567, acc 0.921875
2017-03-02T18:23:07.072860: step 39112, loss 0.105808, acc 0.96875
2017-03-02T18:23:07.145175: step 39113, loss 0.143471, acc 0.9375
2017-03-02T18:23:07.220797: step 39114, loss 0.113977, acc 0.953125
2017-03-02T18:23:07.285210: step 39115, loss 0.0730173, acc 0.984375
2017-03-02T18:23:07.359644: step 39116, loss 0.148297, acc 0.921875
2017-03-02T18:23:07.458550: step 39117, loss 0.144824, acc 0.953125
2017-03-02T18:23:07.535426: step 39118, loss 0.127512, acc 0.9375
2017-03-02T18:23:07.610919: step 39119, loss 0.139744, acc 0.953125
2017-03-02T18:23:07.683490: step 39120, loss 0.117312, acc 0.9375
2017-03-02T18:23:07.742478: step 39121, loss 0.0707683, acc 0.953125
2017-03-02T18:23:07.804768: step 39122, loss 0.158857, acc 0.9375
2017-03-02T18:23:07.871962: step 39123, loss 0.229028, acc 0.90625
2017-03-02T18:23:07.936680: step 39124, loss 0.204863, acc 0.921875
2017-03-02T18:23:08.013669: step 39125, loss 0.184771, acc 0.90625
2017-03-02T18:23:08.081987: step 39126, loss 0.0807435, acc 0.96875
2017-03-02T18:23:08.151479: step 39127, loss 0.0918717, acc 0.953125
2017-03-02T18:23:08.214424: step 39128, loss 0.164432, acc 0.921875
2017-03-02T18:23:08.286959: step 39129, loss 0.223501, acc 0.859375
2017-03-02T18:23:08.357256: step 39130, loss 0.115152, acc 0.9375
2017-03-02T18:23:08.437161: step 39131, loss 0.153279, acc 0.96875
2017-03-02T18:23:08.503633: step 39132, loss 0.190158, acc 0.890625
2017-03-02T18:23:08.574304: step 39133, loss 0.0707308, acc 0.96875
2017-03-02T18:23:08.651465: step 39134, loss 0.0769393, acc 0.96875
2017-03-02T18:23:08.716702: step 39135, loss 0.201048, acc 0.921875
2017-03-02T18:23:08.787280: step 39136, loss 0.253979, acc 0.875
2017-03-02T18:23:08.860824: step 39137, loss 0.210987, acc 0.875
2017-03-02T18:23:08.935318: step 39138, loss 0.148429, acc 0.921875
2017-03-02T18:23:09.004926: step 39139, loss 0.0537624, acc 0.96875
2017-03-02T18:23:09.075586: step 39140, loss 0.128506, acc 0.921875
2017-03-02T18:23:09.143641: step 39141, loss 0.14716, acc 0.921875
2017-03-02T18:23:09.218146: step 39142, loss 0.114573, acc 0.921875
2017-03-02T18:23:09.289597: step 39143, loss 0.114748, acc 0.953125
2017-03-02T18:23:09.354134: step 39144, loss 0.146726, acc 0.921875
2017-03-02T18:23:09.422208: step 39145, loss 0.192794, acc 0.9375
2017-03-02T18:23:09.504547: step 39146, loss 0.0829368, acc 0.984375
2017-03-02T18:23:09.587371: step 39147, loss 0.0509313, acc 0.984375
2017-03-02T18:23:09.658581: step 39148, loss 0.202573, acc 0.890625
2017-03-02T18:23:09.732856: step 39149, loss 0.0858212, acc 0.96875
2017-03-02T18:23:09.808455: step 39150, loss 0.0762296, acc 0.96875
2017-03-02T18:23:09.883054: step 39151, loss 0.16624, acc 0.890625
2017-03-02T18:23:09.953316: step 39152, loss 0.179278, acc 0.875
2017-03-02T18:23:10.022699: step 39153, loss 0.217252, acc 0.890625
2017-03-02T18:23:10.090248: step 39154, loss 0.112556, acc 0.96875
2017-03-02T18:23:10.157961: step 39155, loss 0.111894, acc 0.9375
2017-03-02T18:23:10.238211: step 39156, loss 0.0230807, acc 0.984375
2017-03-02T18:23:10.306457: step 39157, loss 0.168319, acc 0.90625
2017-03-02T18:23:10.385730: step 39158, loss 0.177657, acc 0.9375
2017-03-02T18:23:10.463837: step 39159, loss 0.121669, acc 0.953125
2017-03-02T18:23:10.535441: step 39160, loss 0.184285, acc 0.890625
2017-03-02T18:23:10.614767: step 39161, loss 0.129077, acc 0.9375
2017-03-02T18:23:10.674480: step 39162, loss 0.174607, acc 0.890625
2017-03-02T18:23:10.740167: step 39163, loss 0.0780143, acc 0.953125
2017-03-02T18:23:10.826407: step 39164, loss 0.193623, acc 0.921875
2017-03-02T18:23:10.900395: step 39165, loss 0.161431, acc 0.921875
2017-03-02T18:23:10.970818: step 39166, loss 0.224355, acc 0.921875
2017-03-02T18:23:11.044201: step 39167, loss 0.241636, acc 0.9375
2017-03-02T18:23:11.116847: step 39168, loss 0.200301, acc 0.90625
2017-03-02T18:23:11.189119: step 39169, loss 0.114342, acc 0.953125
2017-03-02T18:23:11.270086: step 39170, loss 0.0710742, acc 0.984375
2017-03-02T18:23:11.334859: step 39171, loss 0.139974, acc 0.9375
2017-03-02T18:23:11.398472: step 39172, loss 0.260118, acc 0.84375
2017-03-02T18:23:11.471667: step 39173, loss 0.0747624, acc 0.953125
2017-03-02T18:23:11.548517: step 39174, loss 0.118726, acc 0.953125
2017-03-02T18:23:11.622693: step 39175, loss 0.182319, acc 0.875
2017-03-02T18:23:11.708153: step 39176, loss 0.17408, acc 0.90625
2017-03-02T18:23:11.775674: step 39177, loss 0.115696, acc 0.921875
2017-03-02T18:23:11.848629: step 39178, loss 0.109507, acc 0.953125
2017-03-02T18:23:11.920777: step 39179, loss 0.15838, acc 0.90625
2017-03-02T18:23:11.991431: step 39180, loss 0.309565, acc 0.890625
2017-03-02T18:23:12.066398: step 39181, loss 0.136637, acc 0.953125
2017-03-02T18:23:12.141319: step 39182, loss 0.166835, acc 0.9375
2017-03-02T18:23:12.210728: step 39183, loss 0.217585, acc 0.921875
2017-03-02T18:23:12.279916: step 39184, loss 0.0810747, acc 0.953125
2017-03-02T18:23:12.350167: step 39185, loss 0.0873469, acc 0.953125
2017-03-02T18:23:12.425433: step 39186, loss 0.125778, acc 0.9375
2017-03-02T18:23:12.506172: step 39187, loss 0.166662, acc 0.9375
2017-03-02T18:23:12.581849: step 39188, loss 0.270179, acc 0.90625
2017-03-02T18:23:12.651547: step 39189, loss 0.230485, acc 0.921875
2017-03-02T18:23:12.732610: step 39190, loss 0.195206, acc 0.921875
2017-03-02T18:23:12.806877: step 39191, loss 0.116874, acc 0.921875
2017-03-02T18:23:12.883552: step 39192, loss 0.136049, acc 0.921875
2017-03-02T18:23:12.953883: step 39193, loss 0.183115, acc 0.953125
2017-03-02T18:23:13.028484: step 39194, loss 0.153553, acc 0.921875
2017-03-02T18:23:13.098699: step 39195, loss 0.107, acc 0.9375
2017-03-02T18:23:13.165705: step 39196, loss 0.0917211, acc 0.9375
2017-03-02T18:23:13.238210: step 39197, loss 0.103237, acc 0.96875
2017-03-02T18:23:13.312516: step 39198, loss 0.100818, acc 0.953125
2017-03-02T18:23:13.387456: step 39199, loss 0.120904, acc 0.9375
2017-03-02T18:23:13.452051: step 39200, loss 0, acc 1

Evaluation:
2017-03-02T18:23:13.483818: step 39200, loss 6.0543, acc 0.6323

Saved model checkpoint to /atlas/u/jdunnmon/tutorial/cnn-text-classification-tf/runs/1488504630/checkpoints/model-39200

